{
 "metadata": {
  "name": "",
  "signature": "sha256:75aaf47c9f342ffb8d8736250631a0dd7c780615cfe34e3148882fc079f985d1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Hadoop, Mapreduce and More \u2013 Part 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all.\n",
      "What I understood finally is that Hadoop is basically comprised of 3 elements:\n",
      "\n",
      "1. A File System\n",
      "2. Map \u2013 Reduce\n",
      "3. Its many individual Components.\n",
      "Let\u2019s go through each of them one by one.\n",
      "\n",
      "###1. Hadoop as a File System:\n",
      "One of the main things that Hadoop provides is cheap data storage. What happens intrinsically is that the Hadoop system takes a file, cuts it into chunks and keeps those chunks at different places in a cluster. Suppose you have a big big file in your local system and you want that file to be:\n",
      "\n",
      "1. On the cloud for easy access\n",
      "2. Processable in human time\n",
      "\n",
      "The one thing you can look forward to is Hadoop.\n",
      "\n",
      "Assuming that you have got hadoop instaled on the amazon cluster you are working on.(If you don\u2019t know how to setup a hadoop cluster, go to this guy)\n",
      "\n",
      "####Start the Hadoop Cluster:\n",
      "You need to run the following commands to start the hadoop cluster(Based on location of hadoop installation directory):\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /usr/local/hadoop/\n",
      "bin/start-all.sh\n",
      "jps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adding File to HDFS: Every command in Hadoop starts with hadoop fs and the rest of it works like the UNIX syntax. To add a file \u201cpurchases.txt\u201d to the hdfs system:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hadoop fs -put purchases.txt /usr/purchases.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2. Hadoop for Map-Reduce:\n",
      "\n",
      "MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.\n",
      "\n",
      "While Hadoop is implemented in Java, you can use almost any language to do map-reduce in hadoop using hadoop streaming. Suppose you have a big file containing the Name of store and sales of store each hour. And you want to find out the sales per store using map-reduce. Lets Write a sample code for that: \n",
      "\n",
      "\n",
      "InputFile"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A,300,12:00\n",
      "B,234,1:00\n",
      "C,234,2:00\n",
      "D,123,3:00\n",
      "A,123,1:00\n",
      "B,346,2:00"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mapper.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "def mapper():\n",
      "    # The Mapper takes inputs from stdin and prints out store name and value\n",
      "    for line in sys.stdin:\n",
      "        data = line.strip().split(\",\")\n",
      "        storeName,Value,time=data\n",
      "        print \"{0},{1}\".format(storeName,Value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reducer.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "def reducer():\n",
      "    # The reducer takes inputs from mapper and prints out aggregated store name and value\n",
      "    salesTotal = 0\n",
      "    oldKey = None\n",
      "    for line in sys.stdin:\n",
      "        data = line.strip().split(\",\")\n",
      "        #Adding a little bit of Defensive programming\n",
      "        if len(data) != 2:\n",
      "            continue\n",
      "        curKey,curVal = data\n",
      "        if oldKey adn oldKey != curKey:\n",
      "            print \"{0},{1}\".format(oldKey,salesTotal)\n",
      "            salesTotal=0\n",
      "        oldKey=curKey\n",
      "        salesTotal += curVal\n",
      "    if oldkey!=None:\n",
      "        print \"{0},{1}\".format(oldKey,salesTotal)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the program on shell using pipes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textfile.txt | ./mapper.py | sort | ./reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running the program on mapreduce using Hadoop Streaming "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hadoop jar contrib/streaming/hadoop-*streaming*.jar /\n",
      "-file mapper.py -mapper mapper.py /\n",
      "-file reducer.py -reducer reducer.py /\n",
      "-input /inputfile -output /outputfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3. Hadoop Components:\n",
      "\n",
      "Now if you have been following Hadoop you might have heard about Apache, Cloudera, HortonWorks etc. All of these are Hadoop vendors who provide Hadoop Along with its components. I will talk about the main component of Hadoop here \u2013 Hive.\n",
      "So what exactly is Hive: Hive is a SQL like interface to map-reduce queries. So if you don\u2019t understand all the hocus-pocus of map-reduce but know SQL, you can do map-reduce via Hive.\n",
      "Seems Promising? It is.\n",
      "While the syntax is mainly SQL, it is still a little different and there are some quirks that we need to understand to work with Hive.\n",
      "First of all lets open hive command prompt: For that you just have to type \u201chive\u201d, and voila you are in.\n",
      "Here are some general commands"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show databases  #   -- See all Databases\n",
      "use database     #     -- Use a particular Database\n",
      "show tables       #     -- See all tables in a particular Database\n",
      "describe table    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating an external table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CREATE EXTERNAL TABLE IF NOT EXISTS BXDataSet\n",
      "(ISBN STRING,BookTitle STRING, ImageURLL STRING)\n",
      "ROW FORMAT DELIMITED  FIELDS TERMINATED BY \u2018;\u2019 STORED AS TEXTFILE;\n",
      "LOAD DATA INPATH \u2018/user/book.csv\u2019 OVERWRITE INTO TABLE BXDataSet;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The query commands work the same way as in SQL. You can do all the group by and hive will automatically convert it in map-reduce:\n",
      "\n",
      "select * from tablename;\n",
      "\n",
      "Stay Tuned for Part 2 \u2013 Where we will talk about another components of Hadoop \u2013 PIG"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}