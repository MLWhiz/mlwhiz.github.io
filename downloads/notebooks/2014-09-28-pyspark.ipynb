{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Learning pyspark – Installation – Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.\n",
    "\n",
    "The installation is pretty simple. These steps were done on Mac OS Mavericks but should work for Linux too. Here are the steps for the installation:\n",
    "\n",
    "###1. Download the Binaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark : http://spark.apache.org/downloads.html\n",
    "Scala : http://www.scala-lang.org/download/\n",
    "\n",
    "Dont use Latest Version of Scala, Use Scala 2.10.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Add these lines to your .bash_profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export SCALA_HOME=your_path_to_scala\n",
    "export SPARK_HOME=your_path_to_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. Build Spark(This will take time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew install sbt\n",
    "cd $SPARK_HOME\n",
    "sbt/sbt assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4. Start the Pyspark Shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$SPARK_HOME/bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Voila. You are running pyspark on your Machine\n",
    "\n",
    "To check that everything is properly installed, Lets run a simple program:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.parallelize([1,2,3])\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return 3.\n",
    "\n",
    "So Now Just Run Hadoop On your Machine and then run pyspark Using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /usr/local/hadoop/\n",
    "bin/start-all.sh\n",
    "jps\n",
    "$SPARK_HOME/bin/pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
