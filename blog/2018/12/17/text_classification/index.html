<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>What Kagglers are using for Text Classification - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="An overview of the newest methods in text classification"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="What Kagglers are using for Text Classification - MLWhiz"><meta property="og:description" content="An overview of the newest methods in text classification"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2018/12/17/text_classification/"><meta property="og:image" content="https://mlwhiz.com/images/text_convolution.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/text_convolution.png"><meta property="article:published_time" content="2018-12-17T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-08T15:10:01+01:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/text_convolution.png"><meta name=twitter:title content="What Kagglers are using for Text Classification - MLWhiz"><meta name=twitter:description content="An overview of the newest methods in text classification"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2018/12/17/text_classification/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2018/12/17/text_classification/#primaryimage","url":"https://mlwhiz.com/images/text_convolution.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2018/12/17/text_classification/#webpage","url":"https://mlwhiz.com/blog/2018/12/17/text_classification/","inLanguage":"en-US","name":"What Kagglers are using for Text Classification - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2018/12/17/text_classification/#primaryimage"},"datePublished":"2018-12-17T00:00:00.00Z","dateModified":"2023-07-08T15:10:01.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"An overview of the newest methods in text classification"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>What Kagglers are using for Text Classification</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>17 December 2018</span></div><img src=https://mlwhiz.com/images/text_convolution.png class="img-fluid w-100 mb-4" alt="What Kagglers are using for Text Classification"><div class="content mb-5"><p>With the problem of Image Classification is more or less solved by Deep learning, <em>Text Classification is the next new developing theme in deep learning</em>. For those who don&rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that?</p><ul><li>To find sentiment of a review.</li><li>Find toxic comments in a platform like Facebook</li><li>Find Insincere questions on Quora. A current ongoing competition on kaggle</li><li>Find fake reviews on websites</li><li>Will a text advert get clicked or not</li></ul><p>And much more. The whole internet is filled with text and to categorise that information algorithmically will only give us incremental benefits to say the least in the field of AI.</p><p>Here I am going to use the data from Quora&rsquo;s Insincere questions to talk about the different models that people are building and sharing to perform this task. Obviously these standalone models are not going to put you on the top of the leaderboard, yet I hope that this ensuing discussion would be helpful for people who want to learn more about text classification. This is going to be a long post in that regard.</p><p>As a side note: if you want to know more about NLP, I would like to recommend this awesome
<a href=https://coursera.pxf.io/9WjZo0 target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>Also take a look at my other post:
<a href=/blog/2019/01/17/deeplearning_nlp_preprocess/>Text Preprocessing Methods for Deep Learning</a>
, which talks about different preprocessing techniques you can use for your NLP task and
<a href=/blog/2019/01/06/pytorch_keras_conversion/>how to switch from Keras to Pytorch</a>
.</p><p>So let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them.</p><h2 id=1-textcnn>1. TextCNN:</h2><p>The idea of using a CNN to classify text was first presented in the paper
<a href=https://www.aclweb.org/anthology/D14-1181 target=_blank rel="nofollow noopener">Convolutional Neural Networks for Sentence Classification</a>
by Yoon Kim. Instead of image pixels, the input to the tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one word vector. That is, each row is word-vector that represents a word. Thus a sequence of max length 70 gives us a image of 70(max sequence length)x300(embedding size)</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/text_convolution.png height=400 width=700></center></div><p>Now for some intuition. While for a image we move our conv filter horizontally also since here we have fixed our kernel size to filter_size x embed_size i.e. (3,300) we are just going to move down for the convolution taking look at three words at once since our filter size is 3 in this case.Also one can think of filter sizes as unigrams, bigrams, trigrams etc. Since we are looking at a context window of 1,2,3, and 5 words respectively. Here is the text classification network coded in Keras:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># https://www.kaggle.com/yekenot/2dcnn-textclassifier</span>
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>model_cnn</span>(embedding_matrix):
    filter_sizes = [<span style=color:#3677a9>1</span>,<span style=color:#3677a9>2</span>,<span style=color:#3677a9>3</span>,<span style=color:#3677a9>5</span>]
    num_filters = <span style=color:#3677a9>36</span>

    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
    x = Reshape((maxlen, embed_size, <span style=color:#3677a9>1</span>))(x)

    maxpool_pool = []
    <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#24909d>len</span>(filter_sizes)):
        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),
                                     kernel_initializer=<span style=color:#ed9d13>&#39;he_normal&#39;</span>, activation=<span style=color:#ed9d13>&#39;elu&#39;</span>)(x)
        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + <span style=color:#3677a9>1</span>, <span style=color:#3677a9>1</span>))(conv))

    z = Concatenate(axis=<span style=color:#3677a9>1</span>)(maxpool_pool)
    z = Flatten()(z)
    z = Dropout(<span style=color:#3677a9>0.1</span>)(z)

    outp = Dense(<span style=color:#3677a9>1</span>, activation=<span style=color:#ed9d13>&#34;sigmoid&#34;</span>)(z)

    model = Model(inputs=inp, outputs=outp)
    model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer=<span style=color:#ed9d13>&#39;adam&#39;</span>, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])

    <span style=color:#6ab825;font-weight:700>return</span> model
</code></pre></div><p>I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-textcnn target=_blank rel="nofollow noopener">kaggle kernel</a>
for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.661 on the public leaderboard.</p><h2 id=2-bidirectional-rnnlstmgru>2. BiDirectional RNN(LSTM/GRU):</h2><p>TextCNN takes care of a lot of things. For example it takes care of words in close range. It is able to see &ldquo;new york&rdquo; together. But it still can&rsquo;t take care of all the context provided in a particular text sequence. It still does not learn the seem to learn the sequential structure of the data, where every word is dependednt on the previous word. Or a word in the previous sentence.</p><p>RNN help us with that. <em>They are able to remember previous information using hidden states and connect it to the current task.</em></p><p>Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time. More over the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won&rsquo;t work for a time sweries prediction task).</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/birnn.png height=400 width=700></center></div><p>For a most simplistic explanation of Bidirectional RNN, think of RNN cell as taking as input a hidden state(a vector) and the word vector and giving out an output vector and the next hidden state.</p><pre><code>        Hidden state, Word vector -&gt;(RNN Cell) -&gt; Output Vector , Next Hidden state
</code></pre><p>For a sequence of length 4 like &lsquo;you will never believe&rsquo;, The RNN cell will give 4 output vectors. Which can be concatenated and then used as part of a dense feedforward architecture.</p><p>In the Bidirectional RNN the only change is that we read the text in the normal fashion as well in reverse. So we stack two RNNs in parallel and hence we get 8 output vectors to append.</p><p>Once we get the output vectors we send them through a series of dense layers and finally a softmax layer to build a text classifier.</p><p>Due to the limitations of RNNs like not remembering long term dependencies, in practice we almost always use LSTM/GRU to model long term dependencies. In such a case you can just think of the RNN cell being replaced by a LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangably with CuDNNLSTM, when you build models.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># BiDirectional LSTM</span>
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>model_lstm_du</span>(embedding_matrix):
    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
    <span style=color:#ed9d13>&#39;&#39;&#39;
</span><span style=color:#ed9d13>    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?
</span><span style=color:#ed9d13>        64*70(maxlen)*2(bidirection concat)
</span><span style=color:#ed9d13>    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    x = Bidirectional(CuDNNLSTM(<span style=color:#3677a9>64</span>, return_sequences=True))(x)
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(<span style=color:#3677a9>64</span>, activation=<span style=color:#ed9d13>&#34;relu&#34;</span>)(conc)
    conc = Dropout(<span style=color:#3677a9>0.1</span>)(conc)
    outp = Dense(<span style=color:#3677a9>1</span>, activation=<span style=color:#ed9d13>&#34;sigmoid&#34;</span>)(conc)
    model = Model(inputs=inp, outputs=outp)
    model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer=<span style=color:#ed9d13>&#39;adam&#39;</span>, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])
    <span style=color:#6ab825;font-weight:700>return</span> model
</code></pre></div><p>I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-bidirectionalrnn target=_blank rel="nofollow noopener">kaggle kernel</a>
for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.671 on the public leaderboard.</p><h2 id=3-attention-models>3. Attention Models</h2><p>The concept of Attention is relatively new as it comes from
<a href=https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf target=_blank rel="nofollow noopener">Hierarchical Attention Networks for Document Classification</a>
paper written jointly by CMU and Microsoft guys in 2016.</p><p>So in the past we used to find features from text by doing a keyword extraction. Some word are more helpful in determining the category of a text than others. But in this method we sort of lost the sequential structure of text. With LSTM and deep learning methods while we are able to take case of the sequence structure we lose the ability to give higher weightage to more important words.
Can we have the best of both worlds?</p><p>And that is attention for you. In the author&rsquo;s words:</p><blockquote><p>Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract
such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector</p></blockquote><div style=margin-top:9px;margin-bottom:10px><center><img src="/images/birnn attention.png" height=400 width=700></center></div><p>In essense we want to create scores for every word in the text, which are the attention similarity score for a word.</p><p>To do this we start with a weight matrix(W), a bias vector(b) and a context vector u. All of them will be learned by the optimmization algorithm.</p><p>Then there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as non linearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to an exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v’s to get the Final Scores,s</p><p>These final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>dot_product</span>(x, kernel):
    <span style=color:#ed9d13>&#34;&#34;&#34;
</span><span style=color:#ed9d13>    Wrapper for dot product operation, in order to be compatible with both
</span><span style=color:#ed9d13>    Theano and Tensorflow
</span><span style=color:#ed9d13>    Args:
</span><span style=color:#ed9d13>        x (): input
</span><span style=color:#ed9d13>        kernel (): weights
</span><span style=color:#ed9d13>    Returns:
</span><span style=color:#ed9d13>    &#34;&#34;&#34;</span>
    <span style=color:#6ab825;font-weight:700>if</span> K.backend() == <span style=color:#ed9d13>&#39;tensorflow&#39;</span>:
        <span style=color:#6ab825;font-weight:700>return</span> K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-<span style=color:#3677a9>1</span>)
    <span style=color:#6ab825;font-weight:700>else</span>:
        <span style=color:#6ab825;font-weight:700>return</span> K.dot(x, kernel)


<span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>AttentionWithContext</span>(Layer):
    <span style=color:#ed9d13>&#34;&#34;&#34;
</span><span style=color:#ed9d13>    Attention operation, with a context/query vector, for temporal data.
</span><span style=color:#ed9d13>    Supports Masking.
</span><span style=color:#ed9d13>    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]
</span><span style=color:#ed9d13>    &#34;Hierarchical Attention Networks for Document Classification&#34;
</span><span style=color:#ed9d13>    by using a context vector to assist the attention
</span><span style=color:#ed9d13>    # Input shape
</span><span style=color:#ed9d13>        3D tensor with shape: `(samples, steps, features)`.
</span><span style=color:#ed9d13>    # Output shape
</span><span style=color:#ed9d13>        2D tensor with shape: `(samples, features)`.
</span><span style=color:#ed9d13>    How to use:
</span><span style=color:#ed9d13>    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
</span><span style=color:#ed9d13>    The dimensions are inferred based on the output shape of the RNN.
</span><span style=color:#ed9d13>    Note: The layer has been tested with Keras 2.0.6
</span><span style=color:#ed9d13>    Example:
</span><span style=color:#ed9d13>        model.add(LSTM(64, return_sequences=True))
</span><span style=color:#ed9d13>        model.add(AttentionWithContext())
</span><span style=color:#ed9d13>        # next add a Dense layer (for classification/regression) or whatever...
</span><span style=color:#ed9d13>    &#34;&#34;&#34;</span>

    <span style=color:#6ab825;font-weight:700>def</span> __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        self.init = initializers.get(<span style=color:#ed9d13>&#39;glorot_uniform&#39;</span>)

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        <span style=color:#24909d>super</span>(AttentionWithContext, self).__init__(**kwargs)

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>build</span>(self, input_shape):
        <span style=color:#6ab825;font-weight:700>assert</span> <span style=color:#24909d>len</span>(input_shape) == <span style=color:#3677a9>3</span>

        self.W = self.add_weight((input_shape[-<span style=color:#3677a9>1</span>], input_shape[-<span style=color:#3677a9>1</span>],),
                                 initializer=self.init,
                                 name=<span style=color:#ed9d13>&#39;{}_W&#39;</span>.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        <span style=color:#6ab825;font-weight:700>if</span> self.bias:
            self.b = self.add_weight((input_shape[-<span style=color:#3677a9>1</span>],),
                                     initializer=<span style=color:#ed9d13>&#39;zero&#39;</span>,
                                     name=<span style=color:#ed9d13>&#39;{}_b&#39;</span>.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight((input_shape[-<span style=color:#3677a9>1</span>],),
                                 initializer=self.init,
                                 name=<span style=color:#ed9d13>&#39;{}_u&#39;</span>.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        <span style=color:#24909d>super</span>(AttentionWithContext, self).build(input_shape)

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>compute_mask</span>(self, <span style=color:#24909d>input</span>, input_mask=None):
        <span style=color:#999;font-style:italic># do not pass the mask to the next layers</span>
        <span style=color:#6ab825;font-weight:700>return</span> None

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>call</span>(self, x, mask=None):
        uit = dot_product(x, self.W)

        <span style=color:#6ab825;font-weight:700>if</span> self.bias:
            uit += self.b

        uit = K.tanh(uit)
        ait = dot_product(uit, self.u)

        a = K.exp(ait)

        <span style=color:#999;font-style:italic># apply mask after the exp. will be re-normalized next</span>
        <span style=color:#6ab825;font-weight:700>if</span> mask <span style=color:#6ab825;font-weight:700>is</span> <span style=color:#6ab825;font-weight:700>not</span> None:
            <span style=color:#999;font-style:italic># Cast the mask to floatX to avoid float64 upcasting in theano</span>
            a *= K.cast(mask, K.floatx())

        <span style=color:#999;font-style:italic># in some cases especially in the early stages of training the sum may be almost zero</span>
        <span style=color:#999;font-style:italic># and this results in NaN&#39;s. A workaround is to add a very small positive number ε to the sum.</span>
        <span style=color:#999;font-style:italic># a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())</span>
        a /= K.cast(K.sum(a, axis=<span style=color:#3677a9>1</span>, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        <span style=color:#6ab825;font-weight:700>return</span> K.sum(weighted_input, axis=<span style=color:#3677a9>1</span>)

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>compute_output_shape</span>(self, input_shape):
        <span style=color:#6ab825;font-weight:700>return</span> input_shape[<span style=color:#3677a9>0</span>], input_shape[-<span style=color:#3677a9>1</span>]


<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>model_lstm_atten</span>(embedding_matrix):
    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)
    x = Bidirectional(CuDNNLSTM(<span style=color:#3677a9>128</span>, return_sequences=True))(x)
    x = Bidirectional(CuDNNLSTM(<span style=color:#3677a9>64</span>, return_sequences=True))(x)
    x = AttentionWithContext()(x)
    x = Dense(<span style=color:#3677a9>64</span>, activation=<span style=color:#ed9d13>&#34;relu&#34;</span>)(x)
    x = Dense(<span style=color:#3677a9>1</span>, activation=<span style=color:#ed9d13>&#34;sigmoid&#34;</span>)(x)
    model = Model(inputs=inp, outputs=x)
    model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer=<span style=color:#ed9d13>&#39;adam&#39;</span>, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])
    <span style=color:#6ab825;font-weight:700>return</span> model

</code></pre></div><p>I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-attention target=_blank rel="nofollow noopener">kaggle kernel</a>
for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.682 on the public leaderboard.</p><p>Hope that Helps! Do checkout the kernels for all the networks and see the comments too. I will try to write a part 2 of this post where I would like to talk about capsule networks and more techniques as they get used in this competition.</p><p>Here are the kernel links again:
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-textcnn target=_blank rel="nofollow noopener">TextCNN</a>
,
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-bidirectionalrnn target=_blank rel="nofollow noopener">BiLSTM/GRU</a>
,
<a href=https://www.kaggle.com/mlwhiz/learning-text-classification-attention target=_blank rel="nofollow noopener">Attention</a></p><p>Do upvote the kenels if you find them helpful.</p><h2 id=references>References:</h2><ul><li><a href=http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ target=_blank rel="nofollow noopener">CNN for NLP</a></li><li><a href=https://en.diveintodeeplearning.org/d2l-en.pdf>https://en.diveintodeeplearning.org/d2l-en.pdf</a></li><li><a href=https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2>https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2</a></li><li><a href=http://univagora.ro/jour/index.php/ijccc/article/view/3142>http://univagora.ro/jour/index.php/ijccc/article/view/3142</a></li><li><a href=https://www.kaggle.com/shujian/fork-of-mix-of-nn-models target=_blank rel="nofollow noopener">Shujian&amp;rsquo;s kernel on Kaggle</a></li></ul><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/logo.svg class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>