<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>An End to End Introduction to GANs using Keras - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="An End to End Introduction to GANs using Keras - MLWhiz"><meta property="og:description" content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/06/17/gans/"><meta property="og:image" content="https://mlwhiz.com/images/gans/faces.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/gans/faces.png"><meta property="article:published_time" content="2019-06-17T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T16:18:05+01:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/gans/faces.png"><meta name=twitter:title content="An End to End Introduction to GANs using Keras - MLWhiz"><meta name=twitter:description content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/06/17/gans/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/06/17/gans/#primaryimage","url":"https://mlwhiz.com/images/gans/faces.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/06/17/gans/#webpage","url":"https://mlwhiz.com/blog/2019/06/17/gans/","inLanguage":"en-US","name":"An End to End Introduction to GANs using Keras - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/06/17/gans/#primaryimage"},"datePublished":"2019-06-17T00:00:00.00Z","dateModified":"2023-07-07T16:18:05.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>An End to End Introduction to GANs using Keras</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>17 June 2019</span></div><img src=https://mlwhiz.com/images/gans/faces.png class="img-fluid w-100 mb-4" alt="An End to End Introduction to GANs using Keras"><div class="content mb-5"><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/gans/faces.png "></center></div><p>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.</p><p><em><strong>In this post, I will help the reader to understand how they can create and build such applications on their own.</strong></em></p><p>I will try to keep this post as intuitive as possible for starters while not dumbing it down too much.</p><p><em><strong>This post is about understanding how GANs work.</strong></em></p><hr><h2 id=task-overview>Task Overview</h2><p>I will work on <em><strong>creating our own anime characters using anime characters dataset.</strong></em></p><p>The DC-GAN flavor of GANs which I will use here is widely applicable not only to generate Faces or new anime characters; it can also be used to create modern fashion styles, for general content creation and sometimes for data augmentation purposes as well.</p><p><em><strong>As per my view, GANs will change the way video games and special effects are generated. The approach could create realistic textures or characters on demand.</strong></em></p><p>You can find the full code for this chapter in the
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/GAN_Project target=_blank rel="nofollow noopener">Github Repository</a>
. I have also uploaded the code to
<a href=https://colab.research.google.com/drive/1Mxbfn0BUW4BlgEPc-minaE_M0_PaYIIX target=_blank rel="nofollow noopener">Google Colab</a>
so that you can try it yourself.</p><hr><h2 id=using-dcgan-architecture-to-generate-anime-images>Using DCGAN architecture to generate anime images</h2><p>As always before we get into the coding, it helps to delve a little bit into the theory.</p><p>The main idea of DC-GAN’s stemmed from the paper
<a href=https://arxiv.org/pdf/1511.06434.pdf target=_blank rel="nofollow noopener">UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a>
written in 2016 by Alec Radford, Luke Metz, and Soumith Chintala.</p><p>Although I am going to explain the paper in the next few sections, do take a look at it. It is an excellent paper.</p><hr><h2 id=intuition-brief-intro-to-gans-for-generating-fake-images>INTUITION: Brief Intro to GANs for Generating Fake Images</h2><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/duel.jpeg><figcaption style=font-size:12px>Generator vs. Discriminator</figcaption></figure></center></div><p>Typically, <em><strong>GANs employ two dueling neural networks to train a computer to learn the nature of a data set well enough to generate convincing fakes.</strong></em></p><p>We can think of this as two systems where one Neural Network works to generate fakes (Generator), and another neural network (Discriminator) tries to classify which image is a fake.</p><p>As both generator and discriminator networks do this repetitively, the networks eventually get better at their respective tasks.</p><p><em><strong>Think of this as simple as swordplay.</strong></em> Two noobs start sparring with each other. After a while, both become better at swordplay.</p><p><em><strong>Or you could think of this as a robber(generator) and a policeman(Discriminator).</strong></em> After a lot of thefts, the robber becomes better at thieving while the policeman gets better at catching the robber. <em>In an ideal world.</em></p><p>The Losses in these neural networks are primarily a function of how the other network performs:</p><ul><li><p>Discriminator network loss is a function of generator network quality- Loss is high for the discriminator if it gets fooled by the generator’s fake images</p></li><li><p>Generator network loss is a function of discriminator network quality — Loss is high if the generator is not able to fool the discriminator.</p></li></ul><p>In the training phase, we train our Discriminator and Generator networks sequentially intending to improve both the Discriminator and Generator performance.</p><p>The objective is to end up with weights that help Generators to generate realistic looking images. <em><strong>In the end, we can use the Generator Neural network to generate fake images from Random Noise.</strong></em></p><hr><h2 id=generator-architecture>Generator architecture</h2><p>One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/generator_paper.png></figure></center></div><p>The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.</p><p>Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/gen_logic.png></figure></center></div><p><em><strong>How do we get such an architecture?</strong></em></p><p>In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/gen_logic_more.png></figure>tc</center></div><p>We don’t have to worry about any weights right now as the network itself will learn those while training.</p><p>Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don’t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.</p><h3 id=now-what-are-transpose-convolutions>Now, What are Transpose convolutions?</h3><p>In most simple terms, <em><strong>transpose convolutions provide us with a way to upsample images.</strong></em> While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/tc.png></figure></center></div><p><em><strong>Q:</strong></em> We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don’t we use Un-pooling?</p><p>It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.</p><h2 id=discriminator-architecture>Discriminator architecture</h2><p>Now, as we have understood the generator architecture, here is the discriminator as a black box.</p><p>In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/dis.png></figure></center></div><p>Takes an image as input and predicts if it is real/fake. <em><strong>Every image conv net ever.</strong></em></p><h2 id=data-preprocessing-and-visualization>Data preprocessing and visualization</h2><p>The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>filenames = glob.glob(<span style=color:#ed9d13>&#39;animeface-character-dataset/*/*.pn*&#39;</span>)
plt.figure(figsize=(<span style=color:#3677a9>10</span>, <span style=color:#3677a9>8</span>))
<span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>5</span>):
    img = plt.imread(filenames[i], <span style=color:#3677a9>0</span>)
    plt.subplot(<span style=color:#3677a9>4</span>, <span style=color:#3677a9>5</span>, i+<span style=color:#3677a9>1</span>)
    plt.imshow(img)
    plt.title(img.shape)
    plt.xticks([])
    plt.yticks([])
plt.tight_layout()
plt.show()
</code></pre></div><p>The resultant output is as follows:</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/out.png></figure></center></div><p>We get to see the sizes of the images and the images themselves.</p><p>We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.</p><p>We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># A function to normalize image pixels.</span>
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>norm_img</span>(img):
    <span style=color:#ed9d13>&#39;&#39;&#39;A function to Normalize Images.
</span><span style=color:#ed9d13>    Input:
</span><span style=color:#ed9d13>        img : Original image as numpy array.
</span><span style=color:#ed9d13>    Output: Normailized Image as numpy array
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    img = (img / <span style=color:#3677a9>127.5</span>) - <span style=color:#3677a9>1</span>
    <span style=color:#6ab825;font-weight:700>return</span> img
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>denorm_img</span>(img):
    <span style=color:#ed9d13>&#39;&#39;&#39;A function to Denormailze, i.e. recreate image from normalized image
</span><span style=color:#ed9d13>    Input:
</span><span style=color:#ed9d13>        img : Normalized image as numpy array.
</span><span style=color:#ed9d13>    Output: Original Image as numpy array
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    img = (img + <span style=color:#3677a9>1</span>) * <span style=color:#3677a9>127.5</span>
    <span style=color:#6ab825;font-weight:700>return</span> img.astype(np.uint8)
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>sample_from_dataset</span>(batch_size, image_shape, data_dir=None):
    <span style=color:#ed9d13>&#39;&#39;&#39;Create a batch of image samples by sampling random images from a data directory.
</span><span style=color:#ed9d13>    Resizes the image using image_shape and normalize the images.
</span><span style=color:#ed9d13>    Input:
</span><span style=color:#ed9d13>        batch_size : Sample size required
</span><span style=color:#ed9d13>        image_size : Size that Image should be resized to
</span><span style=color:#ed9d13>        data_dir : Path of directory where training images are placed.
</span><span style=color:#ed9d13>    Output:
</span><span style=color:#ed9d13>        sample : batch of processed images
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    sample_dim = (batch_size,) + image_shape
    sample = np.empty(sample_dim, dtype=np.float32)
    all_data_dirlist = <span style=color:#24909d>list</span>(glob.glob(data_dir))
    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)
    <span style=color:#6ab825;font-weight:700>for</span> index,img_filename <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(sample_imgs_paths):
        image = Image.open(img_filename)
        image = image.resize(image_shape[:-<span style=color:#3677a9>1</span>])
        image = image.convert(<span style=color:#ed9d13>&#39;RGB&#39;</span>)
        image = np.asarray(image)
        image = norm_img(image)
        sample[index,...] = image
    <span style=color:#6ab825;font-weight:700>return</span> sample
</code></pre></div><p>As you will see, we will be using the preceding defined functions in the training part of our code.</p><h2 id=implementation-of-dcgan>Implementation of DCGAN</h2><p>This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.</p><h3 id=generating-noise-vector-for-generator>Generating noise vector for Generator</h3><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/noise.jpeg><figcaption style=font-size:12px>Kids: Normal Noise generators</figcaption></figure></center></div><p>The following code block is a helper function to create a noise vector of predefined length for a Generator. It will generate the noise which we want to convert to an image using our generator architecture.</p><p>We use a normal distribution</p><p><img src=https://cdn-images-1.medium.com/max/3720/0*MQspqgJbMj2BnO22.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>to generate the noise vector:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>gen_noise</span>(batch_size, noise_shape):
    <span style=color:#ed9d13>&#39;&#39;&#39; Generates a numpy vector sampled from normal distribution of shape                                (batch_size,noise_shape)
</span><span style=color:#ed9d13>    Input:
</span><span style=color:#ed9d13>        batch_size : size of batch
</span><span style=color:#ed9d13>        noise_shape: shape of noise vector, normally kept as 100
</span><span style=color:#ed9d13>    Output:a numpy vector sampled from normal distribution of shape                                  (batch_size,noise_shape)     
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    <span style=color:#6ab825;font-weight:700>return</span> np.random.normal(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>1</span>, size=(batch_size,)+noise_shape)
</code></pre></div><h3 id=generator-architecture-1>Generator architecture</h3><p>The Generator is the most crucial part of the GAN.</p><p>Here, I create a generator by adding some transposed convolution layers to upsample the noise vector to an image.</p><p>As you will notice, this generator architecture is not the same as given in the Original DC-GAN paper.</p><p>I needed to make some architectural changes to fit our data better, so I added a convolution layer in the middle and removed all dense layers from the generator architecture, making it fully convolutional.</p><p>I also use a lot of Batchnorm layers with a momentum of 0.5 and leaky ReLU activation. I use Adam optimizer with β=0.5. The following code block is the function I will use to create the generator:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>get_gen_normal</span>(noise_shape):
    <span style=color:#ed9d13>&#39;&#39;&#39; This function takes as input shape of the noise vector and creates the Keras generator    architecture.
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    kernel_init = <span style=color:#ed9d13>&#39;glorot_uniform&#39;</span>    
    gen_input = Input(shape = noise_shape)

    <span style=color:#999;font-style:italic># Transpose 2D conv layer 1.</span>
    generator = Conv2DTranspose(filters = <span style=color:#3677a9>512</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>1</span>,<span style=color:#3677a9>1</span>), padding = <span style=color:#ed9d13>&#34;valid&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(gen_input)
    generator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(generator)
    generator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(generator)

    <span style=color:#999;font-style:italic># Transpose 2D conv layer 2.</span>
    generator = Conv2DTranspose(filters = <span style=color:#3677a9>256</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(generator)
    generator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(generator)
    generator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(generator)

    <span style=color:#999;font-style:italic># Transpose 2D conv layer 3.</span>
    generator = Conv2DTranspose(filters = <span style=color:#3677a9>128</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(generator)
    generator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(generator)
    generator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(generator)

    <span style=color:#999;font-style:italic># Transpose 2D conv layer 4.</span>
    generator = Conv2DTranspose(filters = <span style=color:#3677a9>64</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(generator)
    generator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(generator)
    generator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(generator)

    <span style=color:#999;font-style:italic># conv 2D layer 1.</span>
    generator = Conv2D(filters = <span style=color:#3677a9>64</span>, kernel_size = (<span style=color:#3677a9>3</span>,<span style=color:#3677a9>3</span>), strides = (<span style=color:#3677a9>1</span>,<span style=color:#3677a9>1</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(generator)
    generator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(generator)
    generator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(generator)

    <span style=color:#999;font-style:italic># Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel</span>
    generator = Conv2DTranspose(filters = <span style=color:#3677a9>3</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(generator)

    <span style=color:#999;font-style:italic># Tanh activation to get final normalized image</span>
    generator = Activation(<span style=color:#ed9d13>&#39;tanh&#39;</span>)(generator)

    <span style=color:#999;font-style:italic># defining the optimizer and compiling the generator model.</span>
    gen_opt = Adam(lr=<span style=color:#3677a9>0.00015</span>, beta_1=<span style=color:#3677a9>0.5</span>)
    generator_model = Model(<span style=color:#24909d>input</span> = gen_input, output = generator)
    generator_model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer=gen_opt, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])
    generator_model.summary()
    <span style=color:#6ab825;font-weight:700>return</span> generator_model
</code></pre></div><p>You can plot the final generator model:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>plot_model(generator, to_file=<span style=color:#ed9d13>&#39;gen_plot.png&#39;</span>, show_shapes=True, show_layer_names=True)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/genarch.png><figcaption style=font-size:12px>Generator Architecture</figcaption></figure></center></div><h3 id=discriminator-architecture-1>Discriminator architecture</h3><p>Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.</p><p>Here is the architecture of the discriminator:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>get_disc_normal</span>(image_shape=(<span style=color:#3677a9>64</span>,<span style=color:#3677a9>64</span>,<span style=color:#3677a9>3</span>)):
    dropout_prob = <span style=color:#3677a9>0.4</span>
    kernel_init = <span style=color:#ed9d13>&#39;glorot_uniform&#39;</span>
    dis_input = Input(shape = image_shape)

    <span style=color:#999;font-style:italic># Conv layer 1:</span>
    discriminator = Conv2D(filters = <span style=color:#3677a9>64</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(dis_input)
    discriminator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(discriminator)
    <span style=color:#999;font-style:italic># Conv layer 2:</span>
    discriminator = Conv2D(filters = <span style=color:#3677a9>128</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(discriminator)
    discriminator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(discriminator)
    discriminator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(discriminator)
    <span style=color:#999;font-style:italic># Conv layer 3:   </span>
    discriminator = Conv2D(filters = <span style=color:#3677a9>256</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(discriminator)
    discriminator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(discriminator)
    discriminator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(discriminator)
    <span style=color:#999;font-style:italic># Conv layer 4:</span>
    discriminator = Conv2D(filters = <span style=color:#3677a9>512</span>, kernel_size = (<span style=color:#3677a9>4</span>,<span style=color:#3677a9>4</span>), strides = (<span style=color:#3677a9>2</span>,<span style=color:#3677a9>2</span>), padding = <span style=color:#ed9d13>&#34;same&#34;</span>, data_format = <span style=color:#ed9d13>&#34;channels_last&#34;</span>, kernel_initializer = kernel_init)(discriminator)
    discriminator = BatchNormalization(momentum = <span style=color:#3677a9>0.5</span>)(discriminator)
    discriminator = LeakyReLU(<span style=color:#3677a9>0.2</span>)(discriminator)<span style=color:#999;font-style:italic>#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)</span>
    <span style=color:#999;font-style:italic># Flatten</span>
    discriminator = Flatten()(discriminator)
    <span style=color:#999;font-style:italic># Dense Layer</span>
    discriminator = Dense(<span style=color:#3677a9>1</span>)(discriminator)
    <span style=color:#999;font-style:italic># Sigmoid Activation</span>
    discriminator = Activation(<span style=color:#ed9d13>&#39;sigmoid&#39;</span>)(discriminator)
    <span style=color:#999;font-style:italic># Optimizer and Compiling model</span>
    dis_opt = Adam(lr=<span style=color:#3677a9>0.0002</span>, beta_1=<span style=color:#3677a9>0.5</span>)
    discriminator_model = Model(<span style=color:#24909d>input</span> = dis_input, output = discriminator)
    discriminator_model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer=dis_opt, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])
    discriminator_model.summary()
    <span style=color:#6ab825;font-weight:700>return</span> discriminator_model
</code></pre></div><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>plot_model(discriminator, to_file=<span style=color:#ed9d13>&#39;dis_plot.png&#39;</span>, show_shapes=True, show_layer_names=True)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/disarch.png><figcaption style=font-size:12px>Discriminator Architecture</figcaption></figure></center></div><h2 id=training>Training</h2><p><img src=https://cdn-images-1.medium.com/max/10368/0*OjIw7GFIkonGjfcc alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Understanding how the training works in GAN is essential. And maybe a little interesting too.</p><p>I start by creating our discriminator and generator using the functions defined in the previous section:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>discriminator = get_disc_normal(image_shape)
generator = get_gen_normal(noise_shape)
</code></pre></div><p>The generator and discriminator are then combined to create the final GAN.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>discriminator.trainable = False

<span style=color:#999;font-style:italic># Optimizer for the GAN</span>
opt = Adam(lr=<span style=color:#3677a9>0.00015</span>, beta_1=<span style=color:#3677a9>0.5</span>) <span style=color:#999;font-style:italic>#same as generator</span>
<span style=color:#999;font-style:italic># Input to the generator</span>
gen_inp = Input(shape=noise_shape)

GAN_inp = generator(gen_inp)
GAN_opt = discriminator(GAN_inp)

<span style=color:#999;font-style:italic># Final GAN</span>
gan = Model(<span style=color:#24909d>input</span> = gen_inp, output = GAN_opt)
gan.compile(loss = <span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>, optimizer = opt, metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])

plot_model(gan, to_file=<span style=color:#ed9d13>&#39;gan_plot.png&#39;</span>, show_shapes=True, show_layer_names=True)
</code></pre></div><p>This is the architecture of our whole GAN:</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*Qn0oyAYAK67oawZl.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h3 id=the-training-loop>The Training Loop</h3><p>This is the main region where we need to understand how the blocks we have created until now assemble and work together to work as one.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise.</span>
fixed_noise = gen_noise(<span style=color:#3677a9>16</span>,noise_shape)

<span style=color:#999;font-style:italic># To keep Track of losses</span>
avg_disc_fake_loss = []
avg_disc_real_loss = []
avg_GAN_loss = []

<span style=color:#999;font-style:italic># We will run for num_steps iterations</span>
<span style=color:#6ab825;font-weight:700>for</span> step <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(num_steps):
    tot_step = step
    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Begin step: &#34;</span>, tot_step)
    <span style=color:#999;font-style:italic># to keep track of time per step</span>
    step_begin_time = time.time()

    <span style=color:#999;font-style:italic># sample a batch of normalized images from the dataset</span>
    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir)

    <span style=color:#999;font-style:italic># Genearate noise to send as input to the generator</span>
    noise = gen_noise(batch_size,noise_shape)

    <span style=color:#999;font-style:italic># Use generator to create(predict) images</span>
    fake_data_X = generator.predict(noise)

    <span style=color:#999;font-style:italic># Save predicted images from the generator every 10th step</span>
    <span style=color:#6ab825;font-weight:700>if</span> (tot_step % <span style=color:#3677a9>100</span>) == <span style=color:#3677a9>0</span>:
        step_num = <span style=color:#24909d>str</span>(tot_step).zfill(<span style=color:#3677a9>4</span>)
        save_img_batch(fake_data_X,img_save_dir+step_num+<span style=color:#ed9d13>&#34;_image.png&#34;</span>)

    <span style=color:#999;font-style:italic># Create the labels for real and fake data. We don&#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick</span>
    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*<span style=color:#3677a9>0.2</span>
    fake_data_Y = np.random.random_sample(batch_size)*<span style=color:#3677a9>0.2</span>

    <span style=color:#999;font-style:italic># train the discriminator using data and labels</span>

    discriminator.trainable = True
    generator.trainable = False

    <span style=color:#999;font-style:italic># Training Discriminator seperately on real data</span>
    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y)
    <span style=color:#999;font-style:italic># training Discriminator seperately on fake data</span>
    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y)

    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Disc: real loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13> fake loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (dis_metrics_real[<span style=color:#3677a9>0</span>], dis_metrics_fake[<span style=color:#3677a9>0</span>]))

    <span style=color:#999;font-style:italic># Save the losses to plot later</span>
    avg_disc_fake_loss.append(dis_metrics_fake[<span style=color:#3677a9>0</span>])
    avg_disc_real_loss.append(dis_metrics_real[<span style=color:#3677a9>0</span>])

    <span style=color:#999;font-style:italic># Train the generator using a random vector of noise and its labels (1&#39;s with noise)</span>
    generator.trainable = True
    discriminator.trainable = False

    GAN_X = gen_noise(batch_size,noise_shape)
    GAN_Y = real_data_Y

    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)
    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;GAN loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (gan_metrics[<span style=color:#3677a9>0</span>]))

    <span style=color:#999;font-style:italic># Log results by opening a file in append mode</span>
    text_file = <span style=color:#24909d>open</span>(log_dir+<span style=color:#ed9d13>&#34;</span><span style=color:#ed9d13>\\</span><span style=color:#ed9d13>training_log.txt&#34;</span>, <span style=color:#ed9d13>&#34;a&#34;</span>)
    text_file.write(<span style=color:#ed9d13>&#34;Step: </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13> Disc: real loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13> fake loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13> GAN loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>\n</span><span style=color:#ed9d13>&#34;</span> % (tot_step, dis_metrics_real[<span style=color:#3677a9>0</span>], dis_metrics_fake[<span style=color:#3677a9>0</span>],gan_metrics[<span style=color:#3677a9>0</span>]))
    text_file.close()

    <span style=color:#999;font-style:italic># save GAN loss to plot later</span>
    avg_GAN_loss.append(gan_metrics[<span style=color:#3677a9>0</span>])

    end_time = time.time()
    diff_time = <span style=color:#24909d>int</span>(end_time - step_begin_time)
    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Step </span><span style=color:#ed9d13>%d</span><span style=color:#ed9d13> completed. Time took: </span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13> secs.&#34;</span> % (tot_step, diff_time))

    <span style=color:#999;font-style:italic># save model at every 500 steps</span>
    <span style=color:#6ab825;font-weight:700>if</span> ((tot_step+<span style=color:#3677a9>1</span>) % <span style=color:#3677a9>500</span>) == <span style=color:#3677a9>0</span>:
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;-----------------------------------------------------------------&#34;</span>)
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Average Disc_fake loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (np.mean(avg_disc_fake_loss)))
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Average Disc_real loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (np.mean(avg_disc_real_loss)))
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Average GAN loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (np.mean(avg_GAN_loss)))
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;-----------------------------------------------------------------&#34;</span>)
        discriminator.trainable = False
        generator.trainable = False
        <span style=color:#999;font-style:italic># predict on fixed_noise</span>
        fixed_noise_generate = generator.predict(noise)
        step_num = <span style=color:#24909d>str</span>(tot_step).zfill(<span style=color:#3677a9>4</span>)
        save_img_batch(fixed_noise_generate,img_save_dir+step_num+<span style=color:#ed9d13>&#34;fixed_image.png&#34;</span>)
        generator.save(save_model_dir+<span style=color:#24909d>str</span>(tot_step)+<span style=color:#ed9d13>&#34;_GENERATOR_weights_and_arch.hdf5&#34;</span>)
        discriminator.save(save_model_dir+<span style=color:#24909d>str</span>(tot_step)+<span style=color:#ed9d13>&#34;_DISCRIMINATOR_weights_and_arch.hdf5&#34;</span>)
</code></pre></div><p>Don’t worry, I will try to break the above code step by step here. The main steps in every training iteration are:</p><p><strong>Step 1:</strong> Sample a batch of normalized images from the dataset directory</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise.</span>
fixed_noise = gen_noise(<span style=color:#3677a9>16</span>,noise_shape)

<span style=color:#999;font-style:italic># To keep Track of losses</span>
avg_disc_fake_loss = []
avg_disc_real_loss = []
avg_GAN_loss = []

<span style=color:#999;font-style:italic># We will run for num_steps iterations</span>
<span style=color:#6ab825;font-weight:700>for</span> step <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(num_steps):
    tot_step = step
    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Begin step: &#34;</span>, tot_step)
    <span style=color:#999;font-style:italic># to keep track of time per step</span>
    step_begin_time = time.time()

    <span style=color:#999;font-style:italic># sample a batch of normalized images from the dataset</span>
    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir)
</code></pre></div><p>**Step2:**Generate noise for input to the generator</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Generate noise to send as input to the generator</span>
    noise = gen_noise(batch_size,noise_shape)
</code></pre></div><p>**Step3:**Generate images using random noise using the generator.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Use generator to create(predict) images</span>
    fake_data_X = generator.predict(noise)

    <span style=color:#999;font-style:italic># Save predicted images from the generator every 100th step</span>
    <span style=color:#6ab825;font-weight:700>if</span> (tot_step % <span style=color:#3677a9>100</span>) == <span style=color:#3677a9>0</span>:
        step_num = <span style=color:#24909d>str</span>(tot_step).zfill(<span style=color:#3677a9>4</span>)

save_img_batch(fake_data_X,img_save_dir+step_num+<span style=color:#ed9d13>&#34;_image.png&#34;</span>)
</code></pre></div><p>**Step 4:**Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their noisy labels.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Create the labels for real and fake data. We don&#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick</span>
    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*<span style=color:#3677a9>0.2</span>
    fake_data_Y = np.random.random_sample(batch_size)*<span style=color:#3677a9>0.2</span>

    <span style=color:#999;font-style:italic># train the discriminator using data and labels</span>

discriminator.trainable = True
    generator.trainable = False

<span style=color:#999;font-style:italic># Training Discriminator seperately on real data</span>
    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y)
    <span style=color:#999;font-style:italic># training Discriminator seperately on fake data</span>
    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y)

    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Disc: real loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13> fake loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (dis_metrics_real[<span style=color:#3677a9>0</span>], dis_metrics_fake[<span style=color:#3677a9>0</span>]))

    <span style=color:#999;font-style:italic># Save the losses to plot later</span>
    avg_disc_fake_loss.append(dis_metrics_fake[<span style=color:#3677a9>0</span>])
    avg_disc_real_loss.append(dis_metrics_real[<span style=color:#3677a9>0</span>])
</code></pre></div><p>**Step 5:**Train the GAN using noise as X and 1&rsquo;s(noisy) as Y while keeping discriminator as untrainable.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Train the generator using a random vector of noise and its labels (1&#39;s with noise)</span>
    generator.trainable = True
    discriminator.trainable = False

GAN_X = gen_noise(batch_size,noise_shape)
    GAN_Y = real_data_Y

    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)
    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;GAN loss: </span><span style=color:#ed9d13>%f</span><span style=color:#ed9d13>&#34;</span> % (gan_metrics[<span style=color:#3677a9>0</span>]))
</code></pre></div><p>We repeat the steps using the for loop to end up with a good discriminator and generator.</p><h2 id=results>Results</h2><p>The final output image looks like the following. As we can see, the GAN can generate pretty good images for our content editor friends to work with.</p><p>They might be a little crude for your liking, but still, this project was a starter for our GAN journey.</p><div style=margin-top:9px;margin-bottom:10px><center><figure><img src=/images/gans/res.png><pre><code>&lt;/figure&gt;
</code></pre></center></div><h3 id=loss-over-the-training-period>Loss over the training period</h3><p>Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average and the variance is decreasing too as we do more steps. One might want to train for even more iterations to get better results.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*gB21j4tJpkIzMxnc.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h3 id=image-generated-at-every-1500-steps>Image generated at every 1500 steps</h3><p>You can see the output and running code in
<a href="https://colab.research.google.com/drive/1Mxbfn0BUW4BlgEPc-minaE_M0_PaYIIX#scrollTo=pqPNyVnSqru1" target=_blank rel="nofollow noopener">Colab</a>
:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Generating GIF from PNGs</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>imageio</span>
<span style=color:#999;font-style:italic># create a list of PNGs</span>
generated_images = [img_save_dir+<span style=color:#24909d>str</span>(x).zfill(<span style=color:#3677a9>4</span>)+<span style=color:#ed9d13>&#34;_image.png&#34;</span> <span style=color:#6ab825;font-weight:700>for</span> x <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>0</span>,num_steps,<span style=color:#3677a9>100</span>)]
images = []
<span style=color:#6ab825;font-weight:700>for</span> filename <span style=color:#6ab825;font-weight:700>in</span> generated_images:
    images.append(imageio.imread(filename))
imageio.mimsave(img_save_dir+<span style=color:#ed9d13>&#39;movie.gif&#39;</span>, images)
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>IPython.display</span> <span style=color:#6ab825;font-weight:700>import</span> Image
<span style=color:#6ab825;font-weight:700>with</span> <span style=color:#24909d>open</span>(img_save_dir+<span style=color:#ed9d13>&#39;movie.gif&#39;</span>,<span style=color:#ed9d13>&#39;rb&#39;</span>) <span style=color:#6ab825;font-weight:700>as</span> f:
    display(Image(data=f.read(), format=<span style=color:#ed9d13>&#39;png&#39;</span>))
</code></pre></div><p><img src=https://cdn-images-1.medium.com/max/2268/1*rLPMvOP6EDjn-9zRNgnbJA.gif alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases the images are getting better.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># create a list of 20 PNGs to show</span>
generated_images = [img_save_dir+<span style=color:#24909d>str</span>(x).zfill(<span style=color:#3677a9>4</span>)+<span style=color:#ed9d13>&#34;fixed_image.png&#34;</span> <span style=color:#6ab825;font-weight:700>for</span> x <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>0</span>,num_steps,<span style=color:#3677a9>1500</span>)]
<span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;Displaying generated images&#34;</span>)
<span style=color:#999;font-style:italic># You might need to change grid size and figure size here according to num images.</span>
plt.figure(figsize=(<span style=color:#3677a9>16</span>,<span style=color:#3677a9>20</span>))
gs1 = gridspec.GridSpec(<span style=color:#3677a9>5</span>, <span style=color:#3677a9>4</span>)
gs1.update(wspace=<span style=color:#3677a9>0</span>, hspace=<span style=color:#3677a9>0</span>)
<span style=color:#6ab825;font-weight:700>for</span> i,image <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(generated_images):
    ax1 = plt.subplot(gs1[i])
    ax1.set_aspect(<span style=color:#ed9d13>&#39;equal&#39;</span>)
    step = image.split(<span style=color:#ed9d13>&#34;fixed&#34;</span>)[<span style=color:#3677a9>0</span>]
    image = Image.open(image)
    fig = plt.imshow(image)
    <span style=color:#999;font-style:italic># you might need to change some params here</span>
    fig = plt.text(<span style=color:#3677a9>20</span>,<span style=color:#3677a9>47</span>,<span style=color:#ed9d13>&#34;Step: &#34;</span>+step,bbox=<span style=color:#24909d>dict</span>(facecolor=<span style=color:#ed9d13>&#39;red&#39;</span>, alpha=<span style=color:#3677a9>0.5</span>),fontsize=<span style=color:#3677a9>12</span>)
    plt.axis(<span style=color:#ed9d13>&#39;off&#39;</span>)
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
plt.tight_layout()
plt.savefig(<span style=color:#ed9d13>&#34;GENERATEDimage.png&#34;</span>,bbox_inches=<span style=color:#ed9d13>&#39;tight&#39;</span>,pad_inches=<span style=color:#3677a9>0</span>)
plt.show()
</code></pre></div><p>Given below is the result of the GAN at different time steps:</p><p><img src=https://cdn-images-1.medium.com/max/2270/0*rfKbSQzG8IRliFSM.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h2 id=conclusion>Conclusion</h2><p>In this post, <em><strong>we learned about the basics of GAN</strong></em>. We also learned about the Generator and Discriminator architecture for DC-GANs, and we built a simple DC-GAN to generate anime images from scratch.</p><p>This model is not very good at generating fake images, yet we get to understand the basics of GANs with this project, and we are fired up to build more exciting and complex GANs as we go forward.</p><p>The DC-GAN flavor of GANs is widely applicable not only to generate Faces or new anime characters, but it can also be used to generate new fashion styles, for general content creation and sometimes for data augmentation purposes as well.</p><p><em><strong>We can now conjure up realistic textures or characters on demand if we have the training data at hand, and that is no small feat.</strong></em></p><p>If you want to know more about deep learning applications and use cases, take a look at the
<a href=https://coursera.pxf.io/b3rQ7m target=_blank rel="nofollow noopener">Sequence Models</a>
course in the
<a href=https://coursera.pxf.io/7mKnnY target=_blank rel="nofollow noopener">Deep Learning Specialization</a>
by Andrew NG. Andrew is a great instructor, and this course is great too.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in London, where I am currently working with Facebook .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/bash>Bash</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/machine-learning>Machine Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/opinion>Opinion</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>