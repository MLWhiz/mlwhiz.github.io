<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>An End to End Introduction to GANs</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work.">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="An End to End Introduction to GANs" />
<meta property="og:description" content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2019/06/17/gans/" />
<meta property="og:image" content="https://mlwhiz.com/images/gans/faces.png" />
<meta property="article:published_time" content="2019-06-17T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-06-17T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/gans/faces.png"/>

<meta name="twitter:title" content="An End to End Introduction to GANs"/>
<meta name="twitter:description" content="I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence. In this post, I will help the reader to understand how they can create and build such applications on their own. I will try to keep this post as intuitive as possible for starters while not dumbing it down too much. This post is about understanding how GANs work."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/atom.xml">RSS</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">An End to End Introduction to GANs</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-06-17T00:00:00">June 17, 2019</time>
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/gans/faces.png""></center>
</div>

<p>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.</p>

<p><strong><em>In this post, I will help the reader to understand how they can create and build such applications on their own.</em></strong></p>

<p>I will try to keep this post as intuitive as possible for starters while not dumbing it down too much.</p>

<p><strong><em>This post is about understanding how GANs work.</em></strong></p>

<hr />

<h2 id="task-overview">Task Overview</h2>

<p>I will work on <strong><em>creating our own anime characters using anime characters dataset.</em></strong></p>

<p>The DC-GAN flavor of GANs which I will use here is widely applicable not only to generate Faces or new anime characters; it can also be used to create modern fashion styles, for general content creation and sometimes for data augmentation purposes as well.</p>

<p><strong><em>As per my view, GANs will change the way video games and special effects are generated. The approach could create realistic textures or characters on demand.</em></strong></p>

<p>You can find the full code for this chapter in the <a href="https://github.com/MLWhiz/GAN_Project" rel="nofollow" target="_blank">Github Repository</a>. I have also uploaded the code to <a href="https://colab.research.google.com/drive/1Mxbfn0BUW4BlgEPc-minaE_M0_PaYIIX" rel="nofollow" target="_blank">Google Colab</a> so that you can try it yourself.</p>

<hr />

<h2 id="using-dcgan-architecture-to-generate-anime-images">Using DCGAN architecture to generate anime images</h2>

<p>As always before we get into the coding, it helps to delve a little bit into the theory.</p>

<p>The main idea of DC-GANâ€™s stemmed from the paper <a href="https://arxiv.org/pdf/1511.06434.pdf" rel="nofollow" target="_blank">UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a> written in 2016 by Alec Radford, Luke Metz, and Soumith Chintala.</p>

<p>Although I am going to explain the paper in the next few sections, do take a look at it. It is an excellent paper.</p>

<hr />

<h2 id="intuition-brief-intro-to-gans-for-generating-fake-images">INTUITION: Brief Intro to GANs for Generating Fake Images</h2>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/duel.jpeg">
      <figcaption style="font-size: 12px;">Generator vs. Discriminator</figcaption>
    </figure>
</center>
</div>

<p>Typically, <strong><em>GANs employ two dueling neural networks to train a computer to learn the nature of a data set well enough to generate convincing fakes.</em></strong></p>

<p>We can think of this as two systems where one Neural Network works to generate fakes (Generator), and another neural network (Discriminator) tries to classify which image is a fake.</p>

<p>As both generator and discriminator networks do this repetitively, the networks eventually get better at their respective tasks.</p>

<p><strong><em>Think of this as simple as swordplay.</em></strong> Two noobs start sparring with each other. After a while, both become better at swordplay.</p>

<p><strong><em>Or you could think of this as a robber(generator) and a policeman(Discriminator).</em></strong> After a lot of thefts, the robber becomes better at thieving while the policeman gets better at catching the robber. <em>In an ideal world.</em></p>

<p>The Losses in these neural networks are primarily a function of how the other network performs:</p>

<ul>
<li><p>Discriminator network loss is a function of generator network quality- Loss is high for the discriminator if it gets fooled by the generatorâ€™s fake images</p></li>

<li><p>Generator network loss is a function of discriminator network quality â€” Loss is high if the generator is not able to fool the discriminator.</p></li>
</ul>

<p>In the training phase, we train our Discriminator and Generator networks sequentially intending to improve both the Discriminator and Generator performance.</p>

<p>The objective is to end up with weights that help Generators to generate realistic looking images. <strong><em>In the end, we can use the Generator Neural network to generate fake images from Random Noise.</em></strong></p>

<hr />

<h2 id="generator-architecture">Generator architecture</h2>

<p>One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/generator_paper.png">
    </figure>
</center>
</div>

<p>The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.</p>

<p>Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/gen_logic.png">
    </figure>
</center>
</div>

<p><strong><em>How do we get such an architecture?</em></strong></p>

<p>In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/gen_logic_more.png">
    </figure>tc
</center>
</div>

<p>We donâ€™t have to worry about any weights right now as the network itself will learn those while training.</p>

<p>Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we donâ€™t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.</p>

<h3 id="now-what-are-transpose-convolutions">Now, What are Transpose convolutions?</h3>

<p>In most simple terms, <strong><em>transpose convolutions provide us with a way to upsample images.</em></strong> While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/tc.png">
    </figure>
</center>
</div>

<p><strong><em>Q:</em></strong> We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why donâ€™t we use Un-pooling?</p>

<p>It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.</p>

<h2 id="discriminator-architecture">Discriminator architecture</h2>

<p>Now, as we have understood the generator architecture, here is the discriminator as a black box.</p>

<p>In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/dis.png">
    </figure>
</center>
</div>

<p>Takes an image as input and predicts if it is real/fake. <strong><em>Every image conv net ever.</em></strong></p>

<h2 id="data-preprocessing-and-visualization">Data preprocessing and visualization</h2>

<p>The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">filenames <span style="color:#f92672">=</span> glob<span style="color:#f92672">.</span>glob(<span style="color:#e6db74">&#39;animeface-character-dataset/*/*.pn*&#39;</span>)
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">8</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>):
    img <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>imread(filenames[i], <span style="color:#ae81ff">0</span>)
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    plt<span style="color:#f92672">.</span>imshow(img)
    plt<span style="color:#f92672">.</span>title(img<span style="color:#f92672">.</span>shape)
    plt<span style="color:#f92672">.</span>xticks([])
    plt<span style="color:#f92672">.</span>yticks([])
plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()</code></pre></div>
<p>The resultant output is as follows:</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/out.png">
    </figure>
</center>
</div>

<p>We get to see the sizes of the images and the images themselves.</p>

<p>We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.</p>

<p>We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># A function to normalize image pixels.</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">norm_img</span>(img):
    <span style="color:#e6db74">&#39;&#39;&#39;A function to Normalize Images.
</span><span style="color:#e6db74">    Input:
</span><span style="color:#e6db74">        img : Original image as numpy array.
</span><span style="color:#e6db74">    Output: Normailized Image as numpy array
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    img <span style="color:#f92672">=</span> (img <span style="color:#f92672">/</span> <span style="color:#ae81ff">127.5</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> img
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">denorm_img</span>(img):
    <span style="color:#e6db74">&#39;&#39;&#39;A function to Denormailze, i.e. recreate image from normalized image
</span><span style="color:#e6db74">    Input:
</span><span style="color:#e6db74">        img : Normalized image as numpy array.
</span><span style="color:#e6db74">    Output: Original Image as numpy array
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    img <span style="color:#f92672">=</span> (img <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">127.5</span>
    <span style="color:#66d9ef">return</span> img<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>uint8) 
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_from_dataset</span>(batch_size, image_shape, data_dir<span style="color:#f92672">=</span>None):
    <span style="color:#e6db74">&#39;&#39;&#39;Create a batch of image samples by sampling random images from a data directory.
</span><span style="color:#e6db74">    Resizes the image using image_shape and normalize the images.
</span><span style="color:#e6db74">    Input:
</span><span style="color:#e6db74">        batch_size : Sample size required
</span><span style="color:#e6db74">        image_size : Size that Image should be resized to
</span><span style="color:#e6db74">        data_dir : Path of directory where training images are placed.
</span><span style="color:#e6db74">    Output:
</span><span style="color:#e6db74">        sample : batch of processed images 
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    sample_dim <span style="color:#f92672">=</span> (batch_size,) <span style="color:#f92672">+</span> image_shape
    sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>empty(sample_dim, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32)
    all_data_dirlist <span style="color:#f92672">=</span> list(glob<span style="color:#f92672">.</span>glob(data_dir))
    sample_imgs_paths <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(all_data_dirlist,batch_size)
    <span style="color:#66d9ef">for</span> index,img_filename <span style="color:#f92672">in</span> enumerate(sample_imgs_paths):
        image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(img_filename)
        image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>resize(image_shape[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
        image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>convert(<span style="color:#e6db74">&#39;RGB&#39;</span>) 
        image <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(image)
        image <span style="color:#f92672">=</span> norm_img(image)
        sample[index,<span style="color:#f92672">...</span>] <span style="color:#f92672">=</span> image
    <span style="color:#66d9ef">return</span> sample</code></pre></div>
<p>As you will see, we will be using the preceding defined functions in the training part of our code.</p>

<h2 id="implementation-of-dcgan">Implementation of DCGAN</h2>

<p>This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.</p>

<h3 id="generating-noise-vector-for-generator">Generating noise vector for Generator</h3>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/noise.jpeg">
      <figcaption style="font-size: 12px;">Kids: Normal Noise generators</figcaption>
    </figure>
</center>
</div>

<p>The following code block is a helper function to create a noise vector of predefined length for a Generator. It will generate the noise which we want to convert to an image using our generator architecture.</p>

<p>We use a normal distribution</p>

<p><img src="https://cdn-images-1.medium.com/max/3720/0*MQspqgJbMj2BnO22.png" alt="" /></p>

<p>to generate the noise vector:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gen_noise</span>(batch_size, noise_shape):
    <span style="color:#e6db74">&#39;&#39;&#39; Generates a numpy vector sampled from normal distribution of shape                                (batch_size,noise_shape)
</span><span style="color:#e6db74">    Input:
</span><span style="color:#e6db74">        batch_size : size of batch
</span><span style="color:#e6db74">        noise_shape: shape of noise vector, normally kept as 100 
</span><span style="color:#e6db74">    Output:a numpy vector sampled from normal distribution of shape                                  (batch_size,noise_shape)     
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, size<span style="color:#f92672">=</span>(batch_size,)<span style="color:#f92672">+</span>noise_shape)</code></pre></div>
<h3 id="generator-architecture-1">Generator architecture</h3>

<p>The Generator is the most crucial part of the GAN.</p>

<p>Here, I create a generator by adding some transposed convolution layers to upsample the noise vector to an image.</p>

<p>As you will notice, this generator architecture is not the same as given in the Original DC-GAN paper.</p>

<p>I needed to make some architectural changes to fit our data better, so I added a convolution layer in the middle and removed all dense layers from the generator architecture, making it fully convolutional.</p>

<p>I also use a lot of Batchnorm layers with a momentum of 0.5 and leaky ReLU activation. I use Adam optimizer with Î²=0.5. The following code block is the function I will use to create the generator:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_gen_normal</span>(noise_shape):
    <span style="color:#e6db74">&#39;&#39;&#39; This function takes as input shape of the noise vector and creates the Keras generator    architecture.
</span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
    kernel_init <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;glorot_uniform&#39;</span>    
    gen_input <span style="color:#f92672">=</span> Input(shape <span style="color:#f92672">=</span> noise_shape) 
    
    <span style="color:#75715e"># Transpose 2D conv layer 1. </span>
    generator <span style="color:#f92672">=</span> Conv2DTranspose(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">512</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;valid&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(gen_input)
    generator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(generator)
    generator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(generator)
    
    <span style="color:#75715e"># Transpose 2D conv layer 2.</span>
    generator <span style="color:#f92672">=</span> Conv2DTranspose(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(generator)
    generator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(generator)
    generator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(generator)
    
    <span style="color:#75715e"># Transpose 2D conv layer 3.</span>
    generator <span style="color:#f92672">=</span> Conv2DTranspose(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(generator)
    generator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(generator)
    generator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(generator)
    
    <span style="color:#75715e"># Transpose 2D conv layer 4.</span>
    generator <span style="color:#f92672">=</span> Conv2DTranspose(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(generator)
    generator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(generator)
    generator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(generator)
    
    <span style="color:#75715e"># conv 2D layer 1.</span>
    generator <span style="color:#f92672">=</span> Conv2D(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(generator)
    generator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(generator)
    generator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(generator)
    
    <span style="color:#75715e"># Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel</span>
    generator <span style="color:#f92672">=</span> Conv2DTranspose(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(generator)
    
    <span style="color:#75715e"># Tanh activation to get final normalized image</span>
    generator <span style="color:#f92672">=</span> Activation(<span style="color:#e6db74">&#39;tanh&#39;</span>)(generator)
    
    <span style="color:#75715e"># defining the optimizer and compiling the generator model.</span>
    gen_opt <span style="color:#f92672">=</span> Adam(lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00015</span>, beta_1<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
    generator_model <span style="color:#f92672">=</span> Model(input <span style="color:#f92672">=</span> gen_input, output <span style="color:#f92672">=</span> generator)
    generator_model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, optimizer<span style="color:#f92672">=</span>gen_opt, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
    generator_model<span style="color:#f92672">.</span>summary()
    <span style="color:#66d9ef">return</span> generator_model</code></pre></div>
<p>You can plot the final generator model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">plot_model(generator, to_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gen_plot.png&#39;</span>, show_shapes<span style="color:#f92672">=</span>True, show_layer_names<span style="color:#f92672">=</span>True)</code></pre></div>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/genarch.png">
      <figcaption style="font-size: 12px;">Generator Architecture</figcaption>
    </figure>
</center>
</div>

<h3 id="discriminator-architecture-1">Discriminator architecture</h3>

<p>Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.</p>

<p>Here is the architecture of the discriminator:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_disc_normal</span>(image_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">3</span>)):
    dropout_prob <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>
    kernel_init <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;glorot_uniform&#39;</span>
    dis_input <span style="color:#f92672">=</span> Input(shape <span style="color:#f92672">=</span> image_shape)
    
    <span style="color:#75715e"># Conv layer 1:</span>
    discriminator <span style="color:#f92672">=</span> Conv2D(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(dis_input)
    discriminator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(discriminator)
    <span style="color:#75715e"># Conv layer 2:</span>
    discriminator <span style="color:#f92672">=</span> Conv2D(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(discriminator)
    discriminator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(discriminator)
    discriminator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(discriminator)
    <span style="color:#75715e"># Conv layer 3:   </span>
    discriminator <span style="color:#f92672">=</span> Conv2D(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(discriminator)
    discriminator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(discriminator)
    discriminator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(discriminator)
    <span style="color:#75715e"># Conv layer 4:</span>
    discriminator <span style="color:#f92672">=</span> Conv2D(filters <span style="color:#f92672">=</span> <span style="color:#ae81ff">512</span>, kernel_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">4</span>), strides <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>), padding <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;same&#34;</span>, data_format <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;channels_last&#34;</span>, kernel_initializer <span style="color:#f92672">=</span> kernel_init)(discriminator)
    discriminator <span style="color:#f92672">=</span> BatchNormalization(momentum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>)(discriminator)
    discriminator <span style="color:#f92672">=</span> LeakyReLU(<span style="color:#ae81ff">0.2</span>)(discriminator)<span style="color:#75715e">#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator)</span>
    <span style="color:#75715e"># Flatten</span>
    discriminator <span style="color:#f92672">=</span> Flatten()(discriminator)
    <span style="color:#75715e"># Dense Layer</span>
    discriminator <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">1</span>)(discriminator)
    <span style="color:#75715e"># Sigmoid Activation</span>
    discriminator <span style="color:#f92672">=</span> Activation(<span style="color:#e6db74">&#39;sigmoid&#39;</span>)(discriminator)
    <span style="color:#75715e"># Optimizer and Compiling model</span>
    dis_opt <span style="color:#f92672">=</span> Adam(lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0002</span>, beta_1<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
    discriminator_model <span style="color:#f92672">=</span> Model(input <span style="color:#f92672">=</span> dis_input, output <span style="color:#f92672">=</span> discriminator)
    discriminator_model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, optimizer<span style="color:#f92672">=</span>dis_opt, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
    discriminator_model<span style="color:#f92672">.</span>summary()
    <span style="color:#66d9ef">return</span> discriminator_model</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">plot_model(discriminator, to_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;dis_plot.png&#39;</span>, show_shapes<span style="color:#f92672">=</span>True, show_layer_names<span style="color:#f92672">=</span>True)</code></pre></div>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/disarch.png">
      <figcaption style="font-size: 12px;">Discriminator Architecture</figcaption>
    </figure>
</center>
</div>

<h2 id="training">Training</h2>

<p><img src="https://cdn-images-1.medium.com/max/10368/0*OjIw7GFIkonGjfcc" alt="" /></p>

<p>Understanding how the training works in GAN is essential. And maybe a little interesting too.</p>

<p>I start by creating our discriminator and generator using the functions defined in the previous section:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">discriminator <span style="color:#f92672">=</span> get_disc_normal(image_shape)
generator <span style="color:#f92672">=</span> get_gen_normal(noise_shape)</code></pre></div>
<p>The generator and discriminator are then combined to create the final GAN.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False

<span style="color:#75715e"># Optimizer for the GAN</span>
opt <span style="color:#f92672">=</span> Adam(lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00015</span>, beta_1<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>) <span style="color:#75715e">#same as generator</span>
<span style="color:#75715e"># Input to the generator</span>
gen_inp <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>noise_shape)

GAN_inp <span style="color:#f92672">=</span> generator(gen_inp)
GAN_opt <span style="color:#f92672">=</span> discriminator(GAN_inp)

<span style="color:#75715e"># Final GAN</span>
gan <span style="color:#f92672">=</span> Model(input <span style="color:#f92672">=</span> gen_inp, output <span style="color:#f92672">=</span> GAN_opt)
gan<span style="color:#f92672">.</span>compile(loss <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, optimizer <span style="color:#f92672">=</span> opt, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])

plot_model(gan, to_file<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gan_plot.png&#39;</span>, show_shapes<span style="color:#f92672">=</span>True, show_layer_names<span style="color:#f92672">=</span>True)</code></pre></div>
<p>This is the architecture of our whole GAN:</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/0*Qn0oyAYAK67oawZl.png" alt="" /></p>

<h3 id="the-training-loop">The Training Loop</h3>

<p>This is the main region where we need to understand how the blocks we have created until now assemble and work together to work as one.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. </span>
fixed_noise <span style="color:#f92672">=</span> gen_noise(<span style="color:#ae81ff">16</span>,noise_shape)

<span style="color:#75715e"># To keep Track of losses</span>
avg_disc_fake_loss <span style="color:#f92672">=</span> []
avg_disc_real_loss <span style="color:#f92672">=</span> []
avg_GAN_loss <span style="color:#f92672">=</span> []

<span style="color:#75715e"># We will run for num_steps iterations</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(num_steps): 
    tot_step <span style="color:#f92672">=</span> step
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Begin step: &#34;</span>, tot_step)
    <span style="color:#75715e"># to keep track of time per step</span>
    step_begin_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time() 
    
    <span style="color:#75715e"># sample a batch of normalized images from the dataset</span>
    real_data_X <span style="color:#f92672">=</span> sample_from_dataset(batch_size, image_shape, data_dir<span style="color:#f92672">=</span>data_dir)
    
    <span style="color:#75715e"># Genearate noise to send as input to the generator</span>
    noise <span style="color:#f92672">=</span> gen_noise(batch_size,noise_shape)
    
    <span style="color:#75715e"># Use generator to create(predict) images</span>
    fake_data_X <span style="color:#f92672">=</span> generator<span style="color:#f92672">.</span>predict(noise)
    
    <span style="color:#75715e"># Save predicted images from the generator every 10th step</span>
    <span style="color:#66d9ef">if</span> (tot_step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        step_num <span style="color:#f92672">=</span> str(tot_step)<span style="color:#f92672">.</span>zfill(<span style="color:#ae81ff">4</span>)
        save_img_batch(fake_data_X,img_save_dir<span style="color:#f92672">+</span>step_num<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;_image.png&#34;</span>)
    
    <span style="color:#75715e"># Create the labels for real and fake data. We don&#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick</span>
    real_data_Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(batch_size) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random_sample(batch_size)<span style="color:#f92672">*</span><span style="color:#ae81ff">0.2</span>
    fake_data_Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random_sample(batch_size)<span style="color:#f92672">*</span><span style="color:#ae81ff">0.2</span>
        
    <span style="color:#75715e"># train the discriminator using data and labels</span>

    discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> True
    generator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False

    <span style="color:#75715e"># Training Discriminator seperately on real data</span>
    dis_metrics_real <span style="color:#f92672">=</span> discriminator<span style="color:#f92672">.</span>train_on_batch(real_data_X,real_data_Y) 
    <span style="color:#75715e"># training Discriminator seperately on fake data</span>
    dis_metrics_fake <span style="color:#f92672">=</span> discriminator<span style="color:#f92672">.</span>train_on_batch(fake_data_X,fake_data_Y) 
    
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Disc: real loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74"> fake loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (dis_metrics_real[<span style="color:#ae81ff">0</span>], dis_metrics_fake[<span style="color:#ae81ff">0</span>]))
    
    <span style="color:#75715e"># Save the losses to plot later</span>
    avg_disc_fake_loss<span style="color:#f92672">.</span>append(dis_metrics_fake[<span style="color:#ae81ff">0</span>])
    avg_disc_real_loss<span style="color:#f92672">.</span>append(dis_metrics_real[<span style="color:#ae81ff">0</span>])
    
    <span style="color:#75715e"># Train the generator using a random vector of noise and its labels (1&#39;s with noise)</span>
    generator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> True
    discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False

    GAN_X <span style="color:#f92672">=</span> gen_noise(batch_size,noise_shape)
    GAN_Y <span style="color:#f92672">=</span> real_data_Y
   
    gan_metrics <span style="color:#f92672">=</span> gan<span style="color:#f92672">.</span>train_on_batch(GAN_X,GAN_Y)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;GAN loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (gan_metrics[<span style="color:#ae81ff">0</span>]))
    
    <span style="color:#75715e"># Log results by opening a file in append mode</span>
    text_file <span style="color:#f92672">=</span> open(log_dir<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">training_log.txt&#34;</span>, <span style="color:#e6db74">&#34;a&#34;</span>)
    text_file<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;Step: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> Disc: real loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74"> fake loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74"> GAN loss: </span><span style="color:#e6db74">%f</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (tot_step, dis_metrics_real[<span style="color:#ae81ff">0</span>], dis_metrics_fake[<span style="color:#ae81ff">0</span>],gan_metrics[<span style="color:#ae81ff">0</span>]))
    text_file<span style="color:#f92672">.</span>close()

    <span style="color:#75715e"># save GAN loss to plot later</span>
    avg_GAN_loss<span style="color:#f92672">.</span>append(gan_metrics[<span style="color:#ae81ff">0</span>])
            
    end_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
    diff_time <span style="color:#f92672">=</span> int(end_time <span style="color:#f92672">-</span> step_begin_time)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Step </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> completed. Time took: </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> secs.&#34;</span> <span style="color:#f92672">%</span> (tot_step, diff_time))
    
    <span style="color:#75715e"># save model at every 500 steps</span>
    <span style="color:#66d9ef">if</span> ((tot_step<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">500</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;-----------------------------------------------------------------&#34;</span>)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Average Disc_fake loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (np<span style="color:#f92672">.</span>mean(avg_disc_fake_loss))) 
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Average Disc_real loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (np<span style="color:#f92672">.</span>mean(avg_disc_real_loss))) 
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Average GAN loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (np<span style="color:#f92672">.</span>mean(avg_GAN_loss)))
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;-----------------------------------------------------------------&#34;</span>)
        discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False
        generator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False
        <span style="color:#75715e"># predict on fixed_noise</span>
        fixed_noise_generate <span style="color:#f92672">=</span> generator<span style="color:#f92672">.</span>predict(noise)
        step_num <span style="color:#f92672">=</span> str(tot_step)<span style="color:#f92672">.</span>zfill(<span style="color:#ae81ff">4</span>)
        save_img_batch(fixed_noise_generate,img_save_dir<span style="color:#f92672">+</span>step_num<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;fixed_image.png&#34;</span>)
        generator<span style="color:#f92672">.</span>save(save_model_dir<span style="color:#f92672">+</span>str(tot_step)<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;_GENERATOR_weights_and_arch.hdf5&#34;</span>)
        discriminator<span style="color:#f92672">.</span>save(save_model_dir<span style="color:#f92672">+</span>str(tot_step)<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;_DISCRIMINATOR_weights_and_arch.hdf5&#34;</span>)</code></pre></div>
<p>Donâ€™t worry, I will try to break the above code step by step here. The main steps in every training iteration are:</p>

<p><strong>Step 1:</strong> Sample a batch of normalized images from the dataset directory</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. </span>
fixed_noise <span style="color:#f92672">=</span> gen_noise(<span style="color:#ae81ff">16</span>,noise_shape)

<span style="color:#75715e"># To keep Track of losses</span>
avg_disc_fake_loss <span style="color:#f92672">=</span> []
avg_disc_real_loss <span style="color:#f92672">=</span> []
avg_GAN_loss <span style="color:#f92672">=</span> []

<span style="color:#75715e"># We will run for num_steps iterations</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(num_steps): 
    tot_step <span style="color:#f92672">=</span> step
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Begin step: &#34;</span>, tot_step)
    <span style="color:#75715e"># to keep track of time per step</span>
    step_begin_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time() 
    
    <span style="color:#75715e"># sample a batch of normalized images from the dataset</span>
    real_data_X <span style="color:#f92672">=</span> sample_from_dataset(batch_size, image_shape, data_dir<span style="color:#f92672">=</span>data_dir)</code></pre></div>
<p><strong>Step2:</strong>Generate noise for input to the generator</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Generate noise to send as input to the generator</span>
    noise <span style="color:#f92672">=</span> gen_noise(batch_size,noise_shape)</code></pre></div>
<p><strong>Step3:</strong>Generate images using random noise using the generator.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Use generator to create(predict) images</span>
    fake_data_X <span style="color:#f92672">=</span> generator<span style="color:#f92672">.</span>predict(noise)
    
    <span style="color:#75715e"># Save predicted images from the generator every 100th step</span>
    <span style="color:#66d9ef">if</span> (tot_step <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        step_num <span style="color:#f92672">=</span> str(tot_step)<span style="color:#f92672">.</span>zfill(<span style="color:#ae81ff">4</span>)

save_img_batch(fake_data_X,img_save_dir<span style="color:#f92672">+</span>step_num<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;_image.png&#34;</span>)</code></pre></div>
<p><strong>Step 4:</strong>Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their noisy labels.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Create the labels for real and fake data. We don&#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick</span>
    real_data_Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(batch_size) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random_sample(batch_size)<span style="color:#f92672">*</span><span style="color:#ae81ff">0.2</span>
    fake_data_Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random_sample(batch_size)<span style="color:#f92672">*</span><span style="color:#ae81ff">0.2</span>
        
    <span style="color:#75715e"># train the discriminator using data and labels</span>

discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> True
    generator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False

<span style="color:#75715e"># Training Discriminator seperately on real data</span>
    dis_metrics_real <span style="color:#f92672">=</span> discriminator<span style="color:#f92672">.</span>train_on_batch(real_data_X,real_data_Y) 
    <span style="color:#75715e"># training Discriminator seperately on fake data</span>
    dis_metrics_fake <span style="color:#f92672">=</span> discriminator<span style="color:#f92672">.</span>train_on_batch(fake_data_X,fake_data_Y) 
    
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Disc: real loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74"> fake loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (dis_metrics_real[<span style="color:#ae81ff">0</span>], dis_metrics_fake[<span style="color:#ae81ff">0</span>]))
    
    <span style="color:#75715e"># Save the losses to plot later</span>
    avg_disc_fake_loss<span style="color:#f92672">.</span>append(dis_metrics_fake[<span style="color:#ae81ff">0</span>])
    avg_disc_real_loss<span style="color:#f92672">.</span>append(dis_metrics_real[<span style="color:#ae81ff">0</span>])</code></pre></div>
<p><strong>Step 5:</strong>Train the GAN using noise as X and 1&rsquo;s(noisy) as Y while keeping discriminator as untrainable.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Train the generator using a random vector of noise and its labels (1&#39;s with noise)</span>
    generator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> True
    discriminator<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> False

GAN_X <span style="color:#f92672">=</span> gen_noise(batch_size,noise_shape)
    GAN_Y <span style="color:#f92672">=</span> real_data_Y
   
    gan_metrics <span style="color:#f92672">=</span> gan<span style="color:#f92672">.</span>train_on_batch(GAN_X,GAN_Y)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;GAN loss: </span><span style="color:#e6db74">%f</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (gan_metrics[<span style="color:#ae81ff">0</span>]))</code></pre></div>
<p>We repeat the steps using the for loop to end up with a good discriminator and generator.</p>

<h2 id="results">Results</h2>

<p>The final output image looks like the following. As we can see, the GAN can generate pretty good images for our content editor friends to work with.</p>

<p>They might be a little crude for your liking, but still, this project was a starter for our GAN journey.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center>
    <figure>
      <img src="/images/gans/res.png">
      
    </figure>
</center>
</div>

<h3 id="loss-over-the-training-period">Loss over the training period</h3>

<p>Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average and the variance is decreasing too as we do more steps. One might want to train for even more iterations to get better results.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/0*gB21j4tJpkIzMxnc.png" alt="" /></p>

<h3 id="image-generated-at-every-1500-steps">Image generated at every 1500 steps</h3>

<p>You can see the output and running code in <a href="https://colab.research.google.com/drive/1Mxbfn0BUW4BlgEPc-minaE_M0_PaYIIX#scrollTo=pqPNyVnSqru1" rel="nofollow" target="_blank">Colab</a>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Generating GIF from PNGs</span>
<span style="color:#f92672">import</span> imageio
<span style="color:#75715e"># create a list of PNGs</span>
generated_images <span style="color:#f92672">=</span> [img_save_dir<span style="color:#f92672">+</span>str(x)<span style="color:#f92672">.</span>zfill(<span style="color:#ae81ff">4</span>)<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;_image.png&#34;</span> <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,num_steps,<span style="color:#ae81ff">100</span>)]
images <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> filename <span style="color:#f92672">in</span> generated_images:
    images<span style="color:#f92672">.</span>append(imageio<span style="color:#f92672">.</span>imread(filename))
imageio<span style="color:#f92672">.</span>mimsave(img_save_dir<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;movie.gif&#39;</span>, images)
<span style="color:#f92672">from</span> IPython.display <span style="color:#f92672">import</span> Image
<span style="color:#66d9ef">with</span> open(img_save_dir<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;movie.gif&#39;</span>,<span style="color:#e6db74">&#39;rb&#39;</span>) <span style="color:#66d9ef">as</span> f:
    display(Image(data<span style="color:#f92672">=</span>f<span style="color:#f92672">.</span>read(), format<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;png&#39;</span>))</code></pre></div>
<p><img src="https://cdn-images-1.medium.com/max/2268/1*rLPMvOP6EDjn-9zRNgnbJA.gif" alt="" /></p>

<p>Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases the images are getting better.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># create a list of 20 PNGs to show</span>
generated_images <span style="color:#f92672">=</span> [img_save_dir<span style="color:#f92672">+</span>str(x)<span style="color:#f92672">.</span>zfill(<span style="color:#ae81ff">4</span>)<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;fixed_image.png&#34;</span> <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,num_steps,<span style="color:#ae81ff">1500</span>)]
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Displaying generated images&#34;</span>)
<span style="color:#75715e"># You might need to change grid size and figure size here according to num images. </span>
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>,<span style="color:#ae81ff">20</span>))
gs1 <span style="color:#f92672">=</span> gridspec<span style="color:#f92672">.</span>GridSpec(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>)
gs1<span style="color:#f92672">.</span>update(wspace<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, hspace<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
<span style="color:#66d9ef">for</span> i,image <span style="color:#f92672">in</span> enumerate(generated_images):
    ax1 <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplot(gs1[i])
    ax1<span style="color:#f92672">.</span>set_aspect(<span style="color:#e6db74">&#39;equal&#39;</span>)
    step <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;fixed&#34;</span>)[<span style="color:#ae81ff">0</span>]
    image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(image)
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>imshow(image)
    <span style="color:#75715e"># you might need to change some params here</span>
    fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">47</span>,<span style="color:#e6db74">&#34;Step: &#34;</span><span style="color:#f92672">+</span>step,bbox<span style="color:#f92672">=</span>dict(facecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>),fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
    fig<span style="color:#f92672">.</span>axes<span style="color:#f92672">.</span>get_xaxis()<span style="color:#f92672">.</span>set_visible(False)
    fig<span style="color:#f92672">.</span>axes<span style="color:#f92672">.</span>get_yaxis()<span style="color:#f92672">.</span>set_visible(False)
plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>savefig(<span style="color:#e6db74">&#34;GENERATEDimage.png&#34;</span>,bbox_inches<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;tight&#39;</span>,pad_inches<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
plt<span style="color:#f92672">.</span>show()</code></pre></div>
<p>Given below is the result of the GAN at different time steps:</p>

<p><img src="https://cdn-images-1.medium.com/max/2270/0*rfKbSQzG8IRliFSM.png" alt="" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, <strong><em>we learned about the basics of GAN</em></strong>. We also learned about the Generator and Discriminator architecture for DC-GANs, and we built a simple DC-GAN to generate anime images from scratch.</p>

<p>This model is not very good at generating fake images, yet we get to understand the basics of GANs with this project, and we are fired up to build more exciting and complex GANs as we go forward.</p>

<p>The DC-GAN flavor of GANs is widely applicable not only to generate Faces or new anime characters, but it can also be used to generate new fashion styles, for general content creation and sometimes for data augmentation purposes as well.</p>

<p><strong><em>We can now conjure up realistic textures or characters on demand if we have the training data at hand, and that is no small feat.</em></strong></p>

<p>If you want to know more about deep learning applications and use cases, take a look at the <a href="https://www.coursera.org/learn/nlp-sequence-models?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&amp;siteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Sequence Models</a> course in the <a href="https://www.coursera.org/specializations/deep-learning?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&amp;siteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Deep Learning Specialization</a> by Andrew NG. Andrew is a great instructor, and this course is great too.</p>

<p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at <a href="https://medium.com/@rahul_agarwal" rel="nofollow" target="_blank"><strong>Medium</strong></a> or Subscribe to my <a href="http://eepurl.com/dbQnuX" rel="nofollow" target="_blank"><strong>blog</strong></a> to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter <a href="https://twitter.com/MLWhiz" rel="nofollow" target="_blank">@mlwhiz</a>.</p>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>
<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.415&subid=0&type=4"><IMG border="0"   alt="Deep Learning Specialization on Coursera" src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.415&subid=0&type=4&gridnum=16"></a>


	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2019/05/19/feature_extraction/" rel="prev"><span class="post-nav__caption">Â«&thinsp;Previous</span><p class="post-nav__post-title">The Hitchhikerâ€™s Guide to Feature Extraction</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2019/06/28/jupyter_extensions/" rel="next"><span class="post-nav__caption">Next&thinsp;Â»</span><p class="post-nav__post-title">3 Great Additions for your Jupyter Notebooks</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar">
	     
  <div style="text-align:center">    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2019/10/10/hyperopt2/">Automate Hyperparameter Tuning for your models</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/26/building_ml_system/">6 Important Steps to build  a Machine Learning System</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/23/generative_approach_to_classification/">A Generative Approach to Classification</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/02/graph_algs/">Data Scientists, The 5 Graph Algorithms that you should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/01/regex/">The Ultimate Guide to using the Python regex module</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/12/resources/">How did I learn Data Science?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/07/feature_selection/">The 5 Feature Selection Algorithms every Data Scientist should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/30/sampling/">The 5 Sampling Algorithms every Data Scientist need to know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/21/bandits/">Bayesian Bandits explained simply</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/20/pandas_subset/">Minimal Pandas Subset for Data Scientists</a></li>
		</ul>
	</div>
</div>


<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>


<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;}
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: center;
    padding: 10px -5px 10px 3%;}
   
</style>
<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2019 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>