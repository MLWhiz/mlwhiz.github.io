<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This post is about various evaluation metrics and how and when to use them."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="The Simple Math behind 3 Decision Tree Splitting criterions"><meta property="og:description" content="This post is about various evaluation metrics and how and when to use them."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/11/12/dtsplits/"><meta property="og:image" content="https://mlwhiz.com/images/dtsplits/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/dtsplits/main.png"><meta property="article:published_time" content="2019-11-12T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/dtsplits/main.png"><meta name=twitter:title content="The Simple Math behind 3 Decision Tree Splitting criterions"><meta name=twitter:description content="This post is about various evaluation metrics and how and when to use them."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/11/12/dtsplits/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/11/12/dtsplits/#primaryimage","url":"https://mlwhiz.com/images/dtsplits/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/11/12/dtsplits/#webpage","url":"https://mlwhiz.com/blog/2019/11/12/dtsplits/","inLanguage":"en-US","name":"The Simple Math behind 3 Decision Tree Splitting criterions - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/11/12/dtsplits/#primaryimage"},"datePublished":"2019-11-12T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"This post is about various evaluation metrics and how and when to use them."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s){if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/data-science class=categoryStyle>Data Science</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>The Simple Math behind 3 Decision Tree Splitting criterions</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>12 November 2019</span></div><img src=https://mlwhiz.com/images/dtsplits/main.png class="img-fluid w-100 mb-4" alt="The Simple Math behind 3 Decision Tree Splitting criterions"><div class="content mb-5"><p>Decision Trees are great and are useful for a variety of tasks. They form the backbone of most of the best performing models in the industry like XGboost and Lightgbm.</p><p>But how do they work exactly? In fact, this is one of the most asked questions in ML/DS interviews.</p><p>We generally know they work in a stepwise manner and have a tree structure where we split a node using some feature on some criterion.</p><p><em><strong>But how do these features get selected and how a particular threshold or value gets chosen for a feature?</strong></em></p><p><em><strong>In this post, I will talk about three of the main splitting criteria used in Decision trees and why they work.</strong></em> This is something that has been written about repeatedly but never really well enough.</p><h2 id=1-gini-impurity>1. Gini Impurity</h2><p>According to Wikipedia,</p><blockquote><p>Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.</p></blockquote><p>In simple terms, Gini impurity is the <em><strong>measure of impurity in a node</strong></em>. Its formula is:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/0_hu8b41bf0e6f257ab0ae4d53c8631c1639_7142_500x0_resize_box_2.png 500w" src=/images/dtsplits/0.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>where J is the number of classes present in the node and p is the distribution of the class in the node.</p><p>So to understand the formula a little better, let us talk specifically about the binary case where we have nodes with only two classes.</p><p>So in the below five examples of candidate nodes labelled A-E and with the distribution of positive and negative class shown, which is the ideal condition to be in?</p><p>I reckon you would say A or E and you are right. What is the worst situation to be in? C, I suppose as the data is precisely 50:50 in that node.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/1_hu1e3902c67bdeebe75e501d9ff6869e14_16103_500x0_resize_box_2.png 500w
, /images/dtsplits/1_hu1e3902c67bdeebe75e501d9ff6869e14_16103_800x0_resize_box_2.png 800w" src=/images/dtsplits/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Now, this all looks good, intuitively. Gini Impurity gives us a way to quantify it.</p><p>Let us calculate the Gini impurity for all five nodes separately and check the values.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/2_hu78796d53b95093cdd89959334f695432_25812_500x0_resize_box_2.png 500w
, /images/dtsplits/2_hu78796d53b95093cdd89959334f695432_25812_800x0_resize_box_2.png 800w" src=/images/dtsplits/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>✅ Gini Impurity works as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Gini Impurity.</p><p>We could also see the plot of Gini Impurity for the binary case to verify the above.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/3_hue7558decd91b32a4bc7d2d8cf7b0c807_142322_500x0_resize_box_2.png 500w
, /images/dtsplits/3_hue7558decd91b32a4bc7d2d8cf7b0c807_142322_800x0_resize_box_2.png 800w
, /images/dtsplits/3_hue7558decd91b32a4bc7d2d8cf7b0c807_142322_1200x0_resize_box_2.png 1200w" src=/images/dtsplits/3.png alt="Gini Impurity"></p><p>❓So how do we exactly use it in a Decision Tree?</p><p>Suppose, we have the UCI Heart Disease data. The “target” field refers to the presence of heart disease in the patient. It is 0 (no presence) or 1.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/4_hu17c14e4918ad27d352601cf9eb05054d_32435_500x0_resize_box_2.png 500w
, /images/dtsplits/4_hu17c14e4918ad27d352601cf9eb05054d_32435_800x0_resize_box_2.png 800w" src=/images/dtsplits/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We now already have a measure in place(Gini Impurity) using which we can evaluate a split on a particular variable with a certain threshold(continuous) or value(categorical).</p><h3 id=categorical-variable-splits>Categorical Variable Splits</h3><p>For simplicity, let us start with a categorical variable — sex.</p><p>If we split by Sex, our tree will look like below:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/11_hu83ba86cb40def1d980d3a6a243831139_20935_500x0_resize_box_2.png 500w" src=/images/dtsplits/11.png alt="If we split on Gender"></p><p>Notice that we use Sex=0 and Sex!=0 so that this generalises well to categories with multiple levels. Our root node has 165 +ve examples and 138 -ve examples. And we get two child nodes when we split by sex.</p><p>We already know how to calculate the impurity for a node. So we calculate the impurity of the left child as well as the right child.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>I_Left <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>72</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>24</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
I_Right <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>93</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>114</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Left Node Impurity:&#34;</span>,I_Left)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Right Node Impurity:&#34;</span>,I_Right)
</code></pre></div><pre><code>Left Node Impurity: 0.375
Right Node Impurity: 0.4948540222642302
</code></pre><p>We get two numbers here. We need to get a single number which provides the impurity of a single split. So what do we do? Should, we take an average? We can take an average, but what will happen if one node gets only one example and another node has all other examples?</p><p>To mitigate the above, we take a weighted average of the two impurities weighted by the number of examples in the individual node. In code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>gender_split_impurity <span style=color:#f92672>=</span> <span style=color:#ae81ff>96</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>96</span><span style=color:#f92672>+</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>I_Left <span style=color:#f92672>+</span> <span style=color:#ae81ff>207</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>96</span><span style=color:#f92672>+</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>I_Right
<span style=color:#66d9ef>print</span>(gender_split_impurity)
</code></pre></div><pre><code>0.45688047065576126
</code></pre><h3 id=continuous-variable-splits>Continuous Variable Splits</h3><p>We can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and created a tree.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/12_hue6aa355143f81eb5ddac5d845981cdfa_23627_500x0_resize_box_2.png 500w" src=/images/dtsplits/12.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>I_Left <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>58</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
I_Right <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>107</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> (<span style=color:#ae81ff>70</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Left Node Impurity:&#34;</span>,I_Left)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Right Node Impurity:&#34;</span>,I_Right)
</code></pre></div><pre><code>Left Node Impurity: 0.49685059208868737
Right Node Impurity: 0.47815123368125373
</code></pre><p>Just by looking at both the impurities close to 0.5, we can infer that it is not a good split. Still, we calculate our weighted Gini impurity as before:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>chol_split_impurity <span style=color:#f92672>=</span> <span style=color:#ae81ff>126</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>126</span><span style=color:#f92672>+</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>I_Left <span style=color:#f92672>+</span> <span style=color:#ae81ff>177</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>126</span><span style=color:#f92672>+</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>I_Right
<span style=color:#66d9ef>print</span>(chol_split_impurity)
</code></pre></div><pre><code>0.48592720450414695
</code></pre><p>Since the chol_split_impurity>gender_split_impurity, we split based on Gender.</p><p>In reality, we evaluate a lot of different splits. With different threshold values for a continuous variable. And all the levels for categorical variables. And then choose the split which provides us with the lowest weighted impurity in the child nodes.</p><hr><h2 id=2-entropy>2. Entropy</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/7_hu9eabd6117842cdb57160e7ef75f5a9bd_805130_500x0_resize_box_2.png 500w
, /images/dtsplits/7_hu9eabd6117842cdb57160e7ef75f5a9bd_805130_800x0_resize_box_2.png 800w
, /images/dtsplits/7_hu9eabd6117842cdb57160e7ef75f5a9bd_805130_1200x0_resize_box_2.png 1200w
, /images/dtsplits/7_hu9eabd6117842cdb57160e7ef75f5a9bd_805130_1500x0_resize_box_2.png 1500w" src=/images/dtsplits/7.png alt="Entropy == Randomness"></p><p>Another very popular way to split nodes in the decision tree is Entropy. Entropy is the measure of Randomness in the system. The formula for Entropy is:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/dtsplits/8.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>where C is the number of classes present in the node and p is the distribution of the class in the node.</p><p>So again talking about the binary case we talked about before. What is the value of Entropy for all the 5 cases from A-E?</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/9_hua3456fc08a773379b379d8bb1e081dcc_32303_500x0_resize_box_2.png 500w
, /images/dtsplits/9_hua3456fc08a773379b379d8bb1e081dcc_32303_800x0_resize_box_2.png 800w" src=/images/dtsplits/9.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Entropy values work as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Entropy.</p><p>We could also see the plot of Entropy for the binary case to verify the above.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/10_hue8f48178bca1d317e350a92fcb394d47_164946_500x0_resize_box_2.png 500w
, /images/dtsplits/10_hue8f48178bca1d317e350a92fcb394d47_164946_800x0_resize_box_2.png 800w
, /images/dtsplits/10_hue8f48178bca1d317e350a92fcb394d47_164946_1200x0_resize_box_2.png 1200w" src=/images/dtsplits/10.png alt=Entropy></p><p>So how do we exactly use Entropy in a Decision Tree?</p><p>We are using the Heartrate example as before. We now already have a measure in place(Entropy) using which we can evaluate a split on an individual variable with a certain threshold(continuous) or value(categorical).</p><h3 id=categorical-variable-splits-1>Categorical Variable Splits</h3><p>For simplicity, let us start with a categorical variable — sex.</p><p>If we split by Sex, our tree will look like below:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/11_hu83ba86cb40def1d980d3a6a243831139_20935_500x0_resize_box_2.png 500w" src=/images/dtsplits/11.png alt="If we split on Gender">
<em>If we split on Gender</em></p><p>We already know how to calculate the randomness for a node. So we calculate the randomness of the left child as well as the right child.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>E_Left <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>(<span style=color:#ae81ff>72</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>72</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>) <span style=color:#f92672>-</span> (<span style=color:#ae81ff>24</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>24</span><span style=color:#f92672>/</span><span style=color:#ae81ff>96</span>)
E_Right <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>(<span style=color:#ae81ff>93</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>93</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>) <span style=color:#f92672>-</span> (<span style=color:#ae81ff>114</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>114</span><span style=color:#f92672>/</span><span style=color:#ae81ff>207</span>)

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Left Node Randomness:&#34;</span>,E_Left)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Right Node Randomness:&#34;</span>,E_Right)
</code></pre></div><pre><code>Left Node Randomness: 0.8112781244591328
Right Node Randomness: 0.992563136012236
</code></pre><p>We get two numbers here. We need to get a single number which provides the Randomness of a single split. So what do we do? We again take a weighted average where we weight by the number of examples in the individual node. In code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>gender_split_randomness <span style=color:#f92672>=</span> <span style=color:#ae81ff>96</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>96</span><span style=color:#f92672>+</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>E_Left <span style=color:#f92672>+</span> <span style=color:#ae81ff>207</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>96</span><span style=color:#f92672>+</span><span style=color:#ae81ff>207</span>)<span style=color:#f92672>*</span>E_Right
<span style=color:#66d9ef>print</span>(gender_split_randomness)
</code></pre></div><pre><code>0.9351263006686785
</code></pre><h3 id=continuous-variable-splits-1>Continuous Variable Splits</h3><p>Again as before, we can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and create a tree.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/12_hue6aa355143f81eb5ddac5d845981cdfa_23627_500x0_resize_box_2.png 500w" src=/images/dtsplits/12.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>E_Left <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>(<span style=color:#ae81ff>58</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>58</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>) <span style=color:#f92672>-</span> (<span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>126</span>)
E_Right <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>(<span style=color:#ae81ff>107</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>107</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>) <span style=color:#f92672>-</span> (<span style=color:#ae81ff>70</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log2(<span style=color:#ae81ff>70</span><span style=color:#f92672>/</span><span style=color:#ae81ff>177</span>)

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Left Node Randomness:&#34;</span>,E_Left)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Right Node Randomness:&#34;</span>,E_Right)
</code></pre></div><pre><code>Left Node Randomness: 0.9954515828457715
Right Node Randomness: 0.9682452182690404
</code></pre><p>Just by looking at both the randomness close to 1, we can infer that it is not a good split. Still, we calculate our weighted Entropy as before:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>chol_split_randomness <span style=color:#f92672>=</span> <span style=color:#ae81ff>126</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>126</span><span style=color:#f92672>+</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>E_Left <span style=color:#f92672>+</span> <span style=color:#ae81ff>177</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>126</span><span style=color:#f92672>+</span><span style=color:#ae81ff>177</span>)<span style=color:#f92672>*</span>E_Right
<span style=color:#66d9ef>print</span>(chol_split_randomness)
</code></pre></div><pre><code>0.9795587560138196
</code></pre><p>Since the chol_split_randomness>gender_split_randomness, we split based on Gender. Precisely the same results we got from Gini.</p><hr><h2 id=3-variance>3. Variance</h2><p>Gini Impurity and Entropy work pretty well for the classification scenario.</p><p>But what about regression?</p><p>In the case of regression, the most common split measure used is just the weighted variance of the nodes. It makes sense too: We want minimum variation in the nodes after the split.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/dtsplits/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We want a regression task for this. So, we have the data for 50 startups, and we want to predict Profit.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/14_hu4698570d3d6885059cc3e427a74109ae_48316_500x0_resize_box_2.png 500w
, /images/dtsplits/14_hu4698570d3d6885059cc3e427a74109ae_48316_800x0_resize_box_2.png 800w" src=/images/dtsplits/14.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h3 id=categorical-variable-splits-2>Categorical Variable Splits</h3><p>Let us try a split by a categorical variable ⇒State=Florida.</p><p>If we split by State=FL, our tree will look like below:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/15_huce47434b44dcd1293aaa8ddaba8c4204_27652_500x0_resize_box_2.png 500w" src=/images/dtsplits/15.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Overall Variance then is just the weighted sums of individual variances:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>overall_variance <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>16</span><span style=color:#f92672>+</span><span style=color:#ae81ff>34</span>)<span style=color:#f92672>*</span>Var_Left <span style=color:#f92672>+</span> <span style=color:#ae81ff>34</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>16</span><span style=color:#f92672>+</span><span style=color:#ae81ff>34</span>)<span style=color:#f92672>*</span>Var_Right
<span style=color:#66d9ef>print</span>(overall_variance)
</code></pre></div><pre><code>1570582843
</code></pre><h3 id=continuous-variable-splits-2>Continuous Variable Splits</h3><p>Again as before, we can split by a continuous variable too. Let us try to split using R&D spend feature in the dataset. We chose a threshold of 100000 and create a tree.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/16_hu6e4c3331c0cac03f243880c287f1a0f1_27815_500x0_resize_box_2.png 500w" src=/images/dtsplits/16.png alt="Splitting on R&amp;amp;D">
<em>Splitting on R&D</em></p><p>Just by looking at this, we can see it is better than our previous split. So, we find the overall variance in this case:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>overall_variance <span style=color:#f92672>=</span> <span style=color:#ae81ff>14</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>14</span><span style=color:#f92672>+</span><span style=color:#ae81ff>36</span>)<span style=color:#f92672>*</span><span style=color:#ae81ff>419828105</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>36</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>14</span><span style=color:#f92672>+</span><span style=color:#ae81ff>36</span>)<span style=color:#f92672>*</span><span style=color:#ae81ff>774641406</span>
<span style=color:#66d9ef>print</span>(overall_variance)
</code></pre></div><pre><code>675293681.7199999
</code></pre><p>Since the overall_variance(R&D>=100000)&lt; overall_variance(State==FL), we prefer a split based on R&D.</p><h2 id=continue-learning>Continue Learning</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/dtsplits/17_hu81c4bdd3448c4bb448bd55ceb050865a_1628169_500x0_resize_box_2.png 500w
, /images/dtsplits/17_hu81c4bdd3448c4bb448bd55ceb050865a_1628169_800x0_resize_box_2.png 800w
, /images/dtsplits/17_hu81c4bdd3448c4bb448bd55ceb050865a_1628169_1200x0_resize_box_2.png 1200w
, /images/dtsplits/17_hu81c4bdd3448c4bb448bd55ceb050865a_1628169_1500x0_resize_box_2.png 1500w" src=/images/dtsplits/17.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>If you want to learn more about Data Science, I would like to call out this
<a href="https://www.coursera.org/learn/machine-learning?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-btd7XBdF681VKxRe2H_Oyg&siteID=lVarvwc5BD0-btd7XBdF681VKxRe2H_Oyg&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">&lt;em>&lt;strong>excellent course&lt;/strong>&lt;/em></a>
by Andrew Ng. This was the one that got me started. Do check it out.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href=https://medium.com/@rahul_agarwal target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>