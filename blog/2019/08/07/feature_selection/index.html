<!doctype html><html lang=en-us><head><meta charset=utf-8><title>The 5 Feature Selection Algorithms every Data Scientist should know - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This post is about some of the most common feature selection techniques one can use while working with data."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.74.3"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="The 5 Feature Selection Algorithms every Data Scientist should know - MLWhiz"><meta property="og:description" content="This post is about some of the most common feature selection techniques one can use while working with data."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/08/07/feature_selection/"><meta property="og:image" content="https://mlwhiz.com/images/fs/1.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/fs/1.png"><meta property="article:published_time" content="2019-08-07T00:00:00+00:00"><meta property="article:modified_time" content="2020-11-27T22:43:58+00:00"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/fs/1.png"><meta name=twitter:title content="The 5 Feature Selection Algorithms every Data Scientist should know - MLWhiz"><meta name=twitter:description content="This post is about some of the most common feature selection techniques one can use while working with data."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/08/07/feature_selection/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/08/07/feature_selection/#primaryimage","url":"https://mlwhiz.com/images/fs/1.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/08/07/feature_selection/#webpage","url":"https://mlwhiz.com/blog/2019/08/07/feature_selection/","inLanguage":"en-US","name":"The 5 Feature Selection Algorithms every Data Scientist should know - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/08/07/feature_selection/#primaryimage"},"datePublished":"2019-08-07T00:00:00.00Z","dateModified":"2020-11-27T22:43:58.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"This post is about some of the most common feature selection techniques one can use while working with data."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/data-science class=categoryStyle>Data Science</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>The 5 Feature Selection Algorithms every Data Scientist should know</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>07 August 2019</span></div><img src=https://mlwhiz.com/images/fs/1.png class="img-fluid w-100 mb-4" alt="The 5 Feature Selection Algorithms every Data Scientist should know"><div class="content mb-5"><p>Data Science is the study of algorithms.</p><p>I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new
<a href=https://towardsdatascience.com/tagged/ds-algorithms target=_blank rel="nofollow noopener">DS Algorithm series</a>
.</p><p>How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.</p><p>We sometimes end up using correlation or tree-based methods to find out the important features.</p><p>Can we add some structure to it?</p><p><em><strong>This post is about some of the most common feature selection techniques one can use while working with data.</strong></em></p><hr><h2 id=why-feature-selection>Why Feature Selection?</h2><p>Before we proceed, we need to answer this question. Why don’t we give all the features to the ML algorithm and let it decide which feature is important?</p><p>So there are three reasons why we don’t:</p><h3 id=1-curse-of-dimensionality--overfitting>1. Curse of dimensionality — Overfitting</h3><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/2_hu293ed32a0d90429d5ab5569a6e356216_90252_500x0_resize_box_2.png 500w" src=/images/fs/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing.</p><h3 id=2-occams-razor>2. Occam’s Razor:</h3><p>We want our <em><strong>models to be simple</strong></em> and explainable. We lose explainability when we have a lot of features.</p><h3 id=3-garbage-in-garbage-out>3. Garbage In Garbage out:</h3><p>Most of the times, we will have many non-informative features. For Example, Name or ID variables. <em><strong>Poor-quality input will produce Poor-Quality output.</strong></em></p><p>Also, a large number of features make a model bulky, time-taking, and harder to implement in production.</p><hr><h2 id=so-what-do-we-do>So What do we do?</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/3_hu2a43bd6240b3f7063fe82334cca87d9f_183421_500x0_resize_q75_box.jpg 500w
, /images/fs/3_hu2a43bd6240b3f7063fe82334cca87d9f_183421_800x0_resize_q75_box.jpg 800w
, /images/fs/3_hu2a43bd6240b3f7063fe82334cca87d9f_183421_1200x0_resize_q75_box.jpg 1200w" src=/images/fs/3.jpg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We select only useful features.</p><p>Fortunately, Scikit-learn has made it pretty much easy for us to make the feature selection. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major buckets</p><ul><li><p><em><strong>Filter based:</strong></em> We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.</p></li><li><p><em><strong>Wrapper-based:</strong></em> Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination</p></li><li><p><em><strong>Embedded:</strong></em> Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.</p></li></ul><p>So enough of theory let us start with our five feature selection methods.</p><p>We will try to do this using a dataset to understand it better.</p><p>I am going to be using a football player dataset to find out <em><strong>what makes a good player great?</strong></em></p><p><em><strong>Don’t worry if you don’t understand football terminologies. I will try to keep it at a minimum.</strong></em></p><p>Here is the Kaggle
<a href=https://www.kaggle.com/mlwhiz/feature-selection-using-football-data target=_blank rel="nofollow noopener">Kernel</a>
with the code to try out yourself.</p><hr><h2 id=some-simple-data-preprocessing>Some Simple Data Preprocessing</h2><p>We have done some basic preprocessing such as removing Nulls and one hot encoding. And converting the problem to a classification problem using:</p><pre><code>y = traindf['Overall']&gt;=87
</code></pre><p>Here we use High Overall as a proxy for a great player.</p><p>Our dataset(X) looks like below and has 223 columns.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/12_hubb170b517eac40b721433858449ce7a8_101925_500x0_resize_box_2.png 500w
, /images/fs/12_hubb170b517eac40b721433858449ce7a8_101925_800x0_resize_box_2.png 800w
, /images/fs/12_hubb170b517eac40b721433858449ce7a8_101925_1200x0_resize_box_2.png 1200w
, /images/fs/12_hubb170b517eac40b721433858449ce7a8_101925_1500x0_resize_box_2.png 1500w" src=/images/fs/12.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=1-pearson-correlation>1. Pearson Correlation</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/fs/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This is a filter-based method.</p><p>We check the <em><strong>absolute value of the Pearson’s correlation</strong></em> between the target and numerical features in our dataset. We keep the top n features based on this criterion.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cor_selector</span>(X, y,num_feats):
    cor_list <span style=color:#f92672>=</span> []
    feature_name <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
    <span style=color:#75715e># calculate the correlation with y for each feature</span>
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> X<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist():
        cor <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>corrcoef(X[i], y)[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
        cor_list<span style=color:#f92672>.</span>append(cor)
    <span style=color:#75715e># replace NaN with 0</span>
    cor_list <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>isnan(i) <span style=color:#66d9ef>else</span> i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> cor_list]
    <span style=color:#75715e># feature name</span>
    cor_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>iloc[:,np<span style=color:#f92672>.</span>argsort(np<span style=color:#f92672>.</span>abs(cor_list))[<span style=color:#f92672>-</span>num_feats:]]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
    <span style=color:#75715e># feature selection? 0 for not select, 1 for select</span>
    cor_support <span style=color:#f92672>=</span> [True <span style=color:#66d9ef>if</span> i <span style=color:#f92672>in</span> cor_feature <span style=color:#66d9ef>else</span> False <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> feature_name]
    <span style=color:#66d9ef>return</span> cor_support, cor_feature
cor_support, cor_feature <span style=color:#f92672>=</span> cor_selector(X, y,num_feats)
<span style=color:#66d9ef>print</span>(str(len(cor_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><hr><h2 id=2-chi-squared>2. Chi-Squared</h2><p>This is another filter-based method.</p><p>In this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/5_hud82eae9a9e053c0f62dad0691a0ea567_398551_500x0_resize_box_2.png 500w
, /images/fs/5_hud82eae9a9e053c0f62dad0691a0ea567_398551_800x0_resize_box_2.png 800w
, /images/fs/5_hud82eae9a9e053c0f62dad0691a0ea567_398551_1200x0_resize_box_2.png 1200w" src=/images/fs/5.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Let us create a small example of how we calculate the chi-squared statistic for a sample.</p><p>So let’s say we have 75 Right-Forwards in our dataset and 25 Non-Right-Forwards. We observe that 40 of the Right-Forwards are good, and 35 are not good. Does this signify that the player being right forward affects the overall performance?</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/6_hufc56de8e22470f3e0108a027c7f6692f_98727_500x0_resize_box_2.png 500w
, /images/fs/6_hufc56de8e22470f3e0108a027c7f6692f_98727_800x0_resize_box_2.png 800w
, /images/fs/6_hufc56de8e22470f3e0108a027c7f6692f_98727_1200x0_resize_box_2.png 1200w
, /images/fs/6_hufc56de8e22470f3e0108a027c7f6692f_98727_1500x0_resize_box_2.png 1500w" src=/images/fs/6.png alt="Observed and Expected Counts"></p><p>We calculate the chi-squared value:</p><p>To do this, we first find out the values we would expect to be falling in each bucket if there was indeed independence between the two categorical variables.</p><p>This is simple. We multiply the row sum and the column sum for each cell and divide it by total observations.</p><p>so Good and NotRightforward Bucket Expected value= 25(Row Sum)*60(Column Sum)/100(Total Observations)</p><p>Why is this expected? Since there are 25% notRightforwards in the data, we would expect 25% of the 60 good players we observed in that cell. Thus 15 players.</p><p>Then we could just use the below formula to sum over all the 4 cells:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/7_hu8f217ca8f37c44409ca6f7189d7df545_17765_500x0_resize_q75_box.jpg 500w" src=/images/fs/7.jpg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>I won’t show it here, but the chi-squared statistic also works in a hand-wavy way with non-negative numerical and categorical features.</p><p>We can get chi-squared features from our dataset as:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> SelectKBest
<span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> chi2
<span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> MinMaxScaler
X_norm <span style=color:#f92672>=</span> MinMaxScaler()<span style=color:#f92672>.</span>fit_transform(X)
chi_selector <span style=color:#f92672>=</span> SelectKBest(chi2, k<span style=color:#f92672>=</span>num_feats)
chi_selector<span style=color:#f92672>.</span>fit(X_norm, y)
chi_support <span style=color:#f92672>=</span> chi_selector<span style=color:#f92672>.</span>get_support()
chi_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>loc[:,chi_support]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
<span style=color:#66d9ef>print</span>(str(len(chi_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><hr><h2 id=3-recursive-feature-elimination>3. Recursive Feature Elimination</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/8_hu8ad977c6bf18f79c4c6b9f3b0d5afe8d_334191_500x0_resize_q75_box.jpg 500w
, /images/fs/8_hu8ad977c6bf18f79c4c6b9f3b0d5afe8d_334191_800x0_resize_q75_box.jpg 800w" src=/images/fs/8.jpg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.</p><p>From sklearn Documentation:</p><blockquote><p>The goal of recursive feature elimination (RFE) is to select features by <strong>recursively considering smaller and smaller sets of features.</strong> First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a <code>coef_</code> attribute or through a <code>feature_importances_</code> attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</p></blockquote><p>As you would have guessed, we could use any estimator with the method. In this case, we use <code>LogisticRegression</code>, and the RFE observes the <code>coef_</code> attribute of the <code>LogisticRegression</code> object</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> RFE
<span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
rfe_selector <span style=color:#f92672>=</span> RFE(estimator<span style=color:#f92672>=</span>LogisticRegression(), n_features_to_select<span style=color:#f92672>=</span>num_feats, step<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
rfe_selector<span style=color:#f92672>.</span>fit(X_norm, y)
rfe_support <span style=color:#f92672>=</span> rfe_selector<span style=color:#f92672>.</span>get_support()
rfe_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>loc[:,rfe_support]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
<span style=color:#66d9ef>print</span>(str(len(rfe_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><hr><h2 id=4-lasso-selectfrommodel>4. Lasso: SelectFromModel</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/9_hu0e242923cc6e1e1dd4844375cb2762be_110602_500x0_resize_box_2.png 500w" src=/images/fs/9.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.</p><p>For example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.</p><p>Here we use Lasso to select variables.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> SelectFromModel
<span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression

embeded_lr_selector <span style=color:#f92672>=</span> SelectFromModel(LogisticRegression(penalty<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;l1&#34;</span>), max_features<span style=color:#f92672>=</span>num_feats)
embeded_lr_selector<span style=color:#f92672>.</span>fit(X_norm, y)

embeded_lr_support <span style=color:#f92672>=</span> embeded_lr_selector<span style=color:#f92672>.</span>get_support()
embeded_lr_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>loc[:,embeded_lr_support]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
<span style=color:#66d9ef>print</span>(str(len(embeded_lr_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><hr><h2 id=5-tree-based-selectfrommodel>5. Tree-based: SelectFromModel</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/10_hu60e7c4ab6b6ddcd4b0c072ddd41affa1_815919_500x0_resize_q75_box.jpeg 500w
, /images/fs/10_hu60e7c4ab6b6ddcd4b0c072ddd41affa1_815919_800x0_resize_q75_box.jpeg 800w
, /images/fs/10_hu60e7c4ab6b6ddcd4b0c072ddd41affa1_815919_1200x0_resize_q75_box.jpeg 1200w" src=/images/fs/10.jpeg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.</p><p>We can also use RandomForest to select features based on feature importance.</p><p>We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> SelectFromModel
<span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier

embeded_rf_selector <span style=color:#f92672>=</span> SelectFromModel(RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>), max_features<span style=color:#f92672>=</span>num_feats)
embeded_rf_selector<span style=color:#f92672>.</span>fit(X, y)

embeded_rf_support <span style=color:#f92672>=</span> embeded_rf_selector<span style=color:#f92672>.</span>get_support()
embeded_rf_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>loc[:,embeded_rf_support]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
<span style=color:#66d9ef>print</span>(str(len(embeded_rf_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><p>We could also have used a LightGBM. Or an XGBoost object as long it has a <code>feature_importances_</code> attribute.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> sklearn.feature_selection <span style=color:#f92672>import</span> SelectFromModel
<span style=color:#f92672>from</span> lightgbm <span style=color:#f92672>import</span> LGBMClassifier

lgbc<span style=color:#f92672>=</span>LGBMClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>, num_leaves<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, colsample_bytree<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>,
            reg_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, reg_lambda<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, min_split_gain<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, min_child_weight<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>)

embeded_lgb_selector <span style=color:#f92672>=</span> SelectFromModel(lgbc, max_features<span style=color:#f92672>=</span>num_feats)
embeded_lgb_selector<span style=color:#f92672>.</span>fit(X, y)

embeded_lgb_support <span style=color:#f92672>=</span> embeded_lgb_selector<span style=color:#f92672>.</span>get_support()
embeded_lgb_feature <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>loc[:,embeded_lgb_support]<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
<span style=color:#66d9ef>print</span>(str(len(embeded_lgb_feature)), <span style=color:#e6db74>&#39;selected features&#39;</span>)
</code></pre></div><hr><h2 id=bonus>Bonus</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/11_hu0fe252ada9f2b5da584d071a45f8074c_188625_500x0_resize_q75_box.jpeg 500w
, /images/fs/11_hu0fe252ada9f2b5da584d071a45f8074c_188625_800x0_resize_q75_box.jpeg 800w
, /images/fs/11_hu0fe252ada9f2b5da584d071a45f8074c_188625_1200x0_resize_q75_box.jpeg 1200w" src=/images/fs/11.jpeg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><em><strong>Why use one, when we can have all?</strong></em></p><p>The answer is sometimes it won’t be possible with a lot of data and time crunch.</p><p>But whenever possible, why not do this?</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># put all selection together</span>
feature_selection_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;Feature&#39;</span>:feature_name, <span style=color:#e6db74>&#39;Pearson&#39;</span>:cor_support, <span style=color:#e6db74>&#39;Chi-2&#39;</span>:chi_support, <span style=color:#e6db74>&#39;RFE&#39;</span>:rfe_support, <span style=color:#e6db74>&#39;Logistics&#39;</span>:embeded_lr_support,
                                    <span style=color:#e6db74>&#39;Random Forest&#39;</span>:embeded_rf_support, <span style=color:#e6db74>&#39;LightGBM&#39;</span>:embeded_lgb_support})
<span style=color:#75715e># count the selected times for each feature</span>
feature_selection_df[<span style=color:#e6db74>&#39;Total&#39;</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(feature_selection_df, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
<span style=color:#75715e># display the top 100</span>
feature_selection_df <span style=color:#f92672>=</span> feature_selection_df<span style=color:#f92672>.</span>sort_values([<span style=color:#e6db74>&#39;Total&#39;</span>,<span style=color:#e6db74>&#39;Feature&#39;</span>] , ascending<span style=color:#f92672>=</span>False)
feature_selection_df<span style=color:#f92672>.</span>index <span style=color:#f92672>=</span> range(<span style=color:#ae81ff>1</span>, len(feature_selection_df)<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/fs/13_hu6a8190a295fc9bf86d8cb86aa2b9233f_113948_500x0_resize_box_2.png 500w
, /images/fs/13_hu6a8190a295fc9bf86d8cb86aa2b9233f_113948_800x0_resize_box_2.png 800w" src=/images/fs/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We check if we get a feature based on all the methods. In this case, as we can see Reactions and LongPassing are excellent attributes to have in a high rated player. And as expected Ballcontrol and Finishing occupy the top spot too.</p><hr><h2 id=conclusion>Conclusion</h2><p><a href=https://towardsdatascience.com/the-hitchhikers-guide-to-feature-extraction-b4c157e96631 target=_blank rel="nofollow noopener">Feature engineering</a>
and feature selection are critical parts of any machine learning pipeline.</p><p>We strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.</p><p>In this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.</p><p>I also tried to provide some intuition into these methods, but you should probably try to see more into it and try to incorporate these methods into your work.</p><p>Do read my
<a href=https://towardsdatascience.com/the-hitchhikers-guide-to-feature-extraction-b4c157e96631 target=_blank rel="nofollow noopener">post on feature engineering</a>
too if you are interested.</p><hr><p>If you want to learn more about Data Science, I would like to call out this
<a href=https://coursera.pxf.io/NKERRq target=_blank rel="nofollow noopener">&lt;strong>excellent course&lt;/strong></a>
by Andrew Ng. This was the one that got me started. Do check it out.</p><hr><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href="https://mlwhiz.medium.com/?source=post_page---------------------------" target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>Hi, Impact!!!
I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.medium.com/>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>