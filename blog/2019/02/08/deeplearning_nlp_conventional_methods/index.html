<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8">
  <title>MLWhiz</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline.">
  <meta name="author" content="Rahul Agarwal">
  <meta name="generator" content="Hugo 0.74.3" />
  <!-- plugins -->
  
  <link rel="stylesheet" href="https://mlwhiz.com/plugins/bootstrap/bootstrap.min.css ">
  
  <link rel="stylesheet" href="https://mlwhiz.com/plugins/slick/slick.css ">
  
  <link rel="stylesheet" href="https://mlwhiz.com/plugins/themify-icons/themify-icons.css ">
  
  <link rel="stylesheet" href="https://mlwhiz.com/plugins/venobox/venobox.css ">
  

	<meta property="og:title" content="NLP  Learning Series: Part 2 - Conventional Methods for Text Classification" />
<meta property="og:description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/" />

<meta property="og:image" content="https://mlwhiz.com/images/tfidf.png" />
<meta property="og:image:secure_url" content="https://mlwhiz.com/images/tfidf.png" />
<meta property="article:published_time" content="2019-02-08T00:00:00+00:00" />

<meta property="article:tag" content="Natural Language Processing" />
<meta property="article:tag" content="Deep Learning" />
<meta property="article:tag" content="Awesome Guides" />

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/tfidf.png"/>

<meta name="twitter:title" content="NLP  Learning Series: Part 2 - Conventional Methods for Text Classification"/>
<meta name="twitter:description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline."/>
<meta name="twitter:site" content="@mlwhiz"/>
<meta name="twitter:creator" content="@mlwhiz"/>



	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">

	<!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="https://mlwhiz.com/scss/style.min.css" media="screen">
  <link rel="stylesheet" href="/css/style.css">

  <link rel="stylesheet" type="text/css" href="/css/font/flaticon.css" >

  <!--Favicon-->
  <link rel="shortcut icon" href="https://mlwhiz.com/images/favicon-200x200.png " type="image/x-icon">
  <link rel="icon" href="https://mlwhiz.com/images/favicon.png " type="image/x-icon">


  
  
  
    <link rel="canonical" href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/" />
  

// layouts/partials/seo_schema.html

<script type="application/ld+json">
{
    "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https://mlwhiz.com/"
    },
    "articleSection" : "blog",
    "name" : "NLP  Learning Series: Part 2 - Conventional Methods for Text Classification",
    "headline" : "NLP  Learning Series: Part 2 - Conventional Methods for Text Classification",
    "description" : "Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline.",
    "inLanguage" : "en-US",
    "author" : "Rahul Agarwal",
    "creator" : "Rahul Agarwal",
    "publisher": "MLWhiz",
    "accountablePerson" : "MLWhiz",
    "copyrightHolder" : "MLWhiz",
    "copyrightYear" : "2019",
    "datePublished": "2019-02-08 00:00:00 \u002b0000 UTC",
    "dateModified" : "2019-02-08 00:00:00 \u002b0000 UTC",
    "url" : "https:\/\/mlwhiz.com\/blog\/2019\/02\/08\/deeplearning_nlp_conventional_methods\/",
    "wordCount" : "2074",
    "keywords" : [ "Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle","Best Content", "Blog" "Natural Language Processing","Deep Learning","Awesome Guides", ]
}
</script>


</head>
<body>
      
   <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
   height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
   
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- navigation -->
<header class="navigation">
  <div class="container">
    
    <nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0">
      <a class="navbar-brand mobile-view" href="https://mlwhiz.com/"><img class="img-fluid"
          src="https://mlwhiz.com/images/logo.png" alt="MLWhiz"></a>
      <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation">
        <i class="ti-menu h3"></i>
      </button>

      <div class="collapse navbar-collapse text-center" id="navigation">
        <div class="desktop-view">
          <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.linkedin.com/in/rahulagwl/"><i class="ti-linkedin"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://medium.com/@rahul_agarwal"><i class="ti-book"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://twitter.com/MLWhiz"><i class="ti-twitter-alt"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.facebook.com/mlwhizblog"><i class="ti-facebook"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://github.com/MLWhiz"><i class="ti-github"></i></a>
            </li>
            
          </ul>
        </div>

        <a class="navbar-brand mx-auto desktop-view" href="https://mlwhiz.com/"><img class="img-fluid-custom"
            src="https://mlwhiz.com/images/logo.png" alt="MLWhiz"></a>

        <ul class="navbar-nav">
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://mlwhiz.com/about">About</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://mlwhiz.com/blog">Blog</a>
          </li>
          
          
          
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-haspopup="true"
              aria-expanded="false">
              Topics
            </a>
            <div class="dropdown-menu">
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/natural-language-processing">NLP</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/deep-learning">Computer Vision</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/deep-learning">Deep Learning</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/data-science">DS/ML</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/big-data">Big Data</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/awesome-guides">My Best Content</a>
              
              <a class="dropdown-item" href="https://mlwhiz.com/categories/learning-resources">Learning Resources</a>
              
            </div>
          </li>
          
          
        </ul>

        
        <!-- search -->
        <div class="search pl-lg-4">
          <button id="searchOpen" class="search-btn"><i class="ti-search"></i></button>
          <div class="search-wrapper">
            <form action="https://mlwhiz.com//search" class="h-100">
              <input class="search-box px-4" id="search-query" name="s" type="search" placeholder="Type & Hit Enter...">
            </form>
            <button id="searchClose" class="search-close"><i class="ti-close text-dark"></i></button>
          </div>
        </div>
        

        
      </div>
    </nav>
  </div>
</header>
<!-- /navigation -->


<section class="section-sm">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 mb-5 mb-lg-0">
        
        
        <a href="/categories/natural-language-processing"
          class="categoryStyle">Natural Language Processing</a>
        
        <a href="/categories/deep-learning"
          class="categoryStyle">Deep Learning</a>
        
        <a href="/categories/awesome-guides"
          class="categoryStyle">Awesome Guides</a>
        
        <h1>NLP  Learning Series: Part 2 - Conventional Methods for Text Classification</h1>
        <div class="mb-3 post-meta">
          <span>By Rahul Agarwal</span>
          
          <svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"></path></svg>

          <span>08 February 2019</span>
          
        </div>
        
        <img src="https://mlwhiz.com/images/tfidf.png" class="img-fluid w-100 mb-4" alt="NLP  Learning Series: Part 2 - Conventional Methods for Text Classification">
        
        <div class="content mb-5">
          <p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The <a href="/blog/2019/01/17/deeplearning_nlp_preprocess/">first post</a> talked about the various <strong>preprocessing techniques that work with Deep learning models</strong> and <strong>increasing embeddings coverage</strong>. In this post, I will try to take you through some <strong>basic conventional models</strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into <strong>Deep learning models</strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like <strong>ULMFit transfer learning</strong> approaches in the fourth post in the series.</p>
<p><strong>As a side note</strong>: if you want to know more about NLP, I would like to <strong>recommend this awesome course</strong> on <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing">Natural Language Processing</a> in the <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0">Advanced machine learning specialization</a>. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.</p>
<p>It might take me a little time to write the whole series. Till then you can take a look at my other posts too: <a href="/blog/2018/12/17/text_classification/">What Kagglers are using for Text Classification</a>, which talks about various deep learning models in use in NLP and <a href="/blog/2019/01/06/pytorch_keras_conversion/">how to switch from Keras to Pytorch</a>.</p>
<p>So again we start with the first step: Preprocessing.</p>
<hr>
<h2 id="basic-preprocessing-techniques-for-text-datacontinued">Basic Preprocessing Techniques for text data(Continued)</h2>
<p>So in the last post, we talked about various preprocessing methods for text for deep learning purpose. Most of the preprocessing for conventional methods remains the same. <strong>We will still remove special characters, punctuations, and contractions</strong>. But We also may want to do stemming/lemmatization when it comes to conventional methods. Let us talk about them.</p>
<blockquote>
<p>For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.</p>
</blockquote>
<p>Since we are going to create features for words in the feature creation step, it makes sense to reduce words to a common denominator so that &lsquo;organize&rsquo;,&lsquo;organizes&rsquo; and &lsquo;organizing&rsquo; could be referred to by a single word &lsquo;organize&rsquo;</p>
<hr>
<h3 id="a-stemming">a) Stemming</h3>
<p>Stemming is the process of converting words to their base forms using crude Heuristic rules. For example, one rule could be to remove &rsquo;s&rsquo; from the end of any word, so that &lsquo;cats&rsquo; becomes &lsquo;cat&rsquo;. or another rule could be to replace &lsquo;ies&rsquo; with &lsquo;i&rsquo; so that &lsquo;ponies becomes &lsquo;poni&rsquo;. One of the main point to note here is that when we stem the word we might get a nonsense word like &lsquo;poni&rsquo;. But it will still work for our use case as we count the number of occurrences of a particular word and not focus on the meanings of these words in conventional methods. It doesn&rsquo;t work with deep learning for precisely the same reason.</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/text_stemming.png"  style="height:90%;width:90%"></center>
</div>
<p>We can do this pretty simply by using this function in python.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span>  SnowballStemmer
<span style="color:#f92672">from</span> nltk.tokenize.toktok <span style="color:#f92672">import</span> ToktokTokenizer
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stem_text</span>(text):
    tokenizer <span style="color:#f92672">=</span> ToktokTokenizer()
    stemmer <span style="color:#f92672">=</span> SnowballStemmer(<span style="color:#e6db74">&#39;english&#39;</span>)
    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(text)
    tokens <span style="color:#f92672">=</span> [token<span style="color:#f92672">.</span>strip() <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens]
    tokens <span style="color:#f92672">=</span> [stemmer<span style="color:#f92672">.</span>stem(token) <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens]
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(tokens)
</code></pre></div><hr>
<h3 id="b-lemmatization">b) Lemmatization</h3>
<p>Lemmatization is very similar to stemming but it aims to remove endings only if the base form is present in a dictionary.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> nltk.stem <span style="color:#f92672">import</span> WordNetLemmatizer
<span style="color:#f92672">from</span> nltk.tokenize.toktok <span style="color:#f92672">import</span> ToktokTokenizer
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">lemma_text</span>(text):
    tokenizer <span style="color:#f92672">=</span> ToktokTokenizer()
    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(text)
    tokens <span style="color:#f92672">=</span> [token<span style="color:#f92672">.</span>strip() <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens]
    tokens <span style="color:#f92672">=</span> [wordnet_lemmatizer<span style="color:#f92672">.</span>lemmatize(token) <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens]
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(tokens)
</code></pre></div><p>Once we are done with processing a text, our text will necessarily go through these following steps.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clean_sentence</span>(x):
    x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>lower()
    x <span style="color:#f92672">=</span> clean_text(x)
    x <span style="color:#f92672">=</span> clean_numbers(x)
    x <span style="color:#f92672">=</span> replace_typical_misspell(x)
    x <span style="color:#f92672">=</span> remove_stopwords(x)
    x <span style="color:#f92672">=</span> replace_contractions(x)
    x <span style="color:#f92672">=</span> lemma_text(x)
    x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;&#39;&#34;</span>,<span style="color:#e6db74">&#34;&#34;</span>)
    <span style="color:#66d9ef">return</span> x
</code></pre></div><hr>
<h2 id="text-representation">Text Representation</h2>
<p>In Conventional Machine learning methods, we ought to create features for a text. There are a lot of representations that are present to achieve this. Let us talk about them one by one.</p>
<h3 id="a-bag-of-words---countvectorizer-features">a) Bag of Words - Countvectorizer Features</h3>
<p>Suppose we have a series of sentences(documents)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">X <span style="color:#f92672">=</span> [
     <span style="color:#e6db74">&#39;This is good&#39;</span>,
     <span style="color:#e6db74">&#39;This is bad&#39;</span>,
     <span style="color:#e6db74">&#39;This is awesome&#39;</span>
     ]  
</code></pre></div><div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/countvectorizer.png"  style="height:90%;width:90%"></center>
</div>
<p>Bag of words will create a dictionary of the most common words in all the sentences. For the example above the dictionary would look like:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">word_index
{<span style="color:#e6db74">&#39;this&#39;</span>:<span style="color:#ae81ff">0</span>,<span style="color:#e6db74">&#39;is&#39;</span>:<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;good&#39;</span>:<span style="color:#ae81ff">2</span>,<span style="color:#e6db74">&#39;bad&#39;</span>:<span style="color:#ae81ff">3</span>,<span style="color:#e6db74">&#39;awesome&#39;</span>:<span style="color:#ae81ff">4</span>}
</code></pre></div><p>And then encode the sentences using the above dict.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">This <span style="color:#f92672">is</span> good <span style="color:#f92672">-</span> [<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>]
This <span style="color:#f92672">is</span> bad <span style="color:#f92672">-</span> [<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>]
This <span style="color:#f92672">is</span> awesome <span style="color:#f92672">-</span> [<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>]
</code></pre></div><p>We could do this pretty simply in Python by using the CountVectorizer class from Python. Don&rsquo;t worry much about the heavy name, it just does what I explained above. It has a lot of parameters most significant of which are:</p>
<ul>
<li><strong>ngram_range:</strong> I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.</li>
<li><strong>min_df:</strong> Minimum no of time an ngram should appear in a corpus to be used as a feature.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">cnt_vectorizer <span style="color:#f92672">=</span> CountVectorizer(dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32,
            strip_accents<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;unicode&#39;</span>, analyzer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;word&#39;</span>,token_pattern<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\w{1,}&#39;</span>,
            ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>),min_df<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)


<span style="color:#75715e"># we fit count vectorizer to get ngrams from both train and test data.</span>
cnt_vectorizer<span style="color:#f92672">.</span>fit(list(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values) <span style="color:#f92672">+</span> list(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values))

xtrain_cntv <span style="color:#f92672">=</span>  cnt_vectorizer<span style="color:#f92672">.</span>transform(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)
xtest_cntv <span style="color:#f92672">=</span> cnt_vectorizer<span style="color:#f92672">.</span>transform(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)
</code></pre></div><p>We could then use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like.
For example:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Fitting a simple Logistic Regression on CV Feats</span>
clf <span style="color:#f92672">=</span> LogisticRegression(C<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
clf<span style="color:#f92672">.</span>fit(xtrain_cntv,y_train)
</code></pre></div><p><a href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/">Here</a> is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&rsquo;t forget to upvote.</p>
<hr>
<h3 id="b-tfidf-features">b) TFIDF Features</h3>
<p>TFIDF is a simple technique to find features from sentences. While in Count features we take count of all the words/ngrams present in a document, with TFIDF we take features only for the significant words. How do we do that? If you think of a document in a corpus, we will consider two things about any word in that document:</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/tfidf.png"  style="height:90%;width:90%"></center>
</div>
<ul>
<li><strong>Term Frequency:</strong> How important is the word in the document?</li>
</ul>
<p>$$TF(word\ in\ a\ document) = \dfrac{No\ of\ occurances\ of\ that\ word\ in\ document}{No\ of\ words\ in\ document}$$</p>
<ul>
<li><strong>Inverse Document Frequency:</strong> How important the term is in the whole corpus?</li>
</ul>
<p>$$IDF(word\ in\ a\ corpus) = -log(ratio\ of\ documents\ that\ include\ the\ word)$$</p>
<p>TFIDF then is just multiplication of these two scores.</p>
<p>Intuitively, One can understand that a word is important if it occurs many times in a document. But that creates a problem. Words like &ldquo;a&rdquo;, &ldquo;the&rdquo; occur many times in sentence. Their TF score will always be high. We solve that by using Inverse Document frequency, which is high if the word is rare, and low if the word is common across the corpus.</p>
<p>In essence, we want to find important words in a document which are also not very common.</p>
<p>We could do this pretty simply in Python by using the TFIDFVectorizer class from Python. It has a lot of parameters most significant of which are:</p>
<ul>
<li><strong>ngram_range:</strong> I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.</li>
<li><strong>min_df:</strong> Minimum no of time an ngram should appear in a corpus to be used as a feature.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Always start with these features. They work (almost) everytime!</span>
tfv <span style="color:#f92672">=</span> TfidfVectorizer(dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32, min_df<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,  max_features<span style="color:#f92672">=</span>None,
            strip_accents<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;unicode&#39;</span>, analyzer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;word&#39;</span>,token_pattern<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\w{1,}&#39;</span>,
            ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>), use_idf<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,smooth_idf<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,sublinear_tf<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
            stop_words <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;english&#39;</span>)

<span style="color:#75715e"># Fitting TF-IDF to both training and test sets (semi-supervised learning)</span>
tfv<span style="color:#f92672">.</span>fit(list(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values) <span style="color:#f92672">+</span> list(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values))
xtrain_tfv <span style="color:#f92672">=</span>  tfv<span style="color:#f92672">.</span>transform(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)
xvalid_tfv <span style="color:#f92672">=</span> tfv<span style="color:#f92672">.</span>transform(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)

</code></pre></div><p>Again, we could use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like. <a href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/">Here</a> is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&rsquo;t forget to upvote.</p>
<hr>
<h3 id="c-hashing-features">c) Hashing Features</h3>
<p>Normally there will be a lot of ngrams in a document corpus. The number of features that our TFIDFVectorizer generated was in excess of 2,00,000 features. This might lead to a problem on very large datasets as we have to hold a very large vocabulary dictionary in memory. One way to counter this is to use the Hash Trick.</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/hashfeats.png"  style="height:90%;width:90%"></center>
</div>
<p>One can think of hashing as a single function which maps any ngram to a number range for example between 0 to 1024. Now we don&rsquo;t have to store our ngrams in a dictionary. We can just use the function to get the index of any word, rather than getting the index from a dictionary.</p>
<p>Since there can be more than 1024 ngrams, different ngrams might map to the same number, and this is called collision. The larger the range we provide our Hashing function, the less is the chance of collisions.</p>
<p>We could do this pretty simply in Python by using the HashingVectorizer class from Python. It has a lot of parameters most significant of which are:</p>
<ul>
<li><strong>ngram_range:</strong> I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.</li>
<li><strong>n_features:</strong> No of features you want to consider. The range I gave above.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Always start with these features. They work (almost) everytime!</span>
hv <span style="color:#f92672">=</span> HashingVectorizer(dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32,
            strip_accents<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;unicode&#39;</span>, analyzer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;word&#39;</span>,
            ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>),n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span><span style="color:#f92672">**</span><span style="color:#ae81ff">12</span>,non_negative<span style="color:#f92672">=</span>True)
<span style="color:#75715e"># Fitting Hash Vectorizer to both training and test sets (semi-supervised learning)</span>
hv<span style="color:#f92672">.</span>fit(list(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values) <span style="color:#f92672">+</span> list(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values))
xtrain_hv <span style="color:#f92672">=</span>  hv<span style="color:#f92672">.</span>transform(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)
xvalid_hv <span style="color:#f92672">=</span> hv<span style="color:#f92672">.</span>transform(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)
y_train <span style="color:#f92672">=</span> train_df<span style="color:#f92672">.</span>target<span style="color:#f92672">.</span>values
</code></pre></div><p><a href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/">Here</a> is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&rsquo;t forget to upvote.</p>
<hr>
<h3 id="d-word2vec-features">d) Word2vec Features</h3>
<p>We already talked a little about word2vec in the previous post. We can use the word to vec features to create sentence level feats also. We want to create a <code>d</code> dimensional vector for sentence. For doing this, we will simply average the word embedding of all the words in a sentence.</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/word2vec_feats.png"  style="height:90%;width:90%"></center>
</div>
<p>We can do this in Python using the following functions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># load the GloVe vectors in a dictionary:</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_glove_index</span>():
    EMBEDDING_FILE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt&#39;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_coefs</span>(word,<span style="color:#f92672">*</span>arr): <span style="color:#66d9ef">return</span> word, np<span style="color:#f92672">.</span>asarray(arr, dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;float32&#39;</span>)[:<span style="color:#ae81ff">300</span>]
    embeddings_index <span style="color:#f92672">=</span> dict(get_coefs(<span style="color:#f92672">*</span>o<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>)) <span style="color:#66d9ef">for</span> o <span style="color:#f92672">in</span> open(EMBEDDING_FILE))
    <span style="color:#66d9ef">return</span> embeddings_index

embeddings_index <span style="color:#f92672">=</span> load_glove_index()

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Found </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> word vectors.&#39;</span> <span style="color:#f92672">%</span> len(embeddings_index))

<span style="color:#f92672">from</span> nltk.corpus <span style="color:#f92672">import</span> stopwords
stop_words <span style="color:#f92672">=</span> stopwords<span style="color:#f92672">.</span>words(<span style="color:#e6db74">&#39;english&#39;</span>)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sent2vec</span>(s):
    words <span style="color:#f92672">=</span> str(s)<span style="color:#f92672">.</span>lower()
    words <span style="color:#f92672">=</span> word_tokenize(words)
    words <span style="color:#f92672">=</span> [w <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> words <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> w <span style="color:#f92672">in</span> stop_words]
    words <span style="color:#f92672">=</span> [w <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> words <span style="color:#66d9ef">if</span> w<span style="color:#f92672">.</span>isalpha()]
    M <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> words:
        <span style="color:#66d9ef">try</span>:
            M<span style="color:#f92672">.</span>append(embeddings_index[w])
        <span style="color:#66d9ef">except</span>:
            <span style="color:#66d9ef">continue</span>
    M <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(M)
    v <span style="color:#f92672">=</span> M<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    <span style="color:#66d9ef">if</span> type(v) <span style="color:#f92672">!=</span> np<span style="color:#f92672">.</span>ndarray:
        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">300</span>)
    <span style="color:#66d9ef">return</span> v <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt((v <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum())

<span style="color:#75715e"># create glove features</span>
xtrain_glove <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([sent2vec(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> tqdm(train_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)])
xtest_glove <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([sent2vec(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> tqdm(test_df<span style="color:#f92672">.</span>cleaned_text<span style="color:#f92672">.</span>values)])
</code></pre></div><p><a href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/">Here</a> is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&rsquo;t forget to upvote.</p>
<hr>
<h2 id="results">Results</h2>
<p>Here are the results of different approaches on the Kaggle Dataset. I ran a 5 fold Stratified CV.</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/results_conv.png"  style="height:40%;width:40%"></center>
</div>
<p><a href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/">Here</a> is the code. If you like it please don&rsquo;t forget to upvote.
Also note that I didn&rsquo;t work on tuning the models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning <a href="/blog/2017/12/28/hyperopt_tuning_ml_model/">using hyperopt</a> or just old fashioned Grid-search and the performance of models may change after that substantially.</p>
<hr>
<h2 id="conclusion">Conclusion</h2>
<p>While Deep Learning works a lot better for NLP classification task, it still makes sense to have an understanding of how these problems were solved in the past, so that we can appreciate the nature of the problem. I have tried to provide a perspective on the conventional methods and one should experiment with them too to create baselines before moving to Deep Learning methods. If you want to <strong>learn more about NLP</strong> <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing">here</a> is an awesome course. You can start for free with the 7-day Free Trial. If you think I can add something to the flow, do mention it in the comments.</p>
<hr>
<h2 id="endnotes-and-references">Endnotes and References</h2>
<p>This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.</p>
<ul>
<li><a href="https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle">Approaching (Almost) Any NLP Problem on Kaggle</a></li>
<li><a href="https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings">How to: Preprocessing when using embeddings</a></li>
</ul>
<hr>


        
<script async data-uid="8d7942551b" src="https://mlwhiz.ck.page/8d7942551b/index.js"></script>


<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088" style="margin-bottom: 1px;"></div>

<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.372&subid=0&type=4" rel="nofollow"><IMG border="0"   alt="Start your future with a Data Science Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.372&subid=0&type=4&gridnum=16"></a>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
      </div>
      <div class="col-lg-4">
  

  <div class="widget">
    
<script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#00aaa1', 'S6S3NPCD');kofiwidget2.draw();</script>

</div>
  <div class="widget">
    
    <h4 class="widget-title">About Me</h4>
    
    <img src="https://mlwhiz.com/images/author.jpg" alt=""
      class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4">
    
    <p>Iâ€™m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p>
    <a href="https://mlwhiz.com/about/" class="btn btn-outline-primary">Know More</a>
    
  </div>


  
  <div class="widget">
    <h4 class="widget-title">Topics</h4>
    <ul class="list-unstyled">
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/awesome-guides">Awesome Guides</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/big-data">Big Data</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/computer-vision">Computer Vision</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/data-science">Data Science</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/deep-learning">Deep Learning</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/learning-resources">Learning Resources</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/natural-language-processing">Natural Language Processing</a>
      </li>
      <li><a class="categoryStyle" style="color:#fff"
          href="/categories/programming">Programming</a>
      </li>
    </ul>
  </div>
  
  <div class="widget">
    <h4 class="widget-title">Tags</h4>
    <ul class="list-inline">


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/algorithms">Algorithms</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/artificial-intelligence">Artificial Intelligence</a></li>
          


          


          


          


          


          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/dask">Dask</a></li>
          


          


          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/deployment">Deployment</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/ec2">Ec2</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/generative-adversarial-networks">Generative Adversarial Networks</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/graphs">Graphs</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/image-classification">Image Classification</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/instance-segmentation">Instance Segmentation</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/interpretability">Interpretability</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/jobs">Jobs</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/kaggle">Kaggle</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/machine-learning">Machine Learning</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/math">Math</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/multiprocessing">Multiprocessing</a></li>
          


          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/object-detection">Object Detection</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/opinion">Opinion</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/pandas">Pandas</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/production">Production</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/productivity">Productivity</a></li>
          


          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/python">Python</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/pytorch">Pytorch</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/spark">Spark</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/sql">Sql</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/statistics">Statistics</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/streamlit">Streamlit</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/text-classification">Text Classification</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/timeseries">Timeseries</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/tools">Tools</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/visualization">Visualization</a></li>
          


          
          <li class="list-inline-item"><a class="tagStyle text-white"
              href="/tags/xgboost">Xgboost</a></li>
          
    </ul>
  </div>
  
  <div class="widget">
    <h4 class="widget-title">Connect With Me</h4>
    <ul class="list-inline social-links">
      
      <li class="list-inline-item"><a href="https://www.linkedin.com/in/rahulagwl/"><i class="ti-linkedin"></i></a></li>
      
      <li class="list-inline-item"><a href="https://medium.com/@rahul_agarwal"><i class="ti-book"></i></a></li>
      
      <li class="list-inline-item"><a href="https://twitter.com/MLWhiz"><i class="ti-twitter-alt"></i></a></li>
      
      <li class="list-inline-item"><a href="https://www.facebook.com/mlwhizblog"><i class="ti-facebook"></i></a></li>
      
      <li class="list-inline-item"><a href="https://github.com/MLWhiz"><i class="ti-github"></i></a></li>
      
    </ul>
  </div>
  
  
<script async data-uid="bfe9f82f10" src="https://mlwhiz.ck.page/bfe9f82f10/index.js"></script>

<script async data-uid="3452d924e2" src="https://mlwhiz.ck.page/3452d924e2/index.js"></script>

</div>

    </div>


  </div>
</section>



<footer>
  <div class="container">
    <div class="row">
      <div class="col-12 text-center mb-5">
        <a href="https://mlwhiz.com/"><img src="https://mlwhiz.com/images/logo.png" class="img-fluid-custom-bottom" alt="MLWhiz"></a>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Contact Me</h6>
        <ul class="list-unstyled">
          <li class="mb-3"><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li>
          <li class="mb-3"><a class="text-dark" href="mailto:rahul@mlwhiz.com"><i
                class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a>
          </li>
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Social Contacts</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="https://www.linkedin.com/in/rahulagwl/">Linkedin</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://medium.com/@rahul_agarwal">Medium</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://twitter.com/MLWhiz">Twitter</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://www.facebook.com/mlwhizblog">Facebook</a></li>
          
          <li class="mb-3"><a class="text-dark" href="https://github.com/MLWhiz">Github</a></li>
          
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Categories</h6>
        <ul class="list-unstyled">
          <li class="mb-3"><a class="text-dark"
              href="/categories/awesome-guides">Awesome Guides</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/big-data">Big Data</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/computer-vision">Computer Vision</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/data-science">Data Science</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/deep-learning">Deep Learning</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/learning-resources">Learning Resources</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/natural-language-processing">Natural Language Processing</a>
          </li>
          <li class="mb-3"><a class="text-dark"
              href="/categories/programming">Programming</a>
          </li>
        </ul>
      </div>
      <div class="col-lg-3 col-sm-6 mb-5">
        <h6 class="mb-4">Quick Links</h6>
        <ul class="list-unstyled">
          
          <li class="mb-3"><a class="text-dark" href="https://mlwhiz.com/about">About</a></li>
          
          
          <li class="mb-3"><a class="text-dark" href="https://mlwhiz.com/blog">Post</a></li>
          
          
          
        </ul>
      </div>
      <div class="col-12 border-top py-4 text-center">
        Copyright Â© 2020 <a href="https://mlwhiz.com">MLWhiz</a> All Rights Reserved
      </div>
    </div>
  </div>
</footer>


<script async data-uid="a0ebaf958d" src="https://mlwhiz.ck.page/a0ebaf958d/index.js"></script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>



<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>


<script>
  var indexURL = "https://mlwhiz.com/index.json"
</script>

<!-- JS Plugins -->

<script src="https://mlwhiz.com/plugins/jQuery/jquery.min.js"></script>

<script src="https://mlwhiz.com/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://mlwhiz.com/plugins/slick/slick.min.js"></script>

<script src="https://mlwhiz.com/plugins/venobox/venobox.min.js"></script>

<script src="https://mlwhiz.com/plugins/search/fuse.min.js"></script>

<script src="https://mlwhiz.com/plugins/search/mark.js"></script>

<script src="https://mlwhiz.com/plugins/search/search.js"></script>

<!-- Main Script -->

<script src="https://mlwhiz.com/js/script.min.js"></script>
<!-- google analitycs -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o),
      m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-54777926-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
