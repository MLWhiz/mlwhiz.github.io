<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Demystifying Object Detection and Instance Segmentation for Data Scientists - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="this post is explaining how permutation importance works and how we can code it using ELI5"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Demystifying Object Detection and Instance Segmentation for Data Scientists - MLWhiz"><meta property="og:description" content="this post is explaining how permutation importance works and how we can code it using ELI5"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/12/05/od/"><meta property="og:image" content="https://mlwhiz.com/images/od/main.jpeg"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/od/main.jpeg"><meta property="article:published_time" content="2019-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-13T13:34:49+01:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/od/main.jpeg"><meta name=twitter:title content="Demystifying Object Detection and Instance Segmentation for Data Scientists - MLWhiz"><meta name=twitter:description content="this post is explaining how permutation importance works and how we can code it using ELI5"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/12/05/od/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/12/05/od/#primaryimage","url":"https://mlwhiz.com/images/od/main.jpeg","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/12/05/od/#webpage","url":"https://mlwhiz.com/blog/2019/12/05/od/","inLanguage":"en-US","name":"Demystifying Object Detection and Instance Segmentation for Data Scientists - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/12/05/od/#primaryimage"},"datePublished":"2019-12-05T00:00:00.00Z","dateModified":"2022-04-13T13:34:49.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"this post is explaining how permutation importance works and how we can code it using ELI5"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Demystifying Object Detection and Instance Segmentation for Data Scientists</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>05 December 2019</span></div><img src=https://mlwhiz.com/images/od/main.jpeg class="img-fluid w-100 mb-4" alt="Demystifying Object Detection and Instance Segmentation for Data Scientists"><div class="content mb-5"><p>I like deep learning a lot but Object Detection is something that doesn’t come easily to me.</p><p>And Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.</p><p>It is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.</p><p>This post is about <em><strong>distilling that history into an easy explanation</strong></em> and explaining the gory details for Object Detection and Instance Segmentation.</p><hr><h2 id=introduction>Introduction</h2><p>We all know about the image classification problem. <em>Given an image can you find out the class the image belongs to?</em></p><p>We can solve any new image classification problem with ConvNets and
<a href=https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8 target=_blank rel="nofollow noopener">Transfer Learning</a>
using pre-trained nets.</p><blockquote><p><strong>ConvNet as fixed feature extractor</strong>. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features <strong>CNN codes</strong>. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.</p></blockquote><p>But there are lots of other interesting problems in the Image domain:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/0_hud3df30181d31b61853f248b52d1b1d62_894026_500x0_resize_box_2.png 500w
, /images/od/0_hud3df30181d31b61853f248b52d1b1d62_894026_800x0_resize_box_2.png 800w
, /images/od/0_hud3df30181d31b61853f248b52d1b1d62_894026_1200x0_resize_box_2.png 1200w
, /images/od/0_hud3df30181d31b61853f248b52d1b1d62_894026_1500x0_resize_box_2.png 1500w" src=/images/od/0.png alt='

<a href="http://cs231n.github.io/transfer-learning/#tf" target="_blank" rel="nofollow noopener">Source</a>
'>
<em><a href=http://cs231n.github.io/transfer-learning/#tf target=_blank rel="nofollow noopener">Source</a></em></p><p>These problems can be divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:</p><ol><li><p><strong>Semantic Segmentation:</strong> <em>Given an image, can we classify each pixel as belonging to a particular class?</em></p></li><li><p><strong>Classification+Localization:</strong> We were able to classify an image as a cat. Great. <em>Can we also get the location of the said cat in that image by drawing a bounding box around the cat?</em> Here we assume that there is a fixed number of objects(commonly 1) in the image.</p></li><li><p><strong>Object Detection:</strong> A More general case of the Classification+Localization problem. In a real-world setting, we don’t know how many objects are in the image beforehand. <em>So can we detect all the objects in the image and draw bounding boxes around them?</em></p></li><li><p><strong>Instance Segmentation:</strong> <em>Can we create masks for each individual object in the image?</em> It is different from semantic segmentation. How? If you look in the 4th image on the top, we won’t be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.</p></li></ol><p>As you can see all the problems have something of a similar flavour but a little different than each other. In this post, I will focus mainly on <strong>Object Detection and Instance segmentation</strong> as they are the most interesting.I will go through the 4 most famous techniques for object detection and how they improved with time and new ideas.</p><hr><h2 id=classificationlocalization>Classification+Localization</h2><p>So lets first try to understand how we can solve the problem when we have a single object in the image. How to solve the <strong>Classification+Localization</strong> case.</p><blockquote><p><em>💡</em> Treat localization as a regression problem!</p></blockquote><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/1_hudc64448fe23fd01e194607b01222cc30_394924_500x0_resize_box_2.png 500w
, /images/od/1_hudc64448fe23fd01e194607b01222cc30_394924_800x0_resize_box_2.png 800w
, /images/od/1_hudc64448fe23fd01e194607b01222cc30_394924_1200x0_resize_box_2.png 1200w
, /images/od/1_hudc64448fe23fd01e194607b01222cc30_394924_1500x0_resize_box_2.png 1500w" src=/images/od/1.png alt='

<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf" target="_blank" rel="nofollow noopener">Source</a>
'></p><h3 id=input-data><strong>Input Data</strong></h3><p>Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting, we used to have data in the form (X,y) where X is the image and y used to be the class label.</p><p>In the Classification+Localization setting, we will have data normally in the form (X,y), where X is still the image and y is an array containing (class_label, x,y,w,h) where,</p><p>x = bounding box top left corner x-coordinate</p><p>y = bounding box top left corner y-coordinate</p><p>w = width of the bounding box in pixels</p><p>h = height of the bounding box in pixels</p><h3 id=model><strong>Model</strong></h3><p>So in this setting, we create a <em>multi-output model</em> which takes an image as the input and has (n_labels + 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).</p><h3 id=loss><strong>Loss</strong></h3><p>Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).</p><blockquote><p>Loss = alpha*Softmax_Loss + (1-alpha)*L2_Loss</p></blockquote><p>Since these two losses would be on a different scale, the alpha hyper-parameter is something that needs to be tuned.</p><p><em>There is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function.</em></p><p><em>And herein lies the essence of the whole Deep Learning framework —</em> <strong>Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem</strong>*. And that is what we are going to see a lot going forward.*</p><hr><h2 id=object-detection>Object Detection</h2><p><em>So how does this idea of localization using regression get mapped to Object Detection?</em> It doesn’t.</p><p>We don’t have a fixed number of objects. So we can’t have 4 outputs denoting, the bounding box coordinates.</p><p>One naive idea could be to apply CNN to many different crops of the image. CNN classifies each crop as an object class or background class. This is intractable. There could be a lot of such crops that you can create.</p><h3 id=region-proposals>Region Proposals:</h3><p>So, if just there was just a method(Normally called Region Proposal Network)which could find some smaller number of cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is the basic idea behind RCNN-The first major success in object detection.</p><p>And that is what selective search (Uijlings et al, “
<a href=http://www.huppelen.nl/publications/selectiveSearchDraft.pdf target=_blank rel="nofollow noopener">Selective Search for Object Recognition</a>
”, IJCV 2013) provided.</p><p><em><strong>So what are Region Proposals?</strong></em></p><ul><li><p>Find <em>“blobby”</em> image regions that are likely to contain objects</p></li><li><p>Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU</p></li></ul><p>So, how exactly the region proposals are made?</p><h3 id=selective-search-for-object-recognitionhttpwwwhuppelennlpublicationsselectivesearchdraftpdf><a href=http://www.huppelen.nl/publications/selectiveSearchDraft.pdf target=_blank rel="nofollow noopener">Selective Search for Object Recognition</a>
:</h3><p>This paper finds regions in two steps.</p><p>First, we start with a set of some initial regions using
<a href=http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf target=_blank rel="nofollow noopener">Efficient GraphBased Image Segmentation</a>
.</p><blockquote><p>Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels.</p></blockquote><p>In this paper they take an approach:</p><blockquote><p>Each edge (vi , vj )∈ E has a corresponding weight w((vi , vj )), which is a non-negative <strong>measure of the similarity</strong> between neighboring elements vi and vj . In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ∈ S corresponds to a connected component in a graph.</p></blockquote><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/2_hu8eb911581ca82a07ce093beac8678cce_314987_500x0_resize_box_2.png 500w
, /images/od/2_hu8eb911581ca82a07ce093beac8678cce_314987_800x0_resize_box_2.png 800w" src=/images/od/2.png alt='

<a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf" target="_blank" rel="nofollow noopener">Efficient graph-based Image Segmentation</a>
 Example'></p><p><em><strong>Put simply, they use graph-based methods to find connected components in an image and the edges are made on some measure of similarity between pixels.</strong></em></p><p>As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. And that is the second step.</p><p>For that, the authors of
<a href=http://www.huppelen.nl/publications/selectiveSearchDraft.pdf target=_blank rel="nofollow noopener">Selective Search for Object Recognition</a>
apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm, they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill to provide us with much better region proposals.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/3_hu67b0a9aefe37905d78ff4cb027110325_10837_500x0_resize_box_2.png 500w" src=/images/od/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/4_huc0a22ffb3ac8be5ca07110ac9580db86_102332_500x0_resize_box_2.png 500w" src=/images/od/4.png alt="The Algorithm for region Proposal used in RCNN"></p><hr><h2 id=1-r-cnn>1. R-CNN</h2><p>So now we have our region proposals. How do we exactly use them in R-CNN?</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/5_hu131be25e3b28b1538a6e2928da1ce688_401929_500x0_resize_box_2.png 500w
, /images/od/5_hu131be25e3b28b1538a6e2928da1ce688_401929_800x0_resize_box_2.png 800w
, /images/od/5_hu131be25e3b28b1538a6e2928da1ce688_401929_1200x0_resize_box_2.png 1200w
, /images/od/5_hu131be25e3b28b1538a6e2928da1ce688_401929_1500x0_resize_box_2.png 1500w" src=/images/od/5.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><blockquote><p>Object detection system overview. Our system
(1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.</p></blockquote><p>Along with this, the authors have also used a class-specific bounding box regressor, that takes:</p><p>Input : (Px, Py, Ph, Pw) — the location of the proposed region.</p><p>Target: (Gx, Gy, Gh, Gw) — Ground truth labels for the region.</p><p>The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G)</p><h3 id=training-r-cnn>Training R-CNN</h3><p>What is the input to an RCNN?</p><p>So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)</p><p>Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. We train class-specific SVM’s</p><p>So every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class-specific data.</p><h3 id=test-time-r-cnn>Test Time R-CNN</h3><p>At test time we predict detection boxes using class-specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Thus, non-maximum suppression is an integral part of the object detection pipeline.</p><p>First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed.</p><p>This process is recursively applied on all the remaining boxes until we are left with good bounding boxes only.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/6_hu5daf1687d6545067808d9d61e741270f_83721_500x0_resize_q75_box.jpeg 500w
, /images/od/6_hu5daf1687d6545067808d9d61e741270f_83721_800x0_resize_q75_box.jpeg 800w" src=/images/od/6.jpeg alt='

<a href="https://www.pyimagesearch.com/wp-content/uploads/2014/10/hog_object_detection_nms.jpg" target="_blank" rel="nofollow noopener">https://www.pyimagesearch.com/wp-content/uploads/2014/10/hog_object_detection_nms.jpg</a>
'></p><h3 id=problems-with-rcnn>Problems with RCNN:</h3><ul><li><p>Training is slow.</p></li><li><p>Inference (detection) is slow. 47s / image with VGG16 — Since the Convnet needs to be run many times.</p></li></ul><p>Need for speed. So Fast R-CNN.</p><hr><h2 id=2-fast-r-cnn>2. Fast R-CNN</h2><blockquote><p><em>💡</em> So the next
<a href=https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf target=_blank rel="nofollow noopener">idea</a>
from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM/classifier to classify those crops.</p></blockquote><p>Something like:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/7_hu567e3569e9179bdb52b35749ec42ff39_328123_500x0_resize_box_2.png 500w
, /images/od/7_hu567e3569e9179bdb52b35749ec42ff39_328123_800x0_resize_box_2.png 800w
, /images/od/7_hu567e3569e9179bdb52b35749ec42ff39_328123_1200x0_resize_box_2.png 1200w
, /images/od/7_hu567e3569e9179bdb52b35749ec42ff39_328123_1500x0_resize_box_2.png 1500w" src=/images/od/7.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><blockquote><p>From
<a href=https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf target=_blank rel="nofollow noopener">Paper</a>
: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.</p></blockquote><h3 id=idea>💡Idea</h3><p>So the <em><strong>basic idea is to have to run the convolution only once in the image rather than so many convolution networks in R-CNN.</strong></em> Then we can map the ROI proposals using some method and filter the last convolution layer and just run a final classifier on that.</p><p>This idea depends a little upon the architecture of the model that gets used too.</p><p>So the architecture that the authors have proposed is:</p><blockquote><p>We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). <em>When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net’s first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network’s last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.</em></p></blockquote><p>Don’t worry if you don’t understand the above. This obviously is a little confusing, so let us break this down. But for that, we need to see VGG16 architecture first.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/od/8.png alt="VGG 16 Architecture"></p><p>The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/9_hu1aa02d10f37d9a13e1bd48fdc89a6d91_29630_500x0_resize_box_2.png 500w
, /images/od/9_hu1aa02d10f37d9a13e1bd48fdc89a6d91_29630_800x0_resize_box_2.png 800w" src=/images/od/9.png alt="We need fixed-sized feature maps for the final classifier"></p><p>Now the location of ROI coordinates is in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512.</p><p>As we are using VGG, we have transformed the image (224 x 224 x 3) into (14 x 14 x 512) — i.e. the height and width are divided by 16. We can map ROIs coordinates onto the feature map just by dividing them by 16.</p><blockquote><p>In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the “things” it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.</p></blockquote><p>What is ROI pooling?</p><p><em>Remember that the final classifier runs for each crop. And so each crop needs to be of the same size. And that is what ROI Pooling does.</em></p><p><img src=/images/od/10.gif alt='

<a href="https://deepsense.ai/region-of-interest-pooling-explained/" target="_blank" rel="nofollow noopener">Source</a>
'></p><p>In the above image, our region proposal is (0,3,5,7) in x,y,w,h format.</p><p>We divide that area into 4 regions since we want to have an ROI pooling layer of 2x2. We divide the whole area into buckets by rounding 5/2 and 7/2 and then just do a max-pool.</p><p><a href=https://stackoverflow.com/questions/48163961/how-do-you-do-roi-pooling-on-areas-smaller-than-the-target-size target=_blank rel="nofollow noopener">How do you do ROI-Pooling on Areas smaller than the target size?</a>
if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens,
<a href=https://stackoverflow.com/questions/48163961/how-do-you-do-roi-pooling-on-areas-smaller-than-the-target-size target=_blank rel="nofollow noopener">we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.</a></p><p>After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K + 1 categories(+1 for Background) and category-specific bounding-box regressors.</p><h3 id=training-fast-rcnn>Training Fast-RCNN</h3><p>What is the input to a Fast- RCNN?</p><p>Pretty much similar to R-CNN: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)</p><p>Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. This time we have a dense layer on top, and we use multi-task loss.</p><p>So every ROI becomes a training example. The main difference is that there is a concept of multi-task loss:</p><p>A Fast R-CNN network has two sibling output layers.</p><p>The first outputs a <em><strong>discrete probability distribution</strong></em> (per RoI), p = (p0, . . . , pK), over K + 1 categories. As usual, p is computed by a softmax over the K+1 outputs of a fully connected layer.</p><p>The second sibling layer outputs <em><strong>bounding-box regression offsets</strong></em>, t= (tx, ty, tw, th), for each of the K object classes. Each training RoI is labelled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labelled RoI to jointly train for classification and bounding-box regression</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/11_hu551a5441d79288cc49fb0e40dac98ed8_7628_500x0_resize_box_2.png 500w" src=/images/od/11.png alt="Classification Loss + regression Loss"></p><p>Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class.</p><h3 id=problem>Problem:</h3><p>Region proposals are still taking up most of the time. Can we reduce the time taken for Region proposals?</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/12_hue5a5af55736b38563f53f4ef693d6840_147586_500x0_resize_box_2.png 500w
, /images/od/12_hue5a5af55736b38563f53f4ef693d6840_147586_800x0_resize_box_2.png 800w" src=/images/od/12.png alt="Runtime dominated by region proposals!"></p><hr><h2 id=3-faster-rcnn>3. Faster-RCNN</h2><p>The next question that got asked was: Can the network itself do region proposals?</p><blockquote><p>The intuition is that: With FastRCNN we’re already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes + bbox coordinates?</p></blockquote><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/13_hu7ad4ad9b1d8bd9600141d45b4b576544_193398_500x0_resize_box_2.png 500w" src=/images/od/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h3 id=how-does-the-region-proposal-network-work>How does the Region Proposal Network work?</h3><p>One of the main ideas in the paper is the idea of Anchors. <strong>Anchors</strong> are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.</p><p>So, first of all, we define anchor centres on the image.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/14_hu76e252da68d1a6c8d2187fb5d52fae2f_772994_500x0_resize_box_2.png 500w
, /images/od/14_hu76e252da68d1a6c8d2187fb5d52fae2f_772994_800x0_resize_box_2.png 800w" src=/images/od/14.png alt="Anchor centers throughout the original image"></p><p>The anchor centres are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224/14).</p><p>This is how anchors look like:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/15_hudc2f185e42518a22741d26e927a229b9_512359_500x0_resize_box_2.png 500w
, /images/od/15_hudc2f185e42518a22741d26e927a229b9_512359_800x0_resize_box_2.png 800w
, /images/od/15_hudc2f185e42518a22741d26e927a229b9_512359_1200x0_resize_box_2.png 1200w" src=/images/od/15.png alt="Left: Anchors, Center: Anchor for a single point, Right: All anchors"></p><ol><li><p>So we start with some predefined regions we think our objects could be with Anchors.</p></li><li><p>Our Region Proposal Network(RPN) classifies which regions have the object and the offset of the object bounding box. Training is done using the same logic. 1 if IOU for anchor with bounding box>0.5 0 otherwise.</p></li><li><p>Non-Maximum suppression to reduce region proposals</p></li><li><p>Fast RCNN detection network on top of proposals</p></li></ol><h3 id=faster-rcnn-loss>Faster-RCNN Loss</h3><p>The whole network is then jointly trained with 4 losses:</p><ol><li><p>RPN classify object / not object</p></li><li><p>RPN regress box coordinates offset</p></li><li><p>Final classification score (object classes)</p></li><li><p>Final box coordinates offset</p></li></ol><h3 id=performance>Performance</h3><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/16_hucd7fc407a1fdf808f91574661fabd43a_28831_500x0_resize_box_2.png 500w" src=/images/od/16.png alt="Results on VOC Dataset for the three different approaches"></p><hr><h2 id=instance-segmentation>Instance Segmentation</h2><p>Now comes the most interesting part — Instance segmentation. <em>Can we create</em> <em><strong>masks</strong></em> <em>for each individual object in the image? Specifically something like:</em></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/17_hucd2ee079d11dafd53e757ad6ae50e7b4_2838717_500x0_resize_box_2.png 500w
, /images/od/17_hucd2ee079d11dafd53e757ad6ae50e7b4_2838717_800x0_resize_box_2.png 800w
, /images/od/17_hucd2ee079d11dafd53e757ad6ae50e7b4_2838717_1200x0_resize_box_2.png 1200w
, /images/od/17_hucd2ee079d11dafd53e757ad6ae50e7b4_2838717_1500x0_resize_box_2.png 1500w" src=/images/od/17.png alt="Some images with masks from the paper"></p><hr><h2 id=mask-rcnn>Mask-RCNN</h2><p>The same authors come to rescue again. The basic idea is to add another output layer that predicts the mask. And to use ROIAlign instead of ROIPooling.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/18_hu4c3e5fdcd30f7b860c1d55308dbc0386_455477_500x0_resize_box_2.png 500w
, /images/od/18_hu4c3e5fdcd30f7b860c1d55308dbc0386_455477_800x0_resize_box_2.png 800w
, /images/od/18_hu4c3e5fdcd30f7b860c1d55308dbc0386_455477_1200x0_resize_box_2.png 1200w
, /images/od/18_hu4c3e5fdcd30f7b860c1d55308dbc0386_455477_1500x0_resize_box_2.png 1500w" src=/images/od/18.png alt='

<a href="https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272" target="_blank" rel="nofollow noopener">Source:</a>
 Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign'>
<em><a href=https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272 target=_blank rel="nofollow noopener">Source:</a>
Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign</em></p><p>Mask R-CNN adopts the same two-stage procedure, with an identical first stage (RPN).</p><p>In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.</p><h3 id=roialign-vs-roipooling>ROIAlign vs ROIPooling</h3><p>In ROI pooling we lose the exact location-based information. See how we arbitrarily divided our region into 4 different sized boxes. For a classification task, it works well.</p><p>But for providing masks on a pixel level, we don’t want to lose this information. And hence we don’t quantize the pooling layer and use bilinear interpolation to find out values that properly aligns the extracted features with the input. See how 0.8 differs from 0.88</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/19_hu70bcab8a99d106bd5210f853a9e66866_14904_500x0_resize_box_2.png 500w" src=/images/od/19.png alt='

<a href="https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272" target="_blank" rel="nofollow noopener">Source</a>
'>
<em><a href=https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272 target=_blank rel="nofollow noopener">Source</a></em></p><h3 id=training>Training</h3><p>During training, we define a multi-task loss on each sampled RoI as</p><p>L = Lcls + Lbox + Lmask</p><p>The classification loss Lcls and bounding-box loss Lbox are identical as in Faster R-CNN. The mask branch has a K × m × m — dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes.</p><p>To this, we apply a per-pixel sigmoid and define Lmask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, Lmask is only defined on the kth mask (other mask outputs do not contribute to the loss).</p><h3 id=mask-prediction>Mask Prediction</h3><p>The mask layer is K × m × m dimensional where K is the number of classes. The m×m floating-number mask output is resized to the RoI size and binarized at a threshold of 0.5 to get final masks.</p><hr><h2 id=conclusion>Conclusion</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/od/20_hu506d839ace9e7d6284b8584601e11ecd_1387294_500x0_resize_box_2.png 500w
, /images/od/20_hu506d839ace9e7d6284b8584601e11ecd_1387294_800x0_resize_box_2.png 800w
, /images/od/20_hu506d839ace9e7d6284b8584601e11ecd_1387294_1200x0_resize_box_2.png 1200w
, /images/od/20_hu506d839ace9e7d6284b8584601e11ecd_1387294_1500x0_resize_box_2.png 1500w" src=/images/od/20.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Congrats for reaching the end. This post was a long one.</p><p><em><strong>In this post, I talked about some of the most important advancements in the field of Object detection and Instance segmentation and tried to explain them as easily as I can.</strong></em></p><p>This is my own understanding of these papers with inputs from many blogs and slides on the internet and I sincerely thank the creators. Let me know if you find something wrong with my understanding.</p><p>Object detection is a vast field and there are a lot of other methods that dominate this field. Some of them being U-net, SSD and YOLO.</p><p>There is no dearth of resources to learn them so I would encourage you to go and take a look at them. You have got a solid backing/understanding now.</p><p><em><strong>In this post, I didn’t write about coding and implementation. So stay tuned for my next post in which we will train a Mask RCNN model for a custom dataset.</strong></em></p><p>If you want to know more about NLP, I would like to recommend this awesome
<a href=https://coursera.pxf.io/9WjZo0 target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in London, where I am currently working with Facebook .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.medium.com/>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>