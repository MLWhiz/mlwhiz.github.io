<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Implementing Object Detection and Instance Segmentation for Data Scientists</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="This post is about implementing and getting an object detector on our custom dataset of weapons">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="Implementing Object Detection and Instance Segmentation for Data Scientists" />
<meta property="og:description" content="This post is about implementing and getting an object detector on our custom dataset of weapons" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2019/12/06/weapons/" />
<meta property="og:image" content="https://mlwhiz.com/images/weapons/main.png" />
<meta property="article:published_time" content="2019-12-06T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-12-06T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/weapons/main.png"/>

<meta name="twitter:title" content="Implementing Object Detection and Instance Segmentation for Data Scientists"/>
<meta name="twitter:description" content="This post is about implementing and getting an object detector on our custom dataset of weapons"/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/atom.xml">RSS</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Implementing Object Detection and Instance Segmentation for Data Scientists</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-12-06T00:00:00">December 06, 2019</time>
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<p><img src="/images/weapons/main.png" alt="" /></p>

<p>Object Detection is a helpful tool to have in your coding repository.</p>

<p>It forms the backbone of many fantastic industrial applications. Some of them being self-driving cars, medical imaging and face detection.</p>

<p>In my last <a href="https://towardsdatascience.com/a-hitchhikers-guide-to-object-detection-and-instance-segmentation-ac0146fe8e11" rel="nofollow" target="_blank">post</a> on Object detection, I talked about how Object detection models evolved.</p>

<p>But what good is theory, if we can’t implement it?</p>

<p><strong><em>This post is about implementing and getting an object detector on our custom dataset of weapons.</em></strong></p>

<p>The problem we will specifically solve today is that of Instance Segmentation using Mask-RCNN.</p>

<hr />

<h2 id="instance-segmentation">Instance Segmentation</h2>

<p><em>Can we create</em> <strong><em>masks</em></strong> <em>for each object in the image? Specifically something like:</em></p>

<p><img src="/images/weapons/0.png" alt="" /></p>

<p>The most common way to solve this problem is by using Mask-RCNN. The architecture of Mask-RCNN looks like below:</p>

<p><img src="/images/weapons/1.png" alt="[Source](https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272)" /><em><a href="https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272" rel="nofollow" target="_blank">Source</a></em></p>

<p>Essentially, it comprises of:</p>

<ul>
<li><p>A backbone network like resnet50/resnet101</p></li>

<li><p>A Region Proposal network</p></li>

<li><p>ROI-Align layers</p></li>

<li><p>Two output layers — one to predict masks and one to predict class and bounding box.</p></li>
</ul>

<p>There is a lot more to it. If you want to learn more about the theory, read my last post&ndash;
<a href="https://towardsdatascience.com/a-hitchhikers-guide-to-object-detection-and-instance-segmentation-ac0146fe8e11" rel="nofollow" target="_blank">Demystifying Object Detection and Instance Segmentation for Data Scientists</a></p>

<p>This post is mostly going to be about the <a href="https://github.com/MLWhiz/data_science_blogs/tree/master/object_detection" rel="nofollow" target="_blank">code</a>.</p>

<hr />

<h2 id="1-creating-your-custom-dataset-for-instance-segmentation">1. Creating your Custom Dataset for Instance Segmentation</h2>

<p><img src="/images/weapons/2.png" alt="Our Dataset" /></p>

<p>The use case we will be working on is a weapon detector. A weapon detector is something that can be used in conjunction with street cameras as well as CCTV’s to fight crime. So it is pretty nifty.</p>

<p>So, I started with downloading 40 images each of guns and swords from the <a href="https://storage.googleapis.com/openimages/web/index.html" rel="nofollow" target="_blank">open image dataset</a> and annotated them using the VIA tool. Now setting up the annotation project in VIA is petty important, so I will try to explain it step by step.</p>

<h3 id="1-set-up-via">1. Set up VIA</h3>

<p>VIA is an annotation tool, using which you can annotate images both bounding boxes as well as masks. I found it as one of the best tools to do annotation as it is online and runs in the browser itself.</p>

<p>To use it, open <a href="http://www.robots.ox.ac.uk/~vgg/software/via/via.html" rel="nofollow" target="_blank">http://www.robots.ox.ac.uk/~vgg/software/via/via.html</a></p>

<p>You will see a page like:</p>

<p><img src="/images/weapons/3.png" alt="" /></p>

<p>The next thing we want to do is to add the different class names in the region_attributes. Here I have added ‘gun’ and ‘sword’ as per our use case as these are the two distinct targets I want to annotate.</p>

<p><img src="/images/weapons/4.png" alt="" /></p>

<h3 id="2-annotate-the-images">2. Annotate the Images</h3>

<p>I have kept all the files in the folder data. Next step is to add the files we want to annotate. We can add files in the data folder using the “Add Files” button in the VIA tool. And start annotating along with labels as shown below after selecting the polyline tool.</p>

<p><img src="/images/weapons/5.png" alt="Click, Click, Enter, Escape, Select" /></p>

<h3 id="3-download-the-annotation-file">3. Download the annotation file</h3>

<p>Click on save project on the top menu of the VIA tool.</p>

<p><img src="/images/weapons/6.png" alt="" /></p>

<p>Save file as via_region_data.json by changing the project name field. This will save the annotations in COCO format.</p>

<h3 id="4-set-up-the-data-directory-structure">4. Set up the data directory structure</h3>

<p>We will need to set up the data directories first so that we can do object detection. In the code below, I am creating a directory structure that is required for the model that we are going to use.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> random <span style="color:#f92672">import</span> random
<span style="color:#f92672">import</span> os
<span style="color:#f92672">from</span> glob <span style="color:#f92672">import</span> glob
<span style="color:#f92672">import</span> json
<span style="color:#75715e"># Path to your images</span>
image_paths <span style="color:#f92672">=</span> glob(<span style="color:#e6db74">&#34;data/*&#34;</span>)
<span style="color:#75715e">#Path to your annotations from VIA tool</span>
annotation_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;via_region_data.json&#39;</span>
<span style="color:#75715e">#clean up the annotations a little</span>
annotations <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(open(annotation_file))
cleaned_annotations <span style="color:#f92672">=</span> {}
<span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> annotations[<span style="color:#e6db74">&#39;_via_img_metadata&#39;</span>]<span style="color:#f92672">.</span>items():
    cleaned_annotations[v[<span style="color:#e6db74">&#39;filename&#39;</span>]] <span style="color:#f92672">=</span> v
<span style="color:#75715e"># create train and validation directories</span>
<span style="color:#960050;background-color:#1e0010">!</span> mkdir procdata
<span style="color:#960050;background-color:#1e0010">!</span> mkdir procdata<span style="color:#f92672">/</span>val
<span style="color:#960050;background-color:#1e0010">!</span> mkdir procdata<span style="color:#f92672">/</span>train
train_annotations <span style="color:#f92672">=</span> {}
valid_annotations <span style="color:#f92672">=</span> {}
<span style="color:#75715e"># 20% of images in validation folder</span>
<span style="color:#66d9ef">for</span> img <span style="color:#f92672">in</span> image_paths:
    <span style="color:#75715e"># Image goes to Validation folder</span>
    <span style="color:#66d9ef">if</span> random()<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.2</span>:
        os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">&#34;cp &#34;</span><span style="color:#f92672">+</span> img <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; procdata/val/&#34;</span>)
        img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;/&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
        valid_annotations[img] <span style="color:#f92672">=</span> cleaned_annotations[img]
    <span style="color:#66d9ef">else</span>:
        os<span style="color:#f92672">.</span>system(<span style="color:#e6db74">&#34;cp &#34;</span><span style="color:#f92672">+</span> img <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; procdata/train/&#34;</span>)
        img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;/&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
        train_annotations[img] <span style="color:#f92672">=</span> cleaned_annotations[img]
<span style="color:#75715e"># put different annotations in different folders</span>
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;procdata/val/via_region_data.json&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> fp:
    json<span style="color:#f92672">.</span>dump(valid_annotations, fp)
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;procdata/train/via_region_data.json&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> fp:
    json<span style="color:#f92672">.</span>dump(train_annotations, fp)</code></pre></div>
<p>After running the above code, we will get the data in the below folder structure:</p>

<pre><code>- procdata
     - train
         - img1.jpg
         - img2.jpg
         - via_region_data.json
     - val
         - img3.jpg
         - img4.jpg
         - via_region_data.json
</code></pre>

<hr />

<h2 id="2-setup-the-coding-environment">2. Setup the Coding Environment</h2>

<p>We will use the code from the <a href="https://github.com/matterport/Mask_RCNN" rel="nofollow" target="_blank">matterport/Mask_RCNN</a> GitHub repository. You can start by cloning the repository and installing the required libraries.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/matterport/Mask_RCNN
cd Mask_RCNN
pip install -r requirements.txt</code></pre></div>
<p>Once we are done with installing the dependencies and cloning the repo, we can start with implementing our project.</p>

<p>We make a copy of the samples/balloon directory in Mask_RCNN folder and create a <strong><em>samples/guns_and_swords</em></strong> directory where we will continue our work:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cp -r samples/balloon samples/guns_and_swords</code></pre></div>
<h3 id="setting-up-the-code">Setting up the Code</h3>

<p>We start by renaming and changing balloon.py in the <code>samples/guns_and_swords</code> directory to <code>gns.py</code>. The <code>balloon.py</code> file right now trains for one target. I have extended it to use multiple targets. In this file, we change:</p>

<ol>
<li><p><code>balloonconfig</code> to <code>gnsConfig</code></p></li>

<li><p><code>BalloonDataset</code> to <code>gnsDataset</code> : We changed some code here to get the target names from our annotation data and also give multiple targets.</p></li>

<li><p>And some changes in the train function</p></li>
</ol>

<p>Showing only the changed <code>gnsConfig</code> here to get you an idea. You can take a look at the whole <a href="https://github.com/MLWhiz/data_science_blogs/blob/master/object_detection/guns_and_swords/gns.py" rel="nofollow" target="_blank">gns.py</a> code here.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">gnsConfig</span>(Config):
    <span style="color:#e6db74">&#34;&#34;&#34;Configuration for training on the toy  dataset.
</span><span style="color:#e6db74">    Derives from the base Config class and overrides some values.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Give the configuration a recognizable name</span>
    NAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;gns&#34;</span>
    <span style="color:#75715e"># We use a GPU with 16GB memory, which can fit three image.</span>
    <span style="color:#75715e"># Adjust down if you use a smaller GPU.</span>
    IMAGES_PER_GPU <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
    <span style="color:#75715e"># Number of classes (including background)</span>
    NUM_CLASSES <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># Background + sword + gun</span>
    <span style="color:#75715e"># Number of training steps per epoch</span></code></pre></div>
<hr />

<h2 id="3-visualizing-images-and-masks">3. Visualizing Images and Masks</h2>

<p>Once we are done with changing the <code>gns.py</code> file,we can visualize our masks and images. You can do simply by following this <a href="hhttps://github.com/MLWhiz/data_science_blogs/blob/master/object_detection/guns_and_swords/1.%20Visualize%20Dataset.ipynb" rel="nofollow" target="_blank">Visualize Dataset.ipynb</a> notebook.</p>

<p><img src="/images/weapons/a.png" alt="" /></p>

<hr />

<h2 id="4-train-the-maskrcnn-model-with-transfer-learning">4. Train the MaskRCNN Model with Transfer Learning</h2>

<p>To train the maskRCNN model, on the Guns and Swords dataset, we need to run one of the following commands on the command line based on if we want to initialise our model with COCO weights or imagenet weights:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Train a new model starting from pre-trained COCO weights</span>
 python3 gns.py train — dataset<span style="color:#f92672">=</span>/path/to/dataset — weights<span style="color:#f92672">=</span>coco

<span style="color:#75715e"># Resume training a model that you had trained earlier</span>
 python3 gns.py train — dataset<span style="color:#f92672">=</span>/path/to/dataset — weights<span style="color:#f92672">=</span>last

<span style="color:#75715e"># Train a new model starting from ImageNet weights</span>
 python3 gns.py train — dataset<span style="color:#f92672">=</span>/path/to/dataset — weights<span style="color:#f92672">=</span>imagenet</code></pre></div>
<p>The command with weights=last will resume training from the last epoch. The weights are going to be saved in the logs directory in the Mask_RCNN folder.</p>

<p>This is how the loss looks after our final epoch.</p>

<p><img src="/images/weapons/12.png" alt="" /></p>

<h3 id="visualize-the-losses-using-tensorboard">Visualize the losses using Tensorboard</h3>

<p>You can take advantage of tensorboard to visualise how your network is performing. Just run:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tensorboard --logdir ~/objectDetection/Mask_RCNN/logs/gns20191010T1234</code></pre></div>
<p>You can get the tensorboard at</p>

<pre><code>https://localhost:6006
</code></pre>

<p>Here is how our mask loss looks like:</p>

<p><img src="/images/weapons/13.png" alt="" /></p>

<p>We can see that the validation loss is performing pretty abruptly. This is expected as we only have kept 20 images in the validation set.</p>

<hr />

<h2 id="5-prediction-on-new-images">5. Prediction on New Images</h2>

<p>Predicting a new image is also pretty easy. Just follow the <a href="https://github.com/MLWhiz/data_science_blogs/blob/master/object_detection/guns_and_swords/2.%20predict.ipynb" rel="nofollow" target="_blank">prediction.ipynb</a> notebook for a minimal example using our trained model. Below is the main part of the code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Function taken from utils.dataset</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_image</span>(image_path):
    <span style="color:#e6db74">&#34;&#34;&#34;Load the specified image and return a [H,W,3] Numpy array.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Load image</span>
    image <span style="color:#f92672">=</span> skimage<span style="color:#f92672">.</span>io<span style="color:#f92672">.</span>imread(image_path)
    <span style="color:#75715e"># If grayscale. Convert to RGB for consistency.</span>
    <span style="color:#66d9ef">if</span> image<span style="color:#f92672">.</span>ndim <span style="color:#f92672">!=</span> <span style="color:#ae81ff">3</span>:
        image <span style="color:#f92672">=</span> skimage<span style="color:#f92672">.</span>color<span style="color:#f92672">.</span>gray2rgb(image)
    <span style="color:#75715e"># If has an alpha channel, remove it for consistency</span>
    <span style="color:#66d9ef">if</span> image<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">4</span>:
        image <span style="color:#f92672">=</span> image[<span style="color:#f92672">...</span>, :<span style="color:#ae81ff">3</span>]
    <span style="color:#66d9ef">return</span> image
<span style="color:#75715e"># path to image to be predicted</span>
image <span style="color:#f92672">=</span> load_image(<span style="color:#e6db74">&#34;../../../data/2c8ce42709516c79.jpg&#34;</span>)
<span style="color:#75715e"># Run object detection</span>
results <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>detect([image], verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
<span style="color:#75715e"># Display results</span>
ax <span style="color:#f92672">=</span> get_ax(<span style="color:#ae81ff">1</span>)
r <span style="color:#f92672">=</span> results[<span style="color:#ae81ff">0</span>]
a <span style="color:#f92672">=</span> visualize<span style="color:#f92672">.</span>display_instances(image, r[<span style="color:#e6db74">&#39;rois&#39;</span>], r[<span style="color:#e6db74">&#39;masks&#39;</span>], r[<span style="color:#e6db74">&#39;class_ids&#39;</span>], dataset<span style="color:#f92672">.</span>class_names, r[<span style="color:#e6db74">&#39;scores&#39;</span>], ax<span style="color:#f92672">=</span>ax,
                            title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Predictions&#34;</span>)</code></pre></div>
<p>This is how the result looks for some images in the validation set:</p>

<p><img src="/images/weapons/b.png" alt="" /></p>

<hr />

<h2 id="improvements">Improvements</h2>

<p>The results don’t look very promising and leave a lot to be desired, but that is to be expected because of very less training data(60 images). One can try to do the below things to improve the model performance for this weapon detector.</p>

<ol>
<li><p>We just trained on 60 images due to time constraints. While we used transfer learning the data is still too less — Annotate more data.</p></li>

<li><p>Train for more epochs and longer time. See how validation loss and training loss looks like.</p></li>

<li><p>Change hyperparameters in the mrcnn/config file in the Mask_RCNN directory. For information on what these hyperparameters mean, take a look at my previous post. The main ones you can look at:</p></li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># if you want to provide different weights to different losses</span>
LOSS_WEIGHTS <span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;rpn_class_loss&#39;</span>: <span style="color:#ae81ff">1.0</span>, <span style="color:#e6db74">&#39;rpn_bbox_loss&#39;</span>: <span style="color:#ae81ff">1.0</span>, <span style="color:#e6db74">&#39;mrcnn_class_loss&#39;</span>: <span style="color:#ae81ff">1.0</span>, <span style="color:#e6db74">&#39;mrcnn_bbox_loss&#39;</span>: <span style="color:#ae81ff">1.0</span>, <span style="color:#e6db74">&#39;mrcnn_mask_loss&#39;</span>: <span style="color:#ae81ff">1.0</span>}

<span style="color:#75715e"># Length of square anchor side in pixels</span>
RPN_ANCHOR_SCALES <span style="color:#f92672">=</span> (<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>)

<span style="color:#75715e"># Ratios of anchors at each cell (width/height)</span>
<span style="color:#75715e"># A value of 1 represents a square anchor, and 0.5 is a wide anchor</span>
RPN_ANCHOR_RATIOS <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>]</code></pre></div>
<hr />

<h2 id="conclusion">Conclusion</h2>

<p><strong><em>In this post, I talked about how to implement Instance segmentation using Mask-RCNN for a custom dataset.</em></strong></p>

<p>I tried to make the coding part as simple as possible and hope you find the code useful. In the next part of this post, I will deploy this model using a web app. So stay tuned.</p>

<p>You can download the annotated weapons data as well as the code at <a href="https://github.com/MLWhiz/data_science_blogs/tree/master/object_detection" rel="nofollow" target="_blank">Github</a>.</p>

<p>If you want to know more about various <strong><em>Object Detection techniques, motion estimation, object tracking in video etc</em></strong>., I would like to recommend this awesome course on <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Deep Learning in Computer Vision</a> in the <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Advanced machine learning specialization</a>.</p>

<p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at <a href="https://medium.com/@rahul_agarwal" rel="nofollow" target="_blank"><strong>Medium</strong></a> or Subscribe to my <a href="http://eepurl.com/dbQnuX" rel="nofollow" target="_blank"><strong>blog</strong></a> to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter <a href="https://twitter.com/MLWhiz" rel="nofollow" target="_blank">@mlwhiz</a>.</p>

<p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.</p>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/statistics/" rel="tag">Statistics</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>

<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.372&subid=0&type=4" rel="nofollow"><IMG border="0"   alt="Start your future with a Data Science Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.372&subid=0&type=4&gridnum=16"></a>





























	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2019/12/05/od/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Demystifying Object Detection and Instance Segmentation for Data Scientists</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2019/12/07/streamlit/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">How to write Web apps using simple Python for Data Scientists?</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<style type="text/css">

  .btn {
    display: inline-block;
    font-weight: 400;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;
    border: 1px solid transparent;
    padding: .375rem .75rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: .25rem;
    transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.btn-session {
    color: #fff;
    background-color: #28a745;
    border-color: #28a745;
}
</style>


<aside class="sidebar">
  <div style="text-align:center">    
    
    <a class="btn btn-session" href="https://www.patreon.com/bePatron?u=28135435" role="button">1:1 Session</a>
	     
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/22/hyperspark/">100x faster Hyperparameter Search Framework with Pyspark</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/22/streamlitec2/">How to Deploy a Streamlit App using an Amazon Free ec2 instance?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/22/pandas_gpu/">Minimal Pandas Subset for Data Scientists on GPU</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/21/ds2020/">Become a Data Scientist in 2020 with these 10 resources</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/21/ci/">Confidence Intervals Explained Simply for Data Scientists</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/21/sql/">Learning SQL the Hard Way</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/02/20/swifter/">Add this single word to make your Pandas Apply faster</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/01/29/altr/">Handling Trees in Data Science Algorithmic Interview</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/01/28/ll/">A simple introduction to Linked Lists for Data Scientists</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/01/28/dp/">Dynamic Programming for Data Scientists</a></li>
		</ul>
	</div>
</div>


<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>


<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;}
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: center;
    padding: 10px -5px 10px 3%;}
   
</style>
<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2020 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>