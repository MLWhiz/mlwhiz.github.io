<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Using Gradient Boosting for Time Series prediction tasks - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="In this post, we will try to solve the time series problem using XGBoost."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Using Gradient Boosting for Time Series prediction tasks - MLWhiz"><meta property="og:description" content="In this post, we will try to solve the time series problem using XGBoost."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/12/28/timeseries/"><meta property="og:image" content="https://mlwhiz.com/images/timeseries/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/timeseries/main.png"><meta property="article:published_time" content="2019-12-28T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-08T22:10:24+01:00"><meta property="article:tag" content="Data Science"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/timeseries/main.png"><meta name=twitter:title content="Using Gradient Boosting for Time Series prediction tasks - MLWhiz"><meta name=twitter:description content="In this post, we will try to solve the time series problem using XGBoost."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/12/28/timeseries/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/12/28/timeseries/#primaryimage","url":"https://mlwhiz.com/images/timeseries/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/12/28/timeseries/#webpage","url":"https://mlwhiz.com/blog/2019/12/28/timeseries/","inLanguage":"en-US","name":"Using Gradient Boosting for Time Series prediction tasks - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/12/28/timeseries/#primaryimage"},"datePublished":"2019-12-28T00:00:00.00Z","dateModified":"2023-07-08T22:10:24.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"In this post, we will try to solve the time series problem using XGBoost."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/data-science class=categoryStyle>Data Science</a><h1>Using Gradient Boosting for Time Series prediction tasks</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>28 December 2019</span></div><img src=https://mlwhiz.com/images/timeseries/main.png class="img-fluid w-100 mb-4" alt="Using Gradient Boosting for Time Series prediction tasks"><div class="content mb-5"><p>Time series prediction problems are pretty frequent in the retail domain.</p><p>Companies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.</p><p>While there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.</p><p><em><strong>In this post, we will try to solve the time series problem using XGBoost.</strong></em></p><p><em><strong>The main things I am going to focus on are the sort of features such a setup takes and how to create such features.</strong></em></p><hr><h2 id=dataset>Dataset</h2><p><img src=https://cdn-images-1.medium.com/max/2000/0*gLUSm0_14D8A3NvR.jpg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Kaggle master Kazanova along with some of his friends released a
<a href=https://coursera.pxf.io/yRPoZB target=_blank rel="nofollow noopener">“How to win a data science competition”</a>
Coursera course. The Course involved a final project which itself was a time series prediction problem.</p><p>In this competition, we are given a challenging time-series dataset consisting of daily sales data, provided by one of the largest Russian software firms — 1C Company.</p><p>We have to predict total sales for every product and store in the next month.</p><p>Here is how the data looks like:</p><p><img src=https://cdn-images-1.medium.com/max/2072/1*hN1eF-iQzfTp6EGg3VBAlA.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We are given the data at a daily level, and we want to build a model which predicts total sales for every product and store in the next month.</p><p>The variable date_block_num is a consecutive month number, used for convenience. January 2013 is 0, and October 2015 is 33. You can think of it as a proxy to month variable. I think all the other variables are self-explanatory.</p><p><em><strong>So how do we approach this sort of a problem?</strong></em></p><hr><h2 id=data-preparation>Data Preparation</h2><p>The main thing that I noticed is that the data preparation and
<a href=https://towardsdatascience.com/the-hitchhikers-guide-to-feature-extraction-b4c157e96631 target=_blank rel="nofollow noopener">feature generation</a>
aspect is by far the most important thing when we attempt to solve the time series problem using regression.</p><h3 id=1-do-basic-eda-and-remove-outliers>1. Do Basic EDA and remove outliers</h3><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>sales = sales[sales[<span style=color:#ed9d13>&#39;item_price&#39;</span>]&lt;<span style=color:#3677a9>100000</span>]
sales = sales[sales[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]&lt;=<span style=color:#3677a9>1000</span>]
</code></pre></div><h3 id=2-group-data-at-a-level-you-want-your-predictions-to-be>2. Group data at a level you want your predictions to be:</h3><p>We start with creating a dataframe of distinct date_block_num, store and item combinations.</p><p>This is important because in the months we don’t have a data for an item store combination, the machine learning algorithm needs to be told explicitly that the sales are zero.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>itertools</span> <span style=color:#6ab825;font-weight:700>import</span> product
<span style=color:#999;font-style:italic># Create &#34;grid&#34; with columns</span>
index_cols = [<span style=color:#ed9d13>&#39;shop_id&#39;</span>, <span style=color:#ed9d13>&#39;item_id&#39;</span>, <span style=color:#ed9d13>&#39;date_block_num&#39;</span>]

<span style=color:#999;font-style:italic># For every month we create a grid from all shops/items combinations from that month</span>
grid = []
<span style=color:#6ab825;font-weight:700>for</span> block_num <span style=color:#6ab825;font-weight:700>in</span> sales[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>].unique():
    cur_shops = sales.loc[sales[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>] == block_num, <span style=color:#ed9d13>&#39;shop_id&#39;</span>].unique()
    cur_items = sales.loc[sales[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>] == block_num, <span style=color:#ed9d13>&#39;item_id&#39;</span>].unique()
    grid.append(np.array(<span style=color:#24909d>list</span>(product(*[cur_shops, cur_items, [block_num]])),dtype=<span style=color:#ed9d13>&#39;int32&#39;</span>))

grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)
grid.head()
</code></pre></div><p><img src=https://cdn-images-1.medium.com/max/2000/1*yDLbk-d9EbYV7EG38MXYeg.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>The grid dataFrame contains all the shop, items and month combinations.</p><p>We then merge the Grid with Sales to get the monthly sales DataFrame. We also replace all the NA’s with zero for months that didn’t have any sales.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>sales_m = sales.groupby([<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>]).agg({<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>: <span style=color:#ed9d13>&#39;sum&#39;</span>,<span style=color:#ed9d13>&#39;item_price&#39;</span>: np.mean}).reset_index()

<span style=color:#999;font-style:italic># Merging sales numbers with the grid dataframe</span>
sales_m = pd.merge(grid,sales_m,on=[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>],how=<span style=color:#ed9d13>&#39;left&#39;</span>).fillna(<span style=color:#3677a9>0</span>)

<span style=color:#999;font-style:italic># adding the category id too from the items table.</span>
sales_m = pd.merge(sales_m,items,on=[<span style=color:#ed9d13>&#39;item_id&#39;</span>],how=<span style=color:#ed9d13>&#39;left&#39;</span>)
</code></pre></div><p><img src=https://cdn-images-1.medium.com/max/3364/1*V_SzSZkoyGT7ce-FtPlLQw.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h3 id=3-create-target-encodings>3. Create Target Encodings</h3><p>To create target encodings, we group by a particular column and take the mean/min/sum etc. of the target column on it. These features are the first features we create in our model.</p><p><em><strong>Please note that these features may induce a lot of leakage/overfitting in our system and thus we don’t use them directly in our models. We will use the lag based version of these features in our models which we will create next.</strong></em></p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>groupcollist = [<span style=color:#ed9d13>&#39;item_id&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_category_id&#39;</span>]

aggregationlist = [(<span style=color:#ed9d13>&#39;item_price&#39;</span>,np.mean,<span style=color:#ed9d13>&#39;avg&#39;</span>),(<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>,np.sum,<span style=color:#ed9d13>&#39;sum&#39;</span>),(<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>,np.mean,<span style=color:#ed9d13>&#39;avg&#39;</span>)]

<span style=color:#6ab825;font-weight:700>for</span> type_id <span style=color:#6ab825;font-weight:700>in</span> groupcollist:
    <span style=color:#6ab825;font-weight:700>for</span> column_id,aggregator,aggtype <span style=color:#6ab825;font-weight:700>in</span> aggregationlist:
        <span style=color:#999;font-style:italic># get numbers from sales data and set column names</span>
        mean_df = sales_m.groupby([type_id,<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]).aggregate(aggregator).reset_index()[[column_id,type_id,<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]]
        mean_df.columns = [type_id+<span style=color:#ed9d13>&#39;_&#39;</span>+aggtype+<span style=color:#ed9d13>&#39;_&#39;</span>+column_id,type_id,<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]
        <span style=color:#999;font-style:italic># merge new columns on sales_m data</span>
        sales_m = pd.merge(sales_m,mean_df,on=[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,type_id],how=<span style=color:#ed9d13>&#39;left&#39;</span>)
</code></pre></div><p>We group by item_id, shop_id, and item_category_id and aggregate on the item_price and item_cnt_day column to create the following new features:</p><p><img src=https://cdn-images-1.medium.com/max/2804/1*TNJdVv0Bka75S5QKHV0D-w.png alt="We create the highlighted target encodings"></p><p>We could also have used
<a href=https://towardsdatascience.com/the-hitchhikers-guide-to-feature-extraction-b4c157e96631 target=_blank rel="nofollow noopener">featuretools</a>
for this. <strong>Featuretools</strong> is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.</p><hr><h3 id=4-create-lag-features>4. Create Lag Features</h3><p>The next set of features our model needs are the lag based Features.</p><p>When we create regular classification models, we treat training examples as fairly independent of each other. But in case of time series problems, at any point in time, the model needs information on what happened in the past.</p><p>We can’t do this for all the past days, but we can provide the models with the most recent information nonetheless using our target encoded features.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>
lag_variables  = [<span style=color:#ed9d13>&#39;item_id_avg_item_price&#39;</span>,<span style=color:#ed9d13>&#39;item_id_sum_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;item_id_avg_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;shop_id_avg_item_price&#39;</span>,<span style=color:#ed9d13>&#39;shop_id_sum_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;shop_id_avg_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;item_category_id_avg_item_price&#39;</span>,<span style=color:#ed9d13>&#39;item_category_id_sum_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;item_category_id_avg_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]
lags = [<span style=color:#3677a9>1</span> ,<span style=color:#3677a9>2</span> ,<span style=color:#3677a9>3</span> ,<span style=color:#3677a9>4</span>, <span style=color:#3677a9>5</span>, <span style=color:#3677a9>12</span>]
<span style=color:#999;font-style:italic># we will keep the results in thsi dataframe</span>
sales_means = sales_m.copy()
<span style=color:#6ab825;font-weight:700>for</span> lag <span style=color:#6ab825;font-weight:700>in</span> lags:
    sales_new_df = sales_m.copy()
    sales_new_df.date_block_num+=lag
    <span style=color:#999;font-style:italic># subset only the lag variables we want</span>
    sales_new_df = sales_new_df[[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>]+lag_variables]
    sales_new_df.columns = [<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>]+ [lag_feat+<span style=color:#ed9d13>&#39;_lag_&#39;</span>+<span style=color:#24909d>str</span>(lag) <span style=color:#6ab825;font-weight:700>for</span> lag_feat <span style=color:#6ab825;font-weight:700>in</span> lag_variables]
    <span style=color:#999;font-style:italic># join with date_block_num,shop_id and item_id</span>
    sales_means = pd.merge(sales_means, sales_new_df,on=[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>,<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>] ,how=<span style=color:#ed9d13>&#39;left&#39;</span>)
</code></pre></div><p>So we aim to add past information for a few features in our data. We do it for all the new features we created and the item_cnt_day feature.</p><p>We fill the NA’s with zeros once we have the lag features.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>for</span> feat <span style=color:#6ab825;font-weight:700>in</span> sales_means.columns:
    <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#ed9d13>&#39;item_cnt&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> feat:
        sales_means[feat]=sales_means[feat].fillna(<span style=color:#3677a9>0</span>)
    <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#39;item_price&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> feat:
    sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median())
</code></pre></div><p>We end up creating a lot of lag features with different lags:</p><pre><code>'item_id_avg_item_price_lag_1','item_id_sum_item_cnt_day_lag_1', 'item_id_avg_item_cnt_day_lag_1','shop_id_avg_item_price_lag_1', 'shop_id_sum_item_cnt_day_lag_1','shop_id_avg_item_cnt_day_lag_1','item_category_id_avg_item_price_lag_1','item_category_id_sum_item_cnt_day_lag_1','item_category_id_avg_item_cnt_day_lag_1', 'item_cnt_day_lag_1',

'item_id_avg_item_price_lag_2', 'item_id_sum_item_cnt_day_lag_2','item_id_avg_item_cnt_day_lag_2', 'shop_id_avg_item_price_lag_2','shop_id_sum_item_cnt_day_lag_2', 'shop_id_avg_item_cnt_day_lag_2','item_category_id_avg_item_price_lag_2','item_category_id_sum_item_cnt_day_lag_2','item_category_id_avg_item_cnt_day_lag_2', 'item_cnt_day_lag_2',

...
</code></pre><hr><h2 id=modelling>Modelling</h2><h3 id=1-drop-the-unrequired-columns>1. Drop the unrequired columns</h3><p>As previously said, we are going to drop the target encoded features as they might induce a lot of overfitting in the model. We also lose the item_name and item_price feature.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>
cols_to_drop = lag_variables[:-<span style=color:#3677a9>1</span>] + [<span style=color:#ed9d13>&#39;item_name&#39;</span>,<span style=color:#ed9d13>&#39;item_price&#39;</span>]

<span style=color:#6ab825;font-weight:700>for</span> col <span style=color:#6ab825;font-weight:700>in</span> cols_to_drop:
    <span style=color:#6ab825;font-weight:700>del</span> sales_means[col]
</code></pre></div><h3 id=2-take-a-recent-bit-of-data-only>2. Take a recent bit of data only</h3><p>When we created the lag variables, we induced a lot of zeroes in the system. We used the maximum lag as 12. To counter that we remove the first 12 months indexes.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>sales_means = sales_means[sales_means[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]&gt;<span style=color:#3677a9>11</span>]
</code></pre></div><h3 id=3-train-and-cv-split>3. Train and CV Split</h3><p>When we do a time series split, we usually don’t take a cross-sectional split as the data is time-dependent. We want to create a model that sees till now and can predict the next month well.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>X_train = sales_means[sales_means[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]&lt;<span style=color:#3677a9>33</span>]
X_cv =  sales_means[sales_means[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]==<span style=color:#3677a9>33</span>]

Y_train = X_train[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]
Y_cv = X_cv[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]

<span style=color:#6ab825;font-weight:700>del</span> X_train[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]
<span style=color:#6ab825;font-weight:700>del</span> X_cv[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]
</code></pre></div><h3 id=4-create-baseline>4. Create Baseline</h3><p><img src=https://cdn-images-1.medium.com/max/2000/0*Ujr-irYHlXd0SAGP.jpg alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Before we proceed with modelling steps, lets check the RMSE of a naive model, as we want to
<a href=https://towardsdatascience.com/take-your-machine-learning-models-to-production-with-these-5-simple-steps-35aa55e3a43c target=_blank rel="nofollow noopener">have an RMSE to compare</a>
to. We assume that we are going to predict the last month sales as current month sale for our baseline model. We can quantify the performance of our model using this baseline RMSE.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>sklearn.metrics</span> <span style=color:#6ab825;font-weight:700>import</span> mean_squared_error
sales_m_test = sales_m[sales_m[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]==<span style=color:#3677a9>33</span>]

preds = sales_m.copy()
preds[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]=preds[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]+<span style=color:#3677a9>1</span>
preds = preds[preds[<span style=color:#ed9d13>&#39;date_block_num&#39;</span>]==<span style=color:#3677a9>33</span>]
preds = preds.rename(columns={<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>:<span style=color:#ed9d13>&#39;preds_item_cnt_day&#39;</span>})
preds = pd.merge(sales_m_test,preds,on = [<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>],how=<span style=color:#ed9d13>&#39;left&#39;</span>)[[<span style=color:#ed9d13>&#39;shop_id&#39;</span>,<span style=color:#ed9d13>&#39;item_id&#39;</span>,<span style=color:#ed9d13>&#39;preds_item_cnt_day&#39;</span>,<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>]].fillna(<span style=color:#3677a9>0</span>)

<span style=color:#999;font-style:italic># We want our predictions clipped at (0,20). Competition Specific</span>
preds[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>] = preds[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>].clip(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>20</span>)
preds[<span style=color:#ed9d13>&#39;preds_item_cnt_day&#39;</span>] = preds[<span style=color:#ed9d13>&#39;preds_item_cnt_day&#39;</span>].clip(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>20</span>)
baseline_rmse = np.sqrt(mean_squared_error(preds[<span style=color:#ed9d13>&#39;item_cnt_day&#39;</span>],preds[<span style=color:#ed9d13>&#39;preds_item_cnt_day&#39;</span>]))

<span style=color:#6ab825;font-weight:700>print</span>(baseline_rmse)
</code></pre></div><pre><code>1.1358170090812756
</code></pre><hr><h3 id=5-train-xgb>5. Train XGB</h3><p>We use the XGBRegressor object from the xgboost scikit API to build our model. Parameters are taken from this
<a href=https://www.kaggle.com/dlarionov/feature-engineering-xgboost target=_blank rel="nofollow noopener">kaggle kernel</a>
. If you have time, you can use hyperopt to
<a href=https://towardsdatascience.com/automate-hyperparameter-tuning-for-your-models-71b18f819604 target=_blank rel="nofollow noopener">automatically find out the hyperparameters</a>
yourself.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>xgboost</span> <span style=color:#6ab825;font-weight:700>import</span> XGBRegressor

model = XGBRegressor(
    max_depth=<span style=color:#3677a9>8</span>,
    n_estimators=<span style=color:#3677a9>1000</span>,
    min_child_weight=<span style=color:#3677a9>300</span>,
    colsample_bytree=<span style=color:#3677a9>0.8</span>,
    subsample=<span style=color:#3677a9>0.8</span>,
    eta=<span style=color:#3677a9>0.3</span>,    
    seed=<span style=color:#3677a9>42</span>)

model.fit(
    X_train,
    Y_train,
    eval_metric=<span style=color:#ed9d13>&#34;rmse&#34;</span>,
    eval_set=[(X_train, Y_train), (X_cv, Y_cv)],
    verbose=True,
    early_stopping_rounds = <span style=color:#3677a9>10</span>)
</code></pre></div><p><img src=https://cdn-images-1.medium.com/max/2288/1*uK9IjFKLEWPGbbeDLc2hfg.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>After running this, we can see RMSE in ranges of <em><strong>0.93</strong></em> on the CV set. And that is pretty impressive based on our baseline validation RMSE of <em><strong>1.13</strong></em>. And so we work on deploying this model as part of our
<a href=https://towardsdatascience.com/take-your-machine-learning-models-to-production-with-these-5-simple-steps-35aa55e3a43c target=_blank rel="nofollow noopener">continuous integration</a>
effort.</p><h3 id=5-plot-feature-importance>5. Plot Feature Importance</h3><p>We can also see the important features that come from XGB.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>feature_importances = pd.DataFrame({<span style=color:#ed9d13>&#39;col&#39;</span>: columns,<span style=color:#ed9d13>&#39;imp&#39;</span>:model.feature_importances_})
feature_importances = feature_importances.sort_values(by=<span style=color:#ed9d13>&#39;imp&#39;</span>,ascending=False)
px.bar(feature_importances,x=<span style=color:#ed9d13>&#39;col&#39;</span>,y=<span style=color:#ed9d13>&#39;imp&#39;</span>)
</code></pre></div><p><img src=https://cdn-images-1.medium.com/max/3716/1*TZ_BawTl6O1kMuTUTMYoHw.png alt="Feature importances"></p><hr><h2 id=conclusion>Conclusion</h2><p>In this post, we talked about how we can use trees for even time series modelling. The purpose was not to get perfect scores on the kaggle leaderboard but to gain an understanding of how such models work.</p><p><img src=https://cdn-images-1.medium.com/max/3004/0*vsyzeBzrG4q4Z33z.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>When I took part in this competition as part of the
<a href=https://coursera.pxf.io/yRPoZB target=_blank rel="nofollow noopener">course</a>
, a couple of years back, using trees I reached near the top of the leaderboard.</p><p>Over time people have worked a lot on tweaking the model, hyperparameter tuning and creating even more informative features. But the basic approach has remained the same.</p><p>You can find the whole running code on
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/time_series_xgb target=_blank rel="nofollow noopener">GitHub</a>
.</p><p>Take a look at the
<a href=https://coursera.pxf.io/yRPoZB target=_blank rel="nofollow noopener">How to Win a Data Science Competition: Learn from Top Kagglers</a>
course in the
<a href=https://coursera.pxf.io/yRPoZB target=_blank rel="nofollow noopener">Advanced machine learning specialization</a>
by Kazanova. This course talks about a lot of ways to improve your models using feature engineering and hyperparameter tuning.</p><p>I am going to be writing more beginner-friendly posts in the future too. Let me know what you think about the series. Follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>