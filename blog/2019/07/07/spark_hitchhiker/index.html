<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.So I thought of giving it a shot. This post is going to be about How to make Spark work?"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="The Hitchhikers guide to handle Big Data using Spark"><meta property="og:description" content="Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.So I thought of giving it a shot. This post is going to be about How to make Spark work?"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/"><meta property="og:image" content="https://mlwhiz.com/images/spark/spark.jpeg"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/spark/spark.jpeg"><meta property="article:published_time" content="2019-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Big Data"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/spark/spark.jpeg"><meta name=twitter:title content="The Hitchhikers guide to handle Big Data using Spark"><meta name=twitter:description content="Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.So I thought of giving it a shot. This post is going to be about How to make Spark work?"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/#primaryimage","url":"https://mlwhiz.com/images/spark/spark.jpeg","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/#webpage","url":"https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/","inLanguage":"en-US","name":"The Hitchhikers guide to handle Big Data using Spark - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/#primaryimage"},"datePublished":"2019-07-07T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.So I thought of giving it a shot. This post is going to be about How to make Spark work?"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/big-data class=categoryStyle>Big Data</a>
<a href=/categories/data-science class=categoryStyle>Data Science</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>The Hitchhikers guide to handle Big Data using Spark</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>07 July 2019</span></div><img src=https://mlwhiz.com/images/spark/spark.jpeg class="img-fluid w-100 mb-4" alt="The Hitchhikers guide to handle Big Data using Spark"><div class="content mb-5"><p>Big Data has become synonymous with Data engineering.</p><p>But the line between Data Engineering and Data scientists is blurring day by day.</p><p>At this point in time, I think that Big Data must be in the repertoire of all data scientists.</p><p>Reason: <em><strong>Too much data is getting generated day by day</strong></em></p><p>And that brings us to Spark.</p><p>Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.</p><p>So I thought of giving it a shot.</p><p><strong>This post is going to be about — “How to make Spark work?”</strong></p><p>This post is going to be quite long. Actually my longest post on medium, so go pick up a Coffee.</p><h2 id=how-it-all-started-mapreduce>How it all started?-MapReduce</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/tree.jpeg "></center></div><p><em><strong>Suppose you are tasked with cutting all the trees in the forest.</strong></em> Perhaps not a good business with all the global warming, but here it serves our purpose and we are talking hypothetically, so I will continue. You have two options:</p><ul><li><p><em>Get Batista with an electric powered chainsaw</em> to do your work and make him cut each tree one by one.</p></li><li><p><em>Get 500 normal guys with normal axes</em> and make them work on different trees.</p></li></ul><p><em><strong>Which would you prefer?</strong></em></p><p>Although Option 1 is still the way some people would go, the need for option 2 led to the emergence of MapReduce.</p><p>In Bigdata speak, we call the Batista solution as scaling <em><strong>vertically/scaling-up</strong></em>as in we add/stuff a lot of RAM and hard disk in a single worker.</p><p>And the second solution is called scaling <em><strong>horizontally/scaling-sideways</strong></em>. As in you connect a lot of ordinary machines(with less RAM) together and use them in parallel.</p><p>Now, vertical scaling has certain benefits over Horizontal scaling:</p><ul><li><p><strong>It is fast if the size of the problem is small:</strong> Think 2 trees. Batista would be through with both of them with his awesome chainsaw while our two guys would be still hacking with their axes.</p></li><li><p><strong>It is easy to understand.</strong> This is how we have always done things. We normally think about things in a sequential pattern and that is how our whole computer architecture and design has evolved.</p></li></ul><p>But, Horizontal Scaling is</p><ul><li><p><strong>Less Expensive:</strong> Getting 50 normal guys itself is much cheaper than getting a single guy like Batista. Apart from that Batista needs a lot of care and maintenance to keep him cool and he is very sensitive to even small things just like machines with a high amount of RAM.</p></li><li><p><strong>Faster when the size of the problem is big:</strong> Now imagine 1000 trees and 1000 workers vs a single Batista. With Horizontal Scaling, if we face a very large problem we will just hire 100 or maybe 1000 more cheap workers. It doesn’t work like that with Batista. You have to increase RAM and that means more cooling infrastructure and more maintenance costs.</p></li></ul><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/mpared.png "></center></div><p><em><strong>MapReduce</strong></em> is what makes the second option possible by letting us use a <em><strong>cluster of computers</strong></em> for parallelization.</p><p>Now, MapReduce looks like a fairly technical term. But let us break it a little. MapReduce is made up of two terms:</p><h3 id=map>Map:</h3><p>It is basically the apply/map function. We split our data into n chunks and send each chunk to a different worker(Mapper). If there is any function we would like to apply over the rows of Data our worker does that.</p><h3 id=reduce>Reduce:</h3><p>Aggregate the data using some function based on a groupby key. It is basically a groupby.</p><p>Of course, there is a lot going in the background to make the system work as intended.</p><p>Don’t worry, if you don’t understand it yet. Just keep reading. Maybe you will understand it when we use MapReduce ourselves in the examples I am going to provide.</p><hr><h2 id=why-spark>Why Spark?</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/pyspark.png "></center></div><p>Hadoop was the first open source system that introduced us to the MapReduce paradigm of programming and Spark is the system that made it faster, much much faster(100x).</p><p>There used to be a lot of data movement in Hadoop as it used to write intermediate results to the file system.</p><p>This affected the speed at which you could do analysis.</p><p>Spark provided us with an in-memory model, so Spark doesn’t write too much to the disk while working.</p><p>Simply, Spark is faster than Hadoop and a lot of people use Spark now.</p><p><em><strong>So without further ado let us get started.</strong></em></p><hr><h2 id=getting-started-with-spark>Getting Started with Spark</h2><p>Installing Spark is actually a headache of its own.</p><p>Since we want to understand how it works and really work with it, I would suggest that you use Sparks on Databricks
<a href="https://databricks.com/try-databricks?utm_source=databricks&utm_medium=homev2tiletest" target=_blank rel="nofollow noopener">here</a>
online with the community edition. Don’t worry it is free.</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/db.png "></center></div><p>Once you register and login will be presented with the following screen.</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/db2.png "></center></div><p>You can start a new notebook here.</p><p>Select the Python notebook and give any name to your notebook.</p><p>Once you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.</p><p>The next step will be to check if the sparkcontext is present. To check if the sparkcontext is present you just have to run this command:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>sc
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/scres.png "></center></div><p>This means that we are set up with a notebook where we can run Spark.</p><hr><h2 id=load-some-data>Load Some Data</h2><p>The next step is to upload some data we will use to learn Spark. Just click on ‘Import and Explore Data’ on the home tab.</p><p>I will end up using multiple datasets by the end of this post but let us start with something very simple.</p><p>Let us add the file <code>shakespeare.txt</code> which you can download from
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post target=_blank rel="nofollow noopener">here</a>
.</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/load.png "></center></div><p>You can see that the file is loaded to <code>/FileStore/tables/shakespeare.txt</code> location.</p><hr><h2 id=our-first-spark-program>Our First Spark Program</h2><p>I like to learn by examples so let’s get done with the “Hello World” of Distributed computing: <em><strong>The WordCount Program.</strong></em></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Distribute the data - Create a RDD</span>
lines <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;/FileStore/tables/shakespeare.txt&#34;</span>)

<span style=color:#75715e># Create a list with all words, Create tuple (word,1), reduce by key i.e. the word</span>
counts <span style=color:#f92672>=</span> (lines<span style=color:#f92672>.</span>flatMap(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; &#39;</span>))          
                  <span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: (x, <span style=color:#ae81ff>1</span>))                 
                  <span style=color:#f92672>.</span>reduceByKey(<span style=color:#66d9ef>lambda</span> x,y : x <span style=color:#f92672>+</span> y))

<span style=color:#75715e># get the output on local</span>
output <span style=color:#f92672>=</span> counts<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>10</span>)                                 
<span style=color:#75715e># print output</span>
<span style=color:#66d9ef>for</span> (word, count) <span style=color:#f92672>in</span> output:                             
    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>: </span><span style=color:#e6db74>%i</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> (word, count))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res1.png "></center></div><p>So that is a small example which counts the number of words in the document and prints 10 of them.</p><p>And most of the work gets done in the second command.</p><p>Don’t worry if you are not able to follow this yet as I still need to tell you about the things that make Spark work.</p><p>But before we get into Spark basics, Let us refresh some of our Python Basics. Understanding Spark becomes a lot easier if you have used functional programming with Python.</p><p>For those of you who haven’t used it, below is a brief intro.</p><hr><h2 id=a-functional-approach-to-programming-in-python>A functional approach to programming in Python</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/func.png "></center></div><h3 id=1-map>1. Map</h3><p>map is used to map a function to an array or a list. Say you want to apply some function to every element in a list.</p><p>You can do this by simply using a for loop but python lambda functions let you do this in a single line in Python.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>my_list <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
<span style=color:#75715e># Lets say I want to square each term in my_list.</span>
squared_list <span style=color:#f92672>=</span> map(<span style=color:#66d9ef>lambda</span> x:x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>,my_list)
<span style=color:#66d9ef>print</span>(list(squared_list))
<span style=color:#f92672>------------------------------------------------------------</span>
[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>36</span>, <span style=color:#ae81ff>49</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>81</span>, <span style=color:#ae81ff>100</span>]
</code></pre></div><p>In the above example, you could think of <code>map</code> as a function which takes two arguments — A function and a list.</p><p>It then applies the function to every element of the list.</p><p>What lambda allows you to do is write an inline function. In here the part <strong><code>lambda x:x**2</code></strong> defines a function that takes x as input and returns x².</p><p>You could have also provided a proper function in place of lambda. For example:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared</span>(x):
    <span style=color:#66d9ef>return</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>

my_list <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
<span style=color:#75715e># Lets say I want to square each term in my_list.</span>
squared_list <span style=color:#f92672>=</span> map(squared,my_list)
<span style=color:#66d9ef>print</span>(list(squared_list))
<span style=color:#f92672>------------------------------------------------------------</span>
[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>36</span>, <span style=color:#ae81ff>49</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>81</span>, <span style=color:#ae81ff>100</span>]
</code></pre></div><p>The same result, but the lambda expressions make the code compact and a lot more readable.</p><h3 id=2-filter>2. Filter</h3><p>The other function that is used extensively is the <code>filter</code> function. This function takes two arguments — A condition and the list to filter.</p><p>If you want to filter your list using some condition you use <code>filter</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>my_list <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
<span style=color:#75715e># Lets say I want only the even numbers in my list.</span>
filtered_list <span style=color:#f92672>=</span> filter(<span style=color:#66d9ef>lambda</span> x:x<span style=color:#f92672>%</span><span style=color:#ae81ff>2</span><span style=color:#f92672>==</span><span style=color:#ae81ff>0</span>,my_list)
<span style=color:#66d9ef>print</span>(list(filtered_list))
<span style=color:#f92672>---------------------------------------------------------------</span>
[<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>10</span>]
</code></pre></div><h3 id=3-reduce>3. Reduce</h3><p>The next function I want to talk about is the reduce function. This function will be the workhorse in Spark.</p><p>This function takes two arguments — a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>import</span> functools
my_list <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]
<span style=color:#75715e># Lets say I want to sum all elements in my list.</span>
sum_list <span style=color:#f92672>=</span> functools<span style=color:#f92672>.</span>reduce(<span style=color:#66d9ef>lambda</span> x,y:x<span style=color:#f92672>+</span>y,my_list)
<span style=color:#66d9ef>print</span>(sum_list)
</code></pre></div><p>In python2 reduce used to be a part of Python, now we have to use <code>reduce</code> as a part of <code>functools</code>.</p><p>Here the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:</p><pre><code>    Reduce function first sends 1,2    ; the lambda function returns 3
    Reduce function then sends 3,3     ; the lambda function returns 6
    Reduce function then sends 6,4     ; the lambda function returns 10
    Reduce function finally sends 10,5 ; the lambda function returns 15
</code></pre><p>A condition on the lambda function we use in reduce is that it must be:</p><ul><li><p>commutative that is a + b = b + a and</p></li><li><p>associative that is (a + b) + c == a + (b + c).</p></li></ul><p>In the above case, we used sum which is <strong>commutative as well as associative</strong>. Other functions that we could have used: <code>max</code>, <code>min</code>, <code>*</code> etc.</p><hr><h2 id=moving-again-to-spark>Moving Again to Spark</h2><p>As we have now got the fundamentals of Python Functional Programming out of the way, lets again head to Spark.</p><p>But first, let us delve a little bit into how spark works. Spark actually consists of two things a driver and workers.</p><p>Workers normally do all the work and the driver makes them do that work.</p><h3 id=rdd>RDD</h3><p>An RDD(Resilient Distributed Dataset) is a parallelized data structure that gets distributed across the worker nodes. They are the basic units of Spark programming.</p><p>In our wordcount example, in the first line</p><pre><code>lines = sc.textFile(&quot;/FileStore/tables/shakespeare.txt&quot;)
</code></pre><p>We took a text file and distributed it across worker nodes so that they can work on it in parallel. We could also parallelize lists using the function <code>sc.parallelize</code></p><p>For example:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
new_rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
new_rdd
<span style=color:#f92672>---------------------------------------------------------------</span>
ParallelCollectionRDD[<span style=color:#ae81ff>22</span>] at parallelize at PythonRDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>267</span>
</code></pre></div><p>In Spark, we can do two different types of operations on RDD: Transformations and Actions.</p><ol><li><p><strong>Transformations:</strong> Create new datasets from existing RDDs</p></li><li><p><strong>Actions:</strong> Mechanism to get results out of Spark</p></li></ol><hr><h2 id=transformation-basics>Transformation Basics</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/trans.png "></center></div><p>So let us say you have got your data in the form of an RDD.</p><p>To requote your data is now accessible to the worker machines. You want to do some transformations on the data now.</p><p>You may want to filter, apply some function, etc.</p><p>In Spark, this is done using Transformation functions.</p><p>Spark provides many transformation functions. You can see a comprehensive list
<a href=http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations target=_blank rel="nofollow noopener">&lt;strong>here&lt;/strong></a>
. Some of the main ones that I use frequently are:</p><h3 id=1-map-1>1. Map:</h3><p>Applies a given function to an RDD.</p><p>Note that the syntax is a little bit different from Python, but it necessarily does the same thing. Don’t worry about <code>collect</code> yet. For now, just think of it as a function that collects the data in squared_rdd back to a list.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
squared_rdd <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x:x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)
squared_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>------------------------------------------------------</span>
[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>36</span>, <span style=color:#ae81ff>49</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>81</span>, <span style=color:#ae81ff>100</span>]
</code></pre></div><h3 id=2-filter-1>2. Filter:</h3><p>Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
filtered_rdd <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>filter(<span style=color:#66d9ef>lambda</span> x:x<span style=color:#f92672>%</span><span style=color:#ae81ff>2</span><span style=color:#f92672>==</span><span style=color:#ae81ff>0</span>)
filtered_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>------------------------------------------------------</span>
[<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>10</span>]
</code></pre></div><h3 id=3-distinct>3. distinct:</h3><p>Returns only distinct elements in an RDD.</p><pre><code>data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]
rdd = sc.parallelize(data,4)
distinct_rdd = rdd.distinct()
distinct_rdd.collect()
------------------------------------------------------
[8, 4, 1, 5, 9, 2, 10, 6, 3, 7]
</code></pre><h3 id=4-flatmap>4. flatmap:</h3><p>Similar to <code>map</code>, but each input item can be mapped to 0 or more output items.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
flat_rdd <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>flatMap(<span style=color:#66d9ef>lambda</span> x:[x,x<span style=color:#f92672>**</span><span style=color:#ae81ff>3</span>])
flat_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>------------------------------------------------------</span>
[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>64</span>]
</code></pre></div><h3 id=5-reduce-by-key>5. Reduce By Key:</h3><p>The parallel to the reduce in Hadoop MapReduce.</p><p>Now Spark cannot provide the value if it just worked with Lists.</p><p>In Spark, there is a concept of pair RDDs that makes it a lot more flexible. Let&rsquo;s assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [(<span style=color:#e6db74>&#39;Apple&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>200</span>),(<span style=color:#e6db74>&#39;Banana&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>24</span>),(<span style=color:#e6db74>&#39;Tomato&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>56</span>),(<span style=color:#e6db74>&#39;Potato&#39;</span>,<span style=color:#e6db74>&#39;Vegetable&#39;</span>,<span style=color:#ae81ff>103</span>),(<span style=color:#e6db74>&#39;Carrot&#39;</span>,<span style=color:#e6db74>&#39;Vegetable&#39;</span>,<span style=color:#ae81ff>34</span>)]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
</code></pre></div><p>Right now our RDD <code>rdd</code> holds tuples.</p><p>Now we want to find out the total sum of revenue that we got from each category.</p><p>To do that we have to transform our <code>rdd</code> to a pair rdd so that it only contains key-value pairs/tuples.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>category_price_rdd <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>1</span>],x[<span style=color:#ae81ff>2</span>]))
category_price_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>-----------------------------------------------------------------</span>
[(<span style=color:#960050;background-color:#1e0010>‘</span>Fruit<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>200</span>), (<span style=color:#960050;background-color:#1e0010>‘</span>Fruit<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>24</span>), (<span style=color:#960050;background-color:#1e0010>‘</span>Fruit<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>56</span>), (<span style=color:#960050;background-color:#1e0010>‘</span>Vegetable<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>103</span>), (<span style=color:#960050;background-color:#1e0010>‘</span>Vegetable<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>34</span>)]
</code></pre></div><p>Here we used the map function to get it in the format we wanted. When working with textfile, the RDD that gets formed has got a lot of strings. We use <code>map</code> to convert it into a format that we want.</p><p>So now our <code>category_price_rdd</code> contains the product category and the price at which the product sold.</p><p>Now we want to reduce on the key category and sum the prices. We can do this by:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>category_total_price_rdd <span style=color:#f92672>=</span> category_price_rdd<span style=color:#f92672>.</span>reduceByKey(<span style=color:#66d9ef>lambda</span> x,y:x<span style=color:#f92672>+</span>y)
category_total_price_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>---------------------------------------------------------</span>
[(<span style=color:#960050;background-color:#1e0010>‘</span>Vegetable<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>137</span>), (<span style=color:#960050;background-color:#1e0010>‘</span>Fruit<span style=color:#960050;background-color:#1e0010>’</span>, <span style=color:#ae81ff>280</span>)]
</code></pre></div><h3 id=6-group-by-key>6. Group By Key:</h3><p>Similar to <code>reduceByKey</code> but does not reduces just puts all the elements in an iterator. For example, if we wanted to keep as key the category and as the value all the products we would use this function.</p><p>Let us again use <code>map</code> to get data in the required form.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>data <span style=color:#f92672>=</span> [(<span style=color:#e6db74>&#39;Apple&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>200</span>),(<span style=color:#e6db74>&#39;Banana&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>24</span>),(<span style=color:#e6db74>&#39;Tomato&#39;</span>,<span style=color:#e6db74>&#39;Fruit&#39;</span>,<span style=color:#ae81ff>56</span>),(<span style=color:#e6db74>&#39;Potato&#39;</span>,<span style=color:#e6db74>&#39;Vegetable&#39;</span>,<span style=color:#ae81ff>103</span>),(<span style=color:#e6db74>&#39;Carrot&#39;</span>,<span style=color:#e6db74>&#39;Vegetable&#39;</span>,<span style=color:#ae81ff>34</span>)]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data,<span style=color:#ae81ff>4</span>)
category_product_rdd <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>1</span>],x[<span style=color:#ae81ff>0</span>]))
category_product_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#f92672>------------------------------------------------------------</span>
[(<span style=color:#e6db74>&#39;Fruit&#39;</span>, <span style=color:#e6db74>&#39;Apple&#39;</span>),  (<span style=color:#e6db74>&#39;Fruit&#39;</span>, <span style=color:#e6db74>&#39;Banana&#39;</span>),  (<span style=color:#e6db74>&#39;Fruit&#39;</span>, <span style=color:#e6db74>&#39;Tomato&#39;</span>),  (<span style=color:#e6db74>&#39;Vegetable&#39;</span>, <span style=color:#e6db74>&#39;Potato&#39;</span>),  (<span style=color:#e6db74>&#39;Vegetable&#39;</span>, <span style=color:#e6db74>&#39;Carrot&#39;</span>)]
</code></pre></div><p>We then use groupByKey as:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>grouped_products_by_category_rdd <span style=color:#f92672>=</span> category_product_rdd<span style=color:#f92672>.</span>groupByKey()
findata <span style=color:#f92672>=</span> grouped_products_by_category_rdd<span style=color:#f92672>.</span>collect()
<span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> findata:
    <span style=color:#66d9ef>print</span>(data[<span style=color:#ae81ff>0</span>],list(data[<span style=color:#ae81ff>1</span>]))
<span style=color:#f92672>------------------------------------------------------------</span>
Vegetable [<span style=color:#e6db74>&#39;Potato&#39;</span>, <span style=color:#e6db74>&#39;Carrot&#39;</span>]
Fruit [<span style=color:#e6db74>&#39;Apple&#39;</span>, <span style=color:#e6db74>&#39;Banana&#39;</span>, <span style=color:#e6db74>&#39;Tomato&#39;</span>]
</code></pre></div><p>Here the <code>groupByKey</code> function worked and it returned the category and the list of products in that category.</p><hr><h2 id=action-basics>Action Basics</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/action.png "></center></div><p>You have filtered your data, mapped some functions on it. Done your computation.</p><p>Now you want to get the data on your local machine or save it to a file or show the results in the form of some graphs in excel or any visualization tool.</p><p>You will need actions for that. A comprehensive list of actions is provided
<a href=http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions target=_blank rel="nofollow noopener">&lt;strong>here&lt;/strong></a>
.</p><p>Some of the most common actions that I tend to use are:</p><h3 id=1-collect>1. collect:</h3><p>We have already used this action many times. It takes the whole RDD and brings it back to the driver program.</p><h3 id=2-reduce>2. reduce:</h3><p>Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>])
rdd<span style=color:#f92672>.</span>reduce(<span style=color:#66d9ef>lambda</span> x,y : x<span style=color:#f92672>+</span>y)
<span style=color:#f92672>---------------------------------</span>
<span style=color:#ae81ff>15</span>
</code></pre></div><h3 id=3-take>3. take:</h3><p>Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. <code>take</code> returns a list with the first n elements of the RDD.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>])
rdd<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>3</span>)
<span style=color:#f92672>---------------------------------</span>
[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>]
</code></pre></div><h3 id=4-takeordered>4. takeOrdered:</h3><p><code>takeOrdered</code> returns the first n elements of the RDD using either their natural order or a custom comparator.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>23</span>])

<span style=color:#75715e># descending order</span>
rdd<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>3</span>,<span style=color:#66d9ef>lambda</span> s:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>*</span>s)
<span style=color:#f92672>----</span>
[<span style=color:#ae81ff>23</span>, <span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>5</span>]

rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([(<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>23</span>),(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>34</span>),(<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>344</span>),(<span style=color:#ae81ff>23</span>,<span style=color:#ae81ff>29</span>)])

<span style=color:#75715e># descending order</span>
rdd<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>3</span>,<span style=color:#66d9ef>lambda</span> s:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>*</span>s[<span style=color:#ae81ff>1</span>])
<span style=color:#f92672>---</span>
[(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>344</span>), (<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>34</span>), (<span style=color:#ae81ff>23</span>, <span style=color:#ae81ff>29</span>)]
</code></pre></div><p>We have our basics covered finally. Let us get back to our wordcount example</p><hr><h2 id=understanding-the-wordcount-example>Understanding The WordCount Example</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/words.jpeg "></center></div><p>Now we sort of understand the transformations and the actions provided to us by Spark.</p><p>It should not be difficult to understand the wordcount program now. Let us go through the program line by line.</p><p>The first line creates an RDD and distributes it to the workers.</p><pre><code>lines = sc.textFile(&quot;/FileStore/tables/shakespeare.txt&quot;)
</code></pre><p>This RDD lines contains a list of sentences in the file. You can see the rdd content using take</p><pre><code>lines.take(5)
--------------------------------------------
['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ',  'William Shakespeare',  '',  'This eBook is for the use of anyone anywhere at no cost and with',  'almost no restrictions whatsoever.  You may copy it, give it away or']
</code></pre><p>This RDD is of the form:</p><pre><code>['word1 word2 word3','word4 word3 word2']
</code></pre><p>This next line is actually the workhorse function in the whole script.</p><pre><code>counts = (lines.flatMap(lambda x: x.split(' '))          
                  .map(lambda x: (x, 1))                 
                  .reduceByKey(lambda x,y : x + y))
</code></pre><p>It contains a series of transformations that we do to the lines RDD. First of all, we do a <code>flatmap</code> transformation.</p><p>The <code>flatmap</code> transformation takes as input the lines and gives words as output. So after the flatmap transformation, the RDD is of the form:</p><pre><code>['word1','word2','word3','word4','word3','word2']
</code></pre><p>Next, we do a <code>map</code> transformation on the <code>flatmap</code> output which converts the RDD to :</p><pre><code>[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]
</code></pre><p>Finally, we do a <code>reduceByKey</code> transformation which counts the number of time each word appeared.</p><p>After which the RDD approaches the final desirable form.</p><pre><code>[('word1',1),('word2',2),('word3',2),('word4',1)]
</code></pre><p>This next line is an action that takes the first 10 elements of the resulting RDD locally.</p><pre><code>output = counts.take(10)
</code></pre><p>This line just prints the output</p><pre><code>for (word, count) in output:                 
    print(&quot;%s: %i&quot; % (word, count))
</code></pre><p>And that is it for the wordcount program. Hope you understand it now.</p><hr><p>So till now, we talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don’t do wordcount in real life.</p><p>We have to work on bigger problems which are much more complex. Worry not! Whatever we have learned till now will let us do that and more.</p><hr><h2 id=spark-in-action-with-example>Spark in Action with Example</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/action.jpeg "></center></div><p>Let us work with a concrete example which takes care of some usual transformations.</p><p>We will work on Movielens
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post target=_blank rel="nofollow noopener">ml-100k.zip</a>
dataset which is a stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies. Released 4/1998.</p><p>The Movielens dataset contains a lot of files but we are going to be working with 3 files only:</p><ol><li><p><strong>Users</strong>: This file name is kept as “u.user”, The columns in this file are:</p><p>[&lsquo;user_id&rsquo;, &lsquo;age&rsquo;, &lsquo;sex&rsquo;, &lsquo;occupation&rsquo;, &lsquo;zip_code&rsquo;]</p></li><li><p><strong>Ratings</strong>: This file name is kept as “u.data”, The columns in this file are:</p><p>[&lsquo;user_id&rsquo;, &lsquo;movie_id&rsquo;, &lsquo;rating&rsquo;, &lsquo;unix_timestamp&rsquo;]</p></li><li><p><strong>Movies</strong>: This file name is kept as “u.item”, The columns in this file are:</p><p>[&lsquo;movie_id&rsquo;, &lsquo;title&rsquo;, &lsquo;release_date&rsquo;, &lsquo;video_release_date&rsquo;, &lsquo;imdb_url&rsquo;, and 18 more columns&mldr;..]</p></li></ol><p>Let us start by importing these 3 files into our spark instance using ‘Import and Explore Data’ on the home tab.</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/load2.png "></center></div><p>Our business partner now comes to us and asks us to find out the <em><strong>25 most rated movie titles</strong></em> from this data. How many times a movie has been rated?</p><p>Let us load the data in different RDDs and see what the data contains.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>userRDD <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;/FileStore/tables/u.user&#34;</span>)
ratingRDD <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;/FileStore/tables/u.data&#34;</span>)
movieRDD <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;/FileStore/tables/u.item&#34;</span>)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;userRDD:&#34;</span>,userRDD<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;ratingRDD:&#34;</span>,ratingRDD<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;movieRDD:&#34;</span>,movieRDD<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))
<span style=color:#f92672>-----------------------------------------------------------</span>
userRDD: [<span style=color:#e6db74>&#39;1|24|M|technician|85711&#39;</span>]
ratingRDD: [<span style=color:#e6db74>&#39;196</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>242</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>3</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>881250949&#39;</span>]
movieRDD: [<span style=color:#e6db74>&#39;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0&#39;</span>]
</code></pre></div><p>We note that to answer this question we will need to use the <code>ratingRDD</code>. But the <code>ratingRDD</code> does not have the movie name.</p><p>So we would have to merge <code>movieRDD</code> and <code>ratingRDD</code> using movie_id.</p><p><strong>How we would do that in Spark?</strong></p><p>Below is the code. We also use a new transformation <code>leftOuterJoin</code>. Do read the docs and comments in the below code.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating.</span>
RDD_movid_rating <span style=color:#f92672>=</span> ratingRDD<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x : (x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>1</span>],x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>2</span>]))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;RDD_movid_rating:&#34;</span>,RDD_movid_rating<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>4</span>))

<span style=color:#75715e># Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title.</span>
RDD_movid_title <span style=color:#f92672>=</span> movieRDD<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x : (x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;|&#34;</span>)[<span style=color:#ae81ff>0</span>],x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;|&#34;</span>)[<span style=color:#ae81ff>1</span>]))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;RDD_movid_title:&#34;</span>,RDD_movid_title<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>2</span>))

<span style=color:#75715e># merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin(). See the transformation document.</span>
rdd_movid_title_rating <span style=color:#f92672>=</span> RDD_movid_rating<span style=color:#f92672>.</span>leftOuterJoin(RDD_movid_title)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_movid_title_rating:&#34;</span>,rdd_movid_title_rating<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))

<span style=color:#75715e># use the RDD in previous step to create (movie,1) tuple pair RDD</span>
rdd_title_rating <span style=color:#f92672>=</span> rdd_movid_title_rating<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>],<span style=color:#ae81ff>1</span> ))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_title_rating:&#34;</span>,rdd_title_rating<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>2</span>))

<span style=color:#75715e># Use the reduceByKey transformation to reduce on the basis of movie_title</span>
rdd_title_ratingcnt <span style=color:#f92672>=</span> rdd_title_rating<span style=color:#f92672>.</span>reduceByKey(<span style=color:#66d9ef>lambda</span> x,y: x<span style=color:#f92672>+</span>y)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_title_ratingcnt:&#34;</span>,rdd_title_ratingcnt<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>2</span>))

<span style=color:#75715e># Get the final answer by using takeOrdered Transformation</span>
<span style=color:#66d9ef>print</span> <span style=color:#e6db74>&#34;#####################################&#34;</span>
<span style=color:#66d9ef>print</span> <span style=color:#e6db74>&#34;25 most rated movies:&#34;</span>,rdd_title_ratingcnt<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>25</span>,<span style=color:#66d9ef>lambda</span> x:<span style=color:#f92672>-</span>x[<span style=color:#ae81ff>1</span>])
<span style=color:#66d9ef>print</span> <span style=color:#e6db74>&#34;#####################################&#34;</span>
</code></pre></div><pre><code>OUTPUT:
--------------------------------------------------------------------RDD_movid_rating: [('242', '3'), ('302', '3'), ('377', '1'), ('51', '2')]
RDD_movid_title: [('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)')]
rdd_movid_title_rating: [('1440', ('3', 'Above the Rim (1994)'))] rdd_title_rating: [('Above the Rim (1994)', 1), ('Above the Rim (1994)', 1)]
rdd_title_ratingcnt: [('Mallrats (1995)', 54), ('Michael Collins (1996)', 92)]

#####################################
25 most rated movies: [('Star Wars (1977)', 583), ('Contact (1997)', 509), ('Fargo (1996)', 508), ('Return of the Jedi (1983)', 507), ('Liar Liar (1997)', 485), ('English Patient, The (1996)', 481), ('Scream (1996)', 478), ('Toy Story (1995)', 452), ('Air Force One (1997)', 431), ('Independence Day (ID4) (1996)', 429), ('Raiders of the Lost Ark (1981)', 420), ('Godfather, The (1972)', 413), ('Pulp Fiction (1994)', 394), ('Twelve Monkeys (1995)', 392), ('Silence of the Lambs, The (1991)', 390), ('Jerry Maguire (1996)', 384), ('Chasing Amy (1997)', 379), ('Rock, The (1996)', 378), ('Empire Strikes Back, The (1980)', 367), ('Star Trek: First Contact (1996)', 365), ('Back to the Future (1985)', 350), ('Titanic (1997)', 350), ('Mission: Impossible (1996)', 344), ('Fugitive, The (1993)', 336), ('Indiana Jones and the Last Crusade (1989)', 331)] #####################################
</code></pre><p>Star Wars is the most rated movie in the Movielens Dataset.</p><p>Now we could have done all this in a single command using the below command but the code is a little messy now.</p><p>I did this to show that you can use chaining functions with Spark and you could bypass the process of variable creation.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>print</span>(((ratingRDD<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x : (x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>1</span>],x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>2</span>])))<span style=color:#f92672>.</span>
     leftOuterJoin(movieRDD<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x : (x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;|&#34;</span>)[<span style=color:#ae81ff>0</span>],x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;|&#34;</span>)[<span style=color:#ae81ff>1</span>]))))<span style=color:#f92672>.</span>
     map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>],<span style=color:#ae81ff>1</span>))<span style=color:#f92672>.</span>
     reduceByKey(<span style=color:#66d9ef>lambda</span> x,y: x<span style=color:#f92672>+</span>y)<span style=color:#f92672>.</span>
     takeOrdered(<span style=color:#ae81ff>25</span>,<span style=color:#66d9ef>lambda</span> x:<span style=color:#f92672>-</span>x[<span style=color:#ae81ff>1</span>]))
</code></pre></div><p>Let us do one more. For practice:</p><p>Now we want to find the most highly rated 25 movies using the same dataset. We actually want only those movies which have been rated at least 100 times.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># We already have the RDD rdd_movid_title_rating: [(u&#39;429&#39;, (u&#39;5&#39;, u&#39;Day the Earth Stood Still, The (1951)&#39;))]</span>
<span style=color:#75715e># We create an RDD that contains sum of all the ratings for a particular movie</span>
rdd_title_ratingsum <span style=color:#f92672>=</span> (rdd_movid_title_rating<span style=color:#f92672>.</span>
                        map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>],int(x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>])))<span style=color:#f92672>.</span>
                        reduceByKey(<span style=color:#66d9ef>lambda</span> x,y:x<span style=color:#f92672>+</span>y))

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_title_ratingsum:&#34;</span>,rdd_title_ratingsum<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>2</span>))
<span style=color:#75715e># Merge this data with the RDD rdd_title_ratingcnt we created in the last step</span>
<span style=color:#75715e># And use Map function to divide ratingsum by rating count.</span>
rdd_title_ratingmean_rating_count <span style=color:#f92672>=</span> (rdd_title_ratingsum<span style=color:#f92672>.</span>
                                    leftOuterJoin(rdd_title_ratingcnt)<span style=color:#f92672>.</span>
                                    map(<span style=color:#66d9ef>lambda</span> x:(x[<span style=color:#ae81ff>0</span>],(float(x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>])<span style=color:#f92672>/</span>x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>],x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>]))))

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_title_ratingmean_rating_count:&#34;</span>,rdd_title_ratingmean_rating_count<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># We could use take ordered here only but we want to only get the movies which have count</span>
<span style=color:#75715e># of ratings more than or equal to 100 so lets filter the data RDD.</span>
rdd_title_rating_rating_count_gt_100 <span style=color:#f92672>=</span> (rdd_title_ratingmean_rating_count<span style=color:#f92672>.</span>
                                        filter(<span style=color:#66d9ef>lambda</span> x: x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>1</span>]<span style=color:#f92672>&gt;=</span><span style=color:#ae81ff>100</span>))

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;rdd_title_rating_rating_count_gt_100:&#34;</span>,rdd_title_rating_rating_count_gt_100<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>))
<span style=color:#75715e># Get the final answer by using takeOrdered Transformation</span>
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;#####################################&#34;</span>)
<span style=color:#66d9ef>print</span> (<span style=color:#e6db74>&#34;25 highly rated movies:&#34;</span>)
<span style=color:#66d9ef>print</span>(rdd_title_rating_rating_count_gt_100<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>25</span>,<span style=color:#66d9ef>lambda</span> x:<span style=color:#f92672>-</span>x[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;#####################################&#34;</span>)
</code></pre></div><pre><code>OUTPUT:
------------------------------------------------------------
rdd_title_ratingsum: [('Mallrats (1995)', 186), ('Michael Collins (1996)', 318)]
rdd_title_ratingmean_rating_count: [('Mallrats (1995)', (3.4444444444444446, 54))]
rdd_title_rating_rating_count_gt_100: [('Butch Cassidy and the Sundance Kid (1969)', (3.949074074074074, 216))]

#####################################
25 highly rated movies: [('Close Shave, A (1995)', (4.491071428571429, 112)), (&quot;Schindler's List (1993)&quot;, (4.466442953020135, 298)), ('Wrong Trousers, The (1993)', (4.466101694915254, 118)), ('Casablanca (1942)', (4.45679012345679, 243)), ('Shawshank Redemption, The (1994)', (4.445229681978798, 283)), ('Rear Window (1954)', (4.3875598086124405, 209)), ('Usual Suspects, The (1995)', (4.385767790262173, 267)), ('Star Wars (1977)', (4.3584905660377355, 583)), ('12 Angry Men (1957)', (4.344, 125)), ('Citizen Kane (1941)', (4.292929292929293, 198)), ('To Kill a Mockingbird (1962)', (4.292237442922374, 219)), (&quot;One Flew Over the Cuckoo's Nest (1975)&quot;, (4.291666666666667, 264)), ('Silence of the Lambs, The (1991)', (4.28974358974359, 390)), ('North by Northwest (1959)', (4.284916201117318, 179)), ('Godfather, The (1972)', (4.283292978208232, 413)), ('Secrets &amp; Lies (1996)', (4.265432098765432, 162)), ('Good Will Hunting (1997)', (4.262626262626263, 198)), ('Manchurian Candidate, The (1962)', (4.259541984732825, 131)), ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', (4.252577319587629, 194)), ('Raiders of the Lost Ark (1981)', (4.252380952380952, 420)), ('Vertigo (1958)', (4.251396648044692, 179)), ('Titanic (1997)', (4.2457142857142856, 350)), ('Lawrence of Arabia (1962)', (4.23121387283237, 173)), ('Maltese Falcon, The (1941)', (4.2101449275362315, 138)), ('Empire Strikes Back, The (1980)', (4.204359673024523, 367))]
#####################################
</code></pre><p>We have talked about RDDs till now as they are very powerful.</p><p>You can use RDDs to work with non-relational databases too.</p><p>They let you do a lot of things that you couldn’t do with SparkSQL?</p><p><em><strong>Yes, you can use SQL with Spark too which I am going to talk about now.</strong></em></p><hr><h2 id=spark-dataframes>Spark DataFrames</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/df.png "></center></div><p>Spark has provided DataFrame API for us Data Scientists to work with relational data. Here is the
<a href=https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html# target=_blank rel="nofollow noopener">documentation</a>
for the adventurous folks.</p><p>Remember that in the background it still is all RDDs and that is why the starting part of this post focussed on RDDs.</p><p>I will start with some common functionalities you will need to work with Spark DataFrames. Would look a lot like Pandas with some syntax changes.</p><h3 id=1-reading-the-file>1. Reading the File</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>ratings <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;/FileStore/tables/u.data&#34;</span>,format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;csv&#34;</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>, inferSchema<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>, header<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;false&#34;</span>)
</code></pre></div><h3 id=2-show-file>2. Show File</h3><p>We have two ways to show files using Spark Dataframes.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>ratings<span style=color:#f92672>.</span>show()
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_2.png "></center></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>display(ratings)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_3.png "></center></div><p>I prefer display as it looks a lot nicer and clean.</p><h3 id=3-change-column-names>3. Change Column names</h3><p>Good functionality. Always required. Don’t forget the <code>*</code> in front of the list.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>ratings <span style=color:#f92672>=</span> ratings<span style=color:#f92672>.</span>toDF(<span style=color:#f92672>*</span>[<span style=color:#e6db74>&#39;user_id&#39;</span>, <span style=color:#e6db74>&#39;movie_id&#39;</span>, <span style=color:#e6db74>&#39;rating&#39;</span>, <span style=color:#e6db74>&#39;unix_timestamp&#39;</span>])
display(ratings)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_4.png "></center></div><h3 id=4-some-basic-stats>4. Some Basic Stats</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>print</span>(ratings<span style=color:#f92672>.</span>count()) <span style=color:#75715e>#Row Count</span>
<span style=color:#66d9ef>print</span>(len(ratings<span style=color:#f92672>.</span>columns)) <span style=color:#75715e>#Column Count</span>
<span style=color:#f92672>---------------------------------------------------------</span>
<span style=color:#ae81ff>100000</span>
<span style=color:#ae81ff>4</span>
</code></pre></div><p>We can also see the dataframe statistics using:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>display(ratings<span style=color:#f92672>.</span>describe())
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_5.png "></center></div><h3 id=5-select-a-few-columns>5. Select a few columns</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>display(ratings<span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#39;user_id&#39;</span>,<span style=color:#e6db74>&#39;movie_id&#39;</span>))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_6.png "></center></div><h3 id=6-filter>6. Filter</h3><p>Filter a dataframe using multiple conditions:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>display(ratings<span style=color:#f92672>.</span>filter((ratings<span style=color:#f92672>.</span>rating<span style=color:#f92672>==</span><span style=color:#ae81ff>5</span>) <span style=color:#f92672>&amp;</span> (ratings<span style=color:#f92672>.</span>user_id<span style=color:#f92672>==</span><span style=color:#ae81ff>253</span>)))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_7.png "></center></div><h3 id=7-groupby>7. Groupby</h3><p>We can use groupby function with a spark dataframe too. Pretty much same as a pandas groupby with the exception that you will need to import <code>pyspark.sql.functions</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> F
display(ratings<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#34;user_id&#34;</span>)<span style=color:#f92672>.</span>agg(F<span style=color:#f92672>.</span>count(<span style=color:#e6db74>&#34;user_id&#34;</span>),F<span style=color:#f92672>.</span>mean(<span style=color:#e6db74>&#34;rating&#34;</span>)))
</code></pre></div><p>Here we have found the count of ratings and average rating from each user_id</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_8.png "></center></div><h2 id=8-sort>8. Sort</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>display(ratings<span style=color:#f92672>.</span>sort(<span style=color:#e6db74>&#34;user_id&#34;</span>))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_9.png "></center></div><p>We can also do a descending sort using <code>F.desc</code> function as below.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># descending Sort</span>
<span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> functions <span style=color:#66d9ef>as</span> F
display(ratings<span style=color:#f92672>.</span>sort(F<span style=color:#f92672>.</span>desc(<span style=color:#e6db74>&#34;user_id&#34;</span>)))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_10.png "></center></div><hr><h2 id=joinsmerging-with-spark-dataframes>Joins/Merging with Spark Dataframes</h2><p>I was not able to find a pandas equivalent of merge with Spark DataFrames but we can use SQL with dataframes and thus we can merge dataframes using SQL.</p><p>Let us try to run some SQL on Ratings.</p><p>We first register the ratings df to a temporary table ratings_table on which we can run sql operations.</p><p>As you can see the result of the SQL select statement is again a Spark Dataframe.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>ratings<span style=color:#f92672>.</span>registerTempTable(<span style=color:#e6db74>&#39;ratings_table&#39;</span>)
newDF <span style=color:#f92672>=</span> sqlContext<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#39;select * from ratings_table where rating&gt;4&#39;</span>)
display(newDF)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_11.png "></center></div><p>Let us now add one more Spark Dataframe to the mix to see if we can use join using the SQL queries:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e>#get one more dataframe to join</span>
movies <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;/FileStore/tables/u.item&#34;</span>,format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;csv&#34;</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;|&#34;</span>, inferSchema<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>, header<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;false&#34;</span>)

<span style=color:#75715e># change column names</span>
movies <span style=color:#f92672>=</span> movies<span style=color:#f92672>.</span>toDF(<span style=color:#f92672>*</span>[<span style=color:#e6db74>&#34;movie_id&#34;</span>,<span style=color:#e6db74>&#34;movie_title&#34;</span>,<span style=color:#e6db74>&#34;release_date&#34;</span>,<span style=color:#e6db74>&#34;video_release_date&#34;</span>,<span style=color:#e6db74>&#34;IMDb_URL&#34;</span>,<span style=color:#e6db74>&#34;unknown&#34;</span>,<span style=color:#e6db74>&#34;Action&#34;</span>,<span style=color:#e6db74>&#34;Adventure&#34;</span>,<span style=color:#e6db74>&#34;Animation &#34;</span>,<span style=color:#e6db74>&#34;Children&#34;</span>,<span style=color:#e6db74>&#34;Comedy&#34;</span>,<span style=color:#e6db74>&#34;Crime&#34;</span>,<span style=color:#e6db74>&#34;Documentary&#34;</span>,<span style=color:#e6db74>&#34;Drama&#34;</span>,<span style=color:#e6db74>&#34;Fantasy&#34;</span>,<span style=color:#e6db74>&#34;Film_Noir&#34;</span>,<span style=color:#e6db74>&#34;Horror&#34;</span>,<span style=color:#e6db74>&#34;Musical&#34;</span>,<span style=color:#e6db74>&#34;Mystery&#34;</span>,<span style=color:#e6db74>&#34;Romance&#34;</span>,<span style=color:#e6db74>&#34;Sci_Fi&#34;</span>,<span style=color:#e6db74>&#34;Thriller&#34;</span>,<span style=color:#e6db74>&#34;War&#34;</span>,<span style=color:#e6db74>&#34;Western&#34;</span>])

display(movies)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_12.png "></center></div><p>Now let us try joining the tables on movie_id to get the name of the movie in the ratings table.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>movies<span style=color:#f92672>.</span>registerTempTable(<span style=color:#e6db74>&#39;movies_table&#39;</span>)

display(sqlContext<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#39;select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id&#39;</span>))
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_13.png "></center></div><p>Let us try to do what we were doing earlier with the RDDs. Finding the top 25 most rated movies:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>mostrateddf <span style=color:#f92672>=</span> sqlContext<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#39;select movie_id,movie_title, count(user_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title order by num_ratings desc &#39;</span>)

display(mostrateddf)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_14.png "></center></div><p>And finding the top 25 highest rated movies having more than 100 votes:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>highrateddf <span style=color:#f92672>=</span> sqlContext<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#39;select movie_id,movie_title, avg(rating) as avg_rating,count(movie_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title having num_ratings&gt;100 order by avg_rating desc &#39;</span>)

display(highrateddf)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_15.png "></center></div><p>I have used GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query. That shows that you can do pretty much complex stuff using <code>sqlContext.sql</code></p><hr><h2 id=a-small-note-about-display>A Small Note About Display</h2><p>You can also use <code>display</code> command to display charts in your notebooks.</p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_16.png "></center></div><p>You can see more options when you select <em><strong>Plot Options.</strong></em></p><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_19.png "></center></div><hr><h2 id=converting-from-spark-dataframe-to-rdd-and-vice-versa>Converting from Spark Dataframe to RDD and vice versa:</h2><p>Sometimes you may want to convert to RDD from a spark Dataframe or vice versa so that you can have the best of both worlds.</p><p>To convert from DF to RDD, you can simply do :</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>highratedrdd <span style=color:#f92672>=</span>highrateddf<span style=color:#f92672>.</span>rdd
highratedrdd<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>2</span>)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_17.png "></center></div><p>To go from an RDD to a dataframe:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> pyspark.sql <span style=color:#f92672>import</span> Row
<span style=color:#75715e># creating a RDD first</span>
data <span style=color:#f92672>=</span> [(<span style=color:#e6db74>&#39;A&#39;</span>,<span style=color:#ae81ff>1</span>),(<span style=color:#e6db74>&#39;B&#39;</span>,<span style=color:#ae81ff>2</span>),(<span style=color:#e6db74>&#39;C&#39;</span>,<span style=color:#ae81ff>3</span>),(<span style=color:#e6db74>&#39;D&#39;</span>,<span style=color:#ae81ff>4</span>)]
rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize(data)

<span style=color:#75715e># map the schema using Row.</span>
rdd_new <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: Row(key<span style=color:#f92672>=</span>x[<span style=color:#ae81ff>0</span>], value<span style=color:#f92672>=</span>int(x[<span style=color:#ae81ff>1</span>])))

<span style=color:#75715e># Convert the rdd to Dataframe</span>
rdd_as_df <span style=color:#f92672>=</span> sqlContext<span style=color:#f92672>.</span>createDataFrame(rdd_new)
display(rdd_as_df)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/res_18.png "></center></div><p>RDD provides you with <em><strong>more control</strong></em> at the cost of time and coding effort. While Dataframes provide you with <em><strong>familiar coding</strong></em> platform. And now you can move back and forth between these two.</p><h2 id=conclusion>Conclusion</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/spark/convo_1.jpeg "></center></div><p>This was a big post and congratulations if you reached the end.</p><p>Spark has provided us with an interface where we could use transformations and actions on our data. Spark also has the Dataframe API to ease the transition of Data scientists to Big Data.</p><p>Hopefully, I’ve covered the basics well enough to pique your interest and help you get started with Spark.</p><p><em><strong>You can find all the code at the
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post target=_blank rel="nofollow noopener">GitHub</a>
repository.</strong></em></p><p>Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on
<a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11468293556&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-essentials" target=_blank rel="nofollow noopener">Big Data Essentials</a>
which is part of the
<a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11468293466&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fbig-data-engineering" target=_blank rel="nofollow noopener">Big Data Specialization</a>
provided by Yandex.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at
<a href=https://medium.com/@rahul_agarwal target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a>
.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>