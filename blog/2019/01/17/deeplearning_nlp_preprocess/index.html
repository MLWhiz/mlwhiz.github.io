<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning" />
<meta property="og:description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/" />
<meta property="og:image" content="https://mlwhiz.com/images/text_processing_flow_1.png" />
<meta property="article:published_time" content="2019-01-17T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-01-17T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/text_processing_flow_1.png"/>

<meta name="twitter:title" content="NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning"/>
<meta name="twitter:description" content="Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/atom.xml">RSS</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-01-17T00:00:00">January 17, 2019</time>
</div>
</div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#a-primer-on-word2vec-embeddings">A Primer on word2vec embeddings:</a></li>
<li><a href="#basic-preprocessing-techniques-for-text-data">Basic Preprocessing Techniques for text data:</a>
<ul>
<li>
<ul>
<li><a href="#a-cleaning-special-characters-and-removing-punctuations">a) Cleaning Special Characters and Removing Punctuations:</a></li>
<li><a href="#b-cleaning-numbers">b) Cleaning Numbers:</a></li>
<li><a href="#c-removing-misspells">c) Removing Misspells:</a></li>
<li><a href="#d-removing-contractions">d) Removing Contractions:</a></li>
</ul></li>
</ul></li>
<li><a href="#representation-sequence-creation">Representation: Sequence Creation</a>
<ul>
<li>
<ul>
<li><a href="#a-tokenizer">a) Tokenizer:</a></li>
<li><a href="#b-pad-sequence">b) Pad Sequence:</a></li>
</ul></li>
</ul></li>
<li><a href="#embedding-enrichment">Embedding Enrichment:</a></li>
<li><a href="#more-engineered-features">More Engineered Features</a></li>
<li><a href="#conclusion">Conclusion:</a></li>
<li><a href="#endnotes-and-references">Endnotes and References</a></li>
</ul></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			

<p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.</p>

<p>Since we have a large amount of material to cover, I am splitting this post into a series of posts. The first post i.e. this one will be based on <strong>preprocessing techniques that work with Deep learning models</strong> and we will also talk about <strong>increasing embeddings coverage</strong>. In the <a href="/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post</a>, I will try to take you through some <strong>basic conventional models</strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into <strong>Deep learning models</strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like <strong>ULMFit transfer learning</strong> approaches in the fourth post in the series.</p>

<p><strong>As a side note</strong>: if you want to know more about NLP, I would like to recommend this awesome course on <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" rel="nofollow" target="_blank">Natural Language Processing</a> in the <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Advanced machine learning specialization</a>. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.</p>

<p>It might take me a little time to write the whole series. Till then you can take a look at my other posts: <a href="/blog/2018/12/17/text_classification/">What Kagglers are using for Text Classification</a>, which talks about various deep learning models in use in NLP and <a href="/blog/2019/01/06/pytorch_keras_conversion/">how to switch from Keras to Pytorch</a>.</p>

<p>So first let me start with explaining a little more about the text classification problem. <strong>Text classification</strong> is a common task in natural language processing, which transforms a sequence of a text of indefinite length into a category of text. How could you use that?</p>

<ul>
<li>To find the sentiment of a review.</li>
<li>Find toxic comments on a platform like Facebook</li>
<li>Find Insincere questions on Quora. A current ongoing competition on kaggle</li>
<li>Find fake reviews on websites</li>
<li>Will a text advert get clicked or not?</li>
</ul>

<p>Now each of these problems has something in common. From a Machine Learning perspective, these are essentially the same problem with just the target labels changing and nothing else. With that said, the addition of business knowledge can help make these models more robust and that is what we want to incorporate while preprocessing the data for test classification. While the preprocessing pipeline I am focussing on in this post is mainly centered around Deep Learning but most of it will also be applicable to conventional machine learning models too.</p>

<p>But let me first go through the flow of a deep learning pipeline for text data before going through all the steps to get a higher level perspective about the whole process.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/text_processing_flow_1.png"  style="height:90%;width:90%"></center>
</div>

<p>We normally start with cleaning up the text data and performing basic EDA. Here we try to improve our data quality by cleaning up the data. We also try to improve the quality of our word2vec embeddings by removing OOV(Out-of-Vocabulary) words. These first two steps normally don&rsquo;t have much order between them and I generally go back and forth between these two steps. Next, we create a representation for text that could be fed into a deep learning model. We then start with creating our models and training them. Finally, we evaluate the models using appropriate metrics and get approval from respective shareholders to deploy our models. Don&rsquo;t worry if these terms don&rsquo;t make much sense now. I will try to explain them through the course of this article.</p>

<p>Here at this junction, let us take a little detour to talk a little about word embeddings. We will have to think about them while preprocessing data for our Deep Learning models.</p>

<h2 id="a-primer-on-word2vec-embeddings">A Primer on word2vec embeddings:</h2>

<p>We need to have a way to represent words in a vocab. One way to do that could be to use One hot encoding of word vectors but that is not really a good choice. One of the major reasons is that the one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity.</p>

<p>$$\frac{\boldsymbol{x}^\top \boldsymbol{y}}{|\boldsymbol{x}| |\boldsymbol{y}|} \in [-1, 1].$$</p>

<p>Given the structure of one hot encoded vectors, the similarity is always going to come as 0 between different words. Another reason is that as the size of vocabulary increases these one hot encoded vectors become very large.</p>

<p>Word2Vec overcomes the above difficulties by providing us with a fixed length vector representation of words and by capturing the similarity and analogy relationships between different words.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/word2vec.png" style="height:80%;width:80%" ></center>
</div>

<p>Word2vec vectors of words are learned in such a way that they allow us to learn different analogies. It enables us to do algebraic manipulations on words which were not possible before. For example: What is king - man + woman? It comes out to be Queen.</p>

<p>Word2Vec vectors also help us to find out the similarity between words. If we try to find similar words to &ldquo;good&rdquo;, we will find awesome, great etc. It is this property of word2vec that makes it invaluable for text classification. Now our deep learning network understands that &ldquo;good&rdquo; and &ldquo;great&rdquo; are essentially words with similar meaning.</p>

<p><strong>Thus in very simple terms, word2vec creates vectors for words. Thus we have a <code>d</code> dimensional vector for every word(common bigrams too) in a dictionary.</strong> We normally use pretrained word vectors which are provided to us by others after training on large corpora of texts like Wikipedia, twitter etc. The most commonly used pretrained word vectors are Glove and Fasttext with 300-dimensional word vectors. We are going to use Glove in this post.</p>

<h2 id="basic-preprocessing-techniques-for-text-data">Basic Preprocessing Techniques for text data:</h2>

<p>In most of the cases, we observe that text data is not entirely clean. Data coming from different sources have different characteristics and that makes Text Preprocessing as one of the most important steps in the classification pipeline. For example, Text data from Twitter is totally different from text data on Quora, or some news/blogging platform, and thus would need to be treated differently. Helpfully, the techniques I am going to talk about in this post are generic enough for any kind of data you might encounter in the jungles of NLP.</p>

<h4 id="a-cleaning-special-characters-and-removing-punctuations">a) Cleaning Special Characters and Removing Punctuations:</h4>

<p>Our preprocessing pipeline depends a lot on the word2vec embeddings we are going to use for our classification task. <em>In principle our preprocessing should match the preprocessing that was used before training the word embedding</em>. Since most of the embeddings don&rsquo;t provide vector values for punctuations and other special chars, the first thing you want to do is to get rid of is the special characters in your text data. These are some of the special chars that were there in the Quora Question data and we use <code>replace</code> function to get rid of these special chars.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Some preprocesssing that will be common to all the text classification methods you will see. </span>

puncts <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;,&#39;</span>, <span style="color:#e6db74">&#39;.&#39;</span>, <span style="color:#e6db74">&#39;&#34;&#39;</span>, <span style="color:#e6db74">&#39;:&#39;</span>, <span style="color:#e6db74">&#39;)&#39;</span>, <span style="color:#e6db74">&#39;(&#39;</span>, <span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;!&#39;</span>, <span style="color:#e6db74">&#39;?&#39;</span>, <span style="color:#e6db74">&#39;|&#39;</span>, <span style="color:#e6db74">&#39;;&#39;</span>, <span style="color:#e6db74">&#34;&#39;&#34;</span>, <span style="color:#e6db74">&#39;$&#39;</span>, <span style="color:#e6db74">&#39;&amp;&#39;</span>, <span style="color:#e6db74">&#39;/&#39;</span>, <span style="color:#e6db74">&#39;[&#39;</span>, <span style="color:#e6db74">&#39;]&#39;</span>, <span style="color:#e6db74">&#39;&gt;&#39;</span>, <span style="color:#e6db74">&#39;%&#39;</span>, <span style="color:#e6db74">&#39;=&#39;</span>, <span style="color:#e6db74">&#39;#&#39;</span>, <span style="color:#e6db74">&#39;*&#39;</span>, <span style="color:#e6db74">&#39;+&#39;</span>, <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">&#39;•&#39;</span>,  <span style="color:#e6db74">&#39;~&#39;</span>, <span style="color:#e6db74">&#39;@&#39;</span>, <span style="color:#e6db74">&#39;£&#39;</span>, 
 <span style="color:#e6db74">&#39;·&#39;</span>, <span style="color:#e6db74">&#39;_&#39;</span>, <span style="color:#e6db74">&#39;{&#39;</span>, <span style="color:#e6db74">&#39;}&#39;</span>, <span style="color:#e6db74">&#39;©&#39;</span>, <span style="color:#e6db74">&#39;^&#39;</span>, <span style="color:#e6db74">&#39;®&#39;</span>, <span style="color:#e6db74">&#39;`&#39;</span>,  <span style="color:#e6db74">&#39;&lt;&#39;</span>, <span style="color:#e6db74">&#39;→&#39;</span>, <span style="color:#e6db74">&#39;°&#39;</span>, <span style="color:#e6db74">&#39;€&#39;</span>, <span style="color:#e6db74">&#39;™&#39;</span>, <span style="color:#e6db74">&#39;›&#39;</span>,  <span style="color:#e6db74">&#39;♥&#39;</span>, <span style="color:#e6db74">&#39;←&#39;</span>, <span style="color:#e6db74">&#39;×&#39;</span>, <span style="color:#e6db74">&#39;§&#39;</span>, <span style="color:#e6db74">&#39;″&#39;</span>, <span style="color:#e6db74">&#39;′&#39;</span>, <span style="color:#e6db74">&#39;Â&#39;</span>, <span style="color:#e6db74">&#39;█&#39;</span>, <span style="color:#e6db74">&#39;½&#39;</span>, <span style="color:#e6db74">&#39;à&#39;</span>, <span style="color:#e6db74">&#39;…&#39;</span>, 
 <span style="color:#e6db74">&#39;“&#39;</span>, <span style="color:#e6db74">&#39;★&#39;</span>, <span style="color:#e6db74">&#39;”&#39;</span>, <span style="color:#e6db74">&#39;–&#39;</span>, <span style="color:#e6db74">&#39;●&#39;</span>, <span style="color:#e6db74">&#39;â&#39;</span>, <span style="color:#e6db74">&#39;►&#39;</span>, <span style="color:#e6db74">&#39;−&#39;</span>, <span style="color:#e6db74">&#39;¢&#39;</span>, <span style="color:#e6db74">&#39;²&#39;</span>, <span style="color:#e6db74">&#39;¬&#39;</span>, <span style="color:#e6db74">&#39;░&#39;</span>, <span style="color:#e6db74">&#39;¶&#39;</span>, <span style="color:#e6db74">&#39;↑&#39;</span>, <span style="color:#e6db74">&#39;±&#39;</span>, <span style="color:#e6db74">&#39;¿&#39;</span>, <span style="color:#e6db74">&#39;▾&#39;</span>, <span style="color:#e6db74">&#39;═&#39;</span>, <span style="color:#e6db74">&#39;¦&#39;</span>, <span style="color:#e6db74">&#39;║&#39;</span>, <span style="color:#e6db74">&#39;―&#39;</span>, <span style="color:#e6db74">&#39;¥&#39;</span>, <span style="color:#e6db74">&#39;▓&#39;</span>, <span style="color:#e6db74">&#39;—&#39;</span>, <span style="color:#e6db74">&#39;‹&#39;</span>, <span style="color:#e6db74">&#39;─&#39;</span>, 
 <span style="color:#e6db74">&#39;▒&#39;</span>, <span style="color:#e6db74">&#39;：&#39;</span>, <span style="color:#e6db74">&#39;¼&#39;</span>, <span style="color:#e6db74">&#39;⊕&#39;</span>, <span style="color:#e6db74">&#39;▼&#39;</span>, <span style="color:#e6db74">&#39;▪&#39;</span>, <span style="color:#e6db74">&#39;†&#39;</span>, <span style="color:#e6db74">&#39;■&#39;</span>, <span style="color:#e6db74">&#39;’&#39;</span>, <span style="color:#e6db74">&#39;▀&#39;</span>, <span style="color:#e6db74">&#39;¨&#39;</span>, <span style="color:#e6db74">&#39;▄&#39;</span>, <span style="color:#e6db74">&#39;♫&#39;</span>, <span style="color:#e6db74">&#39;☆&#39;</span>, <span style="color:#e6db74">&#39;é&#39;</span>, <span style="color:#e6db74">&#39;¯&#39;</span>, <span style="color:#e6db74">&#39;♦&#39;</span>, <span style="color:#e6db74">&#39;¤&#39;</span>, <span style="color:#e6db74">&#39;▲&#39;</span>, <span style="color:#e6db74">&#39;è&#39;</span>, <span style="color:#e6db74">&#39;¸&#39;</span>, <span style="color:#e6db74">&#39;¾&#39;</span>, <span style="color:#e6db74">&#39;Ã&#39;</span>, <span style="color:#e6db74">&#39;⋅&#39;</span>, <span style="color:#e6db74">&#39;‘&#39;</span>, <span style="color:#e6db74">&#39;∞&#39;</span>, 
 <span style="color:#e6db74">&#39;∙&#39;</span>, <span style="color:#e6db74">&#39;）&#39;</span>, <span style="color:#e6db74">&#39;↓&#39;</span>, <span style="color:#e6db74">&#39;、&#39;</span>, <span style="color:#e6db74">&#39;│&#39;</span>, <span style="color:#e6db74">&#39;（&#39;</span>, <span style="color:#e6db74">&#39;»&#39;</span>, <span style="color:#e6db74">&#39;，&#39;</span>, <span style="color:#e6db74">&#39;♪&#39;</span>, <span style="color:#e6db74">&#39;╩&#39;</span>, <span style="color:#e6db74">&#39;╚&#39;</span>, <span style="color:#e6db74">&#39;³&#39;</span>, <span style="color:#e6db74">&#39;・&#39;</span>, <span style="color:#e6db74">&#39;╦&#39;</span>, <span style="color:#e6db74">&#39;╣&#39;</span>, <span style="color:#e6db74">&#39;╔&#39;</span>, <span style="color:#e6db74">&#39;╗&#39;</span>, <span style="color:#e6db74">&#39;▬&#39;</span>, <span style="color:#e6db74">&#39;❤&#39;</span>, <span style="color:#e6db74">&#39;ï&#39;</span>, <span style="color:#e6db74">&#39;Ø&#39;</span>, <span style="color:#e6db74">&#39;¹&#39;</span>, <span style="color:#e6db74">&#39;≤&#39;</span>, <span style="color:#e6db74">&#39;‡&#39;</span>, <span style="color:#e6db74">&#39;√&#39;</span>, ]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clean_text</span>(x):
    x <span style="color:#f92672">=</span> str(x)
    <span style="color:#66d9ef">for</span> punct <span style="color:#f92672">in</span> puncts:
        <span style="color:#66d9ef">if</span> punct <span style="color:#f92672">in</span> x:
            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>replace(punct, f<span style="color:#e6db74">&#39; {punct} &#39;</span>)
    <span style="color:#66d9ef">return</span> x</code></pre></div>
<p>This could also have been done with the help of a simple regex. But I normally like the above way of doing things as it helps to understand the sort of characters we are removing from our data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clean_text</span>(x):
    pattern <span style="color:#f92672">=</span> <span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;[^a-zA-z0-9\s]&#39;</span>
    text <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(pattern, <span style="color:#e6db74">&#39;&#39;</span>, x)
    <span style="color:#66d9ef">return</span> x</code></pre></div>
<h4 id="b-cleaning-numbers">b) Cleaning Numbers:</h4>

<p>Why do we want to replace numbers with <code>#</code>s? Because most embeddings have preprocessed their text like this.</p>

<p><strong>Small Python Trick:</strong> We use an <code>if</code> statement in the code below to check beforehand if a number exists in a text. It is as an <code>if</code> is always fast than a <code>re.sub</code> command and most of our text doesn&rsquo;t contain numbers.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clean_numbers</span>(x):
    <span style="color:#66d9ef">if</span> bool(re<span style="color:#f92672">.</span>search(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\d&#39;</span>, x)):
        x <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;[0-9]{5,}&#39;</span>, <span style="color:#e6db74">&#39;#####&#39;</span>, x)
        x <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;[0-9]{4}&#39;</span>, <span style="color:#e6db74">&#39;####&#39;</span>, x)
        x <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;[0-9]{3}&#39;</span>, <span style="color:#e6db74">&#39;###&#39;</span>, x)
        x <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#39;[0-9]{2}&#39;</span>, <span style="color:#e6db74">&#39;##&#39;</span>, x)
    <span style="color:#66d9ef">return</span> x</code></pre></div>
<h4 id="c-removing-misspells">c) Removing Misspells:</h4>

<p>It always helps to find out misspells in the data. As those word embeddings are not present in the word2vec, we should replace words with their correct spellings to get better embedding coverage. The following code artifact is an adaptation of Peter Norvig&rsquo;s spell checker. It uses word2vec ordering of words to approximate word probabilities. As Google word2vec apparently orders words in decreasing order of frequency in the training corpus. You can use this to find out some misspelled words in the data you have.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># This comes from CPMP script in the Quora questions similarity challenge. </span>
<span style="color:#f92672">import</span> re
<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> Counter
<span style="color:#f92672">import</span> gensim
<span style="color:#f92672">import</span> heapq
<span style="color:#f92672">from</span> operator <span style="color:#f92672">import</span> itemgetter
<span style="color:#f92672">from</span> multiprocessing <span style="color:#f92672">import</span> Pool

model <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>KeyedVectors<span style="color:#f92672">.</span>load_word2vec_format(<span style="color:#e6db74">&#39;../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin&#39;</span>, 
                                                        binary<span style="color:#f92672">=</span>True)
words <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>index2word

w_rank <span style="color:#f92672">=</span> {}
<span style="color:#66d9ef">for</span> i,word <span style="color:#f92672">in</span> enumerate(words):
    w_rank[word] <span style="color:#f92672">=</span> i

WORDS <span style="color:#f92672">=</span> w_rank

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">words</span>(text): <span style="color:#66d9ef">return</span> re<span style="color:#f92672">.</span>findall(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\w+&#39;</span>, text<span style="color:#f92672">.</span>lower())

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">P</span>(word): 
    <span style="color:#e6db74">&#34;Probability of `word`.&#34;</span>
    <span style="color:#75715e"># use inverse of rank as proxy</span>
    <span style="color:#75715e"># returns 0 if the word isn&#39;t in the dictionary</span>
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span> WORDS<span style="color:#f92672">.</span>get(word, <span style="color:#ae81ff">0</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">correction</span>(word): 
    <span style="color:#e6db74">&#34;Most probable spelling correction for word.&#34;</span>
    <span style="color:#66d9ef">return</span> max(candidates(word), key<span style="color:#f92672">=</span>P)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">candidates</span>(word): 
    <span style="color:#e6db74">&#34;Generate possible spelling corrections for word.&#34;</span>
    <span style="color:#66d9ef">return</span> (known([word]) <span style="color:#f92672">or</span> known(edits1(word)) <span style="color:#f92672">or</span> known(edits2(word)) <span style="color:#f92672">or</span> [word])

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">known</span>(words): 
    <span style="color:#e6db74">&#34;The subset of `words` that appear in the dictionary of WORDS.&#34;</span>
    <span style="color:#66d9ef">return</span> set(w <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> words <span style="color:#66d9ef">if</span> w <span style="color:#f92672">in</span> WORDS)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">edits1</span>(word):
    <span style="color:#e6db74">&#34;All edits that are one edit away from `word`.&#34;</span>
    letters    <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;abcdefghijklmnopqrstuvwxyz&#39;</span>
    splits     <span style="color:#f92672">=</span> [(word[:i], word[i:])    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(word) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
    deletes    <span style="color:#f92672">=</span> [L <span style="color:#f92672">+</span> R[<span style="color:#ae81ff">1</span>:]               <span style="color:#66d9ef">for</span> L, R <span style="color:#f92672">in</span> splits <span style="color:#66d9ef">if</span> R]
    transposes <span style="color:#f92672">=</span> [L <span style="color:#f92672">+</span> R[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> R[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> R[<span style="color:#ae81ff">2</span>:] <span style="color:#66d9ef">for</span> L, R <span style="color:#f92672">in</span> splits <span style="color:#66d9ef">if</span> len(R)<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">1</span>]
    replaces   <span style="color:#f92672">=</span> [L <span style="color:#f92672">+</span> c <span style="color:#f92672">+</span> R[<span style="color:#ae81ff">1</span>:]           <span style="color:#66d9ef">for</span> L, R <span style="color:#f92672">in</span> splits <span style="color:#66d9ef">if</span> R <span style="color:#66d9ef">for</span> c <span style="color:#f92672">in</span> letters]
    inserts    <span style="color:#f92672">=</span> [L <span style="color:#f92672">+</span> c <span style="color:#f92672">+</span> R               <span style="color:#66d9ef">for</span> L, R <span style="color:#f92672">in</span> splits <span style="color:#66d9ef">for</span> c <span style="color:#f92672">in</span> letters]
    <span style="color:#66d9ef">return</span> set(deletes <span style="color:#f92672">+</span> transposes <span style="color:#f92672">+</span> replaces <span style="color:#f92672">+</span> inserts)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">edits2</span>(word): 
    <span style="color:#e6db74">&#34;All edits that are two edits away from `word`.&#34;</span>
    <span style="color:#66d9ef">return</span> (e2 <span style="color:#66d9ef">for</span> e1 <span style="color:#f92672">in</span> edits1(word) <span style="color:#66d9ef">for</span> e2 <span style="color:#f92672">in</span> edits1(e1))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_vocab</span>(texts):
    sentences <span style="color:#f92672">=</span> texts<span style="color:#f92672">.</span>apply(<span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>split())<span style="color:#f92672">.</span>values
    vocab <span style="color:#f92672">=</span> {}
    <span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> sentences:
        <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> sentence:
            <span style="color:#66d9ef">try</span>:
                vocab[word] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">KeyError</span>:
                vocab[word] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">return</span> vocab

vocab <span style="color:#f92672">=</span> build_vocab(train<span style="color:#f92672">.</span>question_text)

top_90k_words <span style="color:#f92672">=</span> dict(heapq<span style="color:#f92672">.</span>nlargest(<span style="color:#ae81ff">90000</span>, vocab<span style="color:#f92672">.</span>items(), key<span style="color:#f92672">=</span>itemgetter(<span style="color:#ae81ff">1</span>)))

pool <span style="color:#f92672">=</span> Pool(<span style="color:#ae81ff">4</span>)
corrected_words <span style="color:#f92672">=</span> pool<span style="color:#f92672">.</span>map(correction,list(top_90k_words<span style="color:#f92672">.</span>keys()))

<span style="color:#66d9ef">for</span> word,corrected_word <span style="color:#f92672">in</span> zip(top_90k_words,corrected_words):
    <span style="color:#66d9ef">if</span> word<span style="color:#f92672">!=</span>corrected_word:
        <span style="color:#66d9ef">print</span>(word,<span style="color:#e6db74">&#34;:&#34;</span>,corrected_word)</code></pre></div>
<p>Once we are through with finding misspelled data, the next thing remains to replace them using a misspell mapping and regex functions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">mispell_dict <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;colour&#39;</span>: <span style="color:#e6db74">&#39;color&#39;</span>, <span style="color:#e6db74">&#39;centre&#39;</span>: <span style="color:#e6db74">&#39;center&#39;</span>, <span style="color:#e6db74">&#39;favourite&#39;</span>: <span style="color:#e6db74">&#39;favorite&#39;</span>, <span style="color:#e6db74">&#39;travelling&#39;</span>: <span style="color:#e6db74">&#39;traveling&#39;</span>, <span style="color:#e6db74">&#39;counselling&#39;</span>: <span style="color:#e6db74">&#39;counseling&#39;</span>, <span style="color:#e6db74">&#39;theatre&#39;</span>: <span style="color:#e6db74">&#39;theater&#39;</span>, <span style="color:#e6db74">&#39;cancelled&#39;</span>: <span style="color:#e6db74">&#39;canceled&#39;</span>, <span style="color:#e6db74">&#39;labour&#39;</span>: <span style="color:#e6db74">&#39;labor&#39;</span>, <span style="color:#e6db74">&#39;organisation&#39;</span>: <span style="color:#e6db74">&#39;organization&#39;</span>, <span style="color:#e6db74">&#39;wwii&#39;</span>: <span style="color:#e6db74">&#39;world war 2&#39;</span>, <span style="color:#e6db74">&#39;citicise&#39;</span>: <span style="color:#e6db74">&#39;criticize&#39;</span>, <span style="color:#e6db74">&#39;youtu &#39;</span>: <span style="color:#e6db74">&#39;youtube &#39;</span>, <span style="color:#e6db74">&#39;Qoura&#39;</span>: <span style="color:#e6db74">&#39;Quora&#39;</span>, <span style="color:#e6db74">&#39;sallary&#39;</span>: <span style="color:#e6db74">&#39;salary&#39;</span>, <span style="color:#e6db74">&#39;Whta&#39;</span>: <span style="color:#e6db74">&#39;What&#39;</span>, <span style="color:#e6db74">&#39;narcisist&#39;</span>: <span style="color:#e6db74">&#39;narcissist&#39;</span>, <span style="color:#e6db74">&#39;howdo&#39;</span>: <span style="color:#e6db74">&#39;how do&#39;</span>, <span style="color:#e6db74">&#39;whatare&#39;</span>: <span style="color:#e6db74">&#39;what are&#39;</span>, <span style="color:#e6db74">&#39;howcan&#39;</span>: <span style="color:#e6db74">&#39;how can&#39;</span>, <span style="color:#e6db74">&#39;howmuch&#39;</span>: <span style="color:#e6db74">&#39;how much&#39;</span>, <span style="color:#e6db74">&#39;howmany&#39;</span>: <span style="color:#e6db74">&#39;how many&#39;</span>, <span style="color:#e6db74">&#39;whydo&#39;</span>: <span style="color:#e6db74">&#39;why do&#39;</span>, <span style="color:#e6db74">&#39;doI&#39;</span>: <span style="color:#e6db74">&#39;do I&#39;</span>, <span style="color:#e6db74">&#39;theBest&#39;</span>: <span style="color:#e6db74">&#39;the best&#39;</span>, <span style="color:#e6db74">&#39;howdoes&#39;</span>: <span style="color:#e6db74">&#39;how does&#39;</span>, <span style="color:#e6db74">&#39;mastrubation&#39;</span>: <span style="color:#e6db74">&#39;masturbation&#39;</span>, <span style="color:#e6db74">&#39;mastrubate&#39;</span>: <span style="color:#e6db74">&#39;masturbate&#39;</span>, <span style="color:#e6db74">&#34;mastrubating&#34;</span>: <span style="color:#e6db74">&#39;masturbating&#39;</span>, <span style="color:#e6db74">&#39;pennis&#39;</span>: <span style="color:#e6db74">&#39;penis&#39;</span>, <span style="color:#e6db74">&#39;Etherium&#39;</span>: <span style="color:#e6db74">&#39;Ethereum&#39;</span>, <span style="color:#e6db74">&#39;narcissit&#39;</span>: <span style="color:#e6db74">&#39;narcissist&#39;</span>, <span style="color:#e6db74">&#39;bigdata&#39;</span>: <span style="color:#e6db74">&#39;big data&#39;</span>, <span style="color:#e6db74">&#39;2k17&#39;</span>: <span style="color:#e6db74">&#39;2017&#39;</span>, <span style="color:#e6db74">&#39;2k18&#39;</span>: <span style="color:#e6db74">&#39;2018&#39;</span>, <span style="color:#e6db74">&#39;qouta&#39;</span>: <span style="color:#e6db74">&#39;quota&#39;</span>, <span style="color:#e6db74">&#39;exboyfriend&#39;</span>: <span style="color:#e6db74">&#39;ex boyfriend&#39;</span>, <span style="color:#e6db74">&#39;airhostess&#39;</span>: <span style="color:#e6db74">&#39;air hostess&#39;</span>, <span style="color:#e6db74">&#34;whst&#34;</span>: <span style="color:#e6db74">&#39;what&#39;</span>, <span style="color:#e6db74">&#39;watsapp&#39;</span>: <span style="color:#e6db74">&#39;whatsapp&#39;</span>, <span style="color:#e6db74">&#39;demonitisation&#39;</span>: <span style="color:#e6db74">&#39;demonetization&#39;</span>, <span style="color:#e6db74">&#39;demonitization&#39;</span>: <span style="color:#e6db74">&#39;demonetization&#39;</span>, <span style="color:#e6db74">&#39;demonetisation&#39;</span>: <span style="color:#e6db74">&#39;demonetization&#39;</span>}

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_get_mispell</span>(mispell_dict):
    mispell_re <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;(</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">)&#39;</span> <span style="color:#f92672">%</span> <span style="color:#e6db74">&#39;|&#39;</span><span style="color:#f92672">.</span>join(mispell_dict<span style="color:#f92672">.</span>keys()))
    <span style="color:#66d9ef">return</span> mispell_dict, mispell_re

mispellings, mispellings_re <span style="color:#f92672">=</span> _get_mispell(mispell_dict)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace_typical_misspell</span>(text):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace</span>(match):
        <span style="color:#66d9ef">return</span> mispellings[match<span style="color:#f92672">.</span>group(<span style="color:#ae81ff">0</span>)]
    <span style="color:#66d9ef">return</span> mispellings_re<span style="color:#f92672">.</span>sub(replace, text)

<span style="color:#75715e"># Usage</span>
replace_typical_misspell(<span style="color:#e6db74">&#34;Whta is demonitisation&#34;</span>)</code></pre></div>
<h4 id="d-removing-contractions">d) Removing Contractions:</h4>

<p>Contractions are words that we write with an apostrophe. Examples of contractions are words like &ldquo;ain&rsquo;t&rdquo; or &ldquo;aren&rsquo;t&rdquo;. Since we want to standardize our text, it makes sense to expand these contractions. Below we have done this using a contraction mapping and regex functions.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">contraction_dict <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;ain&#39;t&#34;</span>: <span style="color:#e6db74">&#34;is not&#34;</span>, <span style="color:#e6db74">&#34;aren&#39;t&#34;</span>: <span style="color:#e6db74">&#34;are not&#34;</span>,<span style="color:#e6db74">&#34;can&#39;t&#34;</span>: <span style="color:#e6db74">&#34;cannot&#34;</span>, <span style="color:#e6db74">&#34;&#39;cause&#34;</span>: <span style="color:#e6db74">&#34;because&#34;</span>, <span style="color:#e6db74">&#34;could&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;could have&#34;</span>, <span style="color:#e6db74">&#34;couldn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;could not&#34;</span>, <span style="color:#e6db74">&#34;didn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;did not&#34;</span>,  <span style="color:#e6db74">&#34;doesn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;does not&#34;</span>, <span style="color:#e6db74">&#34;don&#39;t&#34;</span>: <span style="color:#e6db74">&#34;do not&#34;</span>, <span style="color:#e6db74">&#34;hadn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;had not&#34;</span>, <span style="color:#e6db74">&#34;hasn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;has not&#34;</span>, <span style="color:#e6db74">&#34;haven&#39;t&#34;</span>: <span style="color:#e6db74">&#34;have not&#34;</span>, <span style="color:#e6db74">&#34;he&#39;d&#34;</span>: <span style="color:#e6db74">&#34;he would&#34;</span>,<span style="color:#e6db74">&#34;he&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;he will&#34;</span>, <span style="color:#e6db74">&#34;he&#39;s&#34;</span>: <span style="color:#e6db74">&#34;he is&#34;</span>, <span style="color:#e6db74">&#34;how&#39;d&#34;</span>: <span style="color:#e6db74">&#34;how did&#34;</span>, <span style="color:#e6db74">&#34;how&#39;d&#39;y&#34;</span>: <span style="color:#e6db74">&#34;how do you&#34;</span>, <span style="color:#e6db74">&#34;how&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;how will&#34;</span>, <span style="color:#e6db74">&#34;how&#39;s&#34;</span>: <span style="color:#e6db74">&#34;how is&#34;</span>,  <span style="color:#e6db74">&#34;I&#39;d&#34;</span>: <span style="color:#e6db74">&#34;I would&#34;</span>, <span style="color:#e6db74">&#34;I&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;I would have&#34;</span>, <span style="color:#e6db74">&#34;I&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;I will&#34;</span>, <span style="color:#e6db74">&#34;I&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;I will have&#34;</span>,<span style="color:#e6db74">&#34;I&#39;m&#34;</span>: <span style="color:#e6db74">&#34;I am&#34;</span>, <span style="color:#e6db74">&#34;I&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;I have&#34;</span>, <span style="color:#e6db74">&#34;i&#39;d&#34;</span>: <span style="color:#e6db74">&#34;i would&#34;</span>, <span style="color:#e6db74">&#34;i&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;i would have&#34;</span>, <span style="color:#e6db74">&#34;i&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;i will&#34;</span>,  <span style="color:#e6db74">&#34;i&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;i will have&#34;</span>,<span style="color:#e6db74">&#34;i&#39;m&#34;</span>: <span style="color:#e6db74">&#34;i am&#34;</span>, <span style="color:#e6db74">&#34;i&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;i have&#34;</span>, <span style="color:#e6db74">&#34;isn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;is not&#34;</span>, <span style="color:#e6db74">&#34;it&#39;d&#34;</span>: <span style="color:#e6db74">&#34;it would&#34;</span>, <span style="color:#e6db74">&#34;it&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;it would have&#34;</span>, <span style="color:#e6db74">&#34;it&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;it will&#34;</span>, <span style="color:#e6db74">&#34;it&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;it will have&#34;</span>,<span style="color:#e6db74">&#34;it&#39;s&#34;</span>: <span style="color:#e6db74">&#34;it is&#34;</span>, <span style="color:#e6db74">&#34;let&#39;s&#34;</span>: <span style="color:#e6db74">&#34;let us&#34;</span>, <span style="color:#e6db74">&#34;ma&#39;am&#34;</span>: <span style="color:#e6db74">&#34;madam&#34;</span>, <span style="color:#e6db74">&#34;mayn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;may not&#34;</span>, <span style="color:#e6db74">&#34;might&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;might have&#34;</span>,<span style="color:#e6db74">&#34;mightn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;might not&#34;</span>,<span style="color:#e6db74">&#34;mightn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;might not have&#34;</span>, <span style="color:#e6db74">&#34;must&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;must have&#34;</span>, <span style="color:#e6db74">&#34;mustn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;must not&#34;</span>, <span style="color:#e6db74">&#34;mustn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;must not have&#34;</span>, <span style="color:#e6db74">&#34;needn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;need not&#34;</span>, <span style="color:#e6db74">&#34;needn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;need not have&#34;</span>,<span style="color:#e6db74">&#34;o&#39;clock&#34;</span>: <span style="color:#e6db74">&#34;of the clock&#34;</span>, <span style="color:#e6db74">&#34;oughtn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;ought not&#34;</span>, <span style="color:#e6db74">&#34;oughtn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;ought not have&#34;</span>, <span style="color:#e6db74">&#34;shan&#39;t&#34;</span>: <span style="color:#e6db74">&#34;shall not&#34;</span>, <span style="color:#e6db74">&#34;sha&#39;n&#39;t&#34;</span>: <span style="color:#e6db74">&#34;shall not&#34;</span>, <span style="color:#e6db74">&#34;shan&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;shall not have&#34;</span>, <span style="color:#e6db74">&#34;she&#39;d&#34;</span>: <span style="color:#e6db74">&#34;she would&#34;</span>, <span style="color:#e6db74">&#34;she&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;she would have&#34;</span>, <span style="color:#e6db74">&#34;she&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;she will&#34;</span>, <span style="color:#e6db74">&#34;she&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;she will have&#34;</span>, <span style="color:#e6db74">&#34;she&#39;s&#34;</span>: <span style="color:#e6db74">&#34;she is&#34;</span>, <span style="color:#e6db74">&#34;should&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;should have&#34;</span>, <span style="color:#e6db74">&#34;shouldn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;should not&#34;</span>, <span style="color:#e6db74">&#34;shouldn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;should not have&#34;</span>, <span style="color:#e6db74">&#34;so&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;so have&#34;</span>,<span style="color:#e6db74">&#34;so&#39;s&#34;</span>: <span style="color:#e6db74">&#34;so as&#34;</span>, <span style="color:#e6db74">&#34;this&#39;s&#34;</span>: <span style="color:#e6db74">&#34;this is&#34;</span>,<span style="color:#e6db74">&#34;that&#39;d&#34;</span>: <span style="color:#e6db74">&#34;that would&#34;</span>, <span style="color:#e6db74">&#34;that&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;that would have&#34;</span>, <span style="color:#e6db74">&#34;that&#39;s&#34;</span>: <span style="color:#e6db74">&#34;that is&#34;</span>, <span style="color:#e6db74">&#34;there&#39;d&#34;</span>: <span style="color:#e6db74">&#34;there would&#34;</span>, <span style="color:#e6db74">&#34;there&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;there would have&#34;</span>, <span style="color:#e6db74">&#34;there&#39;s&#34;</span>: <span style="color:#e6db74">&#34;there is&#34;</span>, <span style="color:#e6db74">&#34;here&#39;s&#34;</span>: <span style="color:#e6db74">&#34;here is&#34;</span>,<span style="color:#e6db74">&#34;they&#39;d&#34;</span>: <span style="color:#e6db74">&#34;they would&#34;</span>, <span style="color:#e6db74">&#34;they&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;they would have&#34;</span>, <span style="color:#e6db74">&#34;they&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;they will&#34;</span>, <span style="color:#e6db74">&#34;they&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;they will have&#34;</span>, <span style="color:#e6db74">&#34;they&#39;re&#34;</span>: <span style="color:#e6db74">&#34;they are&#34;</span>, <span style="color:#e6db74">&#34;they&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;they have&#34;</span>, <span style="color:#e6db74">&#34;to&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;to have&#34;</span>, <span style="color:#e6db74">&#34;wasn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;was not&#34;</span>, <span style="color:#e6db74">&#34;we&#39;d&#34;</span>: <span style="color:#e6db74">&#34;we would&#34;</span>, <span style="color:#e6db74">&#34;we&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;we would have&#34;</span>, <span style="color:#e6db74">&#34;we&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;we will&#34;</span>, <span style="color:#e6db74">&#34;we&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;we will have&#34;</span>, <span style="color:#e6db74">&#34;we&#39;re&#34;</span>: <span style="color:#e6db74">&#34;we are&#34;</span>, <span style="color:#e6db74">&#34;we&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;we have&#34;</span>, <span style="color:#e6db74">&#34;weren&#39;t&#34;</span>: <span style="color:#e6db74">&#34;were not&#34;</span>, <span style="color:#e6db74">&#34;what&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;what will&#34;</span>, <span style="color:#e6db74">&#34;what&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;what will have&#34;</span>, <span style="color:#e6db74">&#34;what&#39;re&#34;</span>: <span style="color:#e6db74">&#34;what are&#34;</span>,  <span style="color:#e6db74">&#34;what&#39;s&#34;</span>: <span style="color:#e6db74">&#34;what is&#34;</span>, <span style="color:#e6db74">&#34;what&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;what have&#34;</span>, <span style="color:#e6db74">&#34;when&#39;s&#34;</span>: <span style="color:#e6db74">&#34;when is&#34;</span>, <span style="color:#e6db74">&#34;when&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;when have&#34;</span>, <span style="color:#e6db74">&#34;where&#39;d&#34;</span>: <span style="color:#e6db74">&#34;where did&#34;</span>, <span style="color:#e6db74">&#34;where&#39;s&#34;</span>: <span style="color:#e6db74">&#34;where is&#34;</span>, <span style="color:#e6db74">&#34;where&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;where have&#34;</span>, <span style="color:#e6db74">&#34;who&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;who will&#34;</span>, <span style="color:#e6db74">&#34;who&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;who will have&#34;</span>, <span style="color:#e6db74">&#34;who&#39;s&#34;</span>: <span style="color:#e6db74">&#34;who is&#34;</span>, <span style="color:#e6db74">&#34;who&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;who have&#34;</span>, <span style="color:#e6db74">&#34;why&#39;s&#34;</span>: <span style="color:#e6db74">&#34;why is&#34;</span>, <span style="color:#e6db74">&#34;why&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;why have&#34;</span>, <span style="color:#e6db74">&#34;will&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;will have&#34;</span>, <span style="color:#e6db74">&#34;won&#39;t&#34;</span>: <span style="color:#e6db74">&#34;will not&#34;</span>, <span style="color:#e6db74">&#34;won&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;will not have&#34;</span>, <span style="color:#e6db74">&#34;would&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;would have&#34;</span>, <span style="color:#e6db74">&#34;wouldn&#39;t&#34;</span>: <span style="color:#e6db74">&#34;would not&#34;</span>, <span style="color:#e6db74">&#34;wouldn&#39;t&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;would not have&#34;</span>, <span style="color:#e6db74">&#34;y&#39;all&#34;</span>: <span style="color:#e6db74">&#34;you all&#34;</span>, <span style="color:#e6db74">&#34;y&#39;all&#39;d&#34;</span>: <span style="color:#e6db74">&#34;you all would&#34;</span>,<span style="color:#e6db74">&#34;y&#39;all&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;you all would have&#34;</span>,<span style="color:#e6db74">&#34;y&#39;all&#39;re&#34;</span>: <span style="color:#e6db74">&#34;you all are&#34;</span>,<span style="color:#e6db74">&#34;y&#39;all&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;you all have&#34;</span>,<span style="color:#e6db74">&#34;you&#39;d&#34;</span>: <span style="color:#e6db74">&#34;you would&#34;</span>, <span style="color:#e6db74">&#34;you&#39;d&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;you would have&#34;</span>, <span style="color:#e6db74">&#34;you&#39;ll&#34;</span>: <span style="color:#e6db74">&#34;you will&#34;</span>, <span style="color:#e6db74">&#34;you&#39;ll&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;you will have&#34;</span>, <span style="color:#e6db74">&#34;you&#39;re&#34;</span>: <span style="color:#e6db74">&#34;you are&#34;</span>, <span style="color:#e6db74">&#34;you&#39;ve&#34;</span>: <span style="color:#e6db74">&#34;you have&#34;</span>}

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_get_contractions</span>(contraction_dict):
    contraction_re <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;(</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">)&#39;</span> <span style="color:#f92672">%</span> <span style="color:#e6db74">&#39;|&#39;</span><span style="color:#f92672">.</span>join(contraction_dict<span style="color:#f92672">.</span>keys()))
    <span style="color:#66d9ef">return</span> contraction_dict, contraction_re

contractions, contractions_re <span style="color:#f92672">=</span> _get_contractions(contraction_dict)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace_contractions</span>(text):
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace</span>(match):
        <span style="color:#66d9ef">return</span> contractions[match<span style="color:#f92672">.</span>group(<span style="color:#ae81ff">0</span>)]
    <span style="color:#66d9ef">return</span> contractions_re<span style="color:#f92672">.</span>sub(replace, text)

<span style="color:#75715e"># Usage</span>
replace_contractions(<span style="color:#e6db74">&#34;this&#39;s a text with contraction&#34;</span>)</code></pre></div>
<p>Apart from the above techniques, there are other preprocessing techniques of text like Stemming, Lemmatization and Stopword Removal. Since these techniques are not used along with Deep Learning NLP models, we won&rsquo;t talk about them.</p>

<h2 id="representation-sequence-creation">Representation: Sequence Creation</h2>

<p>One of the things that have made Deep Learning the goto choice for NLP is the fact that we don&rsquo;t really have to hand-engineer features from the text data. The deep learning algorithms take as input a sequence of text to learn the structure of text just like a human does. Since Machine cannot understand words they expect their data in numerical form. So we would like to represent out text data as a series of numbers. To understand how this is done we need to understand a little about the Keras Tokenizer function. One can use any other tokenizer also but keras tokenizer seems like a good choice for me.</p>

<h4 id="a-tokenizer">a) Tokenizer:</h4>

<p>In simple words, a tokenizer is a utility function to split a sentence into words.
<code>keras.preprocessing.text.Tokenizer</code> tokenizes(splits) the texts into tokens(words) while keeping only the most occurring words in the text corpus.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e">#Signature:</span>
Tokenizer(num_words<span style="color:#f92672">=</span>None, filters<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">]^_`{|}~</span><span style="color:#ae81ff">\t\n</span><span style="color:#e6db74">&#39;</span>, 
lower<span style="color:#f92672">=</span>True, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39; &#39;</span>, char_level<span style="color:#f92672">=</span>False, oov_token<span style="color:#f92672">=</span>None, document_count<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, <span style="color:#f92672">**</span>kwargs)</code></pre></div>
<p>The num_words parameter keeps a prespecified number of words in the text only. This is helpful as we don&rsquo;t want our models to get a lot of noise by considering words that occur very infrequently. In real-world data, most of the words we leave using num_words param are normally misspells. The tokenizer also filters some non-wanted tokens by default and converts the text into lowercase.</p>

<p>The tokenizer once fitted to the data also keeps an index of words(dictionary of words which we can use to assign a unique number to a word) which can be accessed by tokenizer.word_index. The words in the indexed dictionary are ranked in order of frequencies.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/tokenizer_working.png" style="height:80%;width:80%" ></center>
</div>

<p>So the whole code to use tokenizer is as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> keras.preprocessing.text <span style="color:#f92672">import</span> Tokenizer
<span style="color:#75715e">## Tokenize the sentences</span>
tokenizer <span style="color:#f92672">=</span> Tokenizer(num_words<span style="color:#f92672">=</span>max_features)
tokenizer<span style="color:#f92672">.</span>fit_on_texts(list(train_X)<span style="color:#f92672">+</span>list(test_X))
train_X <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences(train_X)
test_X <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>texts_to_sequences(test_X)</code></pre></div>
<p>where <code>train_X</code> and <code>test_X</code> are lists of documents in the corpus.</p>

<h4 id="b-pad-sequence">b) Pad Sequence:</h4>

<p>Normally our model expects that each sequence(each training example) will be of the same length(same number of words/tokens). We can control this using the <code>maxlen</code> parameter.</p>

<p>For example:
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/pad_seq.png" style="height:40%;width:40%" ></center>
</div></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">train_X <span style="color:#f92672">=</span> pad_sequences(train_X, maxlen<span style="color:#f92672">=</span>maxlen)
test_X <span style="color:#f92672">=</span> pad_sequences(test_X, maxlen<span style="color:#f92672">=</span>maxlen)</code></pre></div>
<p>Now our train data contains a list of list of numbers. Each list has the same length. And we also have the <code>word_index</code> which is a dictionary of most occuring words in the text corpus.</p>

<h2 id="embedding-enrichment">Embedding Enrichment:</h2>

<p>As I said I will be using GLoVE Word2Vec embeddings to explain the enrichment. GLoVE pretrained vectors are trained on the Wikipedia corpus. (You can <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow" target="_blank">download them here</a>). That means some of the words that might be present in your data might not be present in the embeddings. How could we deal with that? Let&rsquo;s first load the Glove Embeddings first.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_glove_index</span>():
    EMBEDDING_FILE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt&#39;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_coefs</span>(word,<span style="color:#f92672">*</span>arr): <span style="color:#66d9ef">return</span> word, np<span style="color:#f92672">.</span>asarray(arr, dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;float32&#39;</span>)[:<span style="color:#ae81ff">300</span>]
    embeddings_index <span style="color:#f92672">=</span> dict(get_coefs(<span style="color:#f92672">*</span>o<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>)) <span style="color:#66d9ef">for</span> o <span style="color:#f92672">in</span> open(EMBEDDING_FILE))
    <span style="color:#66d9ef">return</span> embeddings_index

glove_embedding_index <span style="color:#f92672">=</span> load_glove_index()</code></pre></div>
<p>Be sure to put the path of the folder where you download these GLoVE vectors. What does this <code>glove_embedding_index</code> contain? It is just a dictionary in which the key is the word and the value is the word vector, a <code>np.array</code> of length 300. The length of this dictionary is somewhere around a billion. Since we only want the embeddings of words that are in our <code>word_index</code>, we will create a matrix which just contains required embeddings.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/embedding_matrix_creation.png" style="height:100%;width:100%" ></center>
</div>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_glove</span>(word_index,embeddings_index):
    emb_mean,emb_std <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.005838499</span>,<span style="color:#ae81ff">0.48782197</span>
    all_embs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>stack(embeddings_index<span style="color:#f92672">.</span>values())
    embed_size <span style="color:#f92672">=</span> all_embs<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    nb_words <span style="color:#f92672">=</span> min(max_features, len(word_index))
    embedding_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(emb_mean, emb_std, (nb_words, embed_size))
    count_found <span style="color:#f92672">=</span> nb_words
    <span style="color:#66d9ef">for</span> word, i <span style="color:#f92672">in</span> tqdm(word_index<span style="color:#f92672">.</span>items()):
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> max_features: <span style="color:#66d9ef">continue</span>
        embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word)
        <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None: 
            embedding_matrix[i] <span style="color:#f92672">=</span>  embedding_vector
        <span style="color:#66d9ef">else</span>:
                count_found<span style="color:#f92672">-=</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Got embedding for &#34;</span>,count_found,<span style="color:#e6db74">&#34; words.&#34;</span>)
    <span style="color:#66d9ef">return</span> embedding_matrix</code></pre></div>
<p>The above code works fine but is there a way that we can use the preprocessing in GLoVE to our advantage? Yes. When preprocessing was done for glove, the creators didn&rsquo;t convert the words to lowercase. That means that it contains multiple variations of a word like &lsquo;USA&rsquo;, &lsquo;usa&rsquo; and &lsquo;Usa&rsquo;. That also means that in some cases while a word like &lsquo;Word&rsquo; is present, its analog in lowercase i.e. &lsquo;word&rsquo; is not present. We can get through this situation by using the below code.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_glove</span>(word_index,embeddings_index):
    emb_mean,emb_std <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.005838499</span>,<span style="color:#ae81ff">0.48782197</span>
    all_embs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>stack(embeddings_index<span style="color:#f92672">.</span>values())
    embed_size <span style="color:#f92672">=</span> all_embs<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    nb_words <span style="color:#f92672">=</span> min(max_features, len(word_index))
    embedding_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(emb_mean, emb_std, (nb_words, embed_size))

    count_found <span style="color:#f92672">=</span> nb_words
    <span style="color:#66d9ef">for</span> word, i <span style="color:#f92672">in</span> tqdm(word_index<span style="color:#f92672">.</span>items()):
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> max_features: <span style="color:#66d9ef">continue</span>
        embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word)
        <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None: 
            embedding_matrix[i] <span style="color:#f92672">=</span>  embedding_vector
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">if</span> word<span style="color:#f92672">.</span>islower():
                <span style="color:#75715e"># try to get the embedding of word in titlecase if lowercase is not present</span>
                embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word<span style="color:#f92672">.</span>capitalize())
                <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None: 
                    embedding_matrix[i] <span style="color:#f92672">=</span> embedding_vector
                <span style="color:#66d9ef">else</span>:
                    count_found<span style="color:#f92672">-=</span><span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">else</span>:
                count_found<span style="color:#f92672">-=</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Got embedding for &#34;</span>,count_found,<span style="color:#e6db74">&#34; words.&#34;</span>)
    <span style="color:#66d9ef">return</span> embedding_matrix</code></pre></div>
<p>The above was just an example of how we can use our knowledge of an embedding to get better coverage. Sometimes depending on the problem, one might also derive value by adding extra information to the embeddings using some domain knowledge and NLP skills. For example, we can add external knowledge to the embeddings themselves by adding polarity and subjectivity of a word from the TextBlob package in Python.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> textblob <span style="color:#f92672">import</span> TextBlob
word_sent <span style="color:#f92672">=</span> TextBlob(<span style="color:#e6db74">&#34;good&#34;</span>)<span style="color:#f92672">.</span>sentiment
<span style="color:#66d9ef">print</span>(word_sent<span style="color:#f92672">.</span>polarity,word_sent<span style="color:#f92672">.</span>subjectivity)
<span style="color:#75715e"># 0.7 0.6</span></code></pre></div>
<p>We can get the polarity and subjectivity of any word using TextBlob. Pretty neat. So let us try to add this extra information to our embeddings.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_glove</span>(word_index,embeddings_index):
    emb_mean,emb_std <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.005838499</span>,<span style="color:#ae81ff">0.48782197</span>
    all_embs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>stack(embeddings_index<span style="color:#f92672">.</span>values())
    embed_size <span style="color:#f92672">=</span> all_embs<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    nb_words <span style="color:#f92672">=</span> min(max_features, len(word_index))
    embedding_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(emb_mean, emb_std, (nb_words, embed_size<span style="color:#f92672">+</span><span style="color:#ae81ff">4</span>))
    
    count_found <span style="color:#f92672">=</span> nb_words
    <span style="color:#66d9ef">for</span> word, i <span style="color:#f92672">in</span> tqdm(word_index<span style="color:#f92672">.</span>items()):
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> max_features: <span style="color:#66d9ef">continue</span>
        embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word)
        word_sent <span style="color:#f92672">=</span> TextBlob(word)<span style="color:#f92672">.</span>sentiment
        <span style="color:#75715e"># Extra information we are passing to our embeddings</span>
        extra_embed <span style="color:#f92672">=</span> [word_sent<span style="color:#f92672">.</span>polarity,word_sent<span style="color:#f92672">.</span>subjectivity]
        <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None: 
            embedding_matrix[i] <span style="color:#f92672">=</span>  np<span style="color:#f92672">.</span>append(embedding_vector,extra_embed)
        <span style="color:#66d9ef">else</span>:
            <span style="color:#66d9ef">if</span> word<span style="color:#f92672">.</span>islower():
                embedding_vector <span style="color:#f92672">=</span> embeddings_index<span style="color:#f92672">.</span>get(word<span style="color:#f92672">.</span>capitalize())
                <span style="color:#66d9ef">if</span> embedding_vector <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None: 
                    embedding_matrix[i] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(embedding_vector,extra_embed)
                <span style="color:#66d9ef">else</span>:
                    embedding_matrix[i,<span style="color:#ae81ff">300</span>:] <span style="color:#f92672">=</span> extra_embed
                    count_found<span style="color:#f92672">-=</span><span style="color:#ae81ff">1</span>
            <span style="color:#66d9ef">else</span>:
                embedding_matrix[i,<span style="color:#ae81ff">300</span>:] <span style="color:#f92672">=</span> extra_embed
                count_found<span style="color:#f92672">-=</span><span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Got embedding for &#34;</span>,count_found,<span style="color:#e6db74">&#34; words.&#34;</span>)
    <span style="color:#66d9ef">return</span> embedding_matrix</code></pre></div>
<p>Engineering embeddings is an essential part of getting better performance from the Deep learning models at a later stage. Generally, I revisit this part of code multiple times during the stage of a project while trying to improve my models even further. You can show up a lot of creativity here to improve coverage over your <code>word_index</code> and to include extra features in your embedding.</p>

<h2 id="more-engineered-features">More Engineered Features</h2>

<p><div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/example_nlp_network.png" style="height:100%;width:100%" ></center>
</div>
One can always add sentence specific features like sentence length, number of unique words etc. as another input layer to give extra information to the Deep Neural Network. For example: I created these extra features as part of a feature engineering pipeline for Quora Insincerity Classification Challenge.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add_features</span>(df):
    df[<span style="color:#e6db74">&#39;question_text&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;question_text&#39;</span>]<span style="color:#f92672">.</span>progress_apply(<span style="color:#66d9ef">lambda</span> x:str(x))
    df[<span style="color:#e6db74">&#34;lower_question_text&#34;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#34;question_text&#34;</span>]<span style="color:#f92672">.</span>apply(<span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>lower())
    df[<span style="color:#e6db74">&#39;total_length&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;question_text&#39;</span>]<span style="color:#f92672">.</span>progress_apply(len)
    df[<span style="color:#e6db74">&#39;capitals&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;question_text&#39;</span>]<span style="color:#f92672">.</span>progress_apply(<span style="color:#66d9ef">lambda</span> comment: sum(<span style="color:#ae81ff">1</span> <span style="color:#66d9ef">for</span> c <span style="color:#f92672">in</span> comment <span style="color:#66d9ef">if</span> c<span style="color:#f92672">.</span>isupper()))
    df[<span style="color:#e6db74">&#39;caps_vs_length&#39;</span>] <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>progress_apply(<span style="color:#66d9ef">lambda</span> row: float(row[<span style="color:#e6db74">&#39;capitals&#39;</span>])<span style="color:#f92672">/</span>float(row[<span style="color:#e6db74">&#39;total_length&#39;</span>]),
                                axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    df[<span style="color:#e6db74">&#39;num_words&#39;</span>] <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>question_text<span style="color:#f92672">.</span>str<span style="color:#f92672">.</span>count(<span style="color:#e6db74">&#39;\S+&#39;</span>)
    df[<span style="color:#e6db74">&#39;num_unique_words&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;question_text&#39;</span>]<span style="color:#f92672">.</span>progress_apply(<span style="color:#66d9ef">lambda</span> comment: len(set(w <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> comment<span style="color:#f92672">.</span>split())))
    df[<span style="color:#e6db74">&#39;words_vs_unique&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;num_unique_words&#39;</span>] <span style="color:#f92672">/</span> df[<span style="color:#e6db74">&#39;num_words&#39;</span>] 
    <span style="color:#66d9ef">return</span> df</code></pre></div>
<h2 id="conclusion">Conclusion:</h2>

<p>NLP is still a very interesting problem in Deep Learning space and thus I would encourage you to do a lot of experimentation to see what works and what doesn&rsquo;t. I have tried to provide a wholesome perspective of the preprocessing steps for a Deep Learning Neural network for any NLP problem. But that doesn&rsquo;t mean it is definitive. If you want to learn more about NLP <a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" rel="nofollow" target="_blank">here</a> is an awesome course. You can start for free with the 7-day Free Trial. If you think we can add something to the flow, do mention it in the comments.</p>

<h2 id="endnotes-and-references">Endnotes and References</h2>

<p>This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.</p>

<ul>
<li><a href="https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings" rel="nofollow" target="_blank">How to: Preprocessing when using embeddings</a></li>
<li><a href="https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing" rel="nofollow" target="_blank">Improve your Score with some Text Preprocessing</a></li>
<li><a href="https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm" rel="nofollow" target="_blank">Pytorch baseline</a></li>
<li><a href="https://www.kaggle.com/hengzheng/pytorch-starter" rel="nofollow" target="_blank">Pytorch starter</a></li>
</ul>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/artificial-intelligence/" rel="tag">artificial intelligence</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/deep-learning/" rel="tag">deep learning</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/kaggle/" rel="tag">kaggle</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>
<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.415&subid=0&type=4"><IMG border="0"   alt="Deep Learning Specialization on Coursera" src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.415&subid=0&type=4&gridnum=16"></a>


	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2019/01/06/pytorch_keras_conversion/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">A Layman guide to moving from Keras to Pytorch</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2019/02/08/deeplearning_nlp_conventional_methods/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">NLP  Learning Series: Part 2 - Conventional Methods for Text Classification</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar">
	     
  <div style="text-align:center">    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2019/10/10/hyperopt2/">Automate Hyperparameter Tuning for your models</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/26/building_ml_system/">6 Important Steps to build  a Machine Learning System</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/23/generative_approach_to_classification/">A Generative Approach to Classification</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/02/graph_algs/">Data Scientists, The 5 Graph Algorithms that you should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/01/regex/">The Ultimate Guide to using the Python regex module</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/12/resources/">How did I learn Data Science?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/07/feature_selection/">The 5 Feature Selection Algorithms every Data Scientist should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/30/sampling/">The 5 Sampling Algorithms every Data Scientist need to know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/21/bandits/">Bayesian Bandits explained simply</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/20/pandas_subset/">Minimal Pandas Subset for Data Scientists</a></li>
		</ul>
	</div>
</div>


<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>


<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;}
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: center;
    padding: 10px -5px 10px 3%;}
   
</style>
<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2019 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>