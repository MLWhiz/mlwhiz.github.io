<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>A Layman guide to moving from Keras to Pytorch - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="A Layman guide to moving from Keras to Pytorch - MLWhiz"><meta property="og:description" content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/"><meta property="og:image" content="https://mlwhiz.com/images/artificial-neural-network.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/artificial-neural-network.png"><meta property="article:published_time" content="2019-01-06T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-13T13:34:49+01:00"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/artificial-neural-network.png"><meta name=twitter:title content="A Layman guide to moving from Keras to Pytorch - MLWhiz"><meta name=twitter:description content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/#primaryimage","url":"https://mlwhiz.com/images/artificial-neural-network.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/#webpage","url":"https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/","inLanguage":"en-US","name":"A Layman guide to moving from Keras to Pytorch - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/#primaryimage"},"datePublished":"2019-01-06T00:00:00.00Z","dateModified":"2022-04-13T13:34:49.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"><meta name=impact-site-verification value=1670148355></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>A Layman guide to moving from Keras to Pytorch</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>06 January 2019</span></div><img src=https://mlwhiz.com/images/artificial-neural-network.png class="img-fluid w-100 mb-4" alt="A Layman guide to moving from Keras to Pytorch"><div class="content mb-5"><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/artificial-neural-network.png height=350 width=700></center></div><p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the <strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic</strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.</p><p><strong>As a side note</strong>: If you want to know more about NLP, I would like to recommend this awesome
<a href=https://imp.i384100.net/555ABL target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>Also take a look at my other post:
<a href=/blog/2019/01/17/deeplearning_nlp_preprocess/>Text Preprocessing Methods for Deep Learning</a>
, which talks about different preprocessing techniques you can use for your NLP task and
<a href=/blog/2018/12/17/text_classification/>What Kagglers are using for Text Classification</a>
, which talks about various deep learning models in use in NLP.</p><p>Ok back to the task at hand. <em>While Keras is great to start with deep learning, with time you are going to resent some of its limitations.</em> I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole <code>session.run</code> commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.</p><p>Pytorch helps in that since it seems like the <strong>python way to do things</strong>. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:</p><center><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>I've been using PyTorch a few months now and I've never felt better. I have more energy. My skin is clearer. My eye sight has improved.</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw">May 26, 2017</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></center><p>So without further ado let me translate Keras to Pytorch for you.</p><h2 id=the-classy-way-to-write-your-network>The Classy way to write your network?</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/structured.jpeg height=400 width=700></center></div><p>Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch <strong>take any network you have and try porting it to Pytorch</strong>. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post
<a href=/blog/2018/12/17/text_classification/>here</a>
.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>get_model</span>(features,clipvalue=<span style=color:#3677a9>1.</span>,num_filters=<span style=color:#3677a9>40</span>,dropout=<span style=color:#3677a9>0.1</span>,embed_size=<span style=color:#3677a9>501</span>):
    features_input = Input(shape=(features.shape[<span style=color:#3677a9>1</span>],))
    inp = Input(shape=(maxlen, ))

    <span style=color:#999;font-style:italic># Layer 1: Word2Vec Embeddings.</span>
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)

    <span style=color:#999;font-style:italic># Layer 2: SpatialDropout1D(0.1)</span>
    x = SpatialDropout1D(dropout)(x)

    <span style=color:#999;font-style:italic># Layer 3: Bidirectional CuDNNLSTM</span>
    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)

    <span style=color:#999;font-style:italic># Layer 4: Bidirectional CuDNNGRU</span>
    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)

    <span style=color:#999;font-style:italic># Layer 5: some pooling operations</span>
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)

    <span style=color:#999;font-style:italic># Layer 6: A concatenation of the last state, maximum pool, average pool and</span>
    <span style=color:#999;font-style:italic># additional features</span>
    x = concatenate([avg_pool, x_h, max_pool,features_input])

    <span style=color:#999;font-style:italic># Layer 7: A dense layer</span>
    x = Dense(<span style=color:#3677a9>16</span>, activation=<span style=color:#ed9d13>&#34;relu&#34;</span>)(x)

    <span style=color:#999;font-style:italic># Layer 8: A dropout layer</span>
    x = Dropout(<span style=color:#3677a9>0.1</span>)(x)

    <span style=color:#999;font-style:italic># Layer 9: Output dense layer with one output for our Binary Classification problem.</span>
    outp = Dense(<span style=color:#3677a9>1</span>, activation=<span style=color:#ed9d13>&#34;sigmoid&#34;</span>)(x)

    <span style=color:#999;font-style:italic># Some keras model creation and compiling</span>
    model = Model(inputs=[inp,features_input], outputs=outp)
    adam = optimizers.adam(clipvalue=clipvalue)
    model.compile(loss=<span style=color:#ed9d13>&#39;binary_crossentropy&#39;</span>,
                  optimizer=adam,
                  metrics=[<span style=color:#ed9d13>&#39;accuracy&#39;</span>])
    <span style=color:#6ab825;font-weight:700>return</span> model
</code></pre></div><p>So a model in pytorch is defined as a class(therefore a little more classy) which inherits from <code>nn.module</code> . Every class necessarily contains an <code>__init__</code> procedure block and a block for the <code>forward</code> pass.</p><ul><li><p>In the <code>__init__</code> part the user defines all the layers the network is going to have but doesn&rsquo;t yet define how those layers would be connected to each other</p></li><li><p>In the forward pass block, the user defines how data flows from one layer to another inside the network.</p></li></ul><h4 id=why-is-this-classy>Why is this Classy?</h4><p>Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:</p><ol><li><p>It gives you a <strong>lot of control</strong> on how your network is built.</p></li><li><p>You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)</p></li><li><p><strong>Easy to debug</strong> networks. Any time you find any problem with the network just use something like <code>print("avg_pool", avg_pool.size())</code> in the forward pass to check the sizes of the layer and you will debug the network easily</p></li><li><p>You can <strong>return multiple outputs</strong> from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.</p></li><li><p><strong>Pytorch tensors work in a very similar manner to numpy arrays</strong>. For example, I could have used Pytorch Maxpool function to write the maxpool layer but <code>max_pool, _ = torch.max(h_gru, 1)</code> will also work.</p></li><li><p>You can set up <strong>different layers with different initialization schemes</strong>. Something you won&rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.</p></li><li><p>Wait until you see the <strong>training loop in Pytorch</strong> You will be amazed at the sort of <strong>control</strong> it provides.</p></li></ol><p>Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>Alex_NeuralNet_Meta</span>(nn.Module):
    <span style=color:#6ab825;font-weight:700>def</span> __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix):
        <span style=color:#24909d>super</span>(Alex_NeuralNet_Meta, self).__init__()

        <span style=color:#999;font-style:italic># Initialize some parameters for your model</span>
        self.hidden_size = hidden_size
        drp = <span style=color:#3677a9>0.1</span>

        <span style=color:#999;font-style:italic># Layer 1: Word2Vec Embeddings.</span>
        self.embedding = nn.Embedding(max_features, embed_size)
        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        <span style=color:#999;font-style:italic># Layer 2: Dropout1D(0.1)</span>
        self.embedding_dropout = nn.Dropout2d(<span style=color:#3677a9>0.1</span>)

        <span style=color:#999;font-style:italic># Layer 3: Bidirectional CuDNNLSTM</span>
        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)

        <span style=color:#6ab825;font-weight:700>for</span> name, param <span style=color:#6ab825;font-weight:700>in</span> self.lstm.named_parameters():
            <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#ed9d13>&#39;bias&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.constant_(param, <span style=color:#3677a9>0.0</span>)
            <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#39;weight_ih&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.kaiming_normal_(param)
            <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#39;weight_hh&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.orthogonal_(param)

        <span style=color:#999;font-style:italic># Layer 4: Bidirectional CuDNNGRU</span>
        self.gru = nn.GRU(hidden_size*<span style=color:#3677a9>2</span>, hidden_size, bidirectional=True, batch_first=True)

        <span style=color:#6ab825;font-weight:700>for</span> name, param <span style=color:#6ab825;font-weight:700>in</span> self.gru.named_parameters():
            <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#ed9d13>&#39;bias&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.constant_(param, <span style=color:#3677a9>0.0</span>)
            <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#39;weight_ih&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.kaiming_normal_(param)
            <span style=color:#6ab825;font-weight:700>elif</span> <span style=color:#ed9d13>&#39;weight_hh&#39;</span> <span style=color:#6ab825;font-weight:700>in</span> name:
                 nn.init.orthogonal_(param)

        <span style=color:#999;font-style:italic># Layer 7: A dense layer</span>
        self.linear = nn.Linear(hidden_size*<span style=color:#3677a9>6</span> + features.shape[<span style=color:#3677a9>1</span>], lin_size)
        self.relu = nn.ReLU()

        <span style=color:#999;font-style:italic># Layer 8: A dropout layer</span>
        self.dropout = nn.Dropout(drp)

        <span style=color:#999;font-style:italic># Layer 9: Output dense layer with one output for our Binary Classification problem.</span>
        self.out = nn.Linear(lin_size, <span style=color:#3677a9>1</span>)

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>forward</span>(self, x):
        <span style=color:#ed9d13>&#39;&#39;&#39;
</span><span style=color:#ed9d13>        here x[0] represents the first element of the input that is going to be passed.
</span><span style=color:#ed9d13>        We are going to pass a tuple where first one contains the sequences(x[0])
</span><span style=color:#ed9d13>        and the second one is a additional feature vector(x[1])
</span><span style=color:#ed9d13>        &#39;&#39;&#39;</span>
        h_embedding = self.embedding(x[<span style=color:#3677a9>0</span>])
        <span style=color:#999;font-style:italic># Based on comment by Ivank to integrate spatial dropout.</span>
        embeddings = h_embedding.unsqueeze(<span style=color:#3677a9>2</span>)    <span style=color:#999;font-style:italic># (N, T, 1, K)</span>
        embeddings = embeddings.permute(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>3</span>, <span style=color:#3677a9>2</span>, <span style=color:#3677a9>1</span>)  <span style=color:#999;font-style:italic># (N, K, 1, T)</span>
        embeddings = self.embedding_dropout(embeddings)  <span style=color:#999;font-style:italic># (N, K, 1, T), some features are masked</span>
        embeddings = embeddings.permute(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>3</span>, <span style=color:#3677a9>2</span>, <span style=color:#3677a9>1</span>)  <span style=color:#999;font-style:italic># (N, T, 1, K)</span>
        h_embedding = embeddings.squeeze(<span style=color:#3677a9>2</span>)  <span style=color:#999;font-style:italic># (N, T, K)</span>
        <span style=color:#999;font-style:italic>#h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))</span>

        <span style=color:#999;font-style:italic>#print(&#34;emb&#34;, h_embedding.size())</span>
        h_lstm, _ = self.lstm(h_embedding)
        <span style=color:#999;font-style:italic>#print(&#34;lst&#34;,h_lstm.size())</span>
        h_gru, hh_gru = self.gru(h_lstm)
        hh_gru = hh_gru.view(-<span style=color:#3677a9>1</span>, <span style=color:#3677a9>2</span>*self.hidden_size )
        <span style=color:#999;font-style:italic>#print(&#34;gru&#34;, h_gru.size())</span>
        <span style=color:#999;font-style:italic>#print(&#34;h_gru&#34;, hh_gru.size())</span>

        <span style=color:#999;font-style:italic># Layer 5: is defined dynamically as an operation on tensors.</span>
        avg_pool = torch.mean(h_gru, <span style=color:#3677a9>1</span>)
        max_pool, _ = torch.max(h_gru, <span style=color:#3677a9>1</span>)
        <span style=color:#999;font-style:italic>#print(&#34;avg_pool&#34;, avg_pool.size())</span>
        <span style=color:#999;font-style:italic>#print(&#34;max_pool&#34;, max_pool.size())</span>

        <span style=color:#999;font-style:italic># the extra features you want to give to the model</span>
        f = torch.tensor(x[<span style=color:#3677a9>1</span>], dtype=torch.float).cuda()
        <span style=color:#999;font-style:italic>#print(&#34;f&#34;, f.size())</span>

        <span style=color:#999;font-style:italic># Layer 6: A concatenation of the last state, maximum pool, average pool and</span>
        <span style=color:#999;font-style:italic># additional features</span>
        conc = torch.cat(( hh_gru, avg_pool, max_pool,f), <span style=color:#3677a9>1</span>)
        <span style=color:#999;font-style:italic>#print(&#34;conc&#34;, conc.size())</span>

        <span style=color:#999;font-style:italic># passing conc through linear and relu ops</span>
        conc = self.relu(self.linear(conc))
        conc = self.dropout(conc)
        out = self.out(conc)
        <span style=color:#999;font-style:italic># return the final output</span>
        <span style=color:#6ab825;font-weight:700>return</span> out
</code></pre></div><p>Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.</p><h2 id=tailored-or-readymade-the-best-fit-with-a-highly-customizable-training-loop>Tailored or Readymade: The Best Fit with a highly customizable Training Loop</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/sewing-machine.jpg height=300 width=700></center></div><p>In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.</p><p>This training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.</p><p>So a brief summary of this loop are as follows:</p><ul><li>Create stratified splits using train data</li><li>Loop through the splits.<ul><li>Convert your train and CV data to tensor and load your data to the GPU using the
<code>X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()</code> command</li><li>Load the model onto the GPU using the <code>model.cuda()</code> command</li><li>Define Loss function, Scheduler and Optimizer</li><li>create <code>train_loader</code> and valid_loader` to iterate through batches.</li><li>Start running epochs. In each epoch<ul><li>Set the model mode to train using <code>model.train()</code>.</li><li>Go through the batches in <code>train_loader</code> and run the forward pass</li><li>Run a scheduler step to change the learning rate</li><li>Compute loss</li><li>Set the existing gradients in the optimizer to zero</li><li>Backpropagate the losses through the network</li><li>Clip the gradients</li><li>Take an optimizer step to change the weights in the whole network</li><li>Set the model mode to eval using <code>model.eval()</code>.</li><li>Get predictions for the validation data using <code>valid_loader</code> and store in variable <code>valid_preds_fold</code></li><li>Calculate Loss and print</li></ul></li><li>After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final <code>test_preds</code></li><li>Get Out-of-fold(OOF) predictions for train set using <code>train_preds[valid_idx] = valid_preds_fold</code></li><li>These OOF predictions can then be used to calculate the Local CV score for your model.</li></ul></li></ul><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>pytorch_model_run_cv</span>(x_train,y_train,features,x_test, model_obj, feats = False,clip = True):
    seed_everything()
    avg_losses_f = []
    avg_val_losses_f = []
    <span style=color:#999;font-style:italic># matrix for the out-of-fold predictions</span>
    train_preds = np.zeros((<span style=color:#24909d>len</span>(x_train)))
    <span style=color:#999;font-style:italic># matrix for the predictions on the test set</span>
    test_preds = np.zeros((<span style=color:#24909d>len</span>(x_test)))
    splits = <span style=color:#24909d>list</span>(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))
    <span style=color:#6ab825;font-weight:700>for</span> i, (train_idx, valid_idx) <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(splits):
        seed_everything(i*<span style=color:#3677a9>1000</span>+i)
        x_train = np.array(x_train)
        y_train = np.array(y_train)
        <span style=color:#6ab825;font-weight:700>if</span> feats:
            features = np.array(features)
        x_train_fold = torch.tensor(x_train[train_idx.astype(<span style=color:#24909d>int</span>)], dtype=torch.long).cuda()
        y_train_fold = torch.tensor(y_train[train_idx.astype(<span style=color:#24909d>int</span>), np.newaxis], dtype=torch.float32).cuda()
        <span style=color:#6ab825;font-weight:700>if</span> feats:
            kfold_X_features = features[train_idx.astype(<span style=color:#24909d>int</span>)]
            kfold_X_valid_features = features[valid_idx.astype(<span style=color:#24909d>int</span>)]
        x_val_fold = torch.tensor(x_train[valid_idx.astype(<span style=color:#24909d>int</span>)], dtype=torch.long).cuda()
        y_val_fold = torch.tensor(y_train[valid_idx.astype(<span style=color:#24909d>int</span>), np.newaxis], dtype=torch.float32).cuda()

        model = copy.deepcopy(model_obj)

        model.cuda()

        loss_fn = torch.nn.BCEWithLogitsLoss(reduction=<span style=color:#ed9d13>&#39;sum&#39;</span>)

        step_size = <span style=color:#3677a9>300</span>
        base_lr, max_lr = <span style=color:#3677a9>0.001</span>, <span style=color:#3677a9>0.003</span>
        optimizer = torch.optim.Adam(<span style=color:#24909d>filter</span>(<span style=color:#6ab825;font-weight:700>lambda</span> p: p.requires_grad, model.parameters()),
                                 lr=max_lr)

        <span style=color:#999;font-style:italic>################################################################################################</span>
        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,
                   step_size=step_size, mode=<span style=color:#ed9d13>&#39;exp_range&#39;</span>,
                   gamma=<span style=color:#3677a9>0.99994</span>)
        <span style=color:#999;font-style:italic>###############################################################################################</span>

        train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold))
        valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold))

        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)
        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)

        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#39;Fold {i + 1}&#39;</span>)
        <span style=color:#6ab825;font-weight:700>for</span> epoch <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(n_epochs):
            start_time = time.time()
            model.train()

            avg_loss = <span style=color:#3677a9>0.</span>
            <span style=color:#6ab825;font-weight:700>for</span> i, (x_batch, y_batch, index) <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(train_loader):
                <span style=color:#6ab825;font-weight:700>if</span> feats:
                    f = kfold_X_features[index]
                    y_pred = model([x_batch,f])
                <span style=color:#6ab825;font-weight:700>else</span>:
                    y_pred = model(x_batch)

                <span style=color:#6ab825;font-weight:700>if</span> scheduler:
                    scheduler.batch_step()

                <span style=color:#999;font-style:italic># Compute and print loss.</span>
                loss = loss_fn(y_pred, y_batch)
                optimizer.zero_grad()
                loss.backward()
                <span style=color:#6ab825;font-weight:700>if</span> clip:
                    nn.utils.clip_grad_norm_(model.parameters(),<span style=color:#3677a9>1</span>)
                optimizer.step()
                avg_loss += loss.item() / <span style=color:#24909d>len</span>(train_loader)

            model.eval()

            valid_preds_fold = np.zeros((x_val_fold.size(<span style=color:#3677a9>0</span>)))
            test_preds_fold = np.zeros((<span style=color:#24909d>len</span>(x_test)))

            avg_val_loss = <span style=color:#3677a9>0.</span>
            <span style=color:#6ab825;font-weight:700>for</span> i, (x_batch, y_batch,index) <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(valid_loader):
                <span style=color:#6ab825;font-weight:700>if</span> feats:
                    f = kfold_X_valid_features[index]
                    y_pred = model([x_batch,f]).detach()
                <span style=color:#6ab825;font-weight:700>else</span>:
                    y_pred = model(x_batch).detach()

                avg_val_loss += loss_fn(y_pred, y_batch).item() / <span style=color:#24909d>len</span>(valid_loader)
                valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, <span style=color:#3677a9>0</span>]

            elapsed_time = time.time() - start_time
            <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#39;Epoch {}/{} </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> loss={:.4f} </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> val_loss={:.4f} </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> time={:.2f}s&#39;</span>.format(
                epoch + <span style=color:#3677a9>1</span>, n_epochs, avg_loss, avg_val_loss, elapsed_time))
        avg_losses_f.append(avg_loss)
        avg_val_losses_f.append(avg_val_loss)
        <span style=color:#999;font-style:italic># predict all samples in the test set batch per batch</span>
        <span style=color:#6ab825;font-weight:700>for</span> i, (x_batch,) <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(test_loader):
            <span style=color:#6ab825;font-weight:700>if</span> feats:
                f = test_features[i * batch_size:(i+<span style=color:#3677a9>1</span>) * batch_size]
                y_pred = model([x_batch,f]).detach()
            <span style=color:#6ab825;font-weight:700>else</span>:
                y_pred = model(x_batch).detach()

            test_preds_fold[i * batch_size:(i+<span style=color:#3677a9>1</span>) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, <span style=color:#3677a9>0</span>]

        train_preds[valid_idx] = valid_preds_fold
        test_preds += test_preds_fold / <span style=color:#24909d>len</span>(splits)

    <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#39;All </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> loss={:.4f} </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> val_loss={:.4f} </span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13> &#39;</span>.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))
    <span style=color:#6ab825;font-weight:700>return</span> train_preds, test_preds

</code></pre></div><h4 id=but-why-why-so-much-code>But Why? Why so much code?</h4><p>Okay. I get it. That was probably a handful. What you could have done with a simple<code>.fit</code> in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:</p><ul><li>While in Keras you have prespecified schedulers like <code>ReduceLROnPlateau</code> (and it is a task to write them), in Pytorch you can experiment like crazy. <strong>If you know how to write Python you are going to get along just fine</strong></li><li>Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly.</li><li>And much more. It is only your imagination that will stop you.</li></ul><h2 id=wanna-run-it-yourself>Wanna Run it Yourself?</h2><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/tools.jpg alt="You have all the tools it seems" height=400 width=700></center></div><p>So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the <code>seed_everything</code>, <code>MyDataset</code> and <code>CyclicLR</code> (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a
<a href=https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch/edit target=_blank rel="nofollow noopener">Kaggle Kernel</a>
with the whole running code. You can see the code here and include it in your projects.</p><p>If you liked this post, <strong>please don&rsquo;t forget to upvote the
<a href=https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch/edit target=_blank rel="nofollow noopener">Kernel</a>
too.</strong> I will be obliged.</p><h2 id=endnotes-and-references>Endnotes and References</h2><p>This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.</p><ul><li><a href=https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644 target=_blank rel="nofollow noopener">Discussion on 3rd Place winner model in Toxic comment</a></li><li><a href=https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model target=_blank rel="nofollow noopener">3rd Place model in Keras by Larry Freeman</a></li><li><a href=https://www.kaggle.com/spirosrap/bilstm-attention-kfold-clr-extra-features-capsule target=_blank rel="nofollow noopener">Pytorch starter Capsule model</a></li><li><a href=https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings target=_blank rel="nofollow noopener">How to: Preprocessing when using embeddings</a></li><li><a href=https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing target=_blank rel="nofollow noopener">Improve your Score with some Text Preprocessing</a></li><li><a href=https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm target=_blank rel="nofollow noopener">Pytorch baseline</a></li><li><a href=https://www.kaggle.com/hengzheng/pytorch-starter target=_blank rel="nofollow noopener">Pytorch starter</a></li></ul><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="MLWhiz - Your Home for DS, ML, AI!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>