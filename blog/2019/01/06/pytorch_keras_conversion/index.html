<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>A Layman guide to moving from Keras to Pytorch</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings.">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="A Layman guide to moving from Keras to Pytorch" />
<meta property="og:description" content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/" />
<meta property="og:image" content="https://mlwhiz.com/images/artificial-neural-network.png" />
<meta property="article:published_time" content="2019-01-06T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-01-06T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/artificial-neural-network.png"/>

<meta name="twitter:title" content="A Layman guide to moving from Keras to Pytorch"/>
<meta name="twitter:description" content="Recently I started up with a competition on kaggle on text classification, and as a part of the competition I had to somehow move to Pytorch to get deterministic results. Here are some of my findings."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/atom.xml">RSS</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">A Layman guide to moving from Keras to Pytorch</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-01-06T00:00:00">January 06, 2019</time>
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/artificial-neural-network.png"  height="350" width="700" ></center>
</div>

<p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the <strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic</strong>, even after setting the seeds. So Pytorch did come to rescue. And am  I  glad that I moved.</p>

<p>As a <strong>side note</strong>: if you want to know more about <strong>NLP</strong>, I would like to recommend this awesome course on <strong><a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Natural Language Processing</a></strong> in the <strong><a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Advanced machine learning specialization</a></strong>. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: Sentiment Analysis, summarization, dialogue state tracking, to name a few.</p>

<p>Also take a look at my other post: <a href="/blog/2019/01/17/deeplearning_nlp_preprocess/">Text Preprocessing Methods for Deep Learning</a>, which talks about different preprocessing techniques you can use for your NLP task and <a href="/blog/2018/12/17/text_classification/">What Kagglers are using for Text Classification</a>, which talks about various deep learning models in use in NLP.</p>

<p>Ok back to the task at hand. <em>While Keras is great to start with deep learning, with time you are going to resent some of its limitations.</em> I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole <code>session.run</code> commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.</p>

<p>Pytorch helps in that since it seems like the <strong>python way to do things</strong>. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:</p>

<p><center><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;ve been using PyTorch a few months now and I&#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.</p>&mdash; Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw">May 26, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></center></p>

<p>So without further ado let me translate Keras to Pytorch for you.</p>

<h2 id="the-classy-way-to-write-your-network">The Classy way to write your network?</h2>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/structured.jpeg"  height="400" width="700" ></center>
</div>

<p>Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch <strong>take any network you have and try porting it to Pytorch</strong>. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post <a href="/blog/2018/12/17/text_classification/">here</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_model</span>(features,clipvalue<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>,num_filters<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>,dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,embed_size<span style="color:#f92672">=</span><span style="color:#ae81ff">501</span>):
    features_input <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>(features<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>],))
    inp <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>(maxlen, ))
    
    <span style="color:#75715e"># Layer 1: Word2Vec Embeddings.</span>
    x <span style="color:#f92672">=</span> Embedding(max_features, embed_size, weights<span style="color:#f92672">=</span>[embedding_matrix], trainable<span style="color:#f92672">=</span>False)(inp)
    
    <span style="color:#75715e"># Layer 2: SpatialDropout1D(0.1)</span>
    x <span style="color:#f92672">=</span> SpatialDropout1D(dropout)(x)
    
    <span style="color:#75715e"># Layer 3: Bidirectional CuDNNLSTM</span>
    x <span style="color:#f92672">=</span> Bidirectional(LSTM(num_filters, return_sequences<span style="color:#f92672">=</span>True))(x)

    <span style="color:#75715e"># Layer 4: Bidirectional CuDNNGRU</span>
    x, x_h, x_c <span style="color:#f92672">=</span> Bidirectional(GRU(num_filters, return_sequences<span style="color:#f92672">=</span>True, return_state <span style="color:#f92672">=</span> True))(x)  
    
    <span style="color:#75715e"># Layer 5: some pooling operations</span>
    avg_pool <span style="color:#f92672">=</span> GlobalAveragePooling1D()(x)
    max_pool <span style="color:#f92672">=</span> GlobalMaxPooling1D()(x)
    
    <span style="color:#75715e"># Layer 6: A concatenation of the last state, maximum pool, average pool and </span>
    <span style="color:#75715e"># additional features</span>
    x <span style="color:#f92672">=</span> concatenate([avg_pool, x_h, max_pool,features_input])
    
    <span style="color:#75715e"># Layer 7: A dense layer</span>
    x <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">16</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>)(x)

    <span style="color:#75715e"># Layer 8: A dropout layer</span>
    x <span style="color:#f92672">=</span> Dropout(<span style="color:#ae81ff">0.1</span>)(x)
    
    <span style="color:#75715e"># Layer 9: Output dense layer with one output for our Binary Classification problem.</span>
    outp <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)(x)

    <span style="color:#75715e"># Some keras model creation and compiling</span>
    model <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>[inp,features_input], outputs<span style="color:#f92672">=</span>outp)
    adam <span style="color:#f92672">=</span> optimizers<span style="color:#f92672">.</span>adam(clipvalue<span style="color:#f92672">=</span>clipvalue)
    model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
                  optimizer<span style="color:#f92672">=</span>adam,
                  metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
    <span style="color:#66d9ef">return</span> model</code></pre></div>
<p>So a model in pytorch is defined as a class(therefore a little more classy) which inherits from <code>nn.module</code> . Every class necessarily contains an <code>__init__</code> procedure block and a block for the <code>forward</code> pass.</p>

<ul>
<li><p>In the <code>__init__</code> part the user defines all the layers the network is going to have but doesn&rsquo;t yet define how those layers would be connected to each other</p></li>

<li><p>In the forward pass block, the user defines how data flows from one layer to another inside the network.</p></li>
</ul>

<h4 id="why-is-this-classy">Why is this Classy?</h4>

<p>Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:</p>

<p>1) It gives you a <strong>lot of control</strong> on how your network is built.</p>

<p>2) You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)</p>

<p>3) <strong>Easy to debug</strong> networks. Any time you find any problem with the network just use something like <code>print(&quot;avg_pool&quot;, avg_pool.size())</code> in the forward pass to check the sizes of the layer and you will debug the network easily</p>

<p>4) You can <strong>return multiple outputs</strong> from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.</p>

<p>5) <strong>Pytorch tensors work in a very similar manner to numpy arrays</strong>. For example, I could have used Pytorch Maxpool function to write the maxpool layer but <code>max_pool, _ = torch.max(h_gru, 1)</code> will also work.</p>

<p>6) You can set up <strong>different layers with different initialization schemes</strong>. Something you won&rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.</p>

<p>7) Wait until you see the <strong>training loop in Pytorch</strong> You will be amazed at the sort of <strong>control</strong> it provides.</p>

<p>Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Alex_NeuralNet_Meta</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self,hidden_size,lin_size, embedding_matrix<span style="color:#f92672">=</span>embedding_matrix):
        super(Alex_NeuralNet_Meta, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># Initialize some parameters for your model</span>
        self<span style="color:#f92672">.</span>hidden_size <span style="color:#f92672">=</span> hidden_size
        drp <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>

        <span style="color:#75715e"># Layer 1: Word2Vec Embeddings.</span>
        self<span style="color:#f92672">.</span>embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(max_features, embed_size)
        self<span style="color:#f92672">.</span>embedding<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>tensor(embedding_matrix, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32))
        self<span style="color:#f92672">.</span>embedding<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> False

        <span style="color:#75715e"># Layer 2: Dropout1D(0.1)</span>
        self<span style="color:#f92672">.</span>embedding_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout2d(<span style="color:#ae81ff">0.1</span>)

        <span style="color:#75715e"># Layer 3: Bidirectional CuDNNLSTM</span>
        self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embed_size, hidden_size, bidirectional<span style="color:#f92672">=</span>True, batch_first<span style="color:#f92672">=</span>True)

        <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>lstm<span style="color:#f92672">.</span>named_parameters():
            <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;bias&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(param, <span style="color:#ae81ff">0.0</span>)
            <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#39;weight_ih&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>kaiming_normal_(param)
            <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#39;weight_hh&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>orthogonal_(param)

        <span style="color:#75715e"># Layer 4: Bidirectional CuDNNGRU</span>
        self<span style="color:#f92672">.</span>gru <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GRU(hidden_size<span style="color:#f92672">*</span><span style="color:#ae81ff">2</span>, hidden_size, bidirectional<span style="color:#f92672">=</span>True, batch_first<span style="color:#f92672">=</span>True)

        <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>gru<span style="color:#f92672">.</span>named_parameters():
            <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;bias&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(param, <span style="color:#ae81ff">0.0</span>)
            <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#39;weight_ih&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>kaiming_normal_(param)
            <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#39;weight_hh&#39;</span> <span style="color:#f92672">in</span> name:
                 nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>orthogonal_(param)

        <span style="color:#75715e"># Layer 7: A dense layer</span>
        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size<span style="color:#f92672">*</span><span style="color:#ae81ff">6</span> <span style="color:#f92672">+</span> features<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], lin_size)
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        
        <span style="color:#75715e"># Layer 8: A dropout layer </span>
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(drp)

        <span style="color:#75715e"># Layer 9: Output dense layer with one output for our Binary Classification problem.</span>
        self<span style="color:#f92672">.</span>out <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(lin_size, <span style="color:#ae81ff">1</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#e6db74">&#39;&#39;&#39;
</span><span style="color:#e6db74">        here x[0] represents the first element of the input that is going to be passed. 
</span><span style="color:#e6db74">        We are going to pass a tuple where first one contains the sequences(x[0])
</span><span style="color:#e6db74">        and the second one is a additional feature vector(x[1])
</span><span style="color:#e6db74">        &#39;&#39;&#39;</span>
        h_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding(x[<span style="color:#ae81ff">0</span>])
        <span style="color:#75715e"># Based on comment by Ivank to integrate spatial dropout. </span>
        embeddings <span style="color:#f92672">=</span> h_embedding<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>)    <span style="color:#75715e"># (N, T, 1, K)</span>
        embeddings <span style="color:#f92672">=</span> embeddings<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (N, K, 1, T)</span>
        embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_dropout(embeddings)  <span style="color:#75715e"># (N, K, 1, T), some features are masked</span>
        embeddings <span style="color:#f92672">=</span> embeddings<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (N, T, 1, K)</span>
        h_embedding <span style="color:#f92672">=</span> embeddings<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># (N, T, K)</span>
        <span style="color:#75715e">#h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))</span>
        
        <span style="color:#75715e">#print(&#34;emb&#34;, h_embedding.size())</span>
        h_lstm, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(h_embedding)
        <span style="color:#75715e">#print(&#34;lst&#34;,h_lstm.size())</span>
        h_gru, hh_gru <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gru(h_lstm)
        hh_gru <span style="color:#f92672">=</span> hh_gru<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>hidden_size )
        <span style="color:#75715e">#print(&#34;gru&#34;, h_gru.size())</span>
        <span style="color:#75715e">#print(&#34;h_gru&#34;, hh_gru.size())</span>

        <span style="color:#75715e"># Layer 5: is defined dynamically as an operation on tensors.</span>
        avg_pool <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean(h_gru, <span style="color:#ae81ff">1</span>)
        max_pool, _ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(h_gru, <span style="color:#ae81ff">1</span>)
        <span style="color:#75715e">#print(&#34;avg_pool&#34;, avg_pool.size())</span>
        <span style="color:#75715e">#print(&#34;max_pool&#34;, max_pool.size())</span>
        
        <span style="color:#75715e"># the extra features you want to give to the model</span>
        f <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(x[<span style="color:#ae81ff">1</span>], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>cuda()
        <span style="color:#75715e">#print(&#34;f&#34;, f.size())</span>

        <span style="color:#75715e"># Layer 6: A concatenation of the last state, maximum pool, average pool and </span>
        <span style="color:#75715e"># additional features</span>
        conc <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(( hh_gru, avg_pool, max_pool,f), <span style="color:#ae81ff">1</span>)
        <span style="color:#75715e">#print(&#34;conc&#34;, conc.size())</span>

        <span style="color:#75715e"># passing conc through linear and relu ops</span>
        conc <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear(conc))
        conc <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(conc)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>out(conc)
        <span style="color:#75715e"># return the final output</span>
        <span style="color:#66d9ef">return</span> out</code></pre></div>
<p>Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.</p>

<h2 id="tailored-or-readymade-the-best-fit-with-a-highly-customizable-training-loop">Tailored or Readymade: The Best Fit with a highly customizable Training Loop</h2>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/sewing-machine.jpg"  height="300" width="700" ></center>
</div>

<p>In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.</p>

<p>This training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.</p>

<p>So a brief summary of this loop are as follows:</p>

<ul>
<li>Create stratified splits using train data</li>
<li>Loop through the splits.

<ul>
<li>Convert your train and CV data to tensor and load your data to the GPU using the
<code>X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()</code> command</li>
<li>Load the model onto the GPU using the <code>model.cuda()</code> command</li>
<li>Define Loss function, Scheduler and Optimizer</li>
<li>create <code>train_loader</code>    and     valid_loader` to iterate through batches.</li>
<li>Start running epochs. In each epoch

<ul>
<li>Set the model mode to train using <code>model.train()</code>.</li>
<li>Go through the batches in <code>train_loader</code> and run the forward pass</li>
<li>Run a scheduler step to change the learning rate</li>
<li>Compute loss</li>
<li>Set the existing gradients in the optimizer to zero</li>
<li>Backpropagate the losses through the network</li>
<li>Clip the gradients</li>
<li>Take an optimizer step to change the weights in the whole network</li>
<li>Set the model mode to eval using <code>model.eval()</code>.</li>
<li>Get predictions for the validation data using <code>valid_loader</code> and store in variable <code>valid_preds_fold</code></li>
<li>Calculate Loss and print</li>
</ul></li>
<li>After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final <code>test_preds</code></li>
<li>Get Out-of-fold(OOF) predictions for train set using <code>train_preds[valid_idx] = valid_preds_fold</code></li>
<li>These OOF predictions can then be used to calculate the Local CV score for your model.</li>
</ul></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pytorch_model_run_cv</span>(x_train,y_train,features,x_test, model_obj, feats <span style="color:#f92672">=</span> False,clip <span style="color:#f92672">=</span> True):
    seed_everything()
    avg_losses_f <span style="color:#f92672">=</span> []
    avg_val_losses_f <span style="color:#f92672">=</span> []
    <span style="color:#75715e"># matrix for the out-of-fold predictions</span>
    train_preds <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(x_train)))
    <span style="color:#75715e"># matrix for the predictions on the test set</span>
    test_preds <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(x_test)))
    splits <span style="color:#f92672">=</span> list(StratifiedKFold(n_splits<span style="color:#f92672">=</span>n_splits, shuffle<span style="color:#f92672">=</span>True, random_state<span style="color:#f92672">=</span>SEED)<span style="color:#f92672">.</span>split(x_train, y_train))
    <span style="color:#66d9ef">for</span> i, (train_idx, valid_idx) <span style="color:#f92672">in</span> enumerate(splits):
        seed_everything(i<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#f92672">+</span>i)
        x_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(x_train)
        y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(y_train)
        <span style="color:#66d9ef">if</span> feats:
            features <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(features)
        x_train_fold <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(x_train[train_idx<span style="color:#f92672">.</span>astype(int)], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)<span style="color:#f92672">.</span>cuda()
        y_train_fold <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(y_train[train_idx<span style="color:#f92672">.</span>astype(int), np<span style="color:#f92672">.</span>newaxis], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>cuda()
        <span style="color:#66d9ef">if</span> feats:
            kfold_X_features <span style="color:#f92672">=</span> features[train_idx<span style="color:#f92672">.</span>astype(int)]
            kfold_X_valid_features <span style="color:#f92672">=</span> features[valid_idx<span style="color:#f92672">.</span>astype(int)]
        x_val_fold <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(x_train[valid_idx<span style="color:#f92672">.</span>astype(int)], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)<span style="color:#f92672">.</span>cuda()
        y_val_fold <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(y_train[valid_idx<span style="color:#f92672">.</span>astype(int), np<span style="color:#f92672">.</span>newaxis], dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>cuda()
        
        model <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(model_obj)

        model<span style="color:#f92672">.</span>cuda()

        loss_fn <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>BCEWithLogitsLoss(reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sum&#39;</span>)

        step_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span>
        base_lr, max_lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.001</span>, <span style="color:#ae81ff">0.003</span>   
        optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(filter(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>requires_grad, model<span style="color:#f92672">.</span>parameters()), 
                                 lr<span style="color:#f92672">=</span>max_lr)
        
        <span style="color:#75715e">################################################################################################</span>
        scheduler <span style="color:#f92672">=</span> CyclicLR(optimizer, base_lr<span style="color:#f92672">=</span>base_lr, max_lr<span style="color:#f92672">=</span>max_lr,
                   step_size<span style="color:#f92672">=</span>step_size, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;exp_range&#39;</span>,
                   gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.99994</span>)
        <span style="color:#75715e">###############################################################################################</span>

        train <span style="color:#f92672">=</span> MyDataset(torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TensorDataset(x_train_fold, y_train_fold))
        valid <span style="color:#f92672">=</span> MyDataset(torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TensorDataset(x_val_fold, y_val_fold))
        
        train_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(train, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span>True)
        valid_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(valid, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span>False)

        <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#39;Fold {i + 1}&#39;</span>)
        <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epochs):
            start_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()
            model<span style="color:#f92672">.</span>train()

            avg_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>  
            <span style="color:#66d9ef">for</span> i, (x_batch, y_batch, index) <span style="color:#f92672">in</span> enumerate(train_loader):
                <span style="color:#66d9ef">if</span> feats:       
                    f <span style="color:#f92672">=</span> kfold_X_features[index]
                    y_pred <span style="color:#f92672">=</span> model([x_batch,f])
                <span style="color:#66d9ef">else</span>:
                    y_pred <span style="color:#f92672">=</span> model(x_batch)

                <span style="color:#66d9ef">if</span> scheduler:
                    scheduler<span style="color:#f92672">.</span>batch_step()

                <span style="color:#75715e"># Compute and print loss.</span>
                loss <span style="color:#f92672">=</span> loss_fn(y_pred, y_batch)
                optimizer<span style="color:#f92672">.</span>zero_grad()
                loss<span style="color:#f92672">.</span>backward()
                <span style="color:#66d9ef">if</span> clip:
                    nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(),<span style="color:#ae81ff">1</span>)
                optimizer<span style="color:#f92672">.</span>step()
                avg_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item() <span style="color:#f92672">/</span> len(train_loader)
                
            model<span style="color:#f92672">.</span>eval()
            
            valid_preds_fold <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((x_val_fold<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)))
            test_preds_fold <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(x_test)))
            
            avg_val_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
            <span style="color:#66d9ef">for</span> i, (x_batch, y_batch,index) <span style="color:#f92672">in</span> enumerate(valid_loader):
                <span style="color:#66d9ef">if</span> feats:
                    f <span style="color:#f92672">=</span> kfold_X_valid_features[index]            
                    y_pred <span style="color:#f92672">=</span> model([x_batch,f])<span style="color:#f92672">.</span>detach()
                <span style="color:#66d9ef">else</span>:
                    y_pred <span style="color:#f92672">=</span> model(x_batch)<span style="color:#f92672">.</span>detach()
                
                avg_val_loss <span style="color:#f92672">+=</span> loss_fn(y_pred, y_batch)<span style="color:#f92672">.</span>item() <span style="color:#f92672">/</span> len(valid_loader)
                valid_preds_fold[index] <span style="color:#f92672">=</span> sigmoid(y_pred<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy())[:, <span style="color:#ae81ff">0</span>]
            
            elapsed_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time() <span style="color:#f92672">-</span> start_time 
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Epoch {}/{} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> loss={:.4f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> val_loss={:.4f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> time={:.2f}s&#39;</span><span style="color:#f92672">.</span>format(
                epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, n_epochs, avg_loss, avg_val_loss, elapsed_time))
        avg_losses_f<span style="color:#f92672">.</span>append(avg_loss)
        avg_val_losses_f<span style="color:#f92672">.</span>append(avg_val_loss) 
        <span style="color:#75715e"># predict all samples in the test set batch per batch</span>
        <span style="color:#66d9ef">for</span> i, (x_batch,) <span style="color:#f92672">in</span> enumerate(test_loader):
            <span style="color:#66d9ef">if</span> feats:
                f <span style="color:#f92672">=</span> test_features[i <span style="color:#f92672">*</span> batch_size:(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> batch_size]
                y_pred <span style="color:#f92672">=</span> model([x_batch,f])<span style="color:#f92672">.</span>detach()
            <span style="color:#66d9ef">else</span>:
                y_pred <span style="color:#f92672">=</span> model(x_batch)<span style="color:#f92672">.</span>detach()

            test_preds_fold[i <span style="color:#f92672">*</span> batch_size:(i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> batch_size] <span style="color:#f92672">=</span> sigmoid(y_pred<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy())[:, <span style="color:#ae81ff">0</span>]
            
        train_preds[valid_idx] <span style="color:#f92672">=</span> valid_preds_fold
        test_preds <span style="color:#f92672">+=</span> test_preds_fold <span style="color:#f92672">/</span> len(splits)

    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;All </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> loss={:.4f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> val_loss={:.4f} </span><span style="color:#ae81ff">\t</span><span style="color:#e6db74"> &#39;</span><span style="color:#f92672">.</span>format(np<span style="color:#f92672">.</span>average(avg_losses_f),np<span style="color:#f92672">.</span>average(avg_val_losses_f)))
    <span style="color:#66d9ef">return</span> train_preds, test_preds</code></pre></div>
<h4 id="but-why-why-so-much-code">But Why? Why so much code?</h4>

<p>Okay. I get it. That was probably a handful. What you could have done with a simple<code>.fit</code> in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:</p>

<ul>
<li>While in Keras you have prespecified schedulers like <code>ReduceLROnPlateau</code> (and it is a task to write them), in Pytorch you can experiment like crazy. <strong>If you know how to write Python you are going to get along just fine</strong></li>
<li>Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly.</li>
<li>And much more. It is only your imagination that will stop you.</li>
</ul>

<h2 id="wanna-run-it-yourself">Wanna Run it Yourself?</h2>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/tools.jpg" alt="You have all the tools it seems" height="400" width="700" ></center>
</div>

<p>So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the <code>seed_everything</code>, <code>MyDataset</code> and <code>CyclicLR</code> (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a <a href="https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch/edit" rel="nofollow" target="_blank">Kaggle Kernel</a> with the whole running code. You can see the code here and include it in your projects.</p>

<p>If you liked this post, <strong>please don&rsquo;t forget to upvote the <a href="https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch/edit" rel="nofollow" target="_blank">Kernel</a> too.</strong> I will be obliged.</p>

<h2 id="endnotes-and-references">Endnotes and References</h2>

<p>This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.</p>

<ul>
<li><a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644" rel="nofollow" target="_blank">Discussion on 3rd Place winner model in Toxic comment</a></li>
<li><a href="https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model" rel="nofollow" target="_blank">3rd Place model in Keras by Larry Freeman</a></li>
<li><a href="https://www.kaggle.com/spirosrap/bilstm-attention-kfold-clr-extra-features-capsule" rel="nofollow" target="_blank">Pytorch starter Capsule model</a></li>
<li><a href="https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings" rel="nofollow" target="_blank">How to: Preprocessing when using embeddings</a></li>
<li><a href="https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing" rel="nofollow" target="_blank">Improve your Score with some Text Preprocessing</a></li>
<li><a href="https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm" rel="nofollow" target="_blank">Pytorch baseline</a></li>
<li><a href="https://www.kaggle.com/hengzheng/pytorch-starter" rel="nofollow" target="_blank">Pytorch starter</a></li>
</ul>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/deep-learning/" rel="tag">deep learning</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/nlp/" rel="tag">NLP</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>
<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.415&subid=0&type=4"><IMG border="0"   alt="Deep Learning Specialization on Coursera" src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.415&subid=0&type=4&gridnum=16"></a>


	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2018/12/17/text_classification/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">What Kagglers are using for Text Classification</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2019/01/17/deeplearning_nlp_preprocess/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar">
	     
  <div style="text-align:center">    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2019/10/10/hyperopt2/">Automate Hyperparameter Tuning for your models</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/26/building_ml_system/">6 Important Steps to build  a Machine Learning System</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/23/generative_approach_to_classification/">A Generative Approach to Classification</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/02/graph_algs/">Data Scientists, The 5 Graph Algorithms that you should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/09/01/regex/">The Ultimate Guide to using the Python regex module</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/12/resources/">How did I learn Data Science?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/08/07/feature_selection/">The 5 Feature Selection Algorithms every Data Scientist should know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/30/sampling/">The 5 Sampling Algorithms every Data Scientist need to know</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/21/bandits/">Bayesian Bandits explained simply</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2019/07/20/pandas_subset/">Minimal Pandas Subset for Data Scientists</a></li>
		</ul>
	</div>
</div>


<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>


<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;}
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: center;
    padding: 10px -5px 10px 3%;}
   
</style>
<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2019 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>