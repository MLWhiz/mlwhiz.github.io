<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>End to End Text OCR using Deep-Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="This post is about Optical character recognition(OCR) for text recognition in natural scene images. We will learn about why it is a tough problem, approaches used to solve, and the code that goes along with it.">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="End to End Text OCR using Deep-Learning" />
<meta property="og:description" content="This post is about Optical character recognition(OCR) for text recognition in natural scene images. We will learn about why it is a tough problem, approaches used to solve, and the code that goes along with it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2020/05/24/ocr/" />
<meta property="og:image" content="https://mlwhiz.com/images/ocr/main.jpeg" />
<meta property="article:published_time" content="2020-05-24T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2020-05-24T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/ocr/main.jpeg"/>

<meta name="twitter:title" content="End to End Text OCR using Deep-Learning"/>
<meta name="twitter:description" content="This post is about Optical character recognition(OCR) for text recognition in natural scene images. We will learn about why it is a tough problem, approaches used to solve, and the code that goes along with it."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/resources/index.html">Learning Resources</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">End to End Text OCR using Deep-Learning</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2020-05-24T00:00:00">May 24, 2020</time>
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<p><img src="/images/ocr/main.jpeg" alt="" /></p>

<p>We live in times when any organization or company to scale and to stay relevant has to change how they look at technology and adapt to the changing landscapes swiftly. We already know how Google has digitized books. Or how Google earth is using NLP to identify addresses. Or how it is possible to read the text in digital documents like invoices, legal paperwork, etc.</p>

<p>But how does it exactly work?</p>

<p><em>This post is about Optical character recognition(OCR) for text recognition in natural scene images. We will learn about why it is a tough problem, approaches used to solve, and the code that goes along with it.</em></p>

<p>You can see the whole code <a href="https://www.kaggle.com/mlwhiz/text-detection-v1" rel="nofollow" target="_blank">here</a>.</p>

<hr />

<h2 id="but-why-really">But Why Really?</h2>

<p>In this era of digitization, storing, editing, indexing, and finding information in a digital document is much easier than spending hours scrolling through the printed/handwritten/typed documents.</p>

<p>Moreover, searching for something in a sizeable non-digital document is not just time-consuming; it is also likely for us to miss the information while scrolling the text manually. Lucky for us, computers are getting better every day at doing the tasks humans thought only they could do, often performing better than us as well.</p>

<p>Extracting texts from images has found numerous applications.</p>

<p><em>Some of the applications are Passport recognition, automatic number plate recognition, converting handwritten texts to digital text, converting typed text to digital text, etc.</em></p>

<hr />

<h2 id="challenges">Challenges</h2>

<p>Before going through how we need to understand the challenges, we face in OCR.</p>

<p>Many OCR implementations were available even before the boom of deep learning in 2012. While it was popularly believed that OCR was a solved problem, OCR is still a <a href="https://nanonets.com/blog/ocr-apis-to-extract-text-from-images/" rel="nofollow" target="_blank">challenging problem</a>, especially when text images are taken in an unconstrained environment.</p>

<p>I am talking about complex backgrounds, noise, lightning, different font, and geometrical distortions in the image.</p>

<p>It is in such situations that the machine learning OCR tools shine.</p>

<p>Challenges in the OCR problem arise mostly due to the attribute of the OCR tasks at hand. We can generally divide these tasks into two categories:</p>

<p><strong><em>Structured Text-</em></strong> Text in a typed document. In a standard background, proper row, standard font, and mostly dense.</p>

<p><img src="/images/ocr/0.png" alt="" /></p>

<p><strong><em>Unstructured Text-</em></strong>Text at random places in a natural scene. Sparse text, no proper row structure, complex background, at a random location in the image, and no standard font.</p>

<p><img src="/images/ocr/1.png" alt="" /></p>

<p>A lot of earlier techniques solved the OCR problem for structured text.</p>

<p>But these techniques didn’t properly work for a natural scene, which is sparse and has different attributes than structured data.</p>

<p><strong><em>In this blog, we will be focusing more on the unstructured text, which is a more complex problem to solve</em></strong>.</p>

<p>As we know in the deep learning world, no one solution works for all. We will be seeing multiple approaches to solve the task at hand and will work through one approach among them.</p>

<hr />

<h2 id="datasets-for-unstructured-ocr-tasks">Datasets for unstructured OCR tasks</h2>

<p>There are lots of datasets available in English, but it’s harder to find datasets for other languages. Different datasets present different tasks to be solved. Here are a few examples of datasets commonly used for machine learning OCR problems.</p>

<h3 id="svhn-dataset">SVHN dataset</h3>

<p>The Street View House Numbers dataset contains 73257 digits for training, 26032 digits for testing, and 531131 additional as extra training data. The dataset includes ten labels, which are the digits 0–9. The dataset differs from MNIST since <a href="http://www.iapr-tc11.org/mediawiki/index.php?title=The_Street_View_House_Numbers_(SVHN)_Dataset" rel="nofollow" target="_blank">SVHN</a> has images of house numbers with the house numbers against varying backgrounds. The dataset has bounding boxes around each digit instead of having several images of digits like in MNIST.</p>

<h3 id="scene-text-dataset">Scene Text dataset</h3>

<p><a href="http://www.iapr-tc11.org/mediawiki/index.php?title=KAIST_Scene_Text_Database" rel="nofollow" target="_blank">This dataset</a> consists of 3000 images in different settings (indoor and outdoor) and lighting conditions (shadow, light, and night), with text in Korean and English. Some images also contain digits.</p>

<h3 id="devanagri-character-dataset">Devanagri Character dataset</h3>

<p><a href="http://www.iapr-tc11.org/mediawiki/index.php?title=Devanagari_Character_Dataset" rel="nofollow" target="_blank">This dataset</a> provides us with 1800 samples from 36 character classes obtained by 25 different native writers in the Devanagari script.</p>

<p>And there are many others like this one for Chinese<a href="http://www.iapr-tc11.org/mediawiki/index.php?title=Harbin_Institute_of_Technology_Opening_Recognition_Corpus_for_Chinese_Characters_(HIT-OR3C)" rel="nofollow" target="_blank"> characters</a>, this one for <a href="https://www.kaggle.com/fournierp/captcha-version-2-images" rel="nofollow" target="_blank">CAPTCHA</a> or this one for <a href="http://ai.stanford.edu/~btaskar/ocr/" rel="nofollow" target="_blank">handwritten words</a>.</p>

<hr />

<h2 id="reading-text-in-the-wild">Reading text in the wild</h2>

<p>Any Typical machine learning OCR pipeline follows the following steps :</p>

<p><img src="/images/ocr/2.png" alt="" /></p>

<h3 id="preprocessing">Preprocessing</h3>

<ol>
<li><p>Remove the noise from the image</p></li>

<li><p>Remove the complex background from the image</p></li>

<li><p>Handle the different lightning condition in the image</p></li>
</ol>

<p><img src="/images/ocr/3.png" alt="" /></p>

<p>These are the standard ways to preprocess images in a computer vision task. We will not be focusing on the preprocessing step in this blog.</p>

<h2 id="text-detection">Text Detection</h2>

<p><img src="/images/ocr/4.png" alt="" /></p>

<p>Text detection techniques required to detect the text in the image and create and bounding box around the portion of the image having text. Standard objection detection techniques will also work here.</p>

<h3 id="sliding-window-technique">Sliding window technique</h3>

<p>The bounding box can be created around the text through the sliding window technique. However, this is a computationally expensive task. In this technique, a sliding window passes through the image to detect the text in that window, like a convolutional neural network. We try with different window sizes to not miss the text portion with a different size. There is a convolutional implementation of the sliding window, which can reduce the computational time.</p>

<h3 id="single-shot-and-region-based-detectors">Single-Shot and Region-based detectors</h3>

<p>There are single-shot detection techniques like YOLO(you only look once) and region-based text detection techniques for text detection in the image.</p>

<p><img src="/images/ocr/5.png" alt="" /></p>

<p>YOLO is a single-shot technique as you pass the image only once to detect the text in that region, unlike the sliding window.</p>

<p>Region-based approach work in two steps.</p>

<p>First, the network proposes the region which would possibly have the test and then classify the area if it has the text or not. You can refer one of my previous <a href="https://towardsdatascience.com/a-hitchhikers-guide-to-object-detection-and-instance-segmentation-ac0146fe8e11" rel="nofollow" target="_blank">articles</a> to understand techniques for object detection, in our case text detection.</p>

<h3 id="east-efficient-accurate-scene-text-detector">EAST (Efficient accurate scene text detector)</h3>

<p>This is a very robust deep learning method for text detection based on this <a href="https://arxiv.org/abs/1704.03155v2" rel="nofollow" target="_blank">paper</a>. It is worth mentioning as it is only a text detection method. It can find horizontal and rotated bounding boxes. It can be used in combination with any text recognition method.</p>

<p>The text detection pipeline in this paper has excluded redundant and intermediate steps and only has two stages.</p>

<p>One utilizes the fully convolutional network to directly produce word or text-line level prediction. The generated predictions which could be rotated rectangles or quadrangles are further processed through the non-maximum-suppression step to yield the final output.</p>

<p><img src="/images/ocr/6.png" alt="" /></p>

<p>EAST can detect text both in images and in the video. As mentioned in the paper, it runs near real-time at 13FPS on 720p images with high text detection accuracy. Another benefit of this technique is that its implementation is available in OpenCV 3.4.2 and OpenCV 4. We will be seeing this EAST model in action, along with text recognition.</p>

<hr />

<h2 id="text-recognition">Text Recognition</h2>

<p>Once we have detected the bounding boxes having the text, the next step is to recognize text. There are several techniques for identifying the text. We will be discussing some of the best methods in the following section.</p>

<h3 id="crnn">CRNN</h3>

<p>Convolutional Recurrent Neural Network (CRNN) is a combination of CNN, RNN, and CTC(Connectionist Temporal Classification) loss for image-based sequence recognition tasks, such as scene text recognition and OCR. The network architecture has been taken from this <a href="https://arxiv.org/abs/1507.05717" rel="nofollow" target="_blank">paper</a> published in 2015.</p>

<p><img src="/images/ocr/7.png" alt="" /></p>

<p>This neural network architecture integrates feature extraction, sequence modeling, and transcription into a unified framework. This model does not need character segmentation. The convolution neural network extracts features from the input image(text detected region). The deep bidirectional recurrent neural network predicts the label sequence with some relation between the characters. The transcription layer converts the per-frame made by RNN into a label sequence. There are two modes of transcription, namely the lexicon-free and lexicon-based transcription. In the lexicon-based approach, the highest probable label sequence will be predicted.</p>

<h3 id="machine-learning-ocr-with-tesseract">Machine Learning OCR with Tesseract</h3>

<p>Tesseract was originally developed at Hewlett-Packard Laboratories between 1985 and 1994. In 2005, it was open-sourced by HP. As per Wikipedia-</p>

<blockquote>
<p><em>In 2006, Tesseract was considered one of the most accurate open-source OCR engines then available.</em></p>
</blockquote>

<p>The capability of the Tesseract was mostly limited to structured text data. It would perform quite poorly in unstructured text with significant noise. Further development in Tesseract has been sponsored by Google since 2006.</p>

<p>Deep-learning based method performs better for the unstructured data. Tesseract 4 added deep-learning-based capability with the LSTM network(a kind of Recurrent Neural Network) based OCR engine, which is focused on the line recognition but also supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. The latest stable version 4.1.0 is released on July 7, 2019. This version is significantly more accurate on the unstructured text as well.</p>

<p><em>We will use some of the images to show both text detection with the EAST method and text recognition with Tesseract 4. Let’s see text detection and recognition in action in the following code.</em> The article <a href="https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/" rel="nofollow" target="_blank">here</a> proved to be a helpful resource in writing the code for this project.</p>

<pre><code>##Loading the necessary packages  
import numpy as np 
import cv2 
from imutils.object_detection import non_max_suppression 
import pytesseract 
from matplotlib import pyplot as plt

#Creating argument dictionary for the default arguments needed in the code. 
args = {&quot;image&quot;:&quot;../input/text-detection/example-images/Example-images/ex24.jpg&quot;, &quot;east&quot;:&quot;../input/text-detection/east_text_detection.pb&quot;, &quot;min_confidence&quot;:0.5, &quot;width&quot;:320, &quot;height&quot;:320}
</code></pre>

<p>Here, I am working with essential packages. OpenCV package uses the EAST model for text detection. The tesseract package is for recognizing text in the bounding box detected for the text.</p>

<p>Make sure you have tesseract version &gt;= 4. There are several sources available online to guide the installation of Tesseract.</p>

<p>I created a dictionary for the default arguments needed in the code. Let’s see what these arguments mean.</p>

<ul>
<li><p><em>image: The location of the input image for text detection &amp; recognition.</em></p></li>

<li><p><em>east: The location of the file having the pre-trained EAST detector model.</em></p></li>

<li><p><em>min-confidence: Min probability score for the confidence of the geometry shape predicted at the location.</em></p></li>

<li><p><em>width: Image width should be multiple of 32 for the EAST model to work well.</em></p></li>

<li><p><em>height: Image height should be multiple of 32 for the EAST model to work well.</em></p></li>
</ul>

<h3 id="image-processing">Image processing</h3>

<pre><code>#Give location of the image to be read.
#&quot;Example-images/ex24.jpg&quot; image is being loaded here. 

args['image']=&quot;../input/text-detection/example-images/Example-images/ex24.jpg&quot;
image = cv2.imread(args['image'])

#Saving a original image and shape
orig = image.copy()
(origH, origW) = image.shape[:2]

# set the new height and width to default 320 by using args #dictionary.  
(newW, newH) = (args[&quot;width&quot;], args[&quot;height&quot;])

#Calculate the ratio between original and new image for both height and weight. 
#This ratio will be used to translate bounding box location on the original image. 
rW = origW / float(newW)
rH = origH / float(newH)

# resize the original image to new dimensions
image = cv2.resize(image, (newW, newH))
(H, W) = image.shape[:2]

# construct a blob from the image to forward pass it to EAST model
blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),
    (123.68, 116.78, 103.94), swapRB=True, crop=False)
</code></pre>

<h3 id="loading-pre-trained-east-model-and-defining-output-layers">Loading Pre-trained EAST model and defining output layers</h3>

<pre><code># load the pre-trained EAST model for text detection 
net = cv2.dnn.readNet(args[&quot;east&quot;])

# We would like to get two outputs from the EAST model. 
#1. Probabilty scores for the region whether that contains text or not. 
#2. Geometry of the text -- Coordinates of the bounding box detecting a text
# The following two layer need to pulled from EAST model for achieving this. 
layerNames = [
    &quot;feature_fusion/Conv_7/Sigmoid&quot;,
    &quot;feature_fusion/concat_3&quot;]
</code></pre>

<h3 id="forward-pass-the-image-through-east-model">Forward pass the image through EAST model</h3>

<pre><code>#Forward pass the blob from the image to get the desired output layers
net.setInput(blob)
(scores, geometry) = net.forward(layerNames)
</code></pre>

<h3 id="function-to-decode-bounding-box-from-east-model-prediction">Function to decode bounding box from EAST model prediction</h3>

<pre><code>## Returns a bounding box and probability score if it is more than minimum confidence
def predictions(prob_score, geo):
    (numR, numC) = prob_score.shape[2:4]
    boxes = []
    confidence_val = []

    # loop over rows
    for y in range(0, numR):
        scoresData = prob_score[0, 0, y]
        x0 = geo[0, 0, y]
        x1 = geo[0, 1, y]
        x2 = geo[0, 2, y]
        x3 = geo[0, 3, y]
        anglesData = geo[0, 4, y]

        # loop over the number of columns
        for i in range(0, numC):
            if scoresData[i] &lt; args[&quot;min_confidence&quot;]:
                continue

            (offX, offY) = (i * 4.0, y * 4.0)

            # extracting the rotation angle for the prediction and computing the sine and cosine
            angle = anglesData[i]
            cos = np.cos(angle)
            sin = np.sin(angle)

            # using the geo volume to get the dimensions of the bounding box
            h = x0[i] + x2[i]
            w = x1[i] + x3[i]

            # compute start and end for the text pred bbox
            endX = int(offX + (cos * x1[i]) + (sin * x2[i]))
            endY = int(offY - (sin * x1[i]) + (cos * x2[i]))
            startX = int(endX - w)
            startY = int(endY - h)

            boxes.append((startX, startY, endX, endY))
            confidence_val.append(scoresData[i])

    # return bounding boxes and associated confidence_val
    return (boxes, confidence_val)
</code></pre>

<p>In this exercise, we are only decoding horizontal bounding boxes. Decoding rotating bounding boxes from the scores and geometry is more complicated.</p>

<h3 id="getting-final-bounding-boxes-after-non-max-suppression">Getting final bounding boxes after non-max suppression</h3>

<pre><code># Find predictions and  apply non-maxima suppression
(boxes, confidence_val) = predictions(scores, geometry)
boxes = non_max_suppression(np.array(boxes), probs=confidence_val)
</code></pre>

<p>Now that we have derived the bounding boxes after applying <a href="https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/" rel="nofollow" target="_blank">non-max-suppression</a>. We would want to see the bounding boxes on the image and how we can extract the text from the detected bounding boxes. We do this using Tesseract.</p>

<h3 id="generating-list-with-bounding-box-coordinates-and-recognized-text-in-the-boxes">Generating list with bounding box coordinates and recognized text in the boxes</h3>

<pre><code># initialize the list of results
results = []

# loop over the bounding boxes to find the coordinate of bounding boxes
for (startX, startY, endX, endY) in boxes:
    # scale the coordinates based on the respective ratios in order to reflect bounding box on the original image
    startX = int(startX * rW)
    startY = int(startY * rH)
    endX = int(endX * rW)
    endY = int(endY * rH)

    #extract the region of interest
    r = orig[startY:endY, startX:endX]

    #configuration setting to convert image to string.  
    configuration = (&quot;-l eng --oem 1 --psm 8&quot;)
    ##This will recognize the text from the image of bounding box
    text = pytesseract.image_to_string(r, config=configuration)

    # append bbox coordinate and associated text to the list of results 
    results.append(((startX, startY, endX, endY), text))
</code></pre>

<p>The above portion of the code has stored the bounding box coordinates and associated text in a list. We will see how does it look on the image.</p>

<p>In our case, we have used a specific configuration of the Tesseract. There are multiple options available for tesseract configuration.</p>

<ul>
<li><p><strong>l: language</strong>, chosen English in the above code.</p></li>

<li><p><strong>oem(OCR Engine modes):</strong></p>

<ul>
<li>0 Legacy engine only.</li>
<li><strong>1 Neural nets LSTM engine only.</strong></li>
<li>2 Legacy + LSTM engines.</li>
<li>3 Default, based on what is available.</li>
</ul></li>

<li><p><strong>psm(Page segmentation modes):</strong></p>

<ul>
<li>0 Orientation and script detection (OSD) only.</li>
<li>1 Automatic page segmentation with OSD.</li>
<li>2 Automatic page segmentation, but no OSD, or OCR. (not implemented)</li>
<li>3 Fully automatic page segmentation, but no OSD. (Default)</li>
<li>4 Assume a single column of text of variable sizes.</li>
<li>5 Assume a single uniform block of vertically aligned text.</li>
<li>6 Assume a single uniform block of text.</li>
<li>7 Treat the image as a single text line.</li>
<li><strong>8 Treat the image as a single word.</strong></li>
<li>9 Treat the image as a single word in a circle.</li>
<li>10 Treat the image as a single character.</li>
<li>11 Sparse text. Find as much text as possible in no particular order.</li>
<li>12 Sparse text with OSD.</li>
<li>13 Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.</li>
</ul></li>
</ul>

<p>We can choose the specific Tesseract configuration on the basis of our image data.</p>

<h3 id="display-image-with-a-bounding-box-and-recognized-text">Display image with a bounding box and recognized text</h3>

<pre><code>#Display the image with bounding box and recognized text
orig_image = orig.copy()

# Moving over the results and display on the image
for ((start_X, start_Y, end_X, end_Y), text) in results:
    # display the text detected by Tesseract
    print(&quot;{}\n&quot;.format(text))

    # Displaying text
    text = &quot;&quot;.join([x if ord(x) &lt; 128 else &quot;&quot; for x in text]).strip()
    cv2.rectangle(orig_image, (start_X, start_Y), (end_X, end_Y),
        (0, 0, 255), 2)
    cv2.putText(orig_image, text, (start_X, start_Y - 30),
        cv2.FONT_HERSHEY_SIMPLEX, 0.7,(0,0, 255), 2)

plt.imshow(orig_image)
plt.title('Output')
plt.show()
</code></pre>

<hr />

<h2 id="results">Results</h2>

<p>The above code uses the OpenCV EAST model for text detection and Tesseract for text recognition. PSM for the Tesseract has been set accordingly to the image. It is important to note that Tesseract requires a clear image, typically for working well.</p>

<p>In our current implementation, we did not consider rotating bounding boxes due to its complexity to implement. But in the real scenario where the text is rotated, the above code will not work well. Also, whenever the image is not very clear, Tesseract will have difficulty recognizing the text properly.</p>

<p>Some of the output generated through the above code is:</p>

<p><img src="/images/ocr/8.png" alt="" /></p>

<p><img src="/images/ocr/9.png" alt="" /></p>

<p><img src="/images/ocr/10.png" alt="" /></p>

<p>The code could deliver excellent results for all the above three images. The text is clear, and the background behind the text is also uniform in these images.</p>

<p><img src="/images/ocr/11.png" alt="" /></p>

<p>The model performed pretty well here. But some of the alphabets are not recognized correctly. You can see that bounding boxes are mostly correct as they should be. May be slight rotation would help. But our current implementation does not provide rotating bounding boxes. It seems due to image clarity. Tesseract could not recognize it perfectly.</p>

<p><img src="/images/ocr/12.png" alt="" /></p>

<p>The model performed pretty decently here. But some of the texts in bounding boxes are not recognized correctly. Numeric one could not be detected at all. There is a non-uniform background here, maybe generating a uniform background would have helped this case. Also, 24 is not properly bounded in the box. In such a case, padding the bounding box could help.</p>

<p><img src="/images/ocr/13.png" alt="" /></p>

<p>It seems that stylized font with shadow in the background has affected the result in the above case.</p>

<p>We can not expect the OCR model to be 100 % accurate. Still, we have achieved good results with the EAST model and Tesseract. Adding more filters for processing the image might help in improving the performance of the model.</p>

<p>You can also find this code for this project on a <a href="https://www.kaggle.com/mlwhiz/text-detection-v1" rel="nofollow" target="_blank">Kaggle kernel</a> to try it out on your own.</p>

<p>If you want to know more about various <strong><em>Object Detection techniques, motion estimation, object tracking in video, etc</em></strong>., I would like to recommend this awesome course on <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Deep Learning in Computer Vision</a> in the <a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Advanced machine learning specialization</a>.</p>

<p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at <a href="https://medium.com/@rahul_agarwal" rel="nofollow" target="_blank">Medium</a> or Subscribe to my <a href="http://eepurl.com/dbQnuX" rel="nofollow" target="_blank">blog</a> to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter <a href="https://twitter.com/MLWhiz" rel="nofollow" target="_blank">@mlwhiz</a>.</p>

<p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/statistics/" rel="tag">Statistics</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>

<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.372&subid=0&type=4" rel="nofollow"><IMG border="0"   alt="Start your future with a Data Science Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.372&subid=0&type=4&gridnum=16"></a>





























	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2020/05/24/fstring/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">How and Why to use f strings in Python3?</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2020/05/25/dls/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Stop Worrying and Create your Deep Learning Server in 30 minutes</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<style type="text/css">

  .btn {
    display: inline-block;
    font-weight: 400;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;
    border: 1px solid transparent;
    padding: .375rem .75rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: .25rem;
    transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.btn-session {
    color: #fff;
    background-color: #28a745;
    border-color: #28a745;
}
</style>


<aside class="sidebar">
  <div style="text-align:center">     
    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/25/democratize/">Don’t Democratize Data Science</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/25/cogbias/">Five Cognitive Biases In Data Science (And how to avoid them)</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/25/dls/">Stop Worrying and Create your Deep Learning Server in 30 minutes</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/24/ocr/">End to End Text OCR using Deep-Learning</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/24/fstring/">How and Why to use f strings in Python3?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/05/24/multitextclass/">Using Deep Learning for End to End Multiclass Text Classification</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/03/29/coronatimes/">A Newspaper for COVID-19 — The CoronaTimes</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/03/27/covidcourses/">5 Online Courses you can take for free during COVID-19 Epidemic</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/03/24/coronaai/">Can AI help in fighting against Corona?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/03/20/practicalspark/">Practical Spark Tips for Data Scientists</a></li>
		</ul>
	</div>
</div>

<div style="text-align:center">     
    
    <a class="btn btn-session" href="https://www.patreon.com/bePatron?u=28135435" role="button">1:1 Session</a>
  </div>

<br>

<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>


<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;
  }
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: center;
    padding: 10px -5px 10px 3%;}  
   
</style>

<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2020 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>