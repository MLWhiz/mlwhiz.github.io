<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps"><meta property="og:description" content="Data Exploration is a key part of Data Science. And does it take long?"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/08/04/spark_dataproc/"><meta property="og:image" content="https://mlwhiz.com/images/spark_dataproc/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/spark_dataproc/main.png"><meta property="article:published_time" content="2020-08-04T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Big Data"><meta property="article:tag" content="Data Science"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/spark_dataproc/main.png"><meta name=twitter:title content="Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps"><meta name=twitter:description content="Data Exploration is a key part of Data Science. And does it take long?"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/08/04/spark_dataproc/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/08/04/spark_dataproc/#primaryimage","url":"https://mlwhiz.com/images/spark_dataproc/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/08/04/spark_dataproc/#webpage","url":"https://mlwhiz.com/blog/2020/08/04/spark_dataproc/","inLanguage":"en-US","name":"Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/08/04/spark_dataproc/#primaryimage"},"datePublished":"2020-08-04T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Data Exploration is a key part of Data Science. And does it take long?"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/big-data class=categoryStyle>Big Data</a>
<a href=/categories/data-science class=categoryStyle>Data Science</a><h1>Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>04 August 2020</span></div><img src=https://mlwhiz.com/images/spark_dataproc/main.png class="img-fluid w-100 mb-4" alt="Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps"><div class="content mb-5"><p>Data Exploration is a key part of Data Science. And does it take long? Ahh. Don’t even ask. Preparing a data set for ML not only requires understanding the data set, cleaning, and creating new features, it also involves doing these steps repeatedly until we have a fine-tuned system.</p><p>As we moved towards bigger datasets,
<a href=https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a target=_blank rel="nofollow noopener">Apache Spark</a>
came as a ray of hope. It gave us a scalable and distributed in-memory system to work with Big Data. By the by, we also saw frameworks like
<a href=https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79 target=_blank rel="nofollow noopener">Pytorch</a>
and Tensorflow that inherently parallelized matrix computations using thousands of GPU cores.</p><p>But never did we see these two systems working in tandem in the past. We continued to use Spark for Big Data ETL tasks and GPUs for matrix intensive problems in
<a href=https://towardsdatascience.com/stop-worrying-and-create-your-deep-learning-server-in-30-minutes-bb5bd956b8de target=_blank rel="nofollow noopener">Deep Learning</a>
.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/0_hu897b9c22079b8cbece0d30058aa0d9aa_950616_500x0_resize_box_2.png 500w
, /images/spark_dataproc/0_hu897b9c22079b8cbece0d30058aa0d9aa_950616_800x0_resize_box_2.png 800w
, /images/spark_dataproc/0_hu897b9c22079b8cbece0d30058aa0d9aa_950616_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/0_hu897b9c22079b8cbece0d30058aa0d9aa_950616_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/0.png alt='<a href="https://marketing.thepoweroftwo.solutions/acton/attachment/42621/f-0f867d92-c1d0-4112-afef-26f9cbb51499/1/-/-/-/-/NVIDIA%20&amp;amp;%20Google%20Cloud%20Dataproc%20ebook%20July2020.pdf" target="_blank" rel="nofollow noopener">Source</a>'></p><p>And that is where Spark 3.0 comes. It provides us with a way to add NVIDIA GPUs to our Spark cluster nodes. The work done by these nodes can now be parallelized using both the CPU+GPU using the software platform for GPU computing,
<a href="https://towardsdatascience.com/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=post_stats_page---------------------------" target=_blank rel="nofollow noopener">RAPIDS</a>
.</p><blockquote><h1 id=spark--gpu--rapids--spark-30>Spark + GPU + RAPIDS = Spark 3.0</h1></blockquote><p>As per
<a href=https://www.nvidia.com/en-in/deep-learning-ai/solutions/data-science/apache-spark-3/ target=_blank rel="nofollow noopener">NVIDIA</a>
, the early adopters of Spark 3.0 already see a significantly faster performance with their current data loads. Such reductions in processing times can allow Data Scientists to perform more iterations on much bigger datasets, allowing Retailers to improve their forecasting, finance companies to enhance their credit models, and ad tech firms to improve their ability to predict click-through rates.</p><p>Excited yet. So how can you start using Spark 3.0? Luckily, Google Cloud, Spark, and NVIDIA have come together and simplified the cluster creation process for us. With Dataproc on Google Cloud, we can have a fully-managed Apache Spark cluster with GPUs in a few minutes.</p><p><em><strong>This post is about setting up your own Dataproc Spark Cluster with NVIDIA GPUs on Google Cloud.</strong></em></p><hr><h3 id=1-create-a-new-gcp-project>1. Create a New GCP Project</h3><p>After the initial signup on the
<a href=https://cloud.google.com/ target=_blank rel="nofollow noopener">Google Cloud Platform</a>
, we can start a new project. Here I begin by creating a new project namedSparkDataProc.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/1_hu5dcd3caf8b511bee1bb07d1e0fbe41c5_102565_500x0_resize_box_2.png 500w
, /images/spark_dataproc/1_hu5dcd3caf8b511bee1bb07d1e0fbe41c5_102565_800x0_resize_box_2.png 800w
, /images/spark_dataproc/1_hu5dcd3caf8b511bee1bb07d1e0fbe41c5_102565_1200x0_resize_box_2.png 1200w" src=/images/spark_dataproc/1.png alt="Create a New Project">
<em>Create a New Project</em></p><hr><h3 id=2-enable-the-apis-in-the-gcp-project>2. Enable the APIs in the GCP Project</h3><p>Once we add this project, we can go to our new project and start a Cloud Shell instance by clicking the “Activate <strong>Cloud Shell</strong>” button at the top right corner. Doing so will open up a terminal window at the bottom of our screen where we can run our next commands to set up a data proc cluster:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/2_hu6f16ab4a462682776fd889fda0f52706_231861_500x0_resize_box_2.png 500w
, /images/spark_dataproc/2_hu6f16ab4a462682776fd889fda0f52706_231861_800x0_resize_box_2.png 800w
, /images/spark_dataproc/2_hu6f16ab4a462682776fd889fda0f52706_231861_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/2_hu6f16ab4a462682776fd889fda0f52706_231861_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/2.png alt="Activate <strong>Cloud Shell</strong> for putting your commands"></p><p>After this, we will need to run some commands to set up our project in the cloud shell. We start by enabling dataproc services within your project. Enable the Compute and Dataproc APIs to access Dataproc, and enable the Storage API as you’ll need a Google Cloud Storage bucket to house your data. We also set our default region. This may take several minutes:</p><pre><code>gcloud services enable compute.googleapis.com
gcloud services enable dataproc.googleapis.com
gcloud services enable storage-api.googleapis.com
gcloud config set dataproc/region us-central1
</code></pre><hr><h3 id=3-create-and-put-some-data-in-gcs-bucket>3. Create and Put some data in GCS Bucket</h3><p>Once done, we can create a new Google Cloud Storage Bucket, where we will keep all our data in the Cloud Shell:</p><pre><code>#You might need to change this name as this needs to be unique across all the users
export BUCKET_NAME=rahulsparktest

#Create the Bucket
gsutil mb gs://${BUCKET_NAME}
</code></pre><p>We can also put some data in the bucket for later run purposes when we are running our spark cluster.</p><pre><code># Get data in cloudshell terminal
git clone https://github.com/caroljmcdonald/spark3-book
mkdir -p ~/data/cal_housing
tar -xzf spark3-book/data/cal_housing.tgz -C ~/data

# Put data into Bucket using gsutil
gsutil cp ~/data/CaliforniaHousing/cal_housing.data gs://${BUCKET_NAME}/data/cal_housing/cal_housing.csv
</code></pre><hr><h3 id=4-setup-the-dataproc-rapids-cluster>4. Setup the DataProc Rapids Cluster</h3><p>To create a DataProc RAPIDS cluster that uses NVIDIA T4 GPUs, we need to get some initialization scripts that are used to instantiate our cluster. These scripts will install the GPU drivers(
<a href=https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/gpu/install_gpu_driver.sh target=_blank rel="nofollow noopener">install_gpu_driver.sh</a>
) and create the Rapids conda environment(
<a href=https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/rapids/rapids.sh target=_blank rel="nofollow noopener">rapids.sh</a>
) automatically for us. Since these scripts are in the development phase, the best way is to get the scripts from the GitHub source. We can do this using the below commands in our cloud shell in which we get the initialization scripts and copy them into our GS Bucket:</p><pre><code>wget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/rapids/rapids.sh
wget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/gpu/install_gpu_driver.sh

gsutil cp rapids.sh gs://$BUCKET_NAME
gsutil cp install_gpu_driver.sh gs://$BUCKET_NAME
</code></pre><p>We can now create our cluster using the below command in the Cloud Shell. In the below command, we are using a predefined image version(2.0.0-RC2-ubuntu18) which has Spark 3.0 and python 3.7 to create our dataproc cluster. I am using a previous version of this image since the newest version has some issues with running Jupyter and Jupyter Lab. You can get a list of all versions
<a href=https://cloud.google.com/dataproc/docs/release-notes target=_blank rel="nofollow noopener">here</a>
.</p><pre><code>CLUSTER_NAME=sparktestcluster
REGION=us-central1
gcloud beta dataproc clusters create ${CLUSTER_NAME} \
 --image-version 2.0.0-RC2-ubuntu18 \
 --master-machine-type n1-standard-8 \
 --worker-machine-type n1-highmem-32 \
 --worker-accelerator type=nvidia-tesla-t4,count=2 \
 --optional-components ANACONDA,JUPYTER,ZEPPELIN \
 --initialization-actions gs://$BUCKET_NAME/install_gpu_driver.sh,gs://$BUCKET_NAME/rapids.sh \
 --metadata rapids-runtime=SPARK \
 --metadata gpu-driver-provider=NVIDIA \
 --bucket ${BUCKET_NAME} \
 --subnet default \
 --enable-component-gateway \
--properties=&quot;^#^spark:spark.task.resource.gpu.amount=0.125#spark:spark.executor.
cores=8#spark:spark.task.cpus=1#spark:spark.yarn.unmanagedAM.enabled=false&quot;
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/3_hudb2af82c2cd63e08fc4728d72bda58d9_102244_500x0_resize_box_2.png 500w
, /images/spark_dataproc/3_hudb2af82c2cd63e08fc4728d72bda58d9_102244_800x0_resize_box_2.png 800w
, /images/spark_dataproc/3_hudb2af82c2cd63e08fc4728d72bda58d9_102244_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/3_hudb2af82c2cd63e08fc4728d72bda58d9_102244_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/3.png alt="<strong>Cluster Architecture</strong>"></p><p>Our resulting Dataproc cluster has:</p><ul><li><p>One 8-core master node and two 32-core worker nodes</p></li><li><p>Two NVIDIA T4 GPUs attached to each worker node</p></li><li><p>Anaconda, Jupyter, and Zeppelin enabled</p></li><li><p>Component gateway enabled for accessing Web UIs hosted on the cluster</p></li><li><p>Extra Spark config tuning suitable for a notebook environment set using the properties flag. Specifically, we set spark.executor.cores=8 for improved parallelization and spark.yarn.unmanagedAM.enabled=false since it currently breaks <em><strong>SparkUI</strong></em>.</p></li></ul><p><em><strong>Troubleshooting:</strong></em> If you get errors regarding limits after this command, you might want to change some of the quotas in your default
<a href="https://console.cloud.google.com/iam-admin/quotas?project=sparkdataproc" target=_blank rel="nofollow noopener">Google Console Quotas Page</a>
. The limits I ended up changing were:</p><ul><li><p><strong>GPUs (all regions)</strong> to 12 (Minimum:4)</p></li><li><p><strong>CPUs (all regions)</strong> to 164 (Minimum:72)</p></li><li><p><strong>NVIDIA T4 GPUs</strong> in us-central1 to 12 (Minimum:4)</p></li><li><p><strong>CPUs</strong> in us-central1 to 164 (Minimum:72)</p></li></ul><p>I actually requested more limits than I required as the limit increase process might take a little longer and I will spin up some larger clusters later.</p><hr><h3 id=5-run-jupyterlab-on-dataproc-rapids-cluster>5. Run JupyterLab on DataProc Rapids Cluster</h3><p>Once your command succeeds(It might take 10–15 mins) you will be able to see your Dataproc cluster at
<a href=https://console.cloud.google.com/dataproc/clusters target=_blank rel="nofollow noopener">https://console.cloud.google.com/dataproc/clusters</a>
. Or you can go to the Google Cloud Platform console on your browser and search for “Dataproc” and click on the “Dataproc” icon(It looks like three connected circles). This will navigate you to the Dataproc clusters page.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/4_hu29520a0319a1d3a8f0d9cb95f6b3d894_133935_500x0_resize_box_2.png 500w
, /images/spark_dataproc/4_hu29520a0319a1d3a8f0d9cb95f6b3d894_133935_800x0_resize_box_2.png 800w
, /images/spark_dataproc/4_hu29520a0319a1d3a8f0d9cb95f6b3d894_133935_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/4_hu29520a0319a1d3a8f0d9cb95f6b3d894_133935_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/4.png alt="Dataproc Clusters Page"></p><p>Now, you would be able to open a web interface(Jupyter/JupyterLab/Zeppelin) if you click on the sparktestcluster and then “Web Interfaces”.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/5_hu0d80272490cf6315d0d3fa187b202f66_187539_500x0_resize_box_2.png 500w
, /images/spark_dataproc/5_hu0d80272490cf6315d0d3fa187b202f66_187539_800x0_resize_box_2.png 800w
, /images/spark_dataproc/5_hu0d80272490cf6315d0d3fa187b202f66_187539_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/5_hu0d80272490cf6315d0d3fa187b202f66_187539_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/5.png alt="Web Interface Page for our Cluster"></p><p>After opening up your Jupyter Pyspark Notebook, here is some example code for you to run if you are following along with this tutorial. In this code, we load a small dataset, and we see that the df.count() function ran in 252ms which is indeed fast for Spark, but I would do a much detailed benchmarking post later so keep tuned.</p><pre><code>file = &quot;gs://rahulsparktest/data/cal_housing/cal_housing.csv&quot;

df = spark.read.load(file,format=&quot;csv&quot;, sep=&quot;,&quot;, inferSchema=&quot;true&quot;, header=&quot;false&quot;)
colnames = [&quot;longitude&quot;,&quot;latitude&quot;,&quot;medage&quot;,&quot;totalrooms&quot;,&quot;totalbdrms&quot;,&quot;population&quot;,&quot;houshlds&quot;,&quot;medincome&quot;,&quot;medhvalue&quot;]

df = df.toDF(*colnames)
df.count()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/6_hu3fbe0630e769e2c09e4664574f805279_376993_500x0_resize_box_2.png 500w
, /images/spark_dataproc/6_hu3fbe0630e769e2c09e4664574f805279_376993_800x0_resize_box_2.png 800w
, /images/spark_dataproc/6_hu3fbe0630e769e2c09e4664574f805279_376993_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/6_hu3fbe0630e769e2c09e4664574f805279_376993_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/6.png alt="Yes Our <strong>Jupyter Notebook</strong> Works"></p><h3 id=6-access-the-spark-ui>6. Access the Spark UI</h3><p>That is all well and done, but one major problem I faced was that I was not able to access the Spark UI using the link provided in the notebook. I found out that there were two ways to access the Spark UI for debugging purposes:</p><p><strong>A. Using the Web Interface option:</strong></p><p>We can access Spark UI by clicking first on <strong>Yarn Resource Manager</strong> Link on the <strong>Web Interface</strong> and then on Application Master on the corresponding page:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/7_hu33ad206790b352ddca7f467d411d717b_343848_500x0_resize_box_2.png 500w
, /images/spark_dataproc/7_hu33ad206790b352ddca7f467d411d717b_343848_800x0_resize_box_2.png 800w
, /images/spark_dataproc/7_hu33ad206790b352ddca7f467d411d717b_343848_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/7_hu33ad206790b352ddca7f467d411d717b_343848_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/7.png alt="Click on <strong>Application Master</strong> in <strong>Tracking UI</strong> Column to get Spark UI"></p><p>And, you will arrive at the Spark UI Page:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/8_hu2071c1a1ca6fdab5259a778faaf100fe_75356_500x0_resize_box_2.png 500w
, /images/spark_dataproc/8_hu2071c1a1ca6fdab5259a778faaf100fe_75356_800x0_resize_box_2.png 800w
, /images/spark_dataproc/8_hu2071c1a1ca6fdab5259a778faaf100fe_75356_1200x0_resize_box_2.png 1200w" src=/images/spark_dataproc/8.png alt="Yes! We get the Spark UI."></p><p>B. <strong>Using the SSH Tunneling option:</strong></p><p>Another option to access the Spark UI is using Tunneling. To do this, you need to go to the Web Interface Page and click on <em>“Create an SSH tunnel to connect to a web interface”.</em></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/spark_dataproc/9_hu7e2b2c64324bfbff5f28052acfac8cb8_229409_500x0_resize_box_2.png 500w
, /images/spark_dataproc/9_hu7e2b2c64324bfbff5f28052acfac8cb8_229409_800x0_resize_box_2.png 800w
, /images/spark_dataproc/9_hu7e2b2c64324bfbff5f28052acfac8cb8_229409_1200x0_resize_box_2.png 1200w
, /images/spark_dataproc/9_hu7e2b2c64324bfbff5f28052acfac8cb8_229409_1500x0_resize_box_2.png 1500w" src=/images/spark_dataproc/9.png alt="Spark Test Cluster Web Interface using SSH"></p><p>This will give you two commands that you want to run on your <strong>local machine</strong> and not on Cloud shell. But before running them, you need to install google cloud SDK to your machine and set it up for your current project:</p><pre><code>sudo snap install google-cloud-sdk --classic

# This Below command will open the browser where you can authenticate by selecting your own google account.
gcloud auth login

# Set up the project as sparkdataproc (project ID)
gcloud config set project sparkdataproc
</code></pre><p>Once done with this, we can simply run the first command:</p><pre><code>gcloud compute ssh sparktestcluster-m --project=sparkdataproc  --zone=us-central1-b -- -D 1080 -N
</code></pre><p>And then the second one in another tab/window. This command will open up a new chrome window where you can access the Spark UI by clicking on Application Master the same as before.</p><pre><code>/usr/bin/google-chrome --proxy-server=&quot;socks5://localhost:1080&quot;   --user-data-dir=&quot;/tmp/sparktestcluster-m&quot; [http://sparktestcluster-m:8088](http://sparktestcluster-m:8088)
</code></pre><p>And that is it for setting up a Spark3.0 Cluster accelerated by GPUs.</p><p>It took me around 30 mins to go through all these steps if I don’t count the debugging time and the quota increase requests.</p><p>I am totally amazed by the concept of using a GPU on Spark and the different streams of experiments it opens up. Will be working on a lot of these in the coming weeks not only to benchmark but also because it is fun. So stay tuned.</p><hr><h2 id=continue-learning>Continue Learning</h2><p>Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on
<a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11468293556&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-essentials" target=_blank rel="nofollow noopener">Big Data Essentials: HDFS, MapReduce, and Spark RDD</a>
and
<a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11468293488&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbig-data-analysis" target=_blank rel="nofollow noopener">Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames</a>
by Yandex on Coursera.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at
<a href=https://medium.com/@rahul_agarwal target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>