<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="A Layman’s Introduction to GANs for Data Scientists using PyTorch"><meta property="og:description" content="Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/08/27/pyt_gan/"><meta property="og:image" content="https://mlwhiz.com/images/pyt_gan/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/pyt_gan/main.png"><meta property="article:published_time" content="2020-08-27T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/pyt_gan/main.png"><meta name=twitter:title content="A Layman’s Introduction to GANs for Data Scientists using PyTorch"><meta name=twitter:description content="Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/08/27/pyt_gan/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/08/27/pyt_gan/#primaryimage","url":"https://mlwhiz.com/images/pyt_gan/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/08/27/pyt_gan/#webpage","url":"https://mlwhiz.com/blog/2020/08/27/pyt_gan/","inLanguage":"en-US","name":"A Layman’s Introduction to GANs for Data Scientists using PyTorch - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/08/27/pyt_gan/#primaryimage"},"datePublished":"2020-08-27T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>A Layman’s Introduction to GANs for Data Scientists using PyTorch</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>27 August 2020</span></div><img src=https://mlwhiz.com/images/pyt_gan/main.png class="img-fluid w-100 mb-4" alt="A Layman’s Introduction to GANs for Data Scientists using PyTorch"><div class="content mb-5"><p>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and
<a href=https://lionbridge.ai/articles/a-look-at-deepfakes-in-2020/ target=_blank rel="nofollow noopener">faces generated by artificial intelligence</a>
. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.</p><p>In my view, GANs will change the way we generate video games and special effects. Using this approach, we could create realistic textures or characters on demand.</p><p>So in this post, we’re going to look at the generative adversarial networks behind AI-generated images, and help you to understand how to create and build your similar application with PyTorch. We’ll try to keep the post as intuitive as possible for those of you just starting out, but we’ll try not to dumb it down too much.</p><p>At the end of this article, you’ll have a solid understanding of how General Adversarial Networks (GANs) work, and how to build your own.</p><hr><h2 id=task-overview>Task Overview</h2><p>In this post, we will create unique anime characters using the
<a href=https://www.kaggle.com/splcher/animefacedataset target=_blank rel="nofollow noopener">Anime Face Dataset</a>
. It is a dataset consisting of 63,632 high-quality anime faces in a number of styles. It’s a good starter dataset because it’s perfect for our goal.</p><p>We will be using Deep Convolutional Generative Adversarial Networks (DC-GANs) for our project. Though we’ll be using it to generate the faces of new anime characters, DC-GANs can also be used to create modern
<a href=https://syncedreview.com/2019/08/29/ai-creates-fashion-models-with-custom-outfits-and-poses/ target=_blank rel="nofollow noopener">fashion styles</a>
, general content creation, and sometimes for data augmentation as well.</p><p>But before we get into the coding, let’s take a quick look at how GANs work.</p><hr><h2 id=intuition-brief-intro-to-gans-for-generating-fake-images>INTUITION: Brief Intro to GANs for Generating Fake Images</h2><p>GANs typically employ two dueling neural networks to train a computer to learn the nature of a dataset well enough to generate convincing fakes. One of these Neural Networks generates fakes (the generator), and the other tries to classify which images are fake (the discriminator). These networks improve over time by competing against each other.</p><p>Perhaps imagine the generator as a robber and the discriminator as a police officer. The more the robber steals, the better he gets at stealing things. But at the same time, the police officer also gets better at catching the thief. Well, in an ideal world, anyway.</p><p>The losses in these neural networks are primarily a function of how the other network performs:</p><ul><li><p>Discriminator network loss is a function of generator network quality: Loss is high for the discriminator if it gets fooled by the generator’s fake images.</p></li><li><p>Generator network loss is a function of discriminator network quality: Loss is high if the generator is not able to fool the discriminator.</p></li></ul><p>In the training phase, we train our discriminator and generator networks sequentially, intending to improve performance for both. The end goal is to end up with weights that help the generator to create realistic-looking images. In the end, we’ll use the generator neural network to generate high-quality fake images from random noise***.***</p><hr><h2 id=the-generator-architecture>The Generator architecture</h2><p>One of the main problems we face when working with GANs is that the training is not very stable. So we have to come up with a generator architecture that solves our problem and also results in stable training. The diagram below is taken from the paper
<a href=https://arxiv.org/pdf/1511.06434.pdf target=_blank rel="nofollow noopener">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a>
, which explains the DC-GAN generator architecture.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/1_hu685eaec9d6e9a73d60576151f3298de3_142941_500x0_resize_box_2.png 500w
, /images/pyt_gan/1_hu685eaec9d6e9a73d60576151f3298de3_142941_800x0_resize_box_2.png 800w" src=/images/pyt_gan/1.png alt='<a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank" rel="nofollow noopener">Source</a>'></p><p>Though it might look a little bit confusing, essentially you can think of a generator neural network as a black box which takes as input a 100 dimension normally generated vector of numbers and gives us an image:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/2_hu571ff4aeb34d056e87eed5dd6f5238e4_422783_500x0_resize_box_2.png 500w
, /images/pyt_gan/2_hu571ff4aeb34d056e87eed5dd6f5238e4_422783_800x0_resize_box_2.png 800w
, /images/pyt_gan/2_hu571ff4aeb34d056e87eed5dd6f5238e4_422783_1200x0_resize_box_2.png 1200w
, /images/pyt_gan/2_hu571ff4aeb34d056e87eed5dd6f5238e4_422783_1500x0_resize_box_2.png 1500w" src=/images/pyt_gan/2.png alt="Generator as a BlackBox"></p><p>So how do we create such an architecture? Below, we use a dense layer of size 4x4x1024 to create a dense vector out of the 100-d vector. We then reshape the dense vector in the shape of an image of 4×4 with 1024 filters, as shown in the following figure:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/pyt_gan/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/4_hu57bcaa0730989bd2095ee891dfa37a1f_166019_500x0_resize_box_2.png 500w
, /images/pyt_gan/4_hu57bcaa0730989bd2095ee891dfa37a1f_166019_800x0_resize_box_2.png 800w" src=/images/pyt_gan/4.png alt="Generator Architecture"></p><p>Note that we don’t have to worry about any weights right now as the network itself will learn those during training.</p><p>Once we have the 1024 4×4 maps, we do upsampling using a series of transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, however, we don’t halve the number of maps. We reduce the maps to 3 for each RGB channel since we need three channels for the output image.</p><hr><h2 id=now-what-are-transpose-convolutions>Now, What are Transpose convolutions?</h2><p>Put simply, transposing convolutions provides us with a way to upsample images. In a convolution operation, we try to go from a 4×4 image to a 2×2 image. But when we transpose convolutions, we convolve from 2×2 to 4×4 as shown in the following figure:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/pyt_gan/5.png alt="Upsampling a 2x2 image to 4x4 image"></p><p>Some of you may already know that unpooling is commonly used for upsampling input feature maps in convolutional neural networks (CNN). So why don’t we use unpooling here?</p><p>The reason comes down to the fact that unpooling does not involve any learning. However, transposed convolution is learnable, so it’s preferred. Later in the article, we’ll see how the parameters can be learned by the generator.</p><hr><h2 id=the-discriminator-architecture>The Discriminator architecture</h2><p>Now that we’ve covered the generator architecture, let’s look at the discriminator as a black box. In practice, it contains a series of convolutional layers with a dense layer at the end to predict if an image is fake or not. You can see an example in the figure below:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/6_hu27d6e2923b93de63e64b953e162969d9_151177_500x0_resize_box_2.png 500w" src=/images/pyt_gan/6.png alt="Discriminator as a Blackbox"></p><p>Every image convolutional neural network works by taking an image as input, and predicting if it is real or fake using a sequence of convolutional layers.</p><hr><h2 id=data-preprocessing-and-visualization>Data preprocessing and visualization</h2><p>Before going any further with our training, we preprocess our images to a standard size of 64x64x3. We will also need to normalize the image pixels before we train our GAN. You can see the process in the code below, which I’ve commented on for clarity.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Root directory for dataset</span>
dataroot <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;anime_images/&#34;</span>
<span style=color:#75715e># Number of workers for dataloader</span>
workers <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
<span style=color:#75715e># Batch size during training</span>
batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
<span style=color:#75715e># Spatial size of training images. All images will be resized to this size using a transformer.</span>
image_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
<span style=color:#75715e># Number of channels in the training images. For color images this is 3</span>
nc <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># We can use an image folder dataset the way we have it setup.</span>
<span style=color:#75715e># Create the dataset</span>
dataset <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>ImageFolder(root<span style=color:#f92672>=</span>dataroot,
                           transform<span style=color:#f92672>=</span>transforms<span style=color:#f92672>.</span>Compose([
                               transforms<span style=color:#f92672>.</span>Resize(image_size),
                               transforms<span style=color:#f92672>.</span>CenterCrop(image_size),
                               transforms<span style=color:#f92672>.</span>ToTensor(),
                               transforms<span style=color:#f92672>.</span>Normalize((<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>), (<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>)),
                           ]))
<span style=color:#75715e># Create the dataloader</span>
dataloader <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>DataLoader(dataset, batch_size<span style=color:#f92672>=</span>batch_size,
                                         shuffle<span style=color:#f92672>=</span>True, num_workers<span style=color:#f92672>=</span>workers)
<span style=color:#75715e># Decide which device we want to run on</span>
device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda:0&#34;</span> <span style=color:#66d9ef>if</span> (torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#f92672>and</span> ngpu <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>) <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>)
<span style=color:#75715e># Plot some training images</span>
real_batch <span style=color:#f92672>=</span> next(iter(dataloader))
plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>8</span>))
plt<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#34;off&#34;</span>)
plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Training Images&#34;</span>)
plt<span style=color:#f92672>.</span>imshow(np<span style=color:#f92672>.</span>transpose(vutils<span style=color:#f92672>.</span>make_grid(real_batch[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>to(device)[:<span style=color:#ae81ff>64</span>], padding<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, normalize<span style=color:#f92672>=</span>True)<span style=color:#f92672>.</span>cpu(),(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>0</span>)))
</code></pre></div><p>The resultant output of the code is as follows:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/7_hue9122364ade4d7a99e5798da60caea73_999825_500x0_resize_box_2.png 500w
, /images/pyt_gan/7_hue9122364ade4d7a99e5798da60caea73_999825_800x0_resize_box_2.png 800w" src=/images/pyt_gan/7.png alt="So Many different Characters — Can our Generator understand the patterns?"><figcaption>So Many different Characters — Can our Generator understand the patterns?</figcaption></figure></p><hr><h2 id=implementation-of-dcgan>Implementation of DCGAN</h2><p>This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.</p><hr><h2 id=generating-noise-vector-for-generator>Generating noise vector for Generator</h2><p>We need to generate the noise which we want to convert to an image using our generator architecture.</p><p>We use a normal distribution</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/pyt_gan/8.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/9_hufe6163ed055c83221a0b3d67a088413a_25490_500x0_resize_box_2.png 500w
, /images/pyt_gan/9_hufe6163ed055c83221a0b3d67a088413a_25490_800x0_resize_box_2.png 800w
, /images/pyt_gan/9_hufe6163ed055c83221a0b3d67a088413a_25490_1200x0_resize_box_2.png 1200w
, /images/pyt_gan/9_hufe6163ed055c83221a0b3d67a088413a_25490_1500x0_resize_box_2.png 1500w" src=/images/pyt_gan/9.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>to generate the noise vector:</p><pre><code>nz = 100
noise = torch.randn(64, nz, 1, 1, device=device)
</code></pre><hr><h2 id=generator-architecture>Generator architecture</h2><p>The generator is the most crucial part of the GAN. Here, we’ll create a generator by adding some transposed convolution layers to upsample the noise vector to an image. You’ll notice that this generator architecture is not the same as the one given in the DC-GAN paper I linked above.</p><p>In order to make it a better fit for our data, I had to make some architectural changes. I added a convolution layer in the middle and removed all dense layers from the generator architecture to make it fully convolutional.</p><p>I also used a lot of Batchnorm layers and leaky ReLU activation. The following code block is the function I will use to create the generator:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Size of feature maps in generator</span>
ngf <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
<span style=color:#75715e># Number of channels in the training images. For color images this is 3</span>
nc <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># Size of z latent vector (i.e. size of generator input noise)</span>
nz <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Generator</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self, ngpu):
        super(Generator, self)<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>ngpu <span style=color:#f92672>=</span> ngpu
        self<span style=color:#f92672>.</span>main <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
            <span style=color:#75715e># input is noise, going into a convolution</span>
            <span style=color:#75715e># Transpose 2D conv layer 1.</span>
            nn<span style=color:#f92672>.</span>ConvTranspose2d( nz, ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>),
            nn<span style=color:#f92672>.</span>ReLU(True),
            <span style=color:#75715e># Resulting state size - (ngf*8) x 4 x 4 i.e. if ngf= 64 the size is 512 maps of 4x4</span>

            <span style=color:#75715e># Transpose 2D conv layer 2.</span>
            nn<span style=color:#f92672>.</span>ConvTranspose2d(ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>),
            nn<span style=color:#f92672>.</span>ReLU(True),
            <span style=color:#75715e># Resulting state size -(ngf*4) x 8 x 8 i.e 8x8 maps</span>

            <span style=color:#75715e># Transpose 2D conv layer 3.</span>
            nn<span style=color:#f92672>.</span>ConvTranspose2d( ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>),
            nn<span style=color:#f92672>.</span>ReLU(True),
            <span style=color:#75715e># Resulting state size. (ngf*2) x 16 x 16</span>

            <span style=color:#75715e># Transpose 2D conv layer 4.</span>
            nn<span style=color:#f92672>.</span>ConvTranspose2d( ngf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>, ngf, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ngf),
            nn<span style=color:#f92672>.</span>ReLU(True),
            <span style=color:#75715e># Resulting state size. (ngf) x 32 x 32</span>

            <span style=color:#75715e># Final Transpose 2D conv layer 5 to generate final image.</span>
            <span style=color:#75715e># nc is number of channels - 3 for 3 image channel</span>
            nn<span style=color:#f92672>.</span>ConvTranspose2d( ngf, nc, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),

            <span style=color:#75715e># Tanh activation to get final normalized image</span>
            nn<span style=color:#f92672>.</span>Tanh()
            <span style=color:#75715e># Resulting state size. (nc) x 64 x 64</span>
        )

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input):
        <span style=color:#e6db74>&#39;&#39;&#39; This function takes as input the noise vector&#39;&#39;&#39;</span>
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>main(input)
</code></pre></div><p>Now we can instantiate the model using the generator class. We are keeping the default weight initializer for PyTorch even though the paper says to initialize the weights using a mean of 0 and std dev of 0.2. The default weights initializer from Pytorch is more than good enough for our project.</p><pre><code># Create the generator
netG = Generator(ngpu).to(device)
# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu &gt; 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))
# Print the model
print(netG)
</code></pre><p>We can see the final generator model:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/10_hu0cd9843d7e7e8027b94bc0284b85c5ec_186359_500x0_resize_box_2.png 500w
, /images/pyt_gan/10_hu0cd9843d7e7e8027b94bc0284b85c5ec_186359_800x0_resize_box_2.png 800w
, /images/pyt_gan/10_hu0cd9843d7e7e8027b94bc0284b85c5ec_186359_1200x0_resize_box_2.png 1200w
, /images/pyt_gan/10_hu0cd9843d7e7e8027b94bc0284b85c5ec_186359_1500x0_resize_box_2.png 1500w" src=/images/pyt_gan/10.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=the-discriminator-architecture-1>The Discriminator architecture</h2><p>Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Number of channels in the training images. For color images this is 3</span>
nc <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
<span style=color:#75715e># Size of feature maps in discriminator</span>
ndf <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Discriminator</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self, ngpu):
        super(Discriminator, self)<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>ngpu <span style=color:#f92672>=</span> ngpu
        self<span style=color:#f92672>.</span>main <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
            <span style=color:#75715e># input is (nc) x 64 x 64</span>
            nn<span style=color:#f92672>.</span>Conv2d(nc, ndf, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>LeakyReLU(<span style=color:#ae81ff>0.2</span>, inplace<span style=color:#f92672>=</span>True),
            <span style=color:#75715e># state size. (ndf) x 32 x 32</span>
            nn<span style=color:#f92672>.</span>Conv2d(ndf, ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>),
            nn<span style=color:#f92672>.</span>LeakyReLU(<span style=color:#ae81ff>0.2</span>, inplace<span style=color:#f92672>=</span>True),
            <span style=color:#75715e># state size. (ndf*2) x 16 x 16</span>
            nn<span style=color:#f92672>.</span>Conv2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>, ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>),
            nn<span style=color:#f92672>.</span>LeakyReLU(<span style=color:#ae81ff>0.2</span>, inplace<span style=color:#f92672>=</span>True),
            <span style=color:#75715e># state size. (ndf*4) x 8 x 8</span>
            nn<span style=color:#f92672>.</span>Conv2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>BatchNorm2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>),
            nn<span style=color:#f92672>.</span>LeakyReLU(<span style=color:#ae81ff>0.2</span>, inplace<span style=color:#f92672>=</span>True),
            <span style=color:#75715e># state size. (ndf*8) x 4 x 4</span>
            nn<span style=color:#f92672>.</span>Conv2d(ndf <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, bias<span style=color:#f92672>=</span>False),
            nn<span style=color:#f92672>.</span>Sigmoid()
        )

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, input):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>main(input)
</code></pre></div><p>Now we can instantiate the discriminator exactly as we did the generator.</p><pre><code># Create the Discriminator
netD = Discriminator(ngpu).to(device)
# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu &gt; 1):
    netD = nn.DataParallel(netD, list(range(ngpu)))
# Print the model
print(netD)
</code></pre><p>Here is the architecture of the discriminator:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/11_hua2d0979bc3e50f28ffdeb08a7cc0d3c6_184742_500x0_resize_box_2.png 500w
, /images/pyt_gan/11_hua2d0979bc3e50f28ffdeb08a7cc0d3c6_184742_800x0_resize_box_2.png 800w
, /images/pyt_gan/11_hua2d0979bc3e50f28ffdeb08a7cc0d3c6_184742_1200x0_resize_box_2.png 1200w" src=/images/pyt_gan/11.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=training>Training</h2><p>Understanding how the training works in GAN is essential. It’s interesting, too; we can see how training the generator and discriminator together improves them both at the same time.</p><p>Now that we have our discriminator and generator models, next we need to initialize separate optimizers for them.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Initialize BCELoss function</span>
criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BCELoss()

<span style=color:#75715e># Create batch of latent vectors that we will use to visualize</span>
<span style=color:#75715e># the progression of the generator</span>
fixed_noise <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>64</span>, nz, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, device<span style=color:#f92672>=</span>device)

<span style=color:#75715e># Establish convention for real and fake labels during training</span>
real_label <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.</span>
fake_label <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>

<span style=color:#75715e># Setup Adam optimizers for both G and D</span>

<span style=color:#75715e># Learning rate for optimizers</span>
lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0002</span>

<span style=color:#75715e># Beta1 hyperparam for Adam optimizers</span>
beta1 <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>

optimizerD <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(netD<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>lr, betas<span style=color:#f92672>=</span>(beta1, <span style=color:#ae81ff>0.999</span>))
optimizerG <span style=color:#f92672>=</span> optim<span style=color:#f92672>.</span>Adam(netG<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>lr, betas<span style=color:#f92672>=</span>(beta1, <span style=color:#ae81ff>0.999</span>))
</code></pre></div><hr><h2 id=the-training-loop>The Training Loop</h2><p>This is the main area where we need to understand how the blocks we’ve created will assemble and work together.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Lists to keep track of progress/Losses</span>
img_list <span style=color:#f92672>=</span> []
G_losses <span style=color:#f92672>=</span> []
D_losses <span style=color:#f92672>=</span> []
iters <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>

<span style=color:#75715e># Number of training epochs</span>
num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>50</span>
<span style=color:#75715e># Batch size during training</span>
batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Starting Training Loop...&#34;</span>)
<span style=color:#75715e># For each epoch</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
    <span style=color:#75715e># For each batch in the dataloader</span>
    <span style=color:#66d9ef>for</span> i, data <span style=color:#f92672>in</span> enumerate(dataloader, <span style=color:#ae81ff>0</span>):
        <span style=color:#75715e>############################</span>
        <span style=color:#75715e># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span>
        <span style=color:#75715e># Here we:</span>
        <span style=color:#75715e># A. train the discriminator on real data</span>
        <span style=color:#75715e># B. Create some fake images from Generator using Noise</span>
        <span style=color:#75715e># C. train the discriminator on fake data</span>
        <span style=color:#75715e>###########################</span>
        <span style=color:#75715e># Training Discriminator on real data</span>
        netD<span style=color:#f92672>.</span>zero_grad()
        <span style=color:#75715e># Format batch</span>
        real_cpu <span style=color:#f92672>=</span> data[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>to(device)
        b_size <span style=color:#f92672>=</span> real_cpu<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>)
        label <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>full((b_size,), real_label, device<span style=color:#f92672>=</span>device)
        <span style=color:#75715e># Forward pass real batch through D</span>
        output <span style=color:#f92672>=</span> netD(real_cpu)<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
        <span style=color:#75715e># Calculate loss on real batch</span>
        errD_real <span style=color:#f92672>=</span> criterion(output, label)
        <span style=color:#75715e># Calculate gradients for D in backward pass</span>
        errD_real<span style=color:#f92672>.</span>backward()
        D_x <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>item()

        <span style=color:#75715e>## Create a batch of fake images using generator</span>
        <span style=color:#75715e># Generate noise to send as input to the generator</span>
        noise <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(b_size, nz, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, device<span style=color:#f92672>=</span>device)
        <span style=color:#75715e># Generate fake image batch with G</span>
        fake <span style=color:#f92672>=</span> netG(noise)
        label<span style=color:#f92672>.</span>fill_(fake_label)

        <span style=color:#75715e># Classify fake batch with D</span>
        output <span style=color:#f92672>=</span> netD(fake<span style=color:#f92672>.</span>detach())<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
        <span style=color:#75715e># Calculate D&#39;s loss on the fake batch</span>
        errD_fake <span style=color:#f92672>=</span> criterion(output, label)
        <span style=color:#75715e># Calculate the gradients for this batch</span>
        errD_fake<span style=color:#f92672>.</span>backward()
        D_G_z1 <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>item()
        <span style=color:#75715e># Add the gradients from the all-real and all-fake batches</span>
        errD <span style=color:#f92672>=</span> errD_real <span style=color:#f92672>+</span> errD_fake
        <span style=color:#75715e># Update D</span>
        optimizerD<span style=color:#f92672>.</span>step()

        <span style=color:#75715e>############################</span>
        <span style=color:#75715e># (2) Update G network: maximize log(D(G(z)))</span>
        <span style=color:#75715e># Here we:</span>
        <span style=color:#75715e># A. Find the discriminator output on Fake images</span>
        <span style=color:#75715e># B. Calculate Generators loss based on this output. Note that the label is 1 for generator.</span>
        <span style=color:#75715e># C. Update Generator</span>
        <span style=color:#75715e>###########################</span>
        netG<span style=color:#f92672>.</span>zero_grad()
        label<span style=color:#f92672>.</span>fill_(real_label)  <span style=color:#75715e># fake labels are real for generator cost</span>
        <span style=color:#75715e># Since we just updated D, perform another forward pass of all-fake batch through D</span>
        output <span style=color:#f92672>=</span> netD(fake)<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
        <span style=color:#75715e># Calculate G&#39;s loss based on this output</span>
        errG <span style=color:#f92672>=</span> criterion(output, label)
        <span style=color:#75715e># Calculate gradients for G</span>
        errG<span style=color:#f92672>.</span>backward()
        D_G_z2 <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>item()
        <span style=color:#75715e># Update G</span>
        optimizerG<span style=color:#f92672>.</span>step()

        <span style=color:#75715e># Output training stats every 50th Iteration in an epoch</span>
        <span style=color:#66d9ef>if</span> i <span style=color:#f92672>%</span> <span style=color:#ae81ff>1000</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
            <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>][</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>/</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>]</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>Loss_D: </span><span style=color:#e6db74>%.4f</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>Loss_G: </span><span style=color:#e6db74>%.4f</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>D(x): </span><span style=color:#e6db74>%.4f</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>D(G(z)): </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74> / </span><span style=color:#e6db74>%.4f</span><span style=color:#e6db74>&#39;</span>
                  <span style=color:#f92672>%</span> (epoch, num_epochs, i, len(dataloader),
                     errD<span style=color:#f92672>.</span>item(), errG<span style=color:#f92672>.</span>item(), D_x, D_G_z1, D_G_z2))

        <span style=color:#75715e># Save Losses for plotting later</span>
        G_losses<span style=color:#f92672>.</span>append(errG<span style=color:#f92672>.</span>item())
        D_losses<span style=color:#f92672>.</span>append(errD<span style=color:#f92672>.</span>item())

        <span style=color:#75715e># Check how the generator is doing by saving G&#39;s output on a fixed_noise vector</span>
        <span style=color:#66d9ef>if</span> (iters <span style=color:#f92672>%</span> <span style=color:#ae81ff>250</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) <span style=color:#f92672>or</span> ((epoch <span style=color:#f92672>==</span> num_epochs<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>and</span> (i <span style=color:#f92672>==</span> len(dataloader)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)):
            <span style=color:#75715e>#print(iters)</span>
            <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
                fake <span style=color:#f92672>=</span> netG(fixed_noise)<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()
            img_list<span style=color:#f92672>.</span>append(vutils<span style=color:#f92672>.</span>make_grid(fake, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, normalize<span style=color:#f92672>=</span>True))

        iters <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</code></pre></div><p>It may seem complicated, but I’ll break down the code above step by step in this section. The main steps in every training iteration are:</p><p><strong>Step 1:</strong> Sample a batch of normalized images from the dataset</p><pre><code>for i, data in enumerate(dataloader, 0):
</code></pre><p><strong>Step 2:</strong> Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their labels.</p><pre><code>        # Training Discriminator on real data
        netD.zero_grad()
        # Format batch
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()  ## Create a batch of fake images
        # Generate noise to send as input to the generator
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)

        # Classify fake batch with D
        output = netD(fake.detach()).view(-1)
        # Calculate D's loss on the fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()
</code></pre><p><strong>Step 3:</strong> Backpropagate the errors through the generator by computing the loss gathered from discriminator output on fake images as the input and 1’s as the target while keeping the discriminator as untrainable — This ensures that the loss is higher when the generator is not able to fool the discriminator. You can check it yourself like so: if the discriminator gives 0 on the fake image, the loss will be high i.e., BCELoss(0,1).</p><pre><code>        netG.zero_grad()
        label.fill_(real_label)  
        # fake labels are real for generator cost
        output = netD(fake).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()
</code></pre><p>We repeat the steps using the for loop to end up with a good discriminator and generator.</p><hr><h2 id=results>Results</h2><p>The final output of our generator can be seen below. The GAN generates pretty good images for our content editor friends to work with.</p><p>The images might be a little crude, but still, this project was a starter for our GAN journey. The field is constantly advancing with better and more complex GAN architectures, so we’ll likely see further increases in image quality from these architectures. Also, keep in mind that these images are generated from a noise vector only: this means the input is some noise, and the output is an image. It’s quite incredible.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/12_hub665aa32432b11aeb4f2b85c1a2a0481_921538_500x0_resize_box_2.png 500w
, /images/pyt_gan/12_hub665aa32432b11aeb4f2b85c1a2a0481_921538_800x0_resize_box_2.png 800w" src=/images/pyt_gan/12.png alt="ALL THESE IMAGES ARE FAKE">
<em>ALL THESE IMAGES ARE FAKE</em></p><hr><h2 id=1-loss-over-the-training-period>1. Loss over the training period</h2><p>Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average, and the variance is also decreasing as we do more steps. It’s possible that training for even more iterations would give us even better results.</p><pre><code>plt.figure(figsize=(10,5))
plt.title(&quot;Generator and Discriminator Loss During Training&quot;)
plt.plot(G_losses,label=&quot;G&quot;)
plt.plot(D_losses,label=&quot;D&quot;)
plt.xlabel(&quot;iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.legend()
plt.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/13_hu7ca5b72290e169b5273e36945f3c9abe_119972_500x0_resize_box_2.png 500w
, /images/pyt_gan/13_hu7ca5b72290e169b5273e36945f3c9abe_119972_800x0_resize_box_2.png 800w" src=/images/pyt_gan/13.png alt="Generator vs. Discriminator Loss"></p><hr><h2 id=2-image-animation-at-every-250th-iteration-in-jupyter-notebook>2. Image Animation at every 250th Iteration in Jupyter Notebook</h2><p>We can choose to see the output as an animation using the below code:</p><pre><code>#%%capture
fig = plt.figure(figsize=(8,8))
plt.axis(&quot;off&quot;)
ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]
ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml())
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/14_hu8e53c8fc68da3f2ab6ef288f00ceee4a_1006722_500x0_resize_box_2.png 500w
, /images/pyt_gan/14_hu8e53c8fc68da3f2ab6ef288f00ceee4a_1006722_800x0_resize_box_2.png 800w" src=/images/pyt_gan/14.png alt="You can check out the Progression of Training"></p><p>You can choose to save an animation object as a gif as well if you want to send them to some friends.</p><pre><code>ani.save('animation.gif', writer='imagemagick',fps=5)
Image(url='animation.gif')
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/15_hu42c7713f7a6784488db1f9d9d32cc2d9_19260302_500x0_resize_box_2.png 500w" src=/images/pyt_gan/15.png alt="Or Save the Output as a GIF Image as well."></p><hr><h2 id=3-image-generated-at-every-200-iter>3. Image generated at every 200 Iter</h2><p>Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases, the images are getting better.</p><pre><code># create a list of 16 images to show
every_nth_image = np.ceil(len(img_list)/16)
ims = [np.transpose(img,(1,2,0)) for i,img in enumerate(img_list)if i%every_nth_image==0]
print(&quot;Displaying generated images&quot;)
# You might need to change grid size and figure size here according to num images.
plt.figure(figsize=(20,20))
gs1 = gridspec.GridSpec(4, 4)
gs1.update(wspace=0, hspace=0)
step = 0
for i,image in enumerate(ims):
    ax1 = plt.subplot(gs1[i])
    ax1.set_aspect('equal')
    fig = plt.imshow(image)
    # you might need to change some params here
    fig = plt.text(7,30,&quot;Step: &quot;+str(step),bbox=dict(facecolor='red', alpha=0.5),fontsize=12)
    plt.axis('off')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    step+=int(250*every_nth_image)
#plt.tight_layout()
plt.savefig(&quot;GENERATEDimage.png&quot;,bbox_inches='tight',pad_inches=0)
plt.show()
</code></pre><p>Given below is the result of the GAN at different time steps:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pyt_gan/16_hu06ba1784cd3d2fb5a0c95f4f981760da_3784409_500x0_resize_box_2.png 500w
, /images/pyt_gan/16_hu06ba1784cd3d2fb5a0c95f4f981760da_3784409_800x0_resize_box_2.png 800w
, /images/pyt_gan/16_hu06ba1784cd3d2fb5a0c95f4f981760da_3784409_1200x0_resize_box_2.png 1200w
, /images/pyt_gan/16_hu06ba1784cd3d2fb5a0c95f4f981760da_3784409_1500x0_resize_box_2.png 1500w" src=/images/pyt_gan/16.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=conclusion>Conclusion</h2><p>In this post, we covered the basics of GANs for creating fairly believable fake images. We hope you now have an understanding of generator and discriminator architecture for DC-GANs, and how to build a simple DC-GAN to generate anime images from scratch.</p><p>Though this model is not the most perfect anime face generator, using it as a base helps us to understand the basics of generative adversarial networks, which in turn can be used as a stepping stone to more exciting and complex GANs as we move forward.</p><p>Look at it this way, as long as we have the training data at hand, we now have the ability to conjure up realistic textures or characters on demand. That is no small feat.</p><p>For a closer look at the code for this post, please visit my
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/pytorchgan target=_blank rel="nofollow noopener">GitHub</a>
repository where you can find the code for this post as well as all my posts.</p><p>If you want to know more about deep learning applications and use cases, take a look at the
<a href="https://www.coursera.org/learn/nlp-sequence-models?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&siteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Sequence Models</a>
course in the
<a href="https://www.coursera.org/specializations/deep-learning?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&siteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Deep Learning Specialization</a>
by Andrew Ng. Andrew is a great instructor, and this course is excellent too.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at
<a href=https://medium.com/@rahul_agarwal target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><p><em>This post was first published
<a href=https://lionbridge.ai/articles/how-to-generate-anime-faces-using-gans-via-pytorch/ target=_blank rel="nofollow noopener">here</a></em></p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>