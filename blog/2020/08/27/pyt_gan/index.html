<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>A Layman’s Introduction to GANs for Data Scientists using PyTorch</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	

	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="A Layman’s Introduction to GANs for Data Scientists using PyTorch" />
<meta property="og:description" content="Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2020/08/27/pyt_gan/" />
<meta property="og:image" content="https://mlwhiz.com/images/pyt_gan/main.png" />
<meta property="article:published_time" content="2020-08-27T00:00:00&#43;00:00"/>


	<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlwhiz.com/images/pyt_gan/main.png"/>

<meta name="twitter:title" content="A Layman’s Introduction to GANs for Data Scientists using PyTorch"/>
<meta name="twitter:description" content="Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,600,600i,700,700i%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NMQD44T');</script>
	

	
	<script>
	  !function(f,b,e,v,n,t,s)
	  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
	  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
	  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
	  n.queue=[];t=b.createElement(e);t.async=!0;
	  t.src=v;s=b.getElementsByTagName(e)[0];
	  s.parentNode.insertBefore(t,s)}(window, document,'script',
	  'https://connect.facebook.net/en_US/fbevents.js');
	  fbq('init', '1062344757288542');
	  fbq('track', 'PageView');
	</script>
	<noscript><img height="1" width="1" style="display:none"
	  src="https://www.facebook.com/tr?id=1062344757288542&ev=PageView&noscript=1"
	/></noscript>
	


	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/nlpseries/">NLP</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/resources/index.html">Learning Resources</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">A Layman’s Introduction to GANs for Data Scientists using PyTorch</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2020-08-27T00:00:00">August 27, 2020</time>
	
		
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<p><img src="/images/pyt_gan/main.png" alt="" /></p>

<p>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and <a href="https://lionbridge.ai/articles/a-look-at-deepfakes-in-2020/" rel="nofollow" target="_blank">faces generated by artificial intelligence</a>. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.</p>

<p>In my view, GANs will change the way we generate video games and special effects. Using this approach, we could create realistic textures or characters on demand.</p>

<p>So in this post, we’re going to look at the generative adversarial networks behind AI-generated images, and help you to understand how to create and build your similar application with PyTorch. We’ll try to keep the post as intuitive as possible for those of you just starting out, but we’ll try not to dumb it down too much.</p>

<p>At the end of this article, you’ll have a solid understanding of how General Adversarial Networks (GANs) work, and how to build your own.</p>

<hr />

<h2 id="task-overview">Task Overview</h2>

<p>In this post, we will create unique anime characters using the <a href="https://www.kaggle.com/splcher/animefacedataset" rel="nofollow" target="_blank">Anime Face Dataset</a>. It is a dataset consisting of 63,632 high-quality anime faces in a number of styles. It’s a good starter dataset because it’s perfect for our goal.</p>

<p>We will be using Deep Convolutional Generative Adversarial Networks (DC-GANs) for our project. Though we’ll be using it to generate the faces of new anime characters, DC-GANs can also be used to create modern <a href="https://syncedreview.com/2019/08/29/ai-creates-fashion-models-with-custom-outfits-and-poses/" rel="nofollow" target="_blank">fashion styles</a>, general content creation, and sometimes for data augmentation as well.</p>

<p>But before we get into the coding, let’s take a quick look at how GANs work.</p>

<hr />

<h2 id="intuition-brief-intro-to-gans-for-generating-fake-images">INTUITION: Brief Intro to GANs for Generating Fake Images</h2>

<p>GANs typically employ two dueling neural networks to train a computer to learn the nature of a dataset well enough to generate convincing fakes. One of these Neural Networks generates fakes (the generator), and the other tries to classify which images are fake (the discriminator). These networks improve over time by competing against each other.</p>

<p>Perhaps imagine the generator as a robber and the discriminator as a police officer. The more the robber steals, the better he gets at stealing things. But at the same time, the police officer also gets better at catching the thief. Well, in an ideal world, anyway.</p>

<p>The losses in these neural networks are primarily a function of how the other network performs:</p>

<ul>
<li><p>Discriminator network loss is a function of generator network quality: Loss is high for the discriminator if it gets fooled by the generator’s fake images.</p></li>

<li><p>Generator network loss is a function of discriminator network quality: Loss is high if the generator is not able to fool the discriminator.</p></li>
</ul>

<p>In the training phase, we train our discriminator and generator networks sequentially, intending to improve performance for both. The end goal is to end up with weights that help the generator to create realistic-looking images. In the end, we’ll use the generator neural network to generate high-quality fake images from random noise<strong><em>.</em></strong></p>

<hr />

<h2 id="the-generator-architecture">The Generator architecture</h2>

<p>One of the main problems we face when working with GANs is that the training is not very stable. So we have to come up with a generator architecture that solves our problem and also results in stable training. The diagram below is taken from the paper <a href="https://arxiv.org/pdf/1511.06434.pdf" rel="nofollow" target="_blank">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a>, which explains the DC-GAN generator architecture.</p>

<p><img src="/images/pyt_gan/1.png" alt="[Source](https://arxiv.org/pdf/1511.06434.pdf)" /></p>

<p>Though it might look a little bit confusing, essentially you can think of a generator neural network as a black box which takes as input a 100 dimension normally generated vector of numbers and gives us an image:</p>

<p><img src="/images/pyt_gan/2.png" alt="Generator as a BlackBox" /></p>

<p>So how do we create such an architecture? Below, we use a dense layer of size 4x4x1024 to create a dense vector out of the 100-d vector. We then reshape the dense vector in the shape of an image of 4×4 with 1024 filters, as shown in the following figure:</p>

<p><img src="/images/pyt_gan/3.png" alt="" /></p>

<p><img src="/images/pyt_gan/4.png" alt="Generator Architecture" /></p>

<p>Note that we don’t have to worry about any weights right now as the network itself will learn those during training.</p>

<p>Once we have the 1024 4×4 maps, we do upsampling using a series of transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, however, we don’t halve the number of maps. We reduce the maps to 3 for each RGB channel since we need three channels for the output image.</p>

<hr />

<h2 id="now-what-are-transpose-convolutions">Now, What are Transpose convolutions?</h2>

<p>Put simply, transposing convolutions provides us with a way to upsample images. In a convolution operation, we try to go from a 4×4 image to a 2×2 image. But when we transpose convolutions, we convolve from 2×2 to 4×4 as shown in the following figure:</p>

<p><img src="/images/pyt_gan/5.png" alt="Upsampling a 2x2 image to 4x4 image" /></p>

<p>Some of you may already know that unpooling is commonly used for upsampling input feature maps in convolutional neural networks (CNN). So why don’t we use unpooling here?</p>

<p>The reason comes down to the fact that unpooling does not involve any learning. However, transposed convolution is learnable, so it’s preferred. Later in the article, we’ll see how the parameters can be learned by the generator.</p>

<hr />

<h2 id="the-discriminator-architecture">The Discriminator architecture</h2>

<p>Now that we’ve covered the generator architecture, let’s look at the discriminator as a black box. In practice, it contains a series of convolutional layers with a dense layer at the end to predict if an image is fake or not. You can see an example in the figure below:</p>

<p><img src="/images/pyt_gan/6.png" alt="Discriminator as a Blackbox" /></p>

<p>Every image convolutional neural network works by taking an image as input, and predicting if it is real or fake using a sequence of convolutional layers.</p>

<hr />

<h2 id="data-preprocessing-and-visualization">Data preprocessing and visualization</h2>

<p>Before going any further with our training, we preprocess our images to a standard size of 64x64x3. We will also need to normalize the image pixels before we train our GAN. You can see the process in the code below, which I’ve commented on for clarity.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Root directory for dataset</span>
dataroot <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;anime_images/&#34;</span>
<span style="color:#75715e"># Number of workers for dataloader</span>
workers <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
<span style="color:#75715e"># Batch size during training</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
<span style="color:#75715e"># Spatial size of training images. All images will be resized to this size using a transformer.</span>
image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
<span style="color:#75715e"># Number of channels in the training images. For color images this is 3</span>
nc <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
<span style="color:#75715e"># We can use an image folder dataset the way we have it setup.</span>
<span style="color:#75715e"># Create the dataset</span>
dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>ImageFolder(root<span style="color:#f92672">=</span>dataroot,
                           transform<span style="color:#f92672">=</span>transforms<span style="color:#f92672">.</span>Compose([
                               transforms<span style="color:#f92672">.</span>Resize(image_size),
                               transforms<span style="color:#f92672">.</span>CenterCrop(image_size),
                               transforms<span style="color:#f92672">.</span>ToTensor(),
                               transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>), (<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>)),
                           ]))
<span style="color:#75715e"># Create the dataloader</span>
dataloader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(dataset, batch_size<span style="color:#f92672">=</span>batch_size,
                                         shuffle<span style="color:#f92672">=</span>True, num_workers<span style="color:#f92672">=</span>workers)
<span style="color:#75715e"># Decide which device we want to run on</span>
device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> (torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#f92672">and</span> ngpu <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
<span style="color:#75715e"># Plot some training images</span>
real_batch <span style="color:#f92672">=</span> next(iter(dataloader))
plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">8</span>))
plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Training Images&#34;</span>)
plt<span style="color:#f92672">.</span>imshow(np<span style="color:#f92672">.</span>transpose(vutils<span style="color:#f92672">.</span>make_grid(real_batch[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device)[:<span style="color:#ae81ff">64</span>], padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, normalize<span style="color:#f92672">=</span>True)<span style="color:#f92672">.</span>cpu(),(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>)))</code></pre></div>
<p>The resultant output of the code is as follows:</p>

<p><img src="/images/pyt_gan/7.png" alt="So Many different Characters — Can our Generator understand the patterns?" /><em>So Many different Characters — Can our Generator understand the patterns?</em></p>

<hr />

<h2 id="implementation-of-dcgan">Implementation of DCGAN</h2>

<p>This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.</p>

<hr />

<h2 id="generating-noise-vector-for-generator">Generating noise vector for Generator</h2>

<p>We need to generate the noise which we want to convert to an image using our generator architecture.</p>

<p>We use a normal distribution</p>

<p><img src="/images/pyt_gan/8.png" alt="" /></p>

<p><img src="/images/pyt_gan/9.png" alt="" /></p>

<p>to generate the noise vector:</p>

<pre><code>nz = 100
noise = torch.randn(64, nz, 1, 1, device=device)
</code></pre>

<hr />

<h2 id="generator-architecture">Generator architecture</h2>

<p>The generator is the most crucial part of the GAN. Here, we’ll create a generator by adding some transposed convolution layers to upsample the noise vector to an image. You’ll notice that this generator architecture is not the same as the one given in the DC-GAN paper I linked above.</p>

<p>In order to make it a better fit for our data, I had to make some architectural changes. I added a convolution layer in the middle and removed all dense layers from the generator architecture to make it fully convolutional.</p>

<p>I also used a lot of Batchnorm layers and leaky ReLU activation. The following code block is the function I will use to create the generator:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Size of feature maps in generator</span>
ngf <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
<span style="color:#75715e"># Number of channels in the training images. For color images this is 3</span>
nc <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
<span style="color:#75715e"># Size of z latent vector (i.e. size of generator input noise)</span>
nz <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Generator</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, ngpu):
        super(Generator, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>ngpu <span style="color:#f92672">=</span> ngpu
        self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            <span style="color:#75715e"># input is noise, going into a convolution</span>
            <span style="color:#75715e"># Transpose 2D conv layer 1. </span>
            nn<span style="color:#f92672">.</span>ConvTranspose2d( nz, ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>),
            nn<span style="color:#f92672">.</span>ReLU(True),
            <span style="color:#75715e"># Resulting state size - (ngf*8) x 4 x 4 i.e. if ngf= 64 the size is 512 maps of 4x4 </span>
            
            <span style="color:#75715e"># Transpose 2D conv layer 2.</span>
            nn<span style="color:#f92672">.</span>ConvTranspose2d(ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
            nn<span style="color:#f92672">.</span>ReLU(True),
            <span style="color:#75715e"># Resulting state size -(ngf*4) x 8 x 8 i.e 8x8 maps</span>
            
            <span style="color:#75715e"># Transpose 2D conv layer 3.</span>
            nn<span style="color:#f92672">.</span>ConvTranspose2d( ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>ReLU(True),
            <span style="color:#75715e"># Resulting state size. (ngf*2) x 16 x 16</span>
            
            <span style="color:#75715e"># Transpose 2D conv layer 4.</span>
            nn<span style="color:#f92672">.</span>ConvTranspose2d( ngf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, ngf, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ngf),
            nn<span style="color:#f92672">.</span>ReLU(True),
            <span style="color:#75715e"># Resulting state size. (ngf) x 32 x 32</span>
            
            <span style="color:#75715e"># Final Transpose 2D conv layer 5 to generate final image. </span>
            <span style="color:#75715e"># nc is number of channels - 3 for 3 image channel</span>
            nn<span style="color:#f92672">.</span>ConvTranspose2d( ngf, nc, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            
            <span style="color:#75715e"># Tanh activation to get final normalized image</span>
            nn<span style="color:#f92672">.</span>Tanh()
            <span style="color:#75715e"># Resulting state size. (nc) x 64 x 64</span>
        )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
        <span style="color:#e6db74">&#39;&#39;&#39; This function takes as input the noise vector&#39;&#39;&#39;</span>
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>main(input)</code></pre></div>
<p>Now we can instantiate the model using the generator class. We are keeping the default weight initializer for PyTorch even though the paper says to initialize the weights using a mean of 0 and std dev of 0.2. The default weights initializer from Pytorch is more than good enough for our project.</p>

<pre><code># Create the generator
netG = Generator(ngpu).to(device)
# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu &gt; 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))
# Print the model
print(netG)
</code></pre>

<p>We can see the final generator model:</p>

<p><img src="/images/pyt_gan/10.png" alt="" /></p>

<hr />

<h2 id="the-discriminator-architecture-1">The Discriminator architecture</h2>

<p>Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Number of channels in the training images. For color images this is 3</span>
nc <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
<span style="color:#75715e"># Size of feature maps in discriminator</span>
ndf <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Discriminator</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, ngpu):
        super(Discriminator, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>ngpu <span style="color:#f92672">=</span> ngpu
        self<span style="color:#f92672">.</span>main <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
            <span style="color:#75715e"># input is (nc) x 64 x 64</span>
            nn<span style="color:#f92672">.</span>Conv2d(nc, ndf, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span>True),
            <span style="color:#75715e"># state size. (ndf) x 32 x 32</span>
            nn<span style="color:#f92672">.</span>Conv2d(ndf, ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span>True),
            <span style="color:#75715e"># state size. (ndf*2) x 16 x 16</span>
            nn<span style="color:#f92672">.</span>Conv2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span>True),
            <span style="color:#75715e"># state size. (ndf*4) x 8 x 8</span>
            nn<span style="color:#f92672">.</span>Conv2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>BatchNorm2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>),
            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span>True),
            <span style="color:#75715e"># state size. (ndf*8) x 4 x 4</span>
            nn<span style="color:#f92672">.</span>Conv2d(ndf <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, bias<span style="color:#f92672">=</span>False),
            nn<span style="color:#f92672">.</span>Sigmoid()
        )

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>main(input)</code></pre></div>
<p>Now we can instantiate the discriminator exactly as we did the generator.</p>

<pre><code># Create the Discriminator
netD = Discriminator(ngpu).to(device)
# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu &gt; 1):
    netD = nn.DataParallel(netD, list(range(ngpu)))
# Print the model
print(netD)
</code></pre>

<p>Here is the architecture of the discriminator:</p>

<p><img src="/images/pyt_gan/11.png" alt="" /></p>

<hr />

<h2 id="training">Training</h2>

<p>Understanding how the training works in GAN is essential. It’s interesting, too; we can see how training the generator and discriminator together improves them both at the same time.</p>

<p>Now that we have our discriminator and generator models, next we need to initialize separate optimizers for them.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Initialize BCELoss function</span>
criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BCELoss()

<span style="color:#75715e"># Create batch of latent vectors that we will use to visualize</span>
<span style="color:#75715e"># the progression of the generator</span>
fixed_noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">64</span>, nz, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, device<span style="color:#f92672">=</span>device)

<span style="color:#75715e"># Establish convention for real and fake labels during training</span>
real_label <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span>
fake_label <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>

<span style="color:#75715e"># Setup Adam optimizers for both G and D</span>

<span style="color:#75715e"># Learning rate for optimizers</span>
lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0002</span>

<span style="color:#75715e"># Beta1 hyperparam for Adam optimizers</span>
beta1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>

optimizerD <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(netD<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr, betas<span style="color:#f92672">=</span>(beta1, <span style="color:#ae81ff">0.999</span>))
optimizerG <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(netG<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr, betas<span style="color:#f92672">=</span>(beta1, <span style="color:#ae81ff">0.999</span>))</code></pre></div>
<hr />

<h2 id="the-training-loop">The Training Loop</h2>

<p>This is the main area where we need to understand how the blocks we’ve created will assemble and work together.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Lists to keep track of progress/Losses</span>
img_list <span style="color:#f92672">=</span> []
G_losses <span style="color:#f92672">=</span> []
D_losses <span style="color:#f92672">=</span> []
iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

<span style="color:#75715e"># Number of training epochs</span>
num_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
<span style="color:#75715e"># Batch size during training</span>
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Starting Training Loop...&#34;</span>)
<span style="color:#75715e"># For each epoch</span>
<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
    <span style="color:#75715e"># For each batch in the dataloader</span>
    <span style="color:#66d9ef">for</span> i, data <span style="color:#f92672">in</span> enumerate(dataloader, <span style="color:#ae81ff">0</span>):
        <span style="color:#75715e">############################</span>
        <span style="color:#75715e"># (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))</span>
        <span style="color:#75715e"># Here we: </span>
        <span style="color:#75715e"># A. train the discriminator on real data</span>
        <span style="color:#75715e"># B. Create some fake images from Generator using Noise</span>
        <span style="color:#75715e"># C. train the discriminator on fake data</span>
        <span style="color:#75715e">###########################</span>
        <span style="color:#75715e"># Training Discriminator on real data</span>
        netD<span style="color:#f92672">.</span>zero_grad()
        <span style="color:#75715e"># Format batch</span>
        real_cpu <span style="color:#f92672">=</span> data[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device)
        b_size <span style="color:#f92672">=</span> real_cpu<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
        label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full((b_size,), real_label, device<span style="color:#f92672">=</span>device)
        <span style="color:#75715e"># Forward pass real batch through D</span>
        output <span style="color:#f92672">=</span> netD(real_cpu)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        <span style="color:#75715e"># Calculate loss on real batch</span>
        errD_real <span style="color:#f92672">=</span> criterion(output, label)
        <span style="color:#75715e"># Calculate gradients for D in backward pass</span>
        errD_real<span style="color:#f92672">.</span>backward()
        D_x <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item()

        <span style="color:#75715e">## Create a batch of fake images using generator</span>
        <span style="color:#75715e"># Generate noise to send as input to the generator</span>
        noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(b_size, nz, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, device<span style="color:#f92672">=</span>device)
        <span style="color:#75715e"># Generate fake image batch with G</span>
        fake <span style="color:#f92672">=</span> netG(noise)
        label<span style="color:#f92672">.</span>fill_(fake_label)
        
        <span style="color:#75715e"># Classify fake batch with D</span>
        output <span style="color:#f92672">=</span> netD(fake<span style="color:#f92672">.</span>detach())<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        <span style="color:#75715e"># Calculate D&#39;s loss on the fake batch</span>
        errD_fake <span style="color:#f92672">=</span> criterion(output, label)
        <span style="color:#75715e"># Calculate the gradients for this batch</span>
        errD_fake<span style="color:#f92672">.</span>backward()
        D_G_z1 <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item()
        <span style="color:#75715e"># Add the gradients from the all-real and all-fake batches</span>
        errD <span style="color:#f92672">=</span> errD_real <span style="color:#f92672">+</span> errD_fake
        <span style="color:#75715e"># Update D</span>
        optimizerD<span style="color:#f92672">.</span>step()

        <span style="color:#75715e">############################</span>
        <span style="color:#75715e"># (2) Update G network: maximize log(D(G(z)))</span>
        <span style="color:#75715e"># Here we:</span>
        <span style="color:#75715e"># A. Find the discriminator output on Fake images</span>
        <span style="color:#75715e"># B. Calculate Generators loss based on this output. Note that the label is 1 for generator. </span>
        <span style="color:#75715e"># C. Update Generator</span>
        <span style="color:#75715e">###########################</span>
        netG<span style="color:#f92672">.</span>zero_grad()
        label<span style="color:#f92672">.</span>fill_(real_label)  <span style="color:#75715e"># fake labels are real for generator cost</span>
        <span style="color:#75715e"># Since we just updated D, perform another forward pass of all-fake batch through D</span>
        output <span style="color:#f92672">=</span> netD(fake)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
        <span style="color:#75715e"># Calculate G&#39;s loss based on this output</span>
        errG <span style="color:#f92672">=</span> criterion(output, label)
        <span style="color:#75715e"># Calculate gradients for G</span>
        errG<span style="color:#f92672">.</span>backward()
        D_G_z2 <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item()
        <span style="color:#75715e"># Update G</span>
        optimizerG<span style="color:#f92672">.</span>step()

        <span style="color:#75715e"># Output training stats every 50th Iteration in an epoch</span>
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;[</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">/</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">][</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">/</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">]</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss_D: </span><span style="color:#e6db74">%.4f</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">Loss_G: </span><span style="color:#e6db74">%.4f</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">D(x): </span><span style="color:#e6db74">%.4f</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">D(G(z)): </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%.4f</span><span style="color:#e6db74">&#39;</span>
                  <span style="color:#f92672">%</span> (epoch, num_epochs, i, len(dataloader),
                     errD<span style="color:#f92672">.</span>item(), errG<span style="color:#f92672">.</span>item(), D_x, D_G_z1, D_G_z2))

        <span style="color:#75715e"># Save Losses for plotting later</span>
        G_losses<span style="color:#f92672">.</span>append(errG<span style="color:#f92672">.</span>item())
        D_losses<span style="color:#f92672">.</span>append(errD<span style="color:#f92672">.</span>item())

        <span style="color:#75715e"># Check how the generator is doing by saving G&#39;s output on a fixed_noise vector</span>
        <span style="color:#66d9ef">if</span> (iters <span style="color:#f92672">%</span> <span style="color:#ae81ff">250</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">or</span> ((epoch <span style="color:#f92672">==</span> num_epochs<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">and</span> (i <span style="color:#f92672">==</span> len(dataloader)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)):
            <span style="color:#75715e">#print(iters)</span>
            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
                fake <span style="color:#f92672">=</span> netG(fixed_noise)<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>cpu()
            img_list<span style="color:#f92672">.</span>append(vutils<span style="color:#f92672">.</span>make_grid(fake, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, normalize<span style="color:#f92672">=</span>True))

        iters <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span></code></pre></div>
<p>It may seem complicated, but I’ll break down the code above step by step in this section. The main steps in every training iteration are:</p>

<p><strong>Step 1:</strong> Sample a batch of normalized images from the dataset</p>

<pre><code>for i, data in enumerate(dataloader, 0):
</code></pre>

<p><strong>Step 2:</strong> Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their labels.</p>

<pre><code>        # Training Discriminator on real data
        netD.zero_grad()
        # Format batch
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()  ## Create a batch of fake images
        # Generate noise to send as input to the generator
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)

        # Classify fake batch with D
        output = netD(fake.detach()).view(-1)
        # Calculate D's loss on the fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()
</code></pre>

<p><strong>Step 3:</strong> Backpropagate the errors through the generator by computing the loss gathered from discriminator output on fake images as the input and 1’s as the target while keeping the discriminator as untrainable — This ensures that the loss is higher when the generator is not able to fool the discriminator. You can check it yourself like so: if the discriminator gives 0 on the fake image, the loss will be high i.e., BCELoss(0,1).</p>

<pre><code>        netG.zero_grad()
        label.fill_(real_label)  
        # fake labels are real for generator cost
        output = netD(fake).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()
</code></pre>

<p>We repeat the steps using the for loop to end up with a good discriminator and generator.</p>

<hr />

<h2 id="results">Results</h2>

<p>The final output of our generator can be seen below. The GAN generates pretty good images for our content editor friends to work with.</p>

<p>The images might be a little crude, but still, this project was a starter for our GAN journey. The field is constantly advancing with better and more complex GAN architectures, so we’ll likely see further increases in image quality from these architectures. Also, keep in mind that these images are generated from a noise vector only: this means the input is some noise, and the output is an image. It’s quite incredible.</p>

<p><img src="/images/pyt_gan/12.png" alt="ALL THESE IMAGES ARE FAKE" /><em>ALL THESE IMAGES ARE FAKE</em></p>

<hr />

<h2 id="1-loss-over-the-training-period">1. Loss over the training period</h2>

<p>Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average, and the variance is also decreasing as we do more steps. It’s possible that training for even more iterations would give us even better results.</p>

<pre><code>plt.figure(figsize=(10,5))
plt.title(&quot;Generator and Discriminator Loss During Training&quot;)
plt.plot(G_losses,label=&quot;G&quot;)
plt.plot(D_losses,label=&quot;D&quot;)
plt.xlabel(&quot;iterations&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.legend()
plt.show()
</code></pre>

<p><img src="/images/pyt_gan/13.png" alt="Generator vs. Discriminator Loss" /></p>

<hr />

<h2 id="2-image-animation-at-every-250th-iteration-in-jupyter-notebook">2. Image Animation at every 250th Iteration in Jupyter Notebook</h2>

<p>We can choose to see the output as an animation using the below code:</p>

<pre><code>#%%capture
fig = plt.figure(figsize=(8,8))
plt.axis(&quot;off&quot;)
ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]
ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml())
</code></pre>

<p><img src="/images/pyt_gan/14.png" alt="You can check out the Progression of Training" /></p>

<p>You can choose to save an animation object as a gif as well if you want to send them to some friends.</p>

<pre><code>ani.save('animation.gif', writer='imagemagick',fps=5)
Image(url='animation.gif')
</code></pre>

<p><img src="/images/pyt_gan/15.png" alt="Or Save the Output as a GIF Image as well." /></p>

<hr />

<h2 id="3-image-generated-at-every-200-iter">3. Image generated at every 200 Iter</h2>

<p>Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases, the images are getting better.</p>

<pre><code># create a list of 16 images to show
every_nth_image = np.ceil(len(img_list)/16)
ims = [np.transpose(img,(1,2,0)) for i,img in enumerate(img_list)if i%every_nth_image==0]
print(&quot;Displaying generated images&quot;)
# You might need to change grid size and figure size here according to num images. 
plt.figure(figsize=(20,20))
gs1 = gridspec.GridSpec(4, 4)
gs1.update(wspace=0, hspace=0)
step = 0
for i,image in enumerate(ims):
    ax1 = plt.subplot(gs1[i])
    ax1.set_aspect('equal')
    fig = plt.imshow(image)
    # you might need to change some params here
    fig = plt.text(7,30,&quot;Step: &quot;+str(step),bbox=dict(facecolor='red', alpha=0.5),fontsize=12)
    plt.axis('off')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    step+=int(250*every_nth_image)
#plt.tight_layout()
plt.savefig(&quot;GENERATEDimage.png&quot;,bbox_inches='tight',pad_inches=0)
plt.show()
</code></pre>

<p>Given below is the result of the GAN at different time steps:</p>

<p><img src="/images/pyt_gan/16.png" alt="" /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we covered the basics of GANs for creating fairly believable fake images. We hope you now have an understanding of generator and discriminator architecture for DC-GANs, and how to build a simple DC-GAN to generate anime images from scratch.</p>

<p>Though this model is not the most perfect anime face generator, using it as a base helps us to understand the basics of generative adversarial networks, which in turn can be used as a stepping stone to more exciting and complex GANs as we move forward.</p>

<p>Look at it this way, as long as we have the training data at hand, we now have the ability to conjure up realistic textures or characters on demand. That is no small feat.</p>

<p>For a closer look at the code for this post, please visit my <a href="https://github.com/MLWhiz/data_science_blogs/tree/master/pytorchgan" rel="nofollow" target="_blank">GitHub</a> repository where you can find the code for this post as well as all my posts.</p>

<p>If you want to know more about deep learning applications and use cases, take a look at the <a href="https://www.coursera.org/learn/nlp-sequence-models?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&amp;siteID=lVarvwc5BD0-JE1cT4rP0eccd5RvFoTteA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Sequence Models</a> course in the <a href="https://www.coursera.org/specializations/deep-learning?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&amp;siteID=lVarvwc5BD0-wUe8qfWqZWG14SMpBD9rLg&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="nofollow" target="_blank">Deep Learning Specialization</a> by Andrew Ng. Andrew is a great instructor, and this course is excellent too.</p>

<p>I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at <a href="https://medium.com/@rahul_agarwal" rel="nofollow" target="_blank">Medium</a> or Subscribe to my <a href="http://eepurl.com/dbQnuX" rel="nofollow" target="_blank">blog</a> to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter <a href="https://twitter.com/MLWhiz" rel="nofollow" target="_blank">@mlwhiz</a>.</p>

<p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p>

<p><em>This post was first published <a href="https://lionbridge.ai/articles/how-to-generate-anime-faces-using-gans-via-pytorch/" rel="nofollow" target="_blank">here</a></em></p>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/statistics/" rel="tag">Statistics</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>

<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.372&subid=0&type=4" rel="nofollow"><IMG border="0"   alt="Start your future with a Data Science Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.372&subid=0&type=4&gridnum=16"></a>





























	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2020/08/27/mlengineercourses/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Become an ML Engineer with these courses from Amazon and Google</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<style type="text/css">

  .btn {
    display: inline-block;
    font-weight: 400;
    text-align: center;
    white-space: nowrap;
    vertical-align: middle;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;
    border: 1px solid transparent;
    padding: .375rem .75rem;
    font-size: 1rem;
    line-height: 1.5;
    border-radius: .25rem;
    transition: color .15s ease-in-out,background-color .15s ease-in-out,border-color .15s ease-in-out,box-shadow .15s ease-in-out;
}

.btn-session {
    color: #fff;
    background-color: #28a745;
    border-color: #28a745;
}
</style>


<aside class="sidebar">
  <div style="text-align:center">     
    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>
  <br><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://mlwhiz.com/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/27/pyt_gan/">A Layman’s Introduction to GANs for Data Scientists using PyTorch</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/27/mlengineercourses/">Become an ML Engineer with these courses from Amazon and Google</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/27/pandasql/">How to use SQL with Pandas?</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/12/ctskills/">5 Essential Business-Oriented Critical Thinking Skills For Data Scientists</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/09/owndlrig/">Creating my First Deep Learning &#43; Data Science Workstation</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/04/spark_dataproc/">Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/08/deployment_fastapi/">Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/08/08/yolov5/">How to Create an End to End Object Detector using Yolov5</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/06/06/fastapi_for_data_scientists/">A Layman’s Guide for Data Scientists to create APIs in minutes</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog/2020/06/06/dlrig/">A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</a></li>
		</ul>
	</div>
</div>

<div style="text-align:center">     
    
    <a class="btn btn-session" href="https://www.patreon.com/bePatron?u=28135435" role="button">1:1 Session</a>
  </div>

<br>




<div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>

<style type="text/css">
.bookclass .shareaholic-share-buttons-heading .shareaholic-canvas{
  font-family: montserrat,sans-serif;
  font-size:.5em ;
  font-weight: 500;
  }
</style>
<center>



<div class="bookclass">Subscribe to Get
<a href="https://www.amazon.in/Advanced-Python-Tips-explained-Simply-ebook/dp/B07TM3D279">
  <img src="https://d2sofvawe08yqg.cloudfront.net/advancedpythontips/hero2x?1593185350" width="70%" height="70%"></img></a>
  </div>

<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">


<style type="text/css">
  #mc_embed_signup .button {

    background-color: #3f51b5;
 
  }
  #mc_embed_signup form .center{
    display: block;
    position: relative;
    text-align: -webkit-center;
    padding: 10px -5px 10px 3%;}  
 
  #mc_embed_signup form {
    text-align: -webkit-center;
    } 

    #mc_embed_signup input.button {
    min-width: 110px;
}

    #mc_embed_signup #mc_embed_signup_scroll{
      font-family: montserrat,sans-serif;  
      font-size:1em; 
    }
    #mc_embed_signup input.email {
      font-family: montserrat,sans-serif; 
    }


   
</style>

<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
</center>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2020 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>