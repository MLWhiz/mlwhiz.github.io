<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2 - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2 - MLWhiz"><meta property="og:description" content="Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/"><meta property="og:image" content="https://mlwhiz.com/images/deployment_fastapi/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/deployment_fastapi/main.png"><meta property="article:published_time" content="2020-08-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T16:18:05+01:00"><meta property="article:tag" content="Programming"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/deployment_fastapi/main.png"><meta name=twitter:title content="Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2 - MLWhiz"><meta name=twitter:description content="Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/#primaryimage","url":"https://mlwhiz.com/images/deployment_fastapi/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/#webpage","url":"https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/","inLanguage":"en-US","name":"Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2 - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/#primaryimage"},"datePublished":"2020-08-04T00:00:00.00Z","dateModified":"2023-07-07T16:18:05.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/programming class=categoryStyle>Programming</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>04 August 2020</span></div><img src=https://mlwhiz.com/images/deployment_fastapi/main.png class="img-fluid w-100 mb-4" alt="Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2"><div class="content mb-5"><p>Just recently, I had written a simple
<a href=https://towardsdatascience.com/a-layman-guide-for-data-scientists-to-create-apis-in-minutes-31e6f451cd2f target=_blank rel="nofollow noopener">tutorial</a>
on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.</p><p>That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.</p><p>I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS. None of those seemed particularly great or complete to me.</p><p>So, I tried to do this myself using some help from
<a href=https://fastapi.tiangolo.com/deployment/ target=_blank rel="nofollow noopener">FastAPI documentation</a>
. In this post, we will look at predominantly four things:</p><ul><li><p>Setting Up an Amazon Instance</p></li><li><p>Creating a FastAPI API for Object Detection</p></li><li><p>Deploying FastAPI using Docker</p></li><li><p>An End to End App with UI</p></li></ul><p><em><strong>So, without further ado, let’s get started.</strong></em></p><p>You can skip any part you feel you are versed with though I would expect you to go through the whole post, long as it may be, as there’s a lot of interconnection between concepts.</p><hr><h2 id=1-setting-up-amazon-instance>1. Setting Up Amazon Instance</h2><p>Before we start with using the Amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the
<a href=https://aws.amazon.com/ target=_blank rel="nofollow noopener">AWS website</a>
. Works just like a single sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.</p><ul><li><p>Go to AWS Management Console using
<a href=https://us-west-2.console.aws.amazon.com/console target=_blank rel="nofollow noopener">https://us-west-2.console.aws.amazon.com/console</a>
.</p></li><li><p>On the AWS Management Console, you can select “Launch a Virtual Machine.” Here we are trying to set up the machine where we will deploy our FastAPI API.</p></li><li><p>In the first step, you need to choose the AMI template for the machine. I am selecting the 18.04 Ubuntu Server since Ubuntu.</p></li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/0_hud3945a0c6c78397af7fa6af396d2dbcb_159576_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/0_hud3945a0c6c78397af7fa6af396d2dbcb_159576_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/0_hud3945a0c6c78397af7fa6af396d2dbcb_159576_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/0_hud3945a0c6c78397af7fa6af396d2dbcb_159576_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/0.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>In the second step, I select the t2.xlarge machine, which has 4 CPUs and 16GB RAM rather than the free tier since I want to use an Object Detection model and will need some resources.</li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/1_hu948229725d19e20946046f490415e3fd_195437_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/1_hu948229725d19e20946046f490415e3fd_195437_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/1_hu948229725d19e20946046f490415e3fd_195437_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/1_hu948229725d19e20946046f490415e3fd_195437_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>Keep pressing Next until you reach the “6. Configure Security Group” tab. This is the most crucial step here. You will need to add a rule with Type: “HTTP” and Port Range:80.</li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/2_hu6cae36eaf77183ccec37a9cc9859c2eb_159527_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/2_hu6cae36eaf77183ccec37a9cc9859c2eb_159527_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/2_hu6cae36eaf77183ccec37a9cc9859c2eb_159527_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/2_hu6cae36eaf77183ccec37a9cc9859c2eb_159527_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch, you might need to create a new key pair. Here I am creating a new key pair named fastapi and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair</li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/3_hufb56ee1999a360fc4be41debf61eaece_142074_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/3_hufb56ee1999a360fc4be41debf61eaece_142074_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>You can now go to your instances to see if your instance has started. Hint: See the Instance state; it should be showing “Running.”</li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/4_hub19ef9f9c1bd545ba808f388f892b367_168711_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/4_hub19ef9f9c1bd545ba808f388f892b367_168711_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/4_hub19ef9f9c1bd545ba808f388f892b367_168711_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>Also, to note here are the Public DNS(IPv4) address and the IPv4 public IP. We will need it to connect to this machine. For me, they are:</li></ul><pre><code>Public DNS (IPv4): ec2-18-237-28-174.us-west-2.compute.amazonaws.com

IPv4 Public IP: 18.237.28.174
</code></pre><ul><li>Once you have that run the following commands in the folder, you saved the fastapi.pem file. If the file is named fastapi.txt you might need to rename it to fastapi.pem.</li></ul><pre><code># run fist command if fastapi.txt gets downloaded.
# mv fastapi.txt fastapi.pem

chmod 400 fastapi.pem
ssh -i &quot;fastapi.pem&quot; ubuntu@&lt;Your Public DNS(IPv4) Address&gt;
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/5_hu2d79ce12d63527c7810e6cc33a291472_131341_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/5_hu2d79ce12d63527c7810e6cc33a291472_131341_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/5_hu2d79ce12d63527c7810e6cc33a291472_131341_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/5.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Now we have got our Amazon instance up and running. We can move on here to the real part of the post.</p><hr><h2 id=2-creating-a-fastapi-api-for-object-detection>2. Creating a FastAPI API for Object Detection</h2><p>Before we deploy an API, we need to have an API with us, right? In one of my last posts, I had written a simple
<a href=https://towardsdatascience.com/a-layman-guide-for-data-scientists-to-create-apis-in-minutes-31e6f451cd2f target=_blank rel="nofollow noopener">tutorial to understand FastAPI</a>
and API basics. Do read the post if you want to understand FastAPI basics.</p><p>So, here I will try to create an Image detection API. As for how to pass the Image data to the API? The idea is — <em><strong>What is an image but a string?</strong></em> An image is just made up of bytes, and we can encode these bytes as a string. We will use the base64 string representation, which is a popular way to get binary data to ASCII characters. And, we will pass this string representation to give an image to our API.</p><h3 id=a-some-image-basics-what-is-image-but-a-string>A. Some Image Basics: What is Image, But a String?</h3><p>So, let us first see how we can convert an Image to a String. We read the binary data from an image file using the ‘rb’ flag and turn it into a base64 encoded data representation using the base64.b64encode function. We then use the decode to utf-8 function to get the base encoded data into human-readable characters. Don’t worry if it doesn’t make a lot of sense right now. <em><strong>Just understand that any data is binary, and we can convert binary data to its string representation using a series of steps.</strong></em></p><p>As a simple example, if I have a simple image like below, we can convert it to a string using:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/6_hub5a78a8d13589c37a38e660e2afa6a1d_51073_500x0_resize_box_2.png 500w" src=/images/deployment_fastapi/6.png alt=dog_with_ball.jpg></p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>base64</span>

<span style=color:#6ab825;font-weight:700>with</span> <span style=color:#24909d>open</span>(<span style=color:#ed9d13>&#34;sample_images/dog_with_ball.jpg&#34;</span>, <span style=color:#ed9d13>&#34;rb&#34;</span>) <span style=color:#6ab825;font-weight:700>as</span> image_file:
    base64str = base64.b64encode(image_file.read()).decode(<span style=color:#ed9d13>&#34;utf-8&#34;</span>)
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/7_huecda07594ec3e911ee07e25169de3c28_149765_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/7_huecda07594ec3e911ee07e25169de3c28_149765_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/7.png alt="We can get a string representation of any image"></p><p>Here I have got a string representation of a file named dog_with_ball.png on my laptop.</p><p>Great, we now have a string representation of an image. And, we can send this string representation to our FastAPI. But we also need to have a way to read an image back from its string representation. After all, our image detection API using PyTorch and any other package needs to have an image object that they can predict, and those methods don’t work on a string.</p><p>So here is a way to create a PIL image back from an image’s base64 string. Mostly we just do the reverse steps in the same order. We encode in ‘utf-8’ using .encode. We then use base64.b64decode to decode to bytes. We use these bytes to create a bytes object using io.BytesIO and use Image.open to open this bytes IO object as a PIL image, which can easily be used as an input to my PyTorch prediction code.*** Again simply, it is just a way to convert base64 image string to an actual image.***</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>base64</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>io</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>PIL</span> <span style=color:#6ab825;font-weight:700>import</span> Image

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>base64str_to_PILImage</span>(base64str):
   base64_img_bytes = base64str.encode(<span style=color:#ed9d13>&#39;utf-8&#39;</span>)
   base64bytes = base64.b64decode(base64_img_bytes)
   bytesObj = io.BytesIO(base64bytes)
   img = Image.open(bytesObj)
   <span style=color:#6ab825;font-weight:700>return</span> img
</code></pre></div><p>So does this function work? Let’s see for ourselves. We can use just the string to get back the image.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/8_hua69c407afe5991225e3c2c29906c3b80_459636_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/8_hua69c407afe5991225e3c2c29906c3b80_459636_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/8.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>And we have our happy dog back again. Looks better than the string.</p><h3 id=b-writing-the-actual-fastapi-code>B. Writing the Actual FastAPI code</h3><p>So, as now we understand that our API can get an image as a string from our user, let’s create an object detection API that makes use of this image as a string and outputs the bounding boxes for the object with the object classes as well.</p><p>Here, I will be using a Pytorch pre-trained fasterrcnn_resnet50_fpn detection model from the torchvision.models for object detection, which is trained on the COCO dataset to keep the code simple, but one can use any model. You can look at these posts if you want to train your custom
<a href=https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c target=_blank rel="nofollow noopener">image classification</a>
or
<a href=https://lionbridge.ai/articles/create-an-end-to-end-object-detection-pipeline-using-yolov5/ target=_blank rel="nofollow noopener">image detection</a>
model using Pytorch.</p><p>Below is the full code for the FastAPI. Although it may look long, we already know all the parts. In this code, we essentially do the following steps:</p><ul><li><p>Create our fast API app using the FastAPI() constructor.</p></li><li><p>Load our model and the classes it was trained on. I got the list of classes from the PyTorch
<a href=https://pytorch.org/docs/stable/torchvision/models.html target=_blank rel="nofollow noopener">docs</a>
.</p></li><li><p>We also defined a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user. Here the end-user gives the base64str and some score threshold for object detection prediction.</p></li><li><p>We add a function called base64str_to_PILImage which does just what it is named.</p></li><li><p>And we write a predict function called get_predictionbase64 which returns a dict of bounding boxes and classes using a base64 string representation of an image and a threshold as an input. We also add
<a href=http://twitter.com/app target=_blank rel="nofollow noopener">@app</a>
.put(“/predict”) on top of this function to define our endpoint. If you need to understand put and endpoint refer to my previous
<a href=https://towardsdatascience.com/a-layman-guide-for-data-scientists-to-create-apis-in-minutes-31e6f451cd2f target=_blank rel="nofollow noopener">post</a>
on FastAPI.</p></li></ul><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>fastapi</span> <span style=color:#6ab825;font-weight:700>import</span> FastAPI
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>pydantic</span> <span style=color:#6ab825;font-weight:700>import</span> BaseModel
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torchvision</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>torchvision</span> <span style=color:#6ab825;font-weight:700>import</span> transforms
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>torchvision.models.detection.faster_rcnn</span> <span style=color:#6ab825;font-weight:700>import</span> FastRCNNPredictor
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>PIL</span> <span style=color:#6ab825;font-weight:700>import</span> Image
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>numpy</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>np</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>cv2</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>io</span>, <span style=color:#447fcf;text-decoration:underline>json</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>base64</span>


app = FastAPI()

<span style=color:#999;font-style:italic># load a pre-trained Model and convert it to eval mode.</span>
<span style=color:#999;font-style:italic># This model loads just once when we start the API.</span>
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
COCO_INSTANCE_CATEGORY_NAMES = [
    <span style=color:#ed9d13>&#39;__background__&#39;</span>, <span style=color:#ed9d13>&#39;person&#39;</span>, <span style=color:#ed9d13>&#39;bicycle&#39;</span>, <span style=color:#ed9d13>&#39;car&#39;</span>, <span style=color:#ed9d13>&#39;motorcycle&#39;</span>, <span style=color:#ed9d13>&#39;airplane&#39;</span>, <span style=color:#ed9d13>&#39;bus&#39;</span>,
    <span style=color:#ed9d13>&#39;train&#39;</span>, <span style=color:#ed9d13>&#39;truck&#39;</span>, <span style=color:#ed9d13>&#39;boat&#39;</span>, <span style=color:#ed9d13>&#39;traffic light&#39;</span>, <span style=color:#ed9d13>&#39;fire hydrant&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;stop sign&#39;</span>,
    <span style=color:#ed9d13>&#39;parking meter&#39;</span>, <span style=color:#ed9d13>&#39;bench&#39;</span>, <span style=color:#ed9d13>&#39;bird&#39;</span>, <span style=color:#ed9d13>&#39;cat&#39;</span>, <span style=color:#ed9d13>&#39;dog&#39;</span>, <span style=color:#ed9d13>&#39;horse&#39;</span>, <span style=color:#ed9d13>&#39;sheep&#39;</span>, <span style=color:#ed9d13>&#39;cow&#39;</span>,
    <span style=color:#ed9d13>&#39;elephant&#39;</span>, <span style=color:#ed9d13>&#39;bear&#39;</span>, <span style=color:#ed9d13>&#39;zebra&#39;</span>, <span style=color:#ed9d13>&#39;giraffe&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;backpack&#39;</span>, <span style=color:#ed9d13>&#39;umbrella&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>,
    <span style=color:#ed9d13>&#39;handbag&#39;</span>, <span style=color:#ed9d13>&#39;tie&#39;</span>, <span style=color:#ed9d13>&#39;suitcase&#39;</span>, <span style=color:#ed9d13>&#39;frisbee&#39;</span>, <span style=color:#ed9d13>&#39;skis&#39;</span>, <span style=color:#ed9d13>&#39;snowboard&#39;</span>, <span style=color:#ed9d13>&#39;sports ball&#39;</span>,
    <span style=color:#ed9d13>&#39;kite&#39;</span>, <span style=color:#ed9d13>&#39;baseball bat&#39;</span>, <span style=color:#ed9d13>&#39;baseball glove&#39;</span>, <span style=color:#ed9d13>&#39;skateboard&#39;</span>, <span style=color:#ed9d13>&#39;surfboard&#39;</span>, <span style=color:#ed9d13>&#39;tennis racket&#39;</span>,
    <span style=color:#ed9d13>&#39;bottle&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;wine glass&#39;</span>, <span style=color:#ed9d13>&#39;cup&#39;</span>, <span style=color:#ed9d13>&#39;fork&#39;</span>, <span style=color:#ed9d13>&#39;knife&#39;</span>, <span style=color:#ed9d13>&#39;spoon&#39;</span>, <span style=color:#ed9d13>&#39;bowl&#39;</span>,
    <span style=color:#ed9d13>&#39;banana&#39;</span>, <span style=color:#ed9d13>&#39;apple&#39;</span>, <span style=color:#ed9d13>&#39;sandwich&#39;</span>, <span style=color:#ed9d13>&#39;orange&#39;</span>, <span style=color:#ed9d13>&#39;broccoli&#39;</span>, <span style=color:#ed9d13>&#39;carrot&#39;</span>, <span style=color:#ed9d13>&#39;hot dog&#39;</span>, <span style=color:#ed9d13>&#39;pizza&#39;</span>,
    <span style=color:#ed9d13>&#39;donut&#39;</span>, <span style=color:#ed9d13>&#39;cake&#39;</span>, <span style=color:#ed9d13>&#39;chair&#39;</span>, <span style=color:#ed9d13>&#39;couch&#39;</span>, <span style=color:#ed9d13>&#39;potted plant&#39;</span>, <span style=color:#ed9d13>&#39;bed&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;dining table&#39;</span>,
    <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;toilet&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;tv&#39;</span>, <span style=color:#ed9d13>&#39;laptop&#39;</span>, <span style=color:#ed9d13>&#39;mouse&#39;</span>, <span style=color:#ed9d13>&#39;remote&#39;</span>, <span style=color:#ed9d13>&#39;keyboard&#39;</span>, <span style=color:#ed9d13>&#39;cell phone&#39;</span>,
    <span style=color:#ed9d13>&#39;microwave&#39;</span>, <span style=color:#ed9d13>&#39;oven&#39;</span>, <span style=color:#ed9d13>&#39;toaster&#39;</span>, <span style=color:#ed9d13>&#39;sink&#39;</span>, <span style=color:#ed9d13>&#39;refrigerator&#39;</span>, <span style=color:#ed9d13>&#39;N/A&#39;</span>, <span style=color:#ed9d13>&#39;book&#39;</span>,
    <span style=color:#ed9d13>&#39;clock&#39;</span>, <span style=color:#ed9d13>&#39;vase&#39;</span>, <span style=color:#ed9d13>&#39;scissors&#39;</span>, <span style=color:#ed9d13>&#39;teddy bear&#39;</span>, <span style=color:#ed9d13>&#39;hair drier&#39;</span>, <span style=color:#ed9d13>&#39;toothbrush&#39;</span>
]
model.eval()

<span style=color:#999;font-style:italic># define the Input class</span>
<span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>Input</span>(BaseModel):
    base64str : <span style=color:#24909d>str</span>
    threshold : <span style=color:#24909d>float</span>

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>base64str_to_PILImage</span>(base64str):
    base64_img_bytes = base64str.encode(<span style=color:#ed9d13>&#39;utf-8&#39;</span>)
    base64bytes = base64.b64decode(base64_img_bytes)
    bytesObj = io.BytesIO(base64bytes)
    img = Image.open(bytesObj)
    <span style=color:#6ab825;font-weight:700>return</span> img

<span style=color:orange>@app.put</span>(<span style=color:#ed9d13>&#34;/predict&#34;</span>)
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>get_predictionbase64</span>(d:Input):
    <span style=color:#ed9d13>&#39;&#39;&#39;
</span><span style=color:#ed9d13>    FastAPI API will take a base 64 image as input and return a json object
</span><span style=color:#ed9d13>    &#39;&#39;&#39;</span>
    <span style=color:#999;font-style:italic># Load the image</span>
    img = base64str_to_PILImage(d.base64str)
    <span style=color:#999;font-style:italic># Convert image to tensor</span>
    transform = transforms.Compose([transforms.ToTensor()])
    img = transform(img)
    <span style=color:#999;font-style:italic># get prediction on image</span>
    pred = model([img])
    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>list</span>(pred[<span style=color:#3677a9>0</span>][<span style=color:#ed9d13>&#39;labels&#39;</span>].numpy())]
    pred_boxes = [[(<span style=color:#24909d>float</span>(i[<span style=color:#3677a9>0</span>]), <span style=color:#24909d>float</span>(i[<span style=color:#3677a9>1</span>])), (<span style=color:#24909d>float</span>(i[<span style=color:#3677a9>2</span>]), <span style=color:#24909d>float</span>(i[<span style=color:#3677a9>3</span>]))] <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>list</span>(pred[<span style=color:#3677a9>0</span>][<span style=color:#ed9d13>&#39;boxes&#39;</span>].detach().numpy())]
    pred_score = <span style=color:#24909d>list</span>(pred[<span style=color:#3677a9>0</span>][<span style=color:#ed9d13>&#39;scores&#39;</span>].detach().numpy())
    pred_t = [pred_score.index(x) <span style=color:#6ab825;font-weight:700>for</span> x <span style=color:#6ab825;font-weight:700>in</span> pred_score <span style=color:#6ab825;font-weight:700>if</span> x &gt; d.threshold][-<span style=color:#3677a9>1</span>]
    pred_boxes = pred_boxes[:pred_t+<span style=color:#3677a9>1</span>]
    pred_class = pred_class[:pred_t+<span style=color:#3677a9>1</span>]
    <span style=color:#6ab825;font-weight:700>return</span> {<span style=color:#ed9d13>&#39;boxes&#39;</span>: pred_boxes,
        <span style=color:#ed9d13>&#39;classes&#39;</span> : pred_class}
</code></pre></div><h3 id=c-local-before-global-test-the-fastapi-code-locally>C. Local Before Global: Test the FastAPI code locally</h3><p>Before we move on to AWS, let us check if the code works on our local machine. We can start the API on our laptop using:</p><pre><code>uvicorn fastapiapp:app --reload
</code></pre><p>The above means that your API is now running on your local server, and the &ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &ndash;reload flag when you put the API in production.</p><p>You should see something like:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/9_hu77dcfea9d8dfcff2e83f15ef195f4dce_38668_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/9_hu77dcfea9d8dfcff2e83f15ef195f4dce_38668_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/9.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>You can now try to access this API and see if it works using the requests module:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>requests</span>,<span style=color:#447fcf;text-decoration:underline>json</span>

payload = json.dumps({
  <span style=color:#ed9d13>&#34;base64str&#34;</span>: base64str,
  <span style=color:#ed9d13>&#34;threshold&#34;</span>: <span style=color:#3677a9>0.5</span>
})

response = requests.put(<span style=color:#ed9d13>&#34;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&#34;</span>,data = payload)
data_dict = response.json()
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/10_hu9ed536e837ddf35d2d5370c999b5b04b_56477_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/10_hu9ed536e837ddf35d2d5370c999b5b04b_56477_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/10.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>And so we get our results using the API. This image contains a dog and a sports ball. We also have corner 1 (x1,y1) and corner 2 (x2,y2) coordinates of our bounding boxes.</p><h3 id=d-lets-visualize>D. Lets Visualize</h3><p>Although not strictly necessary, we can visualize how the results look in our Jupyter notebook:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>PIL</span> <span style=color:#6ab825;font-weight:700>import</span> Image
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>numpy</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>np</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>cv2</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>matplotlib.pyplot</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>plt</span>

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>PILImage_to_cv2</span>(img):
    <span style=color:#6ab825;font-weight:700>return</span> np.asarray(img)

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>drawboundingbox</span>(img, boxes,pred_cls, rect_th=<span style=color:#3677a9>2</span>, text_size=<span style=color:#3677a9>1</span>, text_th=<span style=color:#3677a9>2</span>):
    img = PILImage_to_cv2(img)
    class_color_dict = {}

    <span style=color:#999;font-style:italic>#initialize some random colors for each class for better looking bounding boxes</span>
    <span style=color:#6ab825;font-weight:700>for</span> cat <span style=color:#6ab825;font-weight:700>in</span> pred_cls:
        class_color_dict[cat] = [random.randint(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>255</span>) <span style=color:#6ab825;font-weight:700>for</span> _ <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>3</span>)]

    <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#24909d>len</span>(boxes)):
        cv2.rectangle(img, (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>0</span>]), <span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>1</span>])),
                      (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>1</span>][<span style=color:#3677a9>0</span>]),<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>1</span>][<span style=color:#3677a9>1</span>])),
                      color=class_color_dict[pred_cls[i]], thickness=rect_th)
        cv2.putText(img,pred_cls[i], (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>0</span>]), <span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>1</span>])),  cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) <span style=color:#999;font-style:italic># Write the prediction class</span>
    plt.figure(figsize=(<span style=color:#3677a9>20</span>,<span style=color:#3677a9>30</span>))
    plt.imshow(img)
    plt.xticks([])
    plt.yticks([])
    plt.show()

img = Image.open(<span style=color:#ed9d13>&#34;sample_images/dog_with_ball.jpg&#34;</span>)
drawboundingbox(img, data_dict[<span style=color:#ed9d13>&#39;boxes&#39;</span>], data_dict[<span style=color:#ed9d13>&#39;classes&#39;</span>])
</code></pre></div><p>Here is the output:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/11_huf07bf7e165d2d0401bde881d000ec16d_785729_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/11_huf07bf7e165d2d0401bde881d000ec16d_785729_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/11.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Here you will note that I got the image from the local file system, and that sort of can be considered as cheating as we don’t want to save every file that the user sends to us through a web UI. We should have been able to use the same base64string object that we also had to create this image. Right?</p><p>Not to worry, we could do that too. Remember our base64str_to_PILImage function? We could have used that also.</p><pre><code>img = base64str_to_PILImage(base64str)
drawboundingbox(img, data_dict['boxes'], data_dict['classes'])
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/12_hu1e0fb50a8efa40cbbe1c5e2a27979dc7_784580_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/12_hu1e0fb50a8efa40cbbe1c5e2a27979dc7_784580_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/12.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>That looks great. We have our working FastAPI, and we also have our amazon instance. We can now move on to Deployment.</p><hr><h2 id=3-deployment-on-amazon-ec2>3. Deployment on Amazon ec2</h2><p>Till now, we have created an AWS instance and, we have also created a FastAPI that takes as input a base64 string representation of an image and returns bounding boxes and the associated class. But all the FastAPI code still resides in our local machine. <em><strong>How do we put it on the ec2 server? And run predictions on the cloud.</strong></em></p><h3 id=a-install-docker>A. Install Docker</h3><p>We will deploy our app using docker, as is suggested by the fastAPI creator himself. I will try to explain how docker works as we go. The below part may look daunting but it just is a series of commands and steps. So stay with me.</p><p>We can start by installing docker using:</p><pre><code>sudo apt-get update
sudo apt install docker.io
</code></pre><p>We then start the docker service using:</p><pre><code>sudo service docker start
</code></pre><h3 id=b-creating-the-folder-structure-for-docker>B. Creating the folder structure for docker</h3><pre><code>└── dockerfastapi
    ├── Dockerfile
    ├── app
    │   └── main.py
    └── requirements.txt
</code></pre><p>Here dockerfastapi is our project’s main folder. And here are the different files in this folder:</p><p><strong>i. requirements.txt:</strong> Docker needs a file, which tells it which all libraries are required for our app to run. Here I have listed all the libraries I used in my Fastapi API.</p><pre><code>numpy
opencv-python
matplotlib
torchvision
torch
fastapi
pydantic
</code></pre><p><strong>ii. Dockerfile:</strong> The second file is Dockerfile.</p><pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7

COPY ./app /app
COPY requirements.txt .
RUN pip --no-cache-dir install -r requirements.txt
</code></pre><p><em><strong>How Docker works?:</strong></em> You can skip this section, but it will help to get some understanding of how docker works.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/13_hu1d751414e9229ccc17cef9fa83236c3b_17310_500x0_resize_box_2.png 500w" src=/images/deployment_fastapi/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>The dockerfile can be thought of something like a sh file,which contains commands to create a docker image that can be run in a container. One can think of a docker image as an environment where everything like Python and Python libraries is installed. A container is a unit which is just an isolated box in our system that uses a dockerimage. The advantage of using docker is that we can create multiple docker images and use them in multiple containers. For example, one image might contain python36, and another can contain python37. And we can spawn multiple containers in a single Linux server.</p><p>Our Dockerfile contains a few things:</p><ul><li><p>FROM command: Here the first line FROM specifies that we start with tiangolo’s (FastAPI creator) Docker image. As per his site: “<em>This image has an “auto-tuning” mechanism included so that you can just add your code and get that same high performance automatically. And without making sacrifices”</em>. What we are doing is just starting from an image that installs python3.7 for us along with some added configurations for uvicorn and gunicorn ASGI servers and a start.sh file for ASGI servers automatically. For adventurous souls, particularly
<a href=https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker/blob/master/docker-images/python3.7.dockerfile target=_blank rel="nofollow noopener">commandset</a>
1 and
<a href=https://github.com/tiangolo/uvicorn-gunicorn-docker/blob/master/docker-images/python3.7.dockerfile target=_blank rel="nofollow noopener">commandset2</a>
get executed through a sort of a daisy-chaining of commands.</p></li><li><p>COPY command: We can think of a docker image also as a folder that contains files and such. Here we copy our app folder and the requirements.txt file, which we created earlier to our docker image.</p></li><li><p>RUN Command: We run pip install command to install all our python dependencies using the requirements.txt file that is now on the docker image.</p></li></ul><p><strong>iii. main.py:</strong> This file contains the fastapiapp.py code we created earlier. Remember to keep the name of the file main.py only.</p><h3 id=c-docker-build>C. Docker Build</h3><p>We have got all our files in the required structure, but we haven’t yet used any docker command. We will first need to build an image containing all dependencies using Dockerfile.</p><p>We can do this simply by:</p><pre><code>sudo docker build -t myimage .
</code></pre><p>This downloads, copies and installs some files and libraries from tiangolo’s image and creates an image called myimage. This myimage has python37 and some python packages as specified by requirements.txt file.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/14_hu8be31cfd0eea5c977fefe93aaedf6ec2_389614_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/14_hu8be31cfd0eea5c977fefe93aaedf6ec2_389614_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/14_hu8be31cfd0eea5c977fefe93aaedf6ec2_389614_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/14_hu8be31cfd0eea5c977fefe93aaedf6ec2_389614_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/14.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We will then just need to start a container that runs this image. We can do this using:</p><pre><code>sudo docker run -d --name mycontainer -p 80:80 myimage
</code></pre><p>This will create a container named mycontainer which runs our docker image myimage. The part 80:80 connects our docker container port 80 to our Linux machine port 80.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/15_hudc5094f1e0b3fbefab69d16f6f5e588e_22588_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/15_hudc5094f1e0b3fbefab69d16f6f5e588e_22588_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/15.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>And actually that’s it. At this point, you should be able to open the below URL in your browser.</p><pre><code># &lt;IPV4 public IP&gt;/docs
URL: 18.237.28.174/docs
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/16_huf284e2c6fafe320032bccfcceb831f32_39388_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/16_huf284e2c6fafe320032bccfcceb831f32_39388_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/16_huf284e2c6fafe320032bccfcceb831f32_39388_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/16_huf284e2c6fafe320032bccfcceb831f32_39388_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/16.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>And we can check our app programmatically using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>payload = json.dumps({
  <span style=color:#ed9d13>&#34;base64str&#34;</span>: base64str,
  <span style=color:#ed9d13>&#34;threshold&#34;</span>: <span style=color:#3677a9>0.5</span>
})

response = requests.put(<span style=color:#ed9d13>&#34;[http://18.237.28.174/predict](http://18.237.28.174/predict)&#34;</span>,data = payload)
data_dict = response.json()
<span style=color:#6ab825;font-weight:700>print</span>(data_dict)

</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/17_hu8e1fc3fd287f2b83f4bf028305a29147_53142_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/17_hu8e1fc3fd287f2b83f4bf028305a29147_53142_800x0_resize_box_2.png 800w" src=/images/deployment_fastapi/17.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><blockquote><h1 id=yup-finally-our-api-is-deployed>Yup, finally our API is deployed.</h1></blockquote><h3 id=d-troubleshooting-as-the-real-world-is-not-perfect>D. Troubleshooting as the real world is not perfect</h3><p>All the above was good and will just work out of the box if you follow the exact instructions, but the real world doesn’t work like that. You will surely get some errors along the way and would need to debug your code. So to help you with that, some docker commands may come handy:</p><ul><li><strong>Logs:</strong> When we ran our container using sudo docker run we don’t get a lot of info, and that is a big problem when you are debugging. You can see the real-time logs using the below command. If you see an error here, you will need to change your code and build the image again.</li></ul><pre><code>    sudo docker logs -f mycontainer
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/18_hu2172bb65045e520e8f27dedd06d2c7d8_227462_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/18_hu2172bb65045e520e8f27dedd06d2c7d8_227462_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/18_hu2172bb65045e520e8f27dedd06d2c7d8_227462_1200x0_resize_box_2.png 1200w
, /images/deployment_fastapi/18_hu2172bb65045e520e8f27dedd06d2c7d8_227462_1500x0_resize_box_2.png 1500w" src=/images/deployment_fastapi/18.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li><strong>Starting and Stopping Docker:</strong> Sometimes, it might help just to restart your docker. In that case, you can use:</li></ul><pre><code>    sudo service docker stop
    sudo service docker start
</code></pre><ul><li><strong>Listing images and containers:</strong> Working with docker, you will end up creating images and containers, but you won’t be able to see them in the working directory. You can list your images and containers using:</li></ul><pre><code>    sudo docker container ls
    sudo docker image ls
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/19_hu94ad30928ba9bc1a02c98076b0f6765f_47975_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/19_hu94ad30928ba9bc1a02c98076b0f6765f_47975_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/19_hu94ad30928ba9bc1a02c98076b0f6765f_47975_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/19.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li><strong>Deleting unused docker images or containers:</strong> You might need to remove some images or containers as these take up a lot of space on the system. Here is how you do that.</li></ul><pre><code>    # the prune command removes the unused containers and images
    sudo docker system prune

    # delete a particular container
    sudo docker rm mycontainer

    # remove myimage
    sudo docker image rm myimage

    # remove all images
    sudo docker image prune — all
</code></pre><ul><li>**Checking localhost:**The Linux server doesn’t have a browser, but we can still see the browser output though it’s a little ugly:</li></ul><pre><code>    curl localhost
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/20_hu4689ff010ed133c38c1b047c70790d14_90450_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/20_hu4689ff010ed133c38c1b047c70790d14_90450_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/20_hu4689ff010ed133c38c1b047c70790d14_90450_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/20.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li><strong>Develop without reloading image again and again:</strong> For development, it’s useful to be able just to change the contents of the code on our machine and test it live, without having to build the image every time. In that case, it’s also useful to run the server with live auto-reload automatically at every code change. Here, we use our app directory on our Linux machine, and we replace the default (/start.sh) with the development alternative /start-reload.sh during development. After everything looks fine, we can build our image again run it inside the container.</li></ul><pre><code>    sudo docker run -d -p 80:80 -v $(pwd):/app myimage /start-reload.sh
</code></pre><p>If this doesn’t seem sufficient, adding here a docker cheat sheet containing useful docker commands:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/21_hu20d6b4fbe2c0ad022ec2596b0e09489e_284236_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/21_hu20d6b4fbe2c0ad022ec2596b0e09489e_284236_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/21_hu20d6b4fbe2c0ad022ec2596b0e09489e_284236_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/21.png alt='

<a href="http://dockerlabs.collabnix.com/docker/cheatsheet/" target="_blank" rel="nofollow noopener">Source</a>
'></p><hr><h2 id=4-an-end-to-end-app-with-ui>4. An End to End App with UI</h2><p>We are done here with our API creation, but we can also create a UI based app using
<a href=https://towardsdatascience.com/how-to-write-web-apps-using-simple-python-for-data-scientists-a227a1a01582 target=_blank rel="nofollow noopener">Streamlit</a>
using our FastAPI API. This is not how you will do it in a production setting (where you might have developers making apps using react, node.js or javascript)but is mostly here to check the end-to-end flow of how to use an image API. I will host this barebones Streamlit app on local rather than the ec2 server, and it will get the bounding box info and classes from the FastAPI API hosted on ec2.</p><p>If you need to learn more about how streamlit works, you can check out this
<a href=https://towardsdatascience.com/how-to-write-web-apps-using-simple-python-for-data-scientists-a227a1a01582 target=_blank rel="nofollow noopener">post</a>
. Also, if you would want to deploy this streamlit app also to ec2, here is a
<a href=https://towardsdatascience.com/how-to-deploy-a-streamlit-app-using-an-amazon-free-ec2-instance-416a41f69dc3 target=_blank rel="nofollow noopener">tutorial</a>
again.</p><p>Here is the flow of the whole app with UI and FastAPI API on ec2:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/deployment_fastapi/22_huafd13a87192d9c5ac7a06cb3f501b3ea_92137_500x0_resize_box_2.png 500w
, /images/deployment_fastapi/22_huafd13a87192d9c5ac7a06cb3f501b3ea_92137_800x0_resize_box_2.png 800w
, /images/deployment_fastapi/22_huafd13a87192d9c5ac7a06cb3f501b3ea_92137_1200x0_resize_box_2.png 1200w" src=/images/deployment_fastapi/22.png alt="Project Architecture">
<em>Project Architecture</em></p><p>The most important problems we need to solve in our streamlit app are:</p><h3 id=how-to-get-an-image-file-from-the-user-using-streamlit>How to get an image file from the user using Streamlit?</h3><p><strong>A. Using File uploader:</strong> We can use the file uploader using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>bytesObj = st.file_uploader(<span style=color:#a61717;background-color:#e3d2d2>“</span>Choose an image <span style=color:#24909d>file</span><span style=color:#a61717;background-color:#e3d2d2>”</span>)
</code></pre></div><p>The next problem is, what is this bytesObj we get from the streamlit file uploader? In streamlit, we will get a bytesIO object from the file_uploader and we will need to convert it to base64str for our FastAPI app input. This can be done using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>bytesioObj_to_base64str</span>(bytesObj):
   <span style=color:#6ab825;font-weight:700>return</span> base64.b64encode(bytesObj.read()).decode(<span style=color:#ed9d13>&#34;utf-8&#34;</span>)

base64str = bytesioObj_to_base64str(bytesObj)
</code></pre></div><p><strong>B. Using URL:</strong> We can also get an image URL from the user using text_input.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>url = st.text_input(<span style=color:#a61717;background-color:#e3d2d2>‘</span>Enter URL<span style=color:#a61717;background-color:#e3d2d2>’</span>)
</code></pre></div><p>We can then get image from URL in base64 string format using the requests module and base64 encode and utf-8 decode:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>ImgURL_to_base64str</span>(url):
    <span style=color:#6ab825;font-weight:700>return</span> base64.b64encode(requests.get(url).content).decode(<span style=color:#ed9d13>&#34;utf-8&#34;</span>)

base64str = ImgURL_to_base64str(url)
</code></pre></div><p>And here is the complete code of our Streamlit app. You have seen most of the code in this post already.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>streamlit</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>st</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>base64</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>io</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>requests</span>,<span style=color:#447fcf;text-decoration:underline>json</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>PIL</span> <span style=color:#6ab825;font-weight:700>import</span> Image
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>cv2</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>numpy</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>np</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>matplotlib.pyplot</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>plt</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>requests</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>random</span>

<span style=color:#999;font-style:italic># use file uploader object to recieve image</span>
<span style=color:#999;font-style:italic># Remember that this bytes object can be used only once</span>
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>bytesioObj_to_base64str</span>(bytesObj):
    <span style=color:#6ab825;font-weight:700>return</span> base64.b64encode(bytesObj.read()).decode(<span style=color:#ed9d13>&#34;utf-8&#34;</span>)

<span style=color:#999;font-style:italic># Image conversion functions</span>

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>base64str_to_PILImage</span>(base64str):
    base64_img_bytes = base64str.encode(<span style=color:#ed9d13>&#39;utf-8&#39;</span>)
    base64bytes = base64.b64decode(base64_img_bytes)
    bytesObj = io.BytesIO(base64bytes)
    img = Image.open(bytesObj)
    <span style=color:#6ab825;font-weight:700>return</span> img

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>PILImage_to_cv2</span>(img):
    <span style=color:#6ab825;font-weight:700>return</span> np.asarray(img)

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>ImgURL_to_base64str</span>(url):
    <span style=color:#6ab825;font-weight:700>return</span> base64.b64encode(requests.get(url).content).decode(<span style=color:#ed9d13>&#34;utf-8&#34;</span>)

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>drawboundingbox</span>(img, boxes,pred_cls, rect_th=<span style=color:#3677a9>2</span>, text_size=<span style=color:#3677a9>1</span>, text_th=<span style=color:#3677a9>2</span>):
    img = PILImage_to_cv2(img)
    class_color_dict = {}

    <span style=color:#999;font-style:italic>#initialize some random colors for each class for better looking bounding boxes</span>
    <span style=color:#6ab825;font-weight:700>for</span> cat <span style=color:#6ab825;font-weight:700>in</span> pred_cls:
        class_color_dict[cat] = [random.randint(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>255</span>) <span style=color:#6ab825;font-weight:700>for</span> _ <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#3677a9>3</span>)]

    <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(<span style=color:#24909d>len</span>(boxes)):
        cv2.rectangle(img, (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>0</span>]), <span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>1</span>])),
                      (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>1</span>][<span style=color:#3677a9>0</span>]),<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>1</span>][<span style=color:#3677a9>1</span>])),
                      color=class_color_dict[pred_cls[i]], thickness=rect_th)
        cv2.putText(img,pred_cls[i], (<span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>0</span>]), <span style=color:#24909d>int</span>(boxes[i][<span style=color:#3677a9>0</span>][<span style=color:#3677a9>1</span>])),  cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th)
    plt.figure(figsize=(<span style=color:#3677a9>20</span>,<span style=color:#3677a9>30</span>))
    plt.imshow(img)
    plt.xticks([])
    plt.yticks([])
    plt.show()

st.markdown(<span style=color:#ed9d13>&#34;&lt;h1&gt;Our Object Detector App using FastAPI&lt;/h1&gt;&lt;br&gt;&#34;</span>, unsafe_allow_html=True)

bytesObj = st.file_uploader(<span style=color:#ed9d13>&#34;Choose an image file&#34;</span>)

st.markdown(<span style=color:#ed9d13>&#34;&lt;center&gt;&lt;h2&gt;or&lt;/h2&gt;&lt;/center&gt;&#34;</span>, unsafe_allow_html=True)

url = st.text_input(<span style=color:#ed9d13>&#39;Enter URL&#39;</span>)

<span style=color:#6ab825;font-weight:700>if</span> bytesObj <span style=color:#6ab825;font-weight:700>or</span> url:
    <span style=color:#999;font-style:italic># In streamlit we will get a bytesIO object from the file_uploader</span>
    <span style=color:#999;font-style:italic># and we convert it to base64str for our FastAPI</span>
    <span style=color:#6ab825;font-weight:700>if</span> bytesObj:
        base64str = bytesioObj_to_base64str(bytesObj)

    <span style=color:#6ab825;font-weight:700>elif</span> url:
        base64str = ImgURL_to_base64str(url)

    <span style=color:#999;font-style:italic># We will also create the image in PIL Image format using this base64 str</span>
    <span style=color:#999;font-style:italic># Will use this image to show in matplotlib in streamlit</span>
    img = base64str_to_PILImage(base64str)

    <span style=color:#999;font-style:italic># Run FastAPI</span>
    payload = json.dumps({
      <span style=color:#ed9d13>&#34;base64str&#34;</span>: base64str,
      <span style=color:#ed9d13>&#34;threshold&#34;</span>: <span style=color:#3677a9>0.5</span>
    })

    response = requests.put(<span style=color:#ed9d13>&#34;http://18.237.28.174/predict&#34;</span>,data = payload)
    data_dict = response.json()


    st.markdown(<span style=color:#ed9d13>&#34;&lt;center&gt;&lt;h1&gt;App Result&lt;/h1&gt;&lt;/center&gt;&#34;</span>, unsafe_allow_html=True)
    drawboundingbox(img, data_dict[<span style=color:#ed9d13>&#39;boxes&#39;</span>], data_dict[<span style=color:#ed9d13>&#39;classes&#39;</span>])
    st.pyplot()
    st.markdown(<span style=color:#ed9d13>&#34;&lt;center&gt;&lt;h1&gt;FastAPI Response&lt;/h1&gt;&lt;/center&gt;&lt;br&gt;&#34;</span>, unsafe_allow_html=True)
    st.write(data_dict)
</code></pre></div><p>We can run this streamlit app in local using:</p><pre><code>streamlit run streamlitapp.py
</code></pre><p>And we can see our app running on our localhost:8501. Works well with user-uploaded images as well as URL based images. Here is a cat image for some of you cat enthusiasts as well.</p><table><tr><td><img src=/images/deployment_fastapi/23.png></td><td><img src=/images/deployment_fastapi/24.png></td></tr></table><p>So that’s it. We have created a whole workflow here to deploy image detection models through FastAPI on ec2 and utilizing those results in Streamlit. I hope this helps your woes around deploying models in production. You can find the code for this post as well as all my posts at my
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/deployFastApi target=_blank rel="nofollow noopener">GitHub</a>
repository.</p><p>Let me know if you like this post and if you would like to include Docker or FastAPI or Streamlit in your day to day deployment needs. I am also looking to create a much detailed post on Docker so follow me up to stay tuned with my writing as well. Details below.</p><hr><h2 id=continue-learning>Continue Learning</h2><p>If you want to learn more about building and putting a Machine Learning model in production, this
<a href=https://coursera.pxf.io/e45BJ6 target=_blank rel="nofollow noopener">course on AWS</a>
for implementing Machine Learning applications promises just that.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href="https://mlwhiz.medium.com/?source=post_page---------------------------" target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a></p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in London, where I am currently working with Facebook .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/bash>Bash</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/machine-learning>Machine Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/opinion>Opinion</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>