<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Understanding Transformers, the Programming Way - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Understanding Transformers, the Programming Way - MLWhiz"><meta property="og:description" content="Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/"><meta property="og:image" content="https://mlwhiz.com/images/create-transformer-from-scratch/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/create-transformer-from-scratch/main.png"><meta property="article:published_time" content="2020-10-10T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T16:34:38+01:00"><meta property="article:tag" content="Programming"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/create-transformer-from-scratch/main.png"><meta name=twitter:title content="Understanding Transformers, the Programming Way - MLWhiz"><meta name=twitter:description content="Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/#primaryimage","url":"https://mlwhiz.com/images/create-transformer-from-scratch/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/#webpage","url":"https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/","inLanguage":"en-US","name":"Understanding Transformers, the Programming Way - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/#primaryimage"},"datePublished":"2020-10-10T00:00:00.00Z","dateModified":"2023-07-07T16:34:38.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/programming class=categoryStyle>Programming</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Understanding Transformers, the Programming Way</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>10 October 2020</span></div><img src=https://mlwhiz.com/images/create-transformer-from-scratch/main.png class="img-fluid w-100 mb-4" alt="Understanding Transformers, the Programming Way"><div class="content mb-5"><p>Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well. I am sure you would all have heard about the GPT3 Transformer or the jokes thereof.</p><p><em><strong>But everything aside, they are still hard to understand as ever.</strong></em> In my
<a href=https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076 target=_blank rel="nofollow noopener">last post</a>
, I talked in quite a detail about transformers and how they work on a basic level. I went through the encoder and decoder architecture and the whole data flow in those different pieces of the neural network.</p><p>But as I like to say we don’t really understand something before we implement it ourselves. So in this post, we will implement an English to German language translator using Transformers.</p><hr><h2 id=task-description>Task Description</h2><p>We want to create a translator that uses transformers to convert English to German. So, if we look at it as a black-box, our network takes as input an English sentence and returns a German sentence.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/0_huf7637c0a5b79ffe2d019977754e4b30a_26347_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/0_huf7637c0a5b79ffe2d019977754e4b30a_26347_800x0_resize_box_2.png 800w
, /images/create-transformer-from-scratch/0_huf7637c0a5b79ffe2d019977754e4b30a_26347_1200x0_resize_box_2.png 1200w
, /images/create-transformer-from-scratch/0_huf7637c0a5b79ffe2d019977754e4b30a_26347_1500x0_resize_box_2.png 1500w" src=/images/create-transformer-from-scratch/0.png alt="Transformer for Translation"><figcaption>Transformer for Translation</figcaption></figure></p><hr><h2 id=data-preprocessing>Data Preprocessing</h2><p>To train our English-German translation Model, we will need translated sentence pairs between English and German.</p><p>Fortunately, there is pretty much a standard way to get these with the IWSLT(International Workshop on Spoken Language Translation) dataset which we can access using <code>torchtext.datasets</code>. This machine translation dataset is sort of the defacto standard used for translation tasks and contains the translation of TED and TEDx talks on various topics in different languages.</p><p>Also, before we really get into the whole coding part, let us understand what we need as input and output to the model while training. We will actually need two matrices to be input to our Network:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/1_hufe1611d4c6eb8e5ab29b33ec96ea9b34_58843_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/1_hufe1611d4c6eb8e5ab29b33ec96ea9b34_58843_800x0_resize_box_2.png 800w
, /images/create-transformer-from-scratch/1_hufe1611d4c6eb8e5ab29b33ec96ea9b34_58843_1200x0_resize_box_2.png 1200w
, /images/create-transformer-from-scratch/1_hufe1611d4c6eb8e5ab29b33ec96ea9b34_58843_1500x0_resize_box_2.png 1500w" src=/images/create-transformer-from-scratch/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li><p><strong>The Source English sentences(Source):</strong> A matrix of shape (batch size x source sentence length). The numbers in this matrix correspond to words based on the English vocabulary we will also need to create. So for example, 234 in the English vocabulary might correspond to the word “the”. Also, do you notice that a lot of sentences end with a word whose index in vocabulary is 6? What is that about? Since all sentences don’t have the same length, they are padded with a word whose index is 6. So, 6 refers to <code>&lt;blank></code> token.</p></li><li><p><strong>The Shifted Target German sentences(Target):</strong> A matrix of shape (batch size x target sentence length). Here also the numbers in this matrix correspond to words based on the German vocabulary we will also need to create. If you notice that there seems to be a pattern to this particular matrix. All sentences start with a word whose index in german vocabulary is 2 and they invariably end with a pattern [3 and 0 or more 1&rsquo;s]. This is intentional as we want to start the target sentence with some start token(so 2 is for <code>&lt;s></code> token) and end the target sentence with some end token(so 3 is <code>&lt;/s></code> token) and a string of blank tokens(so 1 refers to <code>&lt;blank></code> token). This part is covered more in detail in my last post on transformers, so if you are feeling confused here, I would ask you to take a look at that</p></li></ul><p>So now as we know how to preprocess our data we will get into the actual code for preprocessing steps.</p><p><em>Please note, that it really doesn’t matter here if you preprocess using other methods too. What eventually matters is that in the end, you need to send the sentence source and targets to your model in a way that&rsquo;s intended to be used by the transformer. i.e. source sentences should be padded with blank token and target sentences need to have a start token, an end token, and rest padded by blank tokens.</em></p><p>We start by loading the Spacy Models which provides tokenizers to tokenize German and English text.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Load the Spacy Models</span>
spacy_de = spacy.load(<span style=color:#ed9d13>&#39;de&#39;</span>)
spacy_en = spacy.load(<span style=color:#ed9d13>&#39;en&#39;</span>)

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>tokenize_de</span>(text):
    <span style=color:#6ab825;font-weight:700>return</span> [tok.text <span style=color:#6ab825;font-weight:700>for</span> tok <span style=color:#6ab825;font-weight:700>in</span> spacy_de.tokenizer(text)]

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>tokenize_en</span>(text):
    <span style=color:#6ab825;font-weight:700>return</span> [tok.text <span style=color:#6ab825;font-weight:700>for</span> tok <span style=color:#6ab825;font-weight:700>in</span> spacy_en.tokenizer(text)]
</code></pre></div><p>We also define some special tokens we will use for specifying blank/padding words, and beginning and end of sentences as discussed above.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Special Tokens</span>
BOS_WORD = <span style=color:#ed9d13>&#39;&lt;s&gt;&#39;</span>
EOS_WORD = <span style=color:#ed9d13>&#39;&lt;/s&gt;&#39;</span>
BLANK_WORD = <span style=color:#ed9d13>&#34;&lt;blank&gt;&#34;</span>
</code></pre></div><p>We can now define a preprocessing pipeline for both our source and target sentences using <code>data.field</code> from torchtext. You can notice that while we only specify pad_token for source sentence, we mention <code>pad_token</code>, <code>init_token</code> and <code>eos_token</code> for the target sentence. We also define which tokenizers to use.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>SRC = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)
TGT = data.Field(tokenize=tokenize_de, init_token = BOS_WORD,
                 eos_token = EOS_WORD, pad_token=BLANK_WORD)
</code></pre></div><p>If you notice till now we haven’t seen any data. We now use IWSLT data from <code>torchtext.datasets</code> to create a train, validation, and test dataset. We also filter our sentences using the <code>MAX_LEN</code> parameter so that our code runs a lot faster. Notice that we are getting the data with <code>.en</code> and <code>.de</code> extensions. and we specify the preprocessing steps using the <code>fields</code> parameter.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>MAX_LEN = <span style=color:#3677a9>20</span>
train, val, test = datasets.IWSLT.splits(
    exts=(<span style=color:#ed9d13>&#39;.en&#39;</span>, <span style=color:#ed9d13>&#39;.de&#39;</span>), fields=(SRC, TGT),
    filter_pred=<span style=color:#6ab825;font-weight:700>lambda</span> x: <span style=color:#24909d>len</span>(<span style=color:#24909d>vars</span>(x)[<span style=color:#ed9d13>&#39;src&#39;</span>]) &lt;= MAX_LEN
    <span style=color:#6ab825;font-weight:700>and</span> <span style=color:#24909d>len</span>(<span style=color:#24909d>vars</span>(x)[<span style=color:#ed9d13>&#39;trg&#39;</span>]) &lt;= MAX_LEN)
</code></pre></div><p>So now since we have got our train data, let&rsquo;s see how it looks like:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>for</span> i, example <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>([(x.src,x.trg) <span style=color:#6ab825;font-weight:700>for</span> x <span style=color:#6ab825;font-weight:700>in</span> train[<span style=color:#3677a9>0</span>:<span style=color:#3677a9>5</span>]]):
    <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Example_{i}:{example}&#34;</span>)
</code></pre></div><pre><code>Example_0:(['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', &quot;'m&quot;, 'Dave', 'Gallo', '.'], ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'])

Example_1:(['And', 'we', &quot;'re&quot;, 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.'], ['Wir', 'werden', 'Ihnen', 'einige', 'Geschichten', 'über', 'das', 'Meer', 'in', 'Videoform', 'erzählen', '.'])

Example_2:(['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.'], ['Ich', 'denke', ',', 'das', 'Problem', 'ist', ',', 'dass', 'wir', 'das', 'Meer', 'für', 'zu', 'selbstverständlich', 'halten', '.'])

Example_3:(['When', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.'], ['Wenn', 'man', 'darüber', 'nachdenkt', ',', 'machen', 'die', 'Ozeane', '75', '%', 'des', 'Planeten', 'aus', '.'])

Example_4:(['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.'], ['Der', 'Großteil', 'der', 'Erde', 'ist', 'Meerwasser', '.'])
</code></pre><p>You might notice that while the <code>data.field</code> object has done the tokenization, it has not yet applied the start, end, and pad tokens and that is intentional. This is because we don’t have batches yet and the number of pad tokens will inherently depend on the maximum length of a sentence in the particular batch.</p><p>As mentioned in the start, we also create a Source and Target Language vocabulary by using the built-in function in <code>data.field</code> object. We specify a MIN_FREQ of 2 so that any word that doesn’t occur at least twice doesn’t get to be a part of our vocabulary.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>MIN_FREQ = <span style=color:#3677a9>2</span>
SRC.build_vocab(train.src, min_freq=MIN_FREQ)
TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
</code></pre></div><p>Once we are done with this, we can simply use <code>data.Bucketiterator</code> which is used to giver batches of similar lengths to get our train iterator and validation iterator. Note that we use a <code>batch_size</code> of 1 for our validation data. It is optional to do this but done so that we don’t do padding or do minimal padding while checking validation data performance.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>BATCH_SIZE = <span style=color:#3677a9>350</span>

<span style=color:#999;font-style:italic># Create iterators to process text in batches of approx. the same length by sorting on sentence lengths</span>
train_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, repeat=False, sort_key=<span style=color:#6ab825;font-weight:700>lambda</span> x: <span style=color:#24909d>len</span>(x.src))
val_iter = data.BucketIterator(val, batch_size=<span style=color:#3677a9>1</span>, repeat=False, sort_key=<span style=color:#6ab825;font-weight:700>lambda</span> x: <span style=color:#24909d>len</span>(x.src))
</code></pre></div><p>Before we proceed, it is always a good idea to see how our batch looks like and what we are sending to the model as an input while training.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>batch = <span style=color:#24909d>next</span>(<span style=color:#24909d>iter</span>(train_iter))
src_matrix = batch.src.T
<span style=color:#6ab825;font-weight:700>print</span>(src_matrix, src_matrix.size())
</code></pre></div><p>This is our source matrix:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/2_hu6922030370d30552bed898d412d029e8_18162_500x0_resize_box_2.png 500w" src=/images/create-transformer-from-scratch/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>trg_matrix = batch.trg.T
<span style=color:#6ab825;font-weight:700>print</span>(trg_matrix, trg_matrix.size())
</code></pre></div><p>And here is our target matrix:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/3_hu966ffb068d65c21f7678a91faa0ac07e_17171_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/3_hu966ffb068d65c21f7678a91faa0ac07e_17171_800x0_resize_box_2.png 800w" src=/images/create-transformer-from-scratch/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>So in the first batch, the <code>src_matrix</code> contains 350 sentences of length 20 and the <code>trg_matrix</code> is 350 sentences of length 22. Just so we are sure of our preprocessing, let’s see what some of these numbers represent in the <code>src_matrix</code> and the <code>trg_matrix</code>.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>print</span>(SRC.vocab.itos[<span style=color:#3677a9>1</span>])
<span style=color:#6ab825;font-weight:700>print</span>(TGT.vocab.itos[<span style=color:#3677a9>2</span>])
<span style=color:#6ab825;font-weight:700>print</span>(TGT.vocab.itos[<span style=color:#3677a9>1</span>])
</code></pre></div><pre><code>&lt;blank&gt;
&lt;s&gt;
&lt;blank&gt;
</code></pre><p>Just as expected. The opposite method, i.e. string to index also works well.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>print</span>(TGT.vocab.stoi[<span style=color:#ed9d13>&#39;&lt;/s&gt;&#39;</span>])
</code></pre></div><pre><code>3
</code></pre><hr><h2 id=the-transformer>The Transformer</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/4_hue6d9406efb695630b40c04e233fabe9e_155453_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/4_hue6d9406efb695630b40c04e233fabe9e_155453_800x0_resize_box_2.png 800w
, /images/create-transformer-from-scratch/4_hue6d9406efb695630b40c04e233fabe9e_155453_1200x0_resize_box_2.png 1200w
, /images/create-transformer-from-scratch/4_hue6d9406efb695630b40c04e233fabe9e_155453_1500x0_resize_box_2.png 1500w" src=/images/create-transformer-from-scratch/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>So, now that we have a way to send the source sentence and the shifted target to our transformer, we can look at creating the Transformer.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>PositionalEncoding</span>(nn.Module):
    <span style=color:#6ab825;font-weight:700>def</span> __init__(self, d_model, dropout=<span style=color:#3677a9>0.1</span>, max_len=<span style=color:#3677a9>5000</span>):
        <span style=color:#24909d>super</span>(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = d_model
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(<span style=color:#3677a9>0</span>, max_len, dtype=torch.float).unsqueeze(<span style=color:#3677a9>1</span>)
        div_term = torch.exp(torch.arange(<span style=color:#3677a9>0</span>, d_model, <span style=color:#3677a9>2</span>).float() * (-math.log(<span style=color:#3677a9>10000.0</span>) / d_model))
        pe[:, <span style=color:#3677a9>0</span>::<span style=color:#3677a9>2</span>] = torch.sin(position * div_term)
        pe[:, <span style=color:#3677a9>1</span>::<span style=color:#3677a9>2</span>] = torch.cos(position * div_term)
        pe = pe.unsqueeze(<span style=color:#3677a9>0</span>).transpose(<span style=color:#3677a9>0</span>, <span style=color:#3677a9>1</span>)
        self.register_buffer(<span style=color:#ed9d13>&#39;pe&#39;</span>, pe)

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>forward</span>(self, x):
        x = x * math.sqrt(self.d_model)
        x = x + self.pe[:x.size(<span style=color:#3677a9>0</span>), :]
        <span style=color:#6ab825;font-weight:700>return</span> self.dropout(x)


<span style=color:#6ab825;font-weight:700>class</span> <span style=color:#447fcf;text-decoration:underline>MyTransformer</span>(nn.Module):
    <span style=color:#6ab825;font-weight:700>def</span> __init__(self, d_model: <span style=color:#24909d>int</span> = <span style=color:#3677a9>512</span>, nhead: <span style=color:#24909d>int</span> = <span style=color:#3677a9>8</span>, num_encoder_layers: <span style=color:#24909d>int</span> = <span style=color:#3677a9>6</span>,
                 num_decoder_layers: <span style=color:#24909d>int</span> = <span style=color:#3677a9>6</span>, dim_feedforward: <span style=color:#24909d>int</span> = <span style=color:#3677a9>2048</span>, dropout: <span style=color:#24909d>float</span> = <span style=color:#3677a9>0.1</span>,
                 activation: <span style=color:#24909d>str</span> = <span style=color:#ed9d13>&#34;relu&#34;</span>,source_vocab_length: <span style=color:#24909d>int</span> = <span style=color:#3677a9>60000</span>,target_vocab_length: <span style=color:#24909d>int</span> = <span style=color:#3677a9>60000</span>) -&gt; None:
        <span style=color:#24909d>super</span>(MyTransformer, self).__init__()
        self.source_embedding = nn.Embedding(source_vocab_length, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        encoder_norm = nn.LayerNorm(d_model)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
        self.target_embedding = nn.Embedding(target_vocab_length, d_model)
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)
        self.out = nn.Linear(<span style=color:#3677a9>512</span>, target_vocab_length)
        self._reset_parameters()
        self.d_model = d_model
        self.nhead = nhead

    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>forward</span>(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -&gt; Tensor:
        <span style=color:#6ab825;font-weight:700>if</span> src.size(<span style=color:#3677a9>1</span>) != tgt.size(<span style=color:#3677a9>1</span>):
            <span style=color:#6ab825;font-weight:700>raise</span> <span style=color:#bbb>RuntimeError</span>(<span style=color:#ed9d13>&#34;the batch number of src and tgt must be equal&#34;</span>)
        src = self.source_embedding(src)
        src = self.pos_encoder(src)
        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)
        tgt = self.target_embedding(tgt)
        tgt = self.pos_encoder(tgt)
        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
                              tgt_key_padding_mask=tgt_key_padding_mask,
                              memory_key_padding_mask=memory_key_padding_mask)
        output = self.out(output)
        <span style=color:#6ab825;font-weight:700>return</span> output


    <span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>_reset_parameters</span>(self):
        <span style=color:#ed9d13>r</span><span style=color:#ed9d13>&#34;&#34;&#34;Initiate parameters in the transformer model.&#34;&#34;&#34;</span>
        <span style=color:#6ab825;font-weight:700>for</span> p <span style=color:#6ab825;font-weight:700>in</span> self.parameters():
            <span style=color:#6ab825;font-weight:700>if</span> p.dim() &gt; <span style=color:#3677a9>1</span>:
                xavier_uniform_(p)
</code></pre></div><p>A lot of the blocks here are taken from Pytorch <code>nn</code> module. Infact, Pytorch has a
<a href=https://pytorch.org/docs/master/generated/torch.nn.Transformer.html target=_blank rel="nofollow noopener">Transformer</a>
module too but it doesn’t include a lot of functionalities present in the paper like the embedding layer, and the PositionalEncoding layer. So this is sort of a more complete implementation that takes in a lot from pytorch implementation as well.</p><p>We create our Transformer particularly using these various blocks from Pytorch nn module:</p><ul><li><p><a href=https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html target=_blank rel="nofollow noopener">TransformerEncoderLayer</a>
: A single encoder layer</p></li><li><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html target=_blank rel="nofollow noopener">TransformerEncoder</a>
: A stack of <code>num_encoder_layers</code> layers. In the paper, it is by default kept as 6.</p></li><li><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html target=_blank rel="nofollow noopener">TransformerDecoderLayer</a>
: A single decoder layer</p></li><li><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html target=_blank rel="nofollow noopener">TransformerDecoder</a>
: A stack of <code>num_decoder_layers</code> layers. In the paper it is by default kept as 6.</p></li></ul><p>Also, note that whatever is happening in the layers is actually just matrix functions as I mentioned in the explanation post for transformers. See in particular how the decoder stack takes memory from encoder as input. We also create a positional encoding layer which lets us add the positional embedding to our word embedding.</p><p>If you want, you can look at the source code of all these blocks also which I have already linked. I had to look many times into the source code myself to make sure that I was giving the right inputs to these layers.</p><hr><h2 id=define-optimizer-and-model>Define Optimizer and Model</h2><p>Now, we can initialize the transformer and the optimizer using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>source_vocab_length = <span style=color:#24909d>len</span>(SRC.vocab)
target_vocab_length = <span style=color:#24909d>len</span>(TGT.vocab)
model = MyTransformer(source_vocab_length=source_vocab_length,target_vocab_length=target_vocab_length)
optim = torch.optim.Adam(model.parameters(), lr=<span style=color:#3677a9>0.0001</span>, betas=(<span style=color:#3677a9>0.9</span>, <span style=color:#3677a9>0.98</span>), eps=<span style=color:#3677a9>1e-9</span>)
model = model.cuda()
</code></pre></div><p>In the paper, the authors used an Adam optimizer with a scheduled learning rate but here I just using a normal Adam optimizer to keep things simple.</p><hr><h2 id=training-our-translator>Training our Translator</h2><p>Now, we can train our transformer using the train function below. What we are necessarily doing in the training loop is:</p><ul><li><p>Getting the <code>src_matrix</code> and <code>trg_matrix</code> from a batch.</p></li><li><p>Creating a <code>src_mask</code> — This is the mask that tells the model about the padded words in src_matrix data.</p></li><li><p>Creating a <code>trg_mask</code> — So that our model is not able to look at the future subsequent target words at any point in time.</p></li><li><p>Getting the prediction from the model.</p></li><li><p>Calculating loss using cross-entropy. (In the paper they use KL divergence, but this also works fine for understanding)</p></li><li><p>Backprop.</p></li><li><p>We save the best model based on validation loss.</p></li><li><p>We also predict the model output at every epoch for some sentences of our choice as a debug step using the function <code>greedy_decode_sentence</code>. We will discuss this function in the results section.</p></li></ul><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>train</span>(train_iter, val_iter, model, optim, num_epochs,use_gpu=True):
    train_losses = []
    valid_losses = []
    <span style=color:#6ab825;font-weight:700>for</span> epoch <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(num_epochs):
        train_loss = <span style=color:#3677a9>0</span>
        valid_loss = <span style=color:#3677a9>0</span>
        <span style=color:#999;font-style:italic># Train model</span>
        model.train()
        <span style=color:#6ab825;font-weight:700>for</span> i, batch <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(train_iter):
            src = batch.src.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> batch.src
            trg = batch.trg.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> batch.trg
            <span style=color:#999;font-style:italic>#change to shape (bs , max_seq_len)</span>
            src = src.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
            <span style=color:#999;font-style:italic>#change to shape (bs , max_seq_len+1) , Since right shifted</span>
            trg = trg.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
            trg_input = trg[:, :-<span style=color:#3677a9>1</span>]
            targets = trg[:, <span style=color:#3677a9>1</span>:].contiguous().view(-<span style=color:#3677a9>1</span>)
            src_mask = (src != <span style=color:#3677a9>0</span>)
            src_mask = src_mask.float().masked_fill(src_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(src_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
            src_mask = src_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> src_mask
            trg_mask = (trg_input != <span style=color:#3677a9>0</span>)
            trg_mask = trg_mask.float().masked_fill(trg_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(trg_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
            trg_mask = trg_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> trg_mask
            size = trg_input.size(<span style=color:#3677a9>1</span>)
            <span style=color:#999;font-style:italic>#print(size)</span>
            np_mask = torch.triu(torch.ones(size, size)==<span style=color:#3677a9>1</span>).transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
            np_mask = np_mask.float().masked_fill(np_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(np_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
            np_mask = np_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> np_mask
            <span style=color:#999;font-style:italic># Forward, backprop, optimizer</span>
            optim.zero_grad()
            preds = model(src.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>), trg_input.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>), tgt_mask = np_mask)<span style=color:#999;font-style:italic>#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)</span>
            preds = preds.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>).contiguous().view(-<span style=color:#3677a9>1</span>, preds.size(-<span style=color:#3677a9>1</span>))
            loss = F.cross_entropy(preds,targets, ignore_index=<span style=color:#3677a9>0</span>,reduction=<span style=color:#ed9d13>&#39;sum&#39;</span>)
            loss.backward()
            optim.step()
            train_loss += loss.item()/BATCH_SIZE

        model.eval()
        <span style=color:#6ab825;font-weight:700>with</span> torch.no_grad():
            <span style=color:#6ab825;font-weight:700>for</span> i, batch <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(val_iter):
                src = batch.src.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> batch.src
                trg = batch.trg.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> batch.trg
                <span style=color:#999;font-style:italic>#change to shape (bs , max_seq_len)</span>
                src = src.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
                <span style=color:#999;font-style:italic>#change to shape (bs , max_seq_len+1) , Since right shifted</span>
                trg = trg.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
                trg_input = trg[:, :-<span style=color:#3677a9>1</span>]
                targets = trg[:, <span style=color:#3677a9>1</span>:].contiguous().view(-<span style=color:#3677a9>1</span>)
                src_mask = (src != <span style=color:#3677a9>0</span>)
                src_mask = src_mask.float().masked_fill(src_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(src_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
                src_mask = src_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> src_mask
                trg_mask = (trg_input != <span style=color:#3677a9>0</span>)
                trg_mask = trg_mask.float().masked_fill(trg_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(trg_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
                trg_mask = trg_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> trg_mask
                size = trg_input.size(<span style=color:#3677a9>1</span>)
                <span style=color:#999;font-style:italic>#print(size)</span>
                np_mask = torch.triu(torch.ones(size, size)==<span style=color:#3677a9>1</span>).transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
                np_mask = np_mask.float().masked_fill(np_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(np_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
                np_mask = np_mask.cuda() <span style=color:#6ab825;font-weight:700>if</span> use_gpu <span style=color:#6ab825;font-weight:700>else</span> np_mask

                preds = model(src.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>), trg_input.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>), tgt_mask = np_mask)<span style=color:#999;font-style:italic>#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)</span>
                preds = preds.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>).contiguous().view(-<span style=color:#3677a9>1</span>, preds.size(-<span style=color:#3677a9>1</span>))
                loss = F.cross_entropy(preds,targets, ignore_index=<span style=color:#3677a9>0</span>,reduction=<span style=color:#ed9d13>&#39;sum&#39;</span>)
                valid_loss += loss.item()/<span style=color:#3677a9>1</span>

        <span style=color:#999;font-style:italic># Log after each epoch</span>
        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#39;&#39;&#39;Epoch [{epoch+1}/{num_epochs}] complete. Train Loss: {train_loss/len(train_iter):.3f}. Val Loss: {valid_loss/len(val_iter):.3f}&#39;&#39;&#39;</span>)

        <span style=color:#999;font-style:italic>#Save best model till now:</span>
        <span style=color:#6ab825;font-weight:700>if</span> valid_loss/<span style=color:#24909d>len</span>(val_iter)&lt;<span style=color:#24909d>min</span>(valid_losses,default=<span style=color:#3677a9>1e9</span>):
            <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;saving state dict&#34;</span>)
            torch.save(model.state_dict(), f<span style=color:#ed9d13>&#34;checkpoint_best_epoch.pt&#34;</span>)

        train_losses.append(train_loss/<span style=color:#24909d>len</span>(train_iter))
        valid_losses.append(valid_loss/<span style=color:#24909d>len</span>(val_iter))

        <span style=color:#999;font-style:italic># Check Example after each epoch:</span>
        sentences = [<span style=color:#ed9d13>&#34;This is an example to check how our model is performing.&#34;</span>]
        <span style=color:#6ab825;font-weight:700>for</span> sentence <span style=color:#6ab825;font-weight:700>in</span> sentences:
            <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Original Sentence: {sentence}&#34;</span>)
            <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Translated Sentence: {greeedy_decode_sentence(model,sentence)}&#34;</span>)
    <span style=color:#6ab825;font-weight:700>return</span> train_losses,valid_losses
</code></pre></div><p>We can now run our training using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>train_losses,valid_losses = train(train_iter, val_iter, model, optim, <span style=color:#3677a9>35</span>)
</code></pre></div><p>Below is the output of the training loop (shown only for some epochs):</p><pre><code>**Epoch [1/35] complete.** Train Loss: 86.092. Val Loss: 64.514
Original Sentence: This is an example to check how our model is performing.
Translated Sentence:  Und die der der der der der der der der der der der der der der der der der der der der der der der

**Epoch [2/35] complete.** Train Loss: 59.769. Val Loss: 55.631
Original Sentence: This is an example to check how our model is performing.
Translated Sentence:  Das ist ein paar paar paar sehr , die das ist ein paar sehr Jahre . &lt;/s&gt;

.
.
.
.

**Epoch [16/35] complete.** Train Loss: 21.791. Val Loss: 28.000
Original Sentence: This is an example to check how our model is performing.
Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell aussieht . Das ist ein Modell . &lt;/s&gt;

.
.
.
.

**Epoch [34/35] complete.** Train Loss: 9.492. Val Loss: 31.005
Original Sentence: This is an example to check how our model is performing.
Translated Sentence:  Hier ist ein Beispiel , um prüfen zu überprüfen , wie unser Modell ist . Wir spielen . &lt;/s&gt;

**Epoch [35/35] complete.** Train Loss: 9.014. Val Loss: 32.097
Original Sentence: This is an example to check how our model is performing.
Translated Sentence:  Hier ist ein Beispiel , um prüfen wie unser Modell ist . Wir spielen . &lt;/s&gt;
</code></pre><p>We can see how our model starts with a gibberish translation — “Und die der der der der der der der der der der der der der der der der der der der der der der der” and starts giving us something by the end of a few iterations.</p><hr><h2 id=results>Results</h2><p>We can plot the training and validation losses using Plotly express.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>pandas</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>pd</span>
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>plotly.express</span> <span style=color:#6ab825;font-weight:700>as</span> <span style=color:#447fcf;text-decoration:underline>px</span>
losses = pd.DataFrame({<span style=color:#ed9d13>&#39;train_loss&#39;</span>:train_losses,<span style=color:#ed9d13>&#39;val_loss&#39;</span>:valid_losses})
px.line(losses,y = [<span style=color:#ed9d13>&#39;train_loss&#39;</span>,<span style=color:#ed9d13>&#39;val_loss&#39;</span>])
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/5_hu7018e995a07e70e14d61780a626882a0_34312_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/5_hu7018e995a07e70e14d61780a626882a0_34312_800x0_resize_box_2.png 800w
, /images/create-transformer-from-scratch/5_hu7018e995a07e70e14d61780a626882a0_34312_1200x0_resize_box_2.png 1200w" src=/images/create-transformer-from-scratch/5.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>If we want to deploy this model we can load it simply using:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model.load_state_dict(torch.load(f<span style=color:#a61717;background-color:#e3d2d2>”</span>checkpoint_best_epoch.pt<span style=color:#a61717;background-color:#e3d2d2>”</span>))
</code></pre></div><p>and predict for any source sentence using the <code>greeedy_decode_sentence</code> function, which is:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>greeedy_decode_sentence</span>(model,sentence):
    model.eval()
    sentence = SRC.preprocess(sentence)
    indexed = []
    <span style=color:#6ab825;font-weight:700>for</span> tok <span style=color:#6ab825;font-weight:700>in</span> sentence:
        <span style=color:#6ab825;font-weight:700>if</span> SRC.vocab.stoi[tok] != <span style=color:#3677a9>0</span> :
            indexed.append(SRC.vocab.stoi[tok])
        <span style=color:#6ab825;font-weight:700>else</span>:
            indexed.append(<span style=color:#3677a9>0</span>)
    sentence = Variable(torch.LongTensor([indexed])).cuda()
    trg_init_tok = TGT.vocab.stoi[BOS_WORD]
    trg = torch.LongTensor([[trg_init_tok]]).cuda()
    translated_sentence = <span style=color:#ed9d13>&#34;&#34;</span>
    maxlen = <span style=color:#3677a9>25</span>
    <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>range</span>(maxlen):
        size = trg.size(<span style=color:#3677a9>0</span>)
        np_mask = torch.triu(torch.ones(size, size)==<span style=color:#3677a9>1</span>).transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>)
        np_mask = np_mask.float().masked_fill(np_mask == <span style=color:#3677a9>0</span>, <span style=color:#24909d>float</span>(<span style=color:#ed9d13>&#39;-inf&#39;</span>)).masked_fill(np_mask == <span style=color:#3677a9>1</span>, <span style=color:#24909d>float</span>(<span style=color:#3677a9>0.0</span>))
        np_mask = np_mask.cuda()
        pred = model(sentence.transpose(<span style=color:#3677a9>0</span>,<span style=color:#3677a9>1</span>), trg, tgt_mask = np_mask)
        add_word = TGT.vocab.itos[pred.argmax(dim=<span style=color:#3677a9>2</span>)[-<span style=color:#3677a9>1</span>]]
        translated_sentence+=<span style=color:#ed9d13>&#34; &#34;</span>+add_word
        <span style=color:#6ab825;font-weight:700>if</span> add_word==EOS_WORD:
            <span style=color:#6ab825;font-weight:700>break</span>
        trg = torch.cat((trg,torch.LongTensor([[pred.argmax(dim=<span style=color:#3677a9>2</span>)[-<span style=color:#3677a9>1</span>]]]).cuda()))
        <span style=color:#999;font-style:italic>#print(trg)</span>
    <span style=color:#6ab825;font-weight:700>return</span> translated_sentence
</code></pre></div><p><figure><img src=/images/create-transformer-from-scratch/6.gif alt="Predicting with a greedy search using the Transformer"><figcaption>Predicting with a greedy search using the Transformer</figcaption></figure></p><p>This function does piecewise predictions. The greedy search would start with:</p><ul><li><p>Passing the whole English sentence as encoder input and just the start token <code>&lt;s></code> as shifted output(input to the decoder) to the model and doing the forward pass.</p></li><li><p>The model will predict the next word — <code>der</code></p></li><li><p>Then, we pass the whole English sentence as encoder input and add the last predicted word to the shifted output(input to the decoder = <code>&lt;s> der</code>) and do the forward pass.</p></li><li><p>The model will predict the next word — <code>schnelle</code></p></li><li><p>Passing the whole English sentence as encoder input and <code>&lt;s> der schnelle</code> as shifted output(input to the decoder) to the model and doing the forward pass.</p></li><li><p>and so on, until the model predicts the end token <code>&lt;/s></code> or we generate some maximum number of tokens(something we can define) so the translation doesn’t run for an infinite duration in any case it breaks.</p></li></ul><p>Now we can translate any sentence using this:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>sentence = <span style=color:#ed9d13>&#34;Isn&#39;t Natural language processing just awesome? Please do let me know in the comments.&#34;</span>
<span style=color:#6ab825;font-weight:700>print</span>(greeedy_decode_sentence(model,sentence))
</code></pre></div><pre><code>Ist es nicht einfach toll ? Bitte lassen Sie mich gerne in den Kommentare kennen . &lt;/s&gt;
</code></pre><p>Since I don’t have a German Translator at hand, I will use the next best thing to see how our model is performing. Let us take help of google translate service to understand what this german sentence means.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/create-transformer-from-scratch/7_hudf2950df51e1b725b3f39fdcdafb9f7b_95786_500x0_resize_box_2.png 500w
, /images/create-transformer-from-scratch/7_hudf2950df51e1b725b3f39fdcdafb9f7b_95786_800x0_resize_box_2.png 800w
, /images/create-transformer-from-scratch/7_hudf2950df51e1b725b3f39fdcdafb9f7b_95786_1200x0_resize_box_2.png 1200w" src=/images/create-transformer-from-scratch/7.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>There seem to be some mistakes in the translation as “Natural Language Processing” is not there(Ironic?) but it seems like a good enough translation to me as the neural network is somehow able to understand the structure of both the languages with just an hour of training.</p><hr><h2 id=vorbehalte--verbesserungen-caveatsimprovements>Vorbehalte / Verbesserungen (Caveats/Improvements)</h2><p>We might have achieved better results if we did everything in the same way the paper did:</p><ul><li><p>Train on whole data</p></li><li><p>Byte Pair Encoding</p></li><li><p>Learning Rate Scheduling</p></li><li><p>KL Divergence Loss</p></li><li><p>Beam search, and</p></li><li><p>Checkpoint ensembling</p></li></ul><p>I discussed all of these in my
<a href=https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076 target=_blank rel="nofollow noopener">last post</a>
and all of these are easy to implement additions. But this simple implementation was meant to understand how a transformer works so I didn’t include all these so as not to confuse the readers. There have actually been quite a lot of advancements on top of transformers that have allowed us to have much better models for translation. We will discuss those advancements and how they came about in the upcoming post, where I will talk about BERT, one of the most popular NLP models that utilizes a Transformer at its core.</p><hr><h2 id=references>References</h2><ul><li><p><a href=https://arxiv.org/abs/1706.03762 target=_blank rel="nofollow noopener">Attention Is All You Need</a></p></li><li><p><a href=https://nlp.seas.harvard.edu/2018/04/03/attention.html target=_blank rel="nofollow noopener">The Annotated Transformer</a></p></li></ul><p>In this post, We created an English to German translation network almost from scratch using the transformer architecture.</p><p>For a closer look at the code for this post, please visit my
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/transformers target=_blank rel="nofollow noopener">GitHub</a>
repository where you can find the code for this post as well as all my posts.</p><p><strong>As a side note</strong>: If you want to know more about NLP, I would like to recommend this awesome
<a href=https://coursera.pxf.io/9WjZo0 target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><p>And, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><p>This post was first published
<a href=https://lionbridge.ai/articles/transformers-in-nlp-creating-a-translator-model-from-scratch/ target=_blank rel="nofollow noopener">here</a></p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in London, where I am currently working with Facebook .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/bash>Bash</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/machine-learning>Machine Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/opinion>Opinion</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>