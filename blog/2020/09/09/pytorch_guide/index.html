<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="The Most Complete Guide to PyTorch for Data Scientists"><meta property="og:description" content="PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/09/09/pytorch_guide/"><meta property="og:image" content="https://mlwhiz.com/images/pytorch_guide/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/pytorch_guide/main.png"><meta property="article:published_time" content="2020-09-08T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/pytorch_guide/main.png"><meta name=twitter:title content="The Most Complete Guide to PyTorch for Data Scientists"><meta name=twitter:description content="PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/09/09/pytorch_guide/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/09/09/pytorch_guide/#primaryimage","url":"https://mlwhiz.com/images/pytorch_guide/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/09/09/pytorch_guide/#webpage","url":"https://mlwhiz.com/blog/2020/09/09/pytorch_guide/","inLanguage":"en-US","name":"The Most Complete Guide to PyTorch for Data Scientists - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/09/09/pytorch_guide/#primaryimage"},"datePublished":"2020-09-08T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/computer-vision class=categoryStyle>Computer Vision</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>The Most Complete Guide to PyTorch for Data Scientists</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>08 September 2020</span></div><img src=https://mlwhiz.com/images/pytorch_guide/main.png class="img-fluid w-100 mb-4" alt="The Most Complete Guide to PyTorch for Data Scientists"><div class="content mb-5"><p><em><strong>PyTorch</strong></em> has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.</p><p>I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from
<a href=https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79 target=_blank rel="nofollow noopener">Keras to PyTorch</a>
. With its high customizability and pythonic syntax,PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.</p><p>So, in this PyTorch guide, <em><strong>I will try to ease some of the pain with PyTorch for starters</strong></em> and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.</p><p>But, that is not to say that this is aimed at beginners only as <em><strong>I will also talk about the</strong></em> <em><strong>high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions</strong></em>.</p><p>So let’s get some coffee ☕ ️and start it up.</p><hr><h2 id=tensors>Tensors</h2><p>Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.</p><h3 id=1-create-a-tensor>1. Create a Tensor</h3><p>We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of
<a href=https://pytorch.org/docs/stable/tensors.html target=_blank rel="nofollow noopener">more things</a>
with tensors just like you can do with NumPy arrays.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Using torch.Tensor</span>
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Tensor([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>],[<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]])
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor Using torch.Tensor:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)

<span style=color:#75715e># Using torch.randn</span>
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>)
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor Using torch.randn:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)

<span style=color:#75715e># using torch.[ones|zeros](*size)</span>
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>)
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor Using torch.ones:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>)
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor Using torch.zeros:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)

<span style=color:#75715e># using torch.randint - a tensor of size 4,5 with entries between 0 and 10(excluded)</span>
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(low <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,high <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>,size <span style=color:#f92672>=</span> (<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>))
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor Using torch.randint:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)

<span style=color:#75715e># Using from_numpy to convert from Numpy Array to Tensor</span>
a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>],[<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]])
t <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(a)
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Convert to Tensor From Numpy Array:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)

<span style=color:#75715e># Using .numpy() to convert from Tensor to Numpy array</span>
t <span style=color:#f92672>=</span> t<span style=color:#f92672>.</span>numpy()
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Convert to Numpy Array From Tensor:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/0_hu498d9dc39892132ab99f755c5b75f03e_77051_500x0_resize_box_2.png 500w
, /images/pytorch_guide/0_hu498d9dc39892132ab99f755c5b75f03e_77051_800x0_resize_box_2.png 800w
, /images/pytorch_guide/0_hu498d9dc39892132ab99f755c5b75f03e_77051_1200x0_resize_box_2.png 1200w" src=/images/pytorch_guide/0.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><h3 id=2-tensor-operations>2. Tensor Operations</h3><p>Again, there are a lot of operations you can do on these tensors. The full list of functions can be found
<a href="https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations" target=_blank rel="nofollow noopener">here</a>
.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>)
W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>2</span>)
<span style=color:#75715e># Multiply Matrix A and W</span>
t <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>mm(W)
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Created Tensor t by Multiplying A and W:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)
<span style=color:#75715e># Transpose Tensor t</span>
t <span style=color:#f92672>=</span> t<span style=color:#f92672>.</span>t()
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Transpose of Tensor t:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)
<span style=color:#75715e># Square each element of t</span>
t <span style=color:#f92672>=</span> t<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Square each element of Tensor t:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t}&#34;</span>)
<span style=color:#75715e># return the size of a tensor</span>
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#34;Size of Tensor t using .size():</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{t.size()}&#34;</span>)
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/1_hufb8162ba4b3c0b49f946179a1702f5a7_45468_500x0_resize_box_2.png 500w
, /images/pytorch_guide/1_hufb8162ba4b3c0b49f946179a1702f5a7_45468_800x0_resize_box_2.png 800w
, /images/pytorch_guide/1_hufb8162ba4b3c0b49f946179a1702f5a7_45468_1200x0_resize_box_2.png 1200w" src=/images/pytorch_guide/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><strong>Note:</strong> What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is
<a href=https://pytorch.org/docs/stable/autograd.html#variable-deprecated target=_blank rel="nofollow noopener">deprecated</a>
, and all methods for variables work with Tensors. So, if you don’t know about them, it’s fine as they re not needed, and if you know them, you can forget about them.</p><hr><h2 id=the-nnmodule>The nn.Module</h2><p>Photo by
<a href="https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Fernand De Canne</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/2_huf957eb7cc0803733802fcc4099cfd7cb_2203390_500x0_resize_box_2.png 500w
, /images/pytorch_guide/2_huf957eb7cc0803733802fcc4099cfd7cb_2203390_800x0_resize_box_2.png 800w
, /images/pytorch_guide/2_huf957eb7cc0803733802fcc4099cfd7cb_2203390_1200x0_resize_box_2.png 1200w
, /images/pytorch_guide/2_huf957eb7cc0803733802fcc4099cfd7cb_2203390_1500x0_resize_box_2.png 1500w" src=/images/pytorch_guide/2.png alt='<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>'></p><p>Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Moduleto define any model as a class. Every model class necessarily contains an<code> __init__</code> procedure block and a block for the <code>forward</code> pass.</p><ul><li><p>In the <code>__init__</code> part, the user can define all the layers the network is going to have but doesn&rsquo;t yet define how those layers would be connected to each other.</p></li><li><p>In the <code>forward</code> pass block, the user defines how data flows from one layer to another inside the network.</p></li></ul><p>So, put simply, any network we define will look like:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>myNeuralNet</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
        <span style=color:#75715e># Define all Layers Here</span>
        self<span style=color:#f92672>.</span>lin1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>30</span>)
        self<span style=color:#f92672>.</span>lin2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>10</span>)
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#75715e># Connect the layer Outputs here to define the forward pass</span>
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin1(x)
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin2(x)
        <span style=color:#66d9ef>return</span> x
</code></pre></div><p>Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do).</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>myCrazyNeuralNet</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
        <span style=color:#75715e># Define all Layers Here</span>
        self<span style=color:#f92672>.</span>lin1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>30</span>)
        self<span style=color:#f92672>.</span>lin2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>784</span>)
        self<span style=color:#f92672>.</span>lin3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>10</span>)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#75715e># Connect the layer Outputs here to define the forward pass</span>
        x_lin1 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin1(x)
        x_lin2 <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>lin2(x_lin1)
        x_lin2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin1(x_lin2)
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin3(x_lin2)
        <span style=color:#66d9ef>return</span> x
</code></pre></div><p>We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>784</span>))
model <span style=color:#f92672>=</span> myCrazyNeuralNet()
model(x)<span style=color:#f92672>.</span>size()
<span style=color:#f92672>--------------------------</span>
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>10</span>])
</code></pre></div><hr><h2 id=a-word-about-layers>A word about Layers</h2><p>Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using <code>nn.Module</code>. For example, rather than using the predefined Linear Layer <code>nn.Linear</code> from Pytorch above, we could have created our <strong>custom linear layer</strong>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>myCustomLinearLayer</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self,in_size,out_size):
        super()<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>weights <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>randn(in_size, out_size))
        self<span style=color:#f92672>.</span>bias <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Parameter(torch<span style=color:#f92672>.</span>zeros(out_size))
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#66d9ef>return</span> x<span style=color:#f92672>.</span>mm(self<span style=color:#f92672>.</span>weights) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>bias
</code></pre></div><p>You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch
<a href=https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter target=_blank rel="nofollow noopener">docs</a>
:</p><blockquote><p>Parameters are
<a href=https://pytorch.org/docs/stable/tensors.html#torch.Tensor target=_blank rel="nofollow noopener">&lt;code>*Tensor*&lt;/code></a>
subclasses, that have a very special property when used with <em>Module</em> - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in <em><code>parameters()</code></em> iterator</p></blockquote><p>As you will later see, the <code>model.parameters()</code> iterator will be an input to the optimizer. But more on that later.</p><p>Right now, we can now use this custom layer in any PyTorch network, just like any other layer.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>myCustomNeuralNet</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
        <span style=color:#75715e># Define all Layers Here</span>
        self<span style=color:#f92672>.</span>lin1 <span style=color:#f92672>=</span> myCustomLinearLayer(<span style=color:#ae81ff>784</span>,<span style=color:#ae81ff>10</span>)

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#75715e># Connect the layer Outputs here to define the forward pass</span>
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin1(x)
        <span style=color:#66d9ef>return</span> x
x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>784</span>))
model <span style=color:#f92672>=</span> myCustomNeuralNet()
model(x)<span style=color:#f92672>.</span>size()
<span style=color:#f92672>------------------------------------------</span>
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>10</span>])
</code></pre></div><p>But then again, Pytorch would not be so widely used if it didn’t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are:
<a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear target=_blank rel="nofollow noopener">nn.Linear</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d target=_blank rel="nofollow noopener">nn.Conv2d</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d target=_blank rel="nofollow noopener">nn.MaxPool2d</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU target=_blank rel="nofollow noopener">nn.ReLU</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d target=_blank rel="nofollow noopener">nn.BatchNorm2d</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout target=_blank rel="nofollow noopener">nn.Dropout</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding target=_blank rel="nofollow noopener">nn.Embedding</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU target=_blank rel="nofollow noopener">nn.GRU</a>
/
<a href=https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM target=_blank rel="nofollow noopener">nn.LSTM</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax target=_blank rel="nofollow noopener">nn.Softmax</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax target=_blank rel="nofollow noopener">nn.LogSoftmax</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention target=_blank rel="nofollow noopener">nn.MultiheadAttention</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder target=_blank rel="nofollow noopener">nn.TransformerEncoder</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder target=_blank rel="nofollow noopener">nn.TransformerDecoder</a></p><p>I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/3_hu6bac2fdb22e4c85a567731e38a09e400_109832_500x0_resize_box_2.png 500w
, /images/pytorch_guide/3_hu6bac2fdb22e4c85a567731e38a09e400_109832_800x0_resize_box_2.png 800w
, /images/pytorch_guide/3_hu6bac2fdb22e4c85a567731e38a09e400_109832_1200x0_resize_box_2.png 1200w" src=/images/pytorch_guide/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>So, a Conv2d Layer needs as input an Image of height H and width W, with <code>Cin</code> channels. Now, for the first layer in a convnet, the number of <code>in_channels</code> would be 3(RGB), and the number of <code>out_channels</code> can be defined by the user. The <code>kernel_size</code> mostly used is 3x3, and the <code>stride</code> normally used is 1.</p><p>To check a new layer which I don’t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>conv_layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(in_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>, out_channels <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, kernel_size <span style=color:#f92672>=</span> (<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>3</span>), stride <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</code></pre></div><p>And then pass some random input through it. Here 100 is the batch size.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>24</span>,<span style=color:#ae81ff>24</span>))
conv_layer(x)<span style=color:#f92672>.</span>size()
<span style=color:#f92672>--------------------------------</span>
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>24</span>, <span style=color:#ae81ff>24</span>])
</code></pre></div><p>So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.</p><hr><h2 id=datasets-and-dataloaders>Datasets and DataLoaders</h2><p>How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at
<a href=https://pytorch.org/docs/stable/torchvision/datasets.html target=_blank rel="nofollow noopener">torchvision.datasets</a>
and
<a href=https://pytorch.org/text/datasets.html target=_blank rel="nofollow noopener">torchtext.datasets</a>
. But, to give a concrete example for datasets, let’s say we had to pass images to an Image Neural net using a folder which has images in this structure:</p><pre><code>data
    train
        sailboat
        kayak
        .
        .
</code></pre><p>We can use torchvision.datasets.ImageFolder dataset to get an example image like below:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> transforms
<span style=color:#f92672>from</span> torchvision.datasets <span style=color:#f92672>import</span> ImageFolder
traindir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;data/train/&#34;</span>
t <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
        transforms<span style=color:#f92672>.</span>Resize(size<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>),
    transforms<span style=color:#f92672>.</span>CenterCrop(size<span style=color:#f92672>=</span><span style=color:#ae81ff>224</span>),
        transforms<span style=color:#f92672>.</span>ToTensor()])
train_dataset <span style=color:#f92672>=</span> ImageFolder(root<span style=color:#f92672>=</span>traindir,transform<span style=color:#f92672>=</span>t)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Num Images in Dataset:&#34;</span>, len(train_dataset))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Example Image and Label:&#34;</span>, train_dataset[<span style=color:#ae81ff>2</span>])
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/4_hufa73a4f38ef9b26b8ca5301fcfb85ded_105670_500x0_resize_box_2.png 500w
, /images/pytorch_guide/4_hufa73a4f38ef9b26b8ca5301fcfb85ded_105670_800x0_resize_box_2.png 800w" src=/images/pytorch_guide/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>,len(train_dataset)):
    image ,label <span style=color:#f92672>=</span> train_dataset[i]
    pred <span style=color:#f92672>=</span> model(image)
</code></pre></div><p><em><strong>But that is not optimal. We want to do batching.</strong></em> We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>train_dataloader <span style=color:#f92672>=</span> DataLoader(train_dataset,batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, shuffle<span style=color:#f92672>=</span>True, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</code></pre></div><p>We can simply iterate with batches using:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>for</span> image_batch, label_batch <span style=color:#f92672>in</span> train_dataloader:
    <span style=color:#66d9ef>print</span>(image_batch<span style=color:#f92672>.</span>size(),label_batch<span style=color:#f92672>.</span>size())
    <span style=color:#66d9ef>break</span>
<span style=color:#f92672>-------------------------------------------------</span>
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>]) torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>64</span>])
</code></pre></div><p>So actually, the whole process of using datasets and Dataloaders becomes:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>t <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
        transforms<span style=color:#f92672>.</span>Resize(size<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>),
    transforms<span style=color:#f92672>.</span>CenterCrop(size<span style=color:#f92672>=</span><span style=color:#ae81ff>224</span>),
        transforms<span style=color:#f92672>.</span>ToTensor()])

train_dataset <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>ImageFolder(root<span style=color:#f92672>=</span>traindir,transform<span style=color:#f92672>=</span>t)
train_dataloader <span style=color:#f92672>=</span> DataLoader(train_dataset,batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, shuffle<span style=color:#f92672>=</span>True, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)

<span style=color:#66d9ef>for</span> image_batch, label_batch <span style=color:#f92672>in</span> train_dataloader:
    pred <span style=color:#f92672>=</span> myImageNeuralNet(image_batch)
</code></pre></div><p>You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning
<a href=https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c target=_blank rel="nofollow noopener">here</a>
.</p><p>This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case.</p><hr><h3 id=understanding-custom-datasets>Understanding Custom Datasets</h3><p>To write our custom datasets, we can make use of the abstract class <code>torch.utils.data.Dataset</code> provided by Pytorch. We need to inherit this <code>Dataset</code> class and need to define two methods to create a custom Dataset.</p><ul><li><p><code>__len__</code> : a function that returns the size of the dataset. This one is pretty simple to write in most cases.</p></li><li><p><code>__getitem__</code>: a function that takes as input an index i and returns the sample at index <code>i</code>.</p></li></ul><p>For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in <code>__init__</code> part where we use <code>glob.glob</code> to get image names and do some general preprocessing.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>from</span> glob <span style=color:#f92672>import</span> glob
<span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
<span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> Dataset

<span style=color:#66d9ef>class</span> <span style=color:#a6e22e>customImageFolderDataset</span>(Dataset):
    <span style=color:#e6db74>&#34;&#34;&#34;Custom Image Loader dataset.&#34;&#34;&#34;</span>
    <span style=color:#66d9ef>def</span> __init__(self, root, transform<span style=color:#f92672>=</span>None):
        <span style=color:#e6db74>&#34;&#34;&#34;
</span><span style=color:#e6db74>        Args:
</span><span style=color:#e6db74>            root (string): Path to the images organized in a particular folder structure.
</span><span style=color:#e6db74>            transform: Any Pytorch transform to be applied
</span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
        <span style=color:#75715e># Get all image paths from a directory</span>
        self<span style=color:#f92672>.</span>image_paths <span style=color:#f92672>=</span> glob(f<span style=color:#e6db74>&#34;{root}/*/*&#34;</span>)
        <span style=color:#75715e># Get the labels from the image paths</span>
        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> [x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;/&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>image_paths]
        <span style=color:#75715e># Create a dictionary mapping each label to a index from 0 to len(classes).</span>
        self<span style=color:#f92672>.</span>label_to_idx <span style=color:#f92672>=</span> {x:i <span style=color:#66d9ef>for</span> i,x <span style=color:#f92672>in</span> enumerate(set(self<span style=color:#f92672>.</span>labels))}
        self<span style=color:#f92672>.</span>transform <span style=color:#f92672>=</span> transform

    <span style=color:#66d9ef>def</span> __len__(self):
        <span style=color:#75715e># return length of dataset</span>
        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>image_paths)

    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
        <span style=color:#75715e># open and send one image and label</span>
        img_name <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>image_paths[idx]
        label <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>labels[idx]
        image <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(img_name)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>transform:
            image <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transform(image)
        <span style=color:#66d9ef>return</span> image,self<span style=color:#f92672>.</span>label_to_idx[label]
</code></pre></div><p>Also, note that we open our images one at a time in the <code>__getitem__</code> method and not while initializing. This is not done in <code>__init__</code> because we don&rsquo;t want to load all our images in the memory and just need to load the required ones.</p><p>We can now use this dataset with the utility <code>Dataloader</code> just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>t <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
        transforms<span style=color:#f92672>.</span>Resize(size<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>),
    transforms<span style=color:#f92672>.</span>CenterCrop(size<span style=color:#f92672>=</span><span style=color:#ae81ff>224</span>),
        transforms<span style=color:#f92672>.</span>ToTensor()])

train_dataset <span style=color:#f92672>=</span> customImageFolderDataset(root<span style=color:#f92672>=</span>traindir,transform<span style=color:#f92672>=</span>t)
train_dataloader <span style=color:#f92672>=</span> DataLoader(train_dataset,batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, shuffle<span style=color:#f92672>=</span>True, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)

<span style=color:#66d9ef>for</span> image_batch, label_batch <span style=color:#f92672>in</span> train_dataloader:
    pred <span style=color:#f92672>=</span> myImageNeuralNet(image_batch)
</code></pre></div><hr><h3 id=understanding-custom-dataloaders>Understanding Custom DataLoaders</h3><p><strong>This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations.</strong> But I am adding it for completeness here.</p><p>So let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BiLSTM</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>hidden_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>
        drp <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
        max_features, embed_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>,<span style=color:#ae81ff>300</span>
        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(max_features, embed_size)
        self<span style=color:#f92672>.</span>lstm <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LSTM(embed_size, self<span style=color:#f92672>.</span>hidden_size, bidirectional<span style=color:#f92672>=</span>True, batch_first<span style=color:#f92672>=</span>True)
        self<span style=color:#f92672>.</span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(self<span style=color:#f92672>.</span>hidden_size<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span> , <span style=color:#ae81ff>64</span>)
        self<span style=color:#f92672>.</span>relu <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ReLU()
        self<span style=color:#f92672>.</span>dropout <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Dropout(drp)
        self<span style=color:#f92672>.</span>out <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>1</span>)


    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        h_embedding <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embedding(x)
        h_embedding <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>squeeze(torch<span style=color:#f92672>.</span>unsqueeze(h_embedding, <span style=color:#ae81ff>0</span>))

        h_lstm, _ <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lstm(h_embedding)
        avg_pool <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean(h_lstm, <span style=color:#ae81ff>1</span>)
        max_pool, _ <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>max(h_lstm, <span style=color:#ae81ff>1</span>)
        conc <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat(( avg_pool, max_pool), <span style=color:#ae81ff>1</span>)
        conc <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>linear(conc))
        conc <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(conc)
        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>out(conc)
        <span style=color:#66d9ef>return</span> out
</code></pre></div><p>This network expects its input to be of shape (<code>batch_size</code>, <code>seq_length</code>) and works with any <code>seq_length</code>. We can check this by passing our model two random batches with different sequence lengths(10 and 25).</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model <span style=color:#f92672>=</span> BiLSTM()
input_batch_1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(low <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,high <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>, size <span style=color:#f92672>=</span> (<span style=color:#ae81ff>100</span>,<span style=color:#f92672>**</span><span style=color:#ae81ff>10</span><span style=color:#f92672>**</span>))
input_batch_2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(low <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,high <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>, size <span style=color:#f92672>=</span> (<span style=color:#ae81ff>100</span>,<span style=color:#f92672>**</span><span style=color:#ae81ff>25</span><span style=color:#f92672>**</span>))
<span style=color:#66d9ef>print</span>(model(input_batch_1)<span style=color:#f92672>.</span>size())
<span style=color:#66d9ef>print</span>(model(input_batch_2)<span style=color:#f92672>.</span>size())
<span style=color:#f92672>------------------------------------------------------------------</span>
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>1</span>])
torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>1</span>])
</code></pre></div><p>Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.</p><p>So, how do we do this? Let’s write a very simple custom dataset class first.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CustomTextDataset</span>(Dataset):
    <span style=color:#e6db74>&#39;&#39;&#39;
</span><span style=color:#e6db74>    Simple Dataset initializes with X and y vectors
</span><span style=color:#e6db74>    We start by sorting our X and y vectors by sequence lengths
</span><span style=color:#e6db74>    &#39;&#39;&#39;</span>
    <span style=color:#66d9ef>def</span> __init__(self,X,y<span style=color:#f92672>=</span>None):
        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> list(zip(X,y))
        <span style=color:#75715e># Sort by length of first element in tuple</span>
        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> sorted(self<span style=color:#f92672>.</span>data, key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> x: len(x[<span style=color:#ae81ff>0</span>]))

    <span style=color:#66d9ef>def</span> __len__(self):
        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>data)

    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>data[idx]
</code></pre></div><p>Also, let’s generate some random data which we will use with this custom Dataset.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
train_data_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>1024</span>
sizes <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(low<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,high<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>,size<span style=color:#f92672>=</span>(train_data_size,))
X <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>10000</span>, (sizes[i])) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(train_data_size)]
y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(train_data_size)<span style=color:#f92672>.</span>round()
<span style=color:#75715e>#checking one example in dataset</span>
<span style=color:#66d9ef>print</span>((X[<span style=color:#ae81ff>0</span>],y[<span style=color:#ae81ff>0</span>]))
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/5_hu73e9ccc1ce6998d2fdb18c8d5708c0b9_134063_500x0_resize_box_2.png 500w" src=/images/pytorch_guide/5.png alt="Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.">
<em>Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.</em></p><p>We can use the custom dataset now using:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>train_dataset <span style=color:#f92672>=</span> CustomTextDataset(X,y)
</code></pre></div><p>If we now try to use the Dataloader on this dataset with <code>batch_size</code>>1, we will get an error. Why is that?</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>train_dataloader <span style=color:#f92672>=</span> DataLoader(train_dataset,batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, shuffle<span style=color:#f92672>=</span>False, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
<span style=color:#66d9ef>for</span> xb,yb <span style=color:#f92672>in</span> train_dataloader:
    <span style=color:#66d9ef>print</span>(xb<span style=color:#f92672>.</span>size(),yb<span style=color:#f92672>.</span>size())
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/6_hu4132e27c9bf90dc137f1e93415aa5c27_9727_500x0_resize_box_2.png 500w
, /images/pytorch_guide/6_hu4132e27c9bf90dc137f1e93415aa5c27_9727_800x0_resize_box_2.png 800w" src=/images/pytorch_guide/6.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn’t face this error.</p><p><em><strong>So, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?</strong></em></p><p>We can use <code>collate_fn</code> parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (<code>x_batch</code>, <code>y_batch</code> ) with padded sequence lengths based on <code>max_sequence_length</code> in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>collate_text</span>(batch):
    <span style=color:#75715e># get text sequences in batch</span>
    data <span style=color:#f92672>=</span> [item[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> batch]
    <span style=color:#75715e># get labels in batch</span>
    target <span style=color:#f92672>=</span> [item[<span style=color:#ae81ff>1</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> batch]
    <span style=color:#75715e># get max_seq_length in batch</span>
    max_seq_len <span style=color:#f92672>=</span> max([len(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> data])
    <span style=color:#75715e># pad text sequences based on max_seq_len</span>
    data <span style=color:#f92672>=</span> [np<span style=color:#f92672>.</span>pad(p, (<span style=color:#ae81ff>0</span>, max_seq_len <span style=color:#f92672>-</span> len(p)), <span style=color:#e6db74>&#39;constant&#39;</span>) <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> data]
    <span style=color:#75715e># convert data and target to tensor</span>
    data <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>LongTensor(data)
    target <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>LongTensor(target)
    <span style=color:#66d9ef>return</span> [data, target]
</code></pre></div><p>We can now use this <code>collate_fn</code> with our Dataloader as:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>train_dataloader <span style=color:#f92672>=</span> DataLoader(train_dataset,batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, shuffle<span style=color:#f92672>=</span>False, num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,collate_fn <span style=color:#f92672>=</span> collate_text)

<span style=color:#66d9ef>for</span> xb,yb <span style=color:#f92672>in</span> train_dataloader:
    <span style=color:#66d9ef>print</span>(xb<span style=color:#f92672>.</span>size(),yb<span style=color:#f92672>.</span>size())
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/7_hu134bd9cd626a0a63b444e492820afa14_114962_500x0_resize_box_2.png 500w
, /images/pytorch_guide/7_hu134bd9cd626a0a63b444e492820afa14_114962_800x0_resize_box_2.png 800w
, /images/pytorch_guide/7_hu134bd9cd626a0a63b444e492820afa14_114962_1200x0_resize_box_2.png 1200w" src=/images/pytorch_guide/7.png alt="See that the batches have different sequence lengths now"></p><p>It will work this time as we have provided a custom <code>collate_fn</code>. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.</p><hr><h2 id=training-a-neural-network>Training a Neural Network</h2><p>We know how to create a neural network using <code>nn.Module</code>. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
    <span style=color:#75715e># Set model to train mode</span>
    model<span style=color:#f92672>.</span>train()
    <span style=color:#66d9ef>for</span> x_batch,y_batch <span style=color:#f92672>in</span> train_dataloader:
        <span style=color:#75715e># Clear gradients</span>
        optimizer<span style=color:#f92672>.</span>zero_grad()
        <span style=color:#75715e># Forward pass - Predicted outputs</span>
        pred <span style=color:#f92672>=</span> model(x_batch)
        <span style=color:#75715e># Find Loss and backpropagation of gradients</span>
        loss <span style=color:#f92672>=</span> loss_criterion(pred, y_batch)
        loss<span style=color:#f92672>.</span>backward()
        <span style=color:#75715e># Update the parameters</span>
        optimizer<span style=color:#f92672>.</span>step()
    model<span style=color:#f92672>.</span>eval()
    <span style=color:#66d9ef>for</span> x_batch,y_batch <span style=color:#f92672>in</span> valid_dataloader:
        pred <span style=color:#f92672>=</span> model(x_batch)
        val_loss <span style=color:#f92672>=</span> loss_criterion(pred, y_batch)
</code></pre></div><p>In the above code, we are running five epochs and in each epoch:</p><ol><li><p>We iterate through the dataset using a data loader.</p></li><li><p>In each iteration, we do a forward pass using <code>model(x_batch)</code></p></li><li><p>We calculate the Loss using a <code>loss_criterion</code></p></li><li><p>We back-propagate that loss using <code>loss.backward()</code> call. We don&rsquo;t have to worry about the calculation of the gradients at all, as this simple call does it all for us.</p></li><li><p>Take an optimizer step to change the weights in the whole network using <code>optimizer.step()</code>. This is where weights of the network get modified using the gradients calculated in <code>loss.backward()</code> call.</p></li><li><p>We go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using <code>model.eval()</code>.Please note we don&rsquo;t back-propagate losses in eval mode.</p></li></ol><p>Till now, we have talked about how to use <code>nn.Module</code> to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let&rsquo;s talk about the various options available for Loss Functions and Optimizers.</p><hr><h2 id=loss-functions>Loss functions</h2><p>Pytorch provides us with a variety of
<a href=https://pytorch.org/docs/stable/nn.html#loss-functions target=_blank rel="nofollow noopener">loss functions</a>
for our most common tasks, like Classification and Regression. Some most used examples are
<a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss target=_blank rel="nofollow noopener">nn.CrossEntropyLoss</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss target=_blank rel="nofollow noopener">nn.NLLLoss</a>
,
<a href=https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss target=_blank rel="nofollow noopener">nn.KLDivLoss</a>
and
<a href=https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss target=_blank rel="nofollow noopener">nn.MSELoss</a>
. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of
<a href=https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss target=_blank rel="nofollow noopener">nn.NLLLoss</a></p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/8_hu6ada560b229cd1a272efb53bc9878041_203477_500x0_resize_box_2.png 500w
, /images/pytorch_guide/8_hu6ada560b229cd1a272efb53bc9878041_203477_800x0_resize_box_2.png 800w
, /images/pytorch_guide/8_hu6ada560b229cd1a272efb53bc9878041_203477_1200x0_resize_box_2.png 1200w" src=/images/pytorch_guide/8.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:</p><ul><li><p>the input expected needs to be of size (<code>batch_size</code> x <code>Num_Classes</code> ) — These are the predictions from the Neural Network we have created.</p></li><li><p>We need to have the log-probabilities of each class in the input — To get log-probabilities from a Neural Network, we can add a <code>LogSoftmax</code> Layer as the last layer of our network.</p></li><li><p>The target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.</p></li></ul><p>So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don&rsquo;t want to use this LogSoftmax layer, you could have just used
<a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss target=_blank rel="nofollow noopener">&lt;code>nn.CrossEntropyLoss&lt;/code></a></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>myClassificationNet</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
        <span style=color:#75715e># Define all Layers Here</span>
        self<span style=color:#f92672>.</span>lin <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>10</span>)
        self<span style=color:#f92672>.</span>logsoftmax <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>LogSoftmax(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#75715e># Connect the layer Outputs here to define the forward pass</span>
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lin(x)
        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>logsoftmax(x)
        <span style=color:#66d9ef>return</span> x
</code></pre></div><p>Let’s define a random input to pass to our network to test it:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># some random input:</span>

X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>784</span>)
y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(low <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,high <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>,size <span style=color:#f92672>=</span> (<span style=color:#ae81ff>100</span>,))
</code></pre></div><p>And pass it through the model to get predictions:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model <span style=color:#f92672>=</span> myClassificationNet()
preds <span style=color:#f92672>=</span> model(X)
</code></pre></div><p>We can now get the loss as:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>NLLLoss()
loss <span style=color:#f92672>=</span> criterion(preds,y)
loss
<span style=color:#f92672>------------------------------------------</span>
tensor(<span style=color:#ae81ff>2.4852</span>, grad_fn<span style=color:#f92672>=&lt;</span>NllLossBackward<span style=color:#f92672>&gt;</span>)
</code></pre></div><hr><h3 id=custom-loss-function>Custom Loss Function</h3><p>Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>customMseLoss</span>(output,target):
    loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((output <span style=color:#f92672>-</span> target)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)     
    <span style=color:#66d9ef>return</span> loss
</code></pre></div><p>You can use this custom loss just like before. But note that we don’t instantiate the loss using criterion this time as we have defined it as a function.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>output <span style=color:#f92672>=</span> model(x)
loss <span style=color:#f92672>=</span> customMseLoss(output, target)
loss<span style=color:#f92672>.</span>backward()
</code></pre></div><p>If we wanted, we could have also written it as a class using <code>nn.Module</code> , and then we would have been able to use it as an object. Here is an NLLLoss custom example:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CustomNLLLoss</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self):
        super()<span style=color:#f92672>.</span>__init__()
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x, y):
        <span style=color:#75715e># x should be output from LogSoftmax Layer</span>
        log_prob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span> <span style=color:#f92672>*</span> x
        <span style=color:#75715e># Get log_prob based on y class_index as loss=-mean(ylogp)</span>
        loss <span style=color:#f92672>=</span> log_prob<span style=color:#f92672>.</span>gather(<span style=color:#ae81ff>1</span>, y<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>))
        loss <span style=color:#f92672>=</span> loss<span style=color:#f92672>.</span>mean()
        <span style=color:#66d9ef>return</span> loss
criterion <span style=color:#f92672>=</span> CustomNLLLoss()
loss <span style=color:#f92672>=</span> criterion(preds,y)

</code></pre></div><hr><h2 id=optimizers>Optimizers</h2><p>Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example:
<a href=https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta target=_blank rel="nofollow noopener">torch.optim.Adadelta</a>
,
<a href=https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad target=_blank rel="nofollow noopener">torch.optim.Adagrad</a>
,
<a href=https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop target=_blank rel="nofollow noopener">torch.optim.RMSprop</a>
and the most widely used
<a href=https://pytorch.org/docs/stable/optim.html#torch.optim.Adam target=_blank rel="nofollow noopener">torch.optim.Adam</a>
.</p><p>To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, betas<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0.9</span>, <span style=color:#ae81ff>0.999</span>))
</code></pre></div><p>And then use <code>optimizer.zero_grad()</code> and <code>optimizer.step()</code> while training the model.</p><p>I am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the
<a href=https://pytorch-optimizer.readthedocs.io/en/latest/ target=_blank rel="nofollow noopener">pytorch-optimizer</a>
library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in
<a href=https://github.com/pytorch/pytorch/tree/master/torch/optim target=_blank rel="nofollow noopener">PyTorch</a>
or
<a href=https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer target=_blank rel="nofollow noopener">pytorch-optimizers</a>
.</p><p>Other optimizers from
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/pytorch_guide/9_hu2112dd30c5fdc3fb07b27b31310a3c2e_184668_500x0_resize_box_2.png 500w
, /images/pytorch_guide/9_hu2112dd30c5fdc3fb07b27b31310a3c2e_184668_800x0_resize_box_2.png 800w" src=/images/pytorch_guide/9.png alt='<a href="https://github.com/jettify/pytorch-optimizer" target="_blank" rel="nofollow noopener">pytorch-optimizer</a>
 library'>
<em>Other optimizers from
<a href=https://github.com/jettify/pytorch-optimizer target=_blank rel="nofollow noopener">pytorch-optimizer</a>
library</em></p><hr><h2 id=using-gpumultiple-gpus>Using GPU/Multiple GPUs</h2><p>Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using <code>model.to('cuda')</code>. Or if you want to use multiple GPUs, you can use <code>nn.DataParallel</code>. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using <code>DataParallel</code> if needed.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Whether to train on a gpu</span>
train_on_gpu <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available()
<span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#39;Train on gpu: {train_on_gpu}&#39;</span>)<span style=color:#75715e># Number of gpus</span>
<span style=color:#66d9ef>if</span> train_on_gpu:
    gpu_count <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>device_count()
    <span style=color:#66d9ef>print</span>(f<span style=color:#e6db74>&#39;{gpu_count} gpus detected.&#39;</span>)
    <span style=color:#66d9ef>if</span> gpu_count <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>:
        multi_gpu <span style=color:#f92672>=</span> True
    <span style=color:#66d9ef>else</span>:
        multi_gpu <span style=color:#f92672>=</span> False
<span style=color:#66d9ef>if</span> train_on_gpu:
    model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>)
<span style=color:#66d9ef>if</span> multi_gpu:
    model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>DataParallel(model)
</code></pre></div><p>The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It’s as simple as adding a few lines of code to our training loop.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
<span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
    model<span style=color:#f92672>.</span>train()
    <span style=color:#66d9ef>for</span> x_batch,y_batch <span style=color:#f92672>in</span> train_dataloader:
        <span style=color:#66d9ef>if</span> train_on_gpu:
            x_batch,y_batch <span style=color:#f92672>=</span> x_batch<span style=color:#f92672>.</span>cuda(), y_batch<span style=color:#f92672>.</span>cuda()
        optimizer<span style=color:#f92672>.</span>zero_grad()
        pred <span style=color:#f92672>=</span> model(x_batch)
        loss <span style=color:#f92672>=</span> loss_criterion(pred, y_batch)
        loss<span style=color:#f92672>.</span>backward()
        optimizer<span style=color:#f92672>.</span>step()
    model<span style=color:#f92672>.</span>eval()
    <span style=color:#66d9ef>for</span> x_batch,y_batch <span style=color:#f92672>in</span> valid_dataloader:
        <span style=color:#66d9ef>if</span> train_on_gpu:
            x_batch,y_batch <span style=color:#f92672>=</span> x_batch<span style=color:#f92672>.</span>cuda(), y_batch<span style=color:#f92672>.</span>cuda()
        pred <span style=color:#f92672>=</span> model(x_batch)
        val_loss <span style=color:#f92672>=</span> loss_criterion(pred, y_batch)
</code></pre></div><hr><h2 id=conclusion>Conclusion</h2><p>Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.</p><p>You can find the code for this post here on my
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide target=_blank rel="nofollow noopener">GitHub</a>
repo, where I keep codes for all my blogs.</p><p>If you want to learn more about Pytorch using a course based structure, take a look at the
<a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Deep Neural Networks with PyTorch</a>
course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on
<a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Deep Learning in Computer Vision</a>
in the
<a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Advanced machine learning specialization</a>
.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a></p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>