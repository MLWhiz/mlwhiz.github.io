<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Understanding Transformers, the Data Science Way"><meta property="og:description" content="Transformers have become the defacto standard for NLP tasks nowadays."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/09/20/transformers/"><meta property="og:image" content="https://mlwhiz.com/images/transformers/main_transformer.jpg"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/transformers/main_transformer.jpg"><meta property="article:published_time" content="2020-09-20T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-29T22:24:06+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/transformers/main_transformer.jpg"><meta name=twitter:title content="Understanding Transformers, the Data Science Way"><meta name=twitter:description content="Transformers have become the defacto standard for NLP tasks nowadays."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/09/20/transformers/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/09/20/transformers/#primaryimage","url":"https://mlwhiz.com/images/transformers/main_transformer.jpg","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/09/20/transformers/#webpage","url":"https://mlwhiz.com/blog/2020/09/20/transformers/","inLanguage":"en-US","name":"Understanding Transformers, the Data Science Way - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/09/20/transformers/#primaryimage"},"datePublished":"2020-09-20T00:00:00.00Z","dateModified":"2020-10-29T22:24:06.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"Transformers have become the defacto standard for NLP tasks nowadays."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Understanding Transformers, the Data Science Way</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>20 September 2020</span></div><img src=https://mlwhiz.com/images/transformers/main_transformer.jpg class="img-fluid w-100 mb-4" alt="Understanding Transformers, the Data Science Way"><div class="content mb-5"><p>Transformers have become the defacto standard for NLP tasks nowadays.</p><p>While the Transformer architecture was introduced with NLP, they are now being used in Computer Vision and to generate music as well. I am sure you would all have heard about the GPT3 Transformer and its applications thereof.</p><p><em><strong>But all these things aside, they are still hard to understand as ever.</strong></em></p><p>It has taken me multiple readings through the Google research
<a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel="nofollow noopener">paper</a>
that first introduced transformers along with just so many blog posts to really understand how a transformer works.</p><p>So, I thought of putting the whole idea down in as simple words as possible and with some very basic Math and some puns as I am a proponent of having some fun while learning. I will try to keep both the jargon and the technicality to a minimum, yet it is such a topic that I could only do so much. And my goal is to make the reader understand even the most gory details of Transformer by the end of this post.</p><p><em><strong>Also, this is officially my longest post both in terms of time taken to write it as well as length of the post. Hence, I will advice you to Grab A Coffee.</strong></em> ☕️</p><p>So, here goes — This post will be a highly conversational one and it is about “<em><strong>Decoding The Transformer”.</strong></em></p><hr><p><em><strong>Q: So, Why should I even understand Transformer?</strong></em></p><p>In the past, the LSTM and GRU architecture(as explained here in my past
<a href=https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566 target=_blank rel="nofollow noopener">post</a>
on NLP) along with attention mechanism used to be the State of the Art Approach for Language modeling problems (put very simply, predict the next word) and Translation systems. But, the main problem with these architectures is that they are recurrent in nature, and the runtime increases as the sequence length increases. That is, these architectures take a sentence and process each word in a <em><strong>sequential</strong></em> way, and hence with the increase in sentence length the whole runtime increases.</p><p>Transformer, a model architecture first explained in the paper Attention is all you need, lets go of this recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. And that makes it FAST.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/0.png alt='<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="nofollow noopener">Source</a>'><figcaption>From the Paper</figcaption></figure></p><p>This is the picture of the full transformer as taken from the paper. And, it surely is intimidating. So, I will aim to demystify it in this post by going through each individual piece. So read ahead.</p><hr><h2 id=the-big-picture>The Big Picture</h2><p><em><strong>Q: That sounds interesting. So, what does a transformer do exactly?</strong></em></p><p>Essentially, a transformer can perform almost any NLP task. It can be used for language modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one language to another, or for a classification problem will provide the class probability using an appropriate output layer.</p><p>It all will depend on the final outputs layer for the network but, the Transformer basic structure will remain quite the same for any task. For this particular post, I will be continuing with the machine translation example.</p><p>So from a very high place, this is how the transformer looks for a translation task. It takes as input an English sentence and returns a German sentence.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/1_huf7637c0a5b79ffe2d019977754e4b30a_26347_500x0_resize_box_2.png 500w
, /images/transformers/1_huf7637c0a5b79ffe2d019977754e4b30a_26347_800x0_resize_box_2.png 800w
, /images/transformers/1_huf7637c0a5b79ffe2d019977754e4b30a_26347_1200x0_resize_box_2.png 1200w
, /images/transformers/1_huf7637c0a5b79ffe2d019977754e4b30a_26347_1500x0_resize_box_2.png 1500w" src=/images/transformers/1.png alt="Transformer for Translation"><figcaption>Transformer for Translation</figcaption></figure></p><hr><h2 id=the-building-blocks>The Building Blocks</h2><p><em><strong>Q: That was too basic. <em>😎</em> Can you expand on it?</strong></em></p><p>Okay, just remember in the end, you asked for it. Let’s go a little deeper and try to understand what a transformer is composed of.</p><p>So, a transformer is essentially composed of a stack of encoder and decoder layers. The role of an encoder layer is to encode the English sentence into a numerical form using the attention mechanism, while the decoder aims to use the encoded information from the encoder layers to give the German translation for the particular English sentence.</p><p>In the figure below, the transformer is given as input an English sentence, which gets encoded using 6 encoder layers. The output from the final encoder layer then goes to each decoder layer to translate English to German.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/2_hu21bb5496b45a9dbcaf034b628881e1f6_117404_500x0_resize_box_2.png 500w
, /images/transformers/2_hu21bb5496b45a9dbcaf034b628881e1f6_117404_800x0_resize_box_2.png 800w
, /images/transformers/2_hu21bb5496b45a9dbcaf034b628881e1f6_117404_1200x0_resize_box_2.png 1200w
, /images/transformers/2_hu21bb5496b45a9dbcaf034b628881e1f6_117404_1500x0_resize_box_2.png 1500w" src=/images/transformers/2.png alt="Data Flow in a Transformer"><figcaption>Data Flow in a Transformer</figcaption></figure></p><hr><h2 id=1-encoder-architecture>1. Encoder Architecture</h2><p><em><strong>Q: That’s alright but, how does an encoder stack encode an English sentence exactly?</strong></em></p><p>Patience, I am getting to it. So, as I said the encoder stack contains six encoder layers on top of each other(As given in the paper, but the future versions of transformers use even more layers). And each encoder in the stack has essentially two main layers:</p><ul><li><p><strong>a multi-head self-attention Layer, and</strong></p></li><li><p><strong>a position-wise fully connected feed-forward network</strong></p></li></ul><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/3.png alt="Very basic encoder Layer"><figcaption>Very basic encoder Layer</figcaption></figure></p><p>They are a mouthful. Right? Don’t lose me yet as I will explain both of them in the coming sections. Right now, just remember that the encoder layer incorporates attention and a position-wise feed-forward network.</p><p><em><strong>Q: But, how does this layer expect its inputs to be?</strong></em></p><p>This layer expects its inputs to be of the shape <code>SxD</code> (as shown in the figure below) where <code>S</code> is the source sentence(English Sentence) length, and <code>D</code> is the dimension of the embedding whose weights can be trained with the network. In this post, we will be using D as 512 by default throughout. While S will be the maximum length of sentence in a batch. So it normally changes with batches.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/4.png alt="Encoder — Input and Output shapes are the same"><figcaption>Encoder — Input and Output shapes are the same</figcaption></figure></p><p>And what about the outputs of this layer? Remember that the encoder layers are stacked on top of each other. So, we want to be able to have an output of the same dimension as the input so that the output can flow easily into the next encoder. So the output is also of the shape, <code>SxD</code>.</p><p><em><strong>Q: Enough about the sizes talk, I understand what goes in and what goes out but what actually happens in the Encoder layer?</strong></em></p><p>Okay, let’s go through the attention layer and the feedforward layer one by one:</p><h3 id=a-self-attention-layer>A) Self-attention layer</h3><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/5_hu35b92d9bd23859c7c34b055b5523039e_137953_500x0_resize_box_2.png 500w
, /images/transformers/5_hu35b92d9bd23859c7c34b055b5523039e_137953_800x0_resize_box_2.png 800w
, /images/transformers/5_hu35b92d9bd23859c7c34b055b5523039e_137953_1200x0_resize_box_2.png 1200w
, /images/transformers/5_hu35b92d9bd23859c7c34b055b5523039e_137953_1500x0_resize_box_2.png 1500w" src=/images/transformers/5.png alt="How Self-Attention Works"><figcaption>How Self-Attention Works</figcaption></figure></p><p>The above figure must look daunting but it is easy to understand. So just stay with me here.</p><p>Deep Learning is essentially nothing but a lot of matrix calculations and what we are essentially doing in this layer is a lot of matrix calculations intelligently. The self-attention layer initializes with 3 weight matrices — Query($W_q$), Key($W_k$), and Value($W_v$). Each of these matrices has a size of (<code>Dxd</code>) where d is taken as 64 in the paper. The weights for these matrices will be trained when we train the model.</p><p>In the first calculation(Calc 1 in the figure), we create matrices Q, K, and V by multiplying the input with the respective Query, Key, and Value matrix.</p><p>Till now it is trivial and shouldn’t make any sense, but it is at the second calculation where it gets interesting. Let’s try to understand the output of the softmax function. We start by multiplying the Q and Kᵀ matrix to get a matrix of size (<code>SxS</code>) and divide it by the scalar √d. We then take a softmax to make the rows sum to one.</p><p>Intuitively, we can think of the resultant <code>SxS</code> matrix as the contribution of each word in another word. For example, it might look like this:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/6.png alt=Softmax(QxKt/sqrt(d))><figcaption>After Softmax</figcaption></figure></p><p>As you can see the diagonal entries are big. This is because the word contribution to itself is high. That is reasonable. But we can see here that the word “quick” devolves into “quick” and “fox” and the word “brown” also devolves into “brown” and “fox”. That intuitively helps us to say that both the words — “quick” and “brown” each refers to the “fox”.</p><p>Once we have this SxS matrix with contributions we multiply this matrix by the Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64). So, what the operation actually does is that it replaces the embedding vector of a word like “quick” with say .75 x (quick embedding) and .2x(fox embedding) and thus now the resultant output for the word “quick” has attention embedded in itself.</p><p>Note that the output of this layer has the dimension (Sxd) and before we get done with the whole encoder we need to change it back to D=512 as we need the output of this encoder as the input of another encoder.</p><p><em><strong>Q: But, you called this layer Multi-head self-attention Layer. What is the multi-head?</strong></em></p><p>Okay, my bad but in my defense, I was just getting to that.</p><p>It’s called a multi-head because we use many such self-attention layers in parallel. That is, we have many self-attention layers stacked on top of each other. The number of attention layers,h, is kept as 8 in the paper. So the input X goes through many self-attention layers parallelly, each of which gives a z matrix of shape (Sxd) = 4x64. We concatenate these 8(h) matrices and again apply a final output linear layer, $W_o$, of size DxD.</p><p>What size do we get? For the concatenate operation we get a size of SxD(4x(64x8) = 4x512). And multiplying this output by $W_o$, we get the final output Z with the shape of SxD(4x512) as desired.</p><p>Also, note the relation between h,d, and D i.e. h x d = D</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/7_hu956ed1b67493af186b7776ec372bb5e2_62664_500x0_resize_box_2.png 500w
, /images/transformers/7_hu956ed1b67493af186b7776ec372bb5e2_62664_800x0_resize_box_2.png 800w
, /images/transformers/7_hu956ed1b67493af186b7776ec372bb5e2_62664_1200x0_resize_box_2.png 1200w
, /images/transformers/7_hu956ed1b67493af186b7776ec372bb5e2_62664_1500x0_resize_box_2.png 1500w" src=/images/transformers/7.png alt="The Full multi-headed self-attention Layer"><figcaption>The Full multi-headed self-attention Layer</figcaption></figure></p><p>Thus, we finally get the output Z of shape 4x512 as intended. But before it goes into another encoder we pass it through a Feed-Forward Network.</p><h3 id=b-position-wise-feed-forward-network>B) Position-wise feed-forward network</h3><p>Once we understand the multi-headed attention layer, the Feed-forward network is actually pretty easy to understand. It is just a combination of various linear and dropout layers on the output Z. Consequentially, it is again just a lot of Matrix multiplication here.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/8_huc65a66df5c091b1250a15290be6a8151_100130_500x0_resize_box_2.png 500w
, /images/transformers/8_huc65a66df5c091b1250a15290be6a8151_100130_800x0_resize_box_2.png 800w
, /images/transformers/8_huc65a66df5c091b1250a15290be6a8151_100130_1200x0_resize_box_2.png 1200w
, /images/transformers/8_huc65a66df5c091b1250a15290be6a8151_100130_1500x0_resize_box_2.png 1500w" src=/images/transformers/8.png alt="Each word goes into the feed-forward network."><figcaption>Each word goes into the feed-forward network</figcaption></figure></p><p>The feed-forward network applies itself to each position in the output Z parallelly(Each position can be thought of as a word) and hence the name Position-wise feed-forward network. The feed-forward network also shares weight, so that the length of the source sentence doesn’t matter(Also, if it didn’t share weights, we would have to initialize a lot of such networks based on max source sentence length and that is not feasible)</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/9.png alt="It is actually just a linear layer that gets applied to each position(or word)"><figcaption>It is actually just a linear layer that gets applied to each position(or word)</figcaption></figure></p><p>With this, we near an okayish understanding of the encoder part of the Transformer.</p><p><em><strong>Q: Hey, I was just going through the picture in the paper, and the encoder stack has something called “positional encoding” and “Add & Norm” also. What are these?</strong></em></p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/10.png alt="I am back again here so you don’t have to scroll"><figcaption>I am back again here so you don’t have to scroll</figcaption></figure></p><p>Okay, These two concepts are pretty essential to this particular architecture. And I am glad you asked this one. So, we will discuss these steps before moving further to the decoder stack.</p><h3 id=c-positional-encodings>C. Positional Encodings</h3><p>Since, our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of both the encoder and decoder stacks(as we will see later). The positional encodings need to have the same dimension, D as the embeddings have so that the two can be summed.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/11_hu53684e1d3c15e922eb6c1a87d0a0bc06_8468_500x0_resize_box_2.png 500w
, /images/transformers/11_hu53684e1d3c15e922eb6c1a87d0a0bc06_8468_800x0_resize_box_2.png 800w
, /images/transformers/11_hu53684e1d3c15e922eb6c1a87d0a0bc06_8468_1200x0_resize_box_2.png 1200w
, /images/transformers/11_hu53684e1d3c15e922eb6c1a87d0a0bc06_8468_1500x0_resize_box_2.png 1500w" src=/images/transformers/11.png alt="Add a static positional pattern to X"><figcaption>Add a static positional pattern to X</figcaption></figure></p><p>In the paper, the authors used sine and cosine functions to create positional embeddings for different positions.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/12.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This particular mathematical thing actually generates a 2d matrix which is added to the embedding vector that goes into the first encoder step.</p><p>Put simply, it’s just a constant matrix that we add to the sentence so that the network could get the position of the word.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/13_hu8c389fac8aaa1caaeb85483665bcef38_275956_500x0_resize_box_2.png 500w
, /images/transformers/13_hu8c389fac8aaa1caaeb85483665bcef38_275956_800x0_resize_box_2.png 800w" src=/images/transformers/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/14_huc17476ed0c62989e4b0ca8a0cc9a11cf_403663_500x0_resize_box_2.png 500w
, /images/transformers/14_huc17476ed0c62989e4b0ca8a0cc9a11cf_403663_800x0_resize_box_2.png 800w" src=/images/transformers/14.png alt="Positional encoding matrix for the first 300 and 3000 positions"><figcaption>Positional encoding matrix for the first 300 and 3000 positions</figcaption></figure></p><p>Above is the heatmap of the position encoding matrix that we will add to the input that is to be given to the first encoder. I am showing the heatmap for the first 300 positions and the first 3000 positions. We can see that there is a distinct pattern that we provide to our Transformer to understand the position of each word. And since we are using a function comprised of sin and cos, we are able to embed positional embeddings for very high positions also pretty well as we can see in the second picture.</p><p><strong>Interesting Fact:</strong> The authors also let the Transformer learn these encodings too and didn’t see any difference in performance as such. So, they went with the above idea as it doesn’t depend on sentence length and so even if the test sentence is bigger than train samples, we would be fine.</p><h3 id=d-add-and-normalize>D. Add and Normalize</h3><p>Another thing, that I didn’t mention for the sake of simplicity while explaining the encoder is that the encoder(the decoder architecture too) architecture has skip level residual connections(something akin to resnet50) also. So, the exact encoder architecture in the paper looks like below. Simply put, it helps traverse information for a much greater length in a Deep Neural Network. This can be thought of as akin(intuitively) to information passing in an organization where you have access to your manager as well as to your manager’s manager.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/15_hu08fef8dba1d9273c744d53408caeff4b_39526_500x0_resize_box_2.png 500w
, /images/transformers/15_hu08fef8dba1d9273c744d53408caeff4b_39526_800x0_resize_box_2.png 800w
, /images/transformers/15_hu08fef8dba1d9273c744d53408caeff4b_39526_1200x0_resize_box_2.png 1200w
, /images/transformers/15_hu08fef8dba1d9273c744d53408caeff4b_39526_1500x0_resize_box_2.png 1500w" src=/images/transformers/15.png alt="The Skip level connections help information flow in the network"><figcaption>The Skip level connections help information flow in the network</figcaption></figure></p><hr><h2 id=2-decoder-architecture>2. Decoder Architecture</h2><p><em><strong>Q: Okay, so till now we have learned that an encoder takes an input sentence and encodes its information in a matrix of size SxD(4x512). That’s all great but how does it help the decoder decode it to German?</strong></em></p><p>Good things come to those who wait. So, before understanding how the decoder does that, let us understand the decoder stack.</p><p>The decoder stack contains 6 decoder layers in a stack (As given in the paper again) and each decoder in the stack is comprised of these main three layers:</p><ul><li><p><strong>Masked multi-head self-attention Layer</strong></p></li><li><p><strong>multi-head self-attention Layer, and</strong></p></li><li><p><strong>a position-wise fully connected feed-forward network</strong></p></li></ul><p>It also has the same positional encoding as well as the skip level connection as well. We already know how the multi-head attention and feed-forward network layers work, so we will get straight into what is different in the decoder as compared to the encoder.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/16_huf4611689821365a04fedacfa295c8e0d_185260_500x0_resize_box_2.png 500w
, /images/transformers/16_huf4611689821365a04fedacfa295c8e0d_185260_800x0_resize_box_2.png 800w
, /images/transformers/16_huf4611689821365a04fedacfa295c8e0d_185260_1200x0_resize_box_2.png 1200w
, /images/transformers/16_huf4611689821365a04fedacfa295c8e0d_185260_1500x0_resize_box_2.png 1500w" src=/images/transformers/16.png alt="Decoder Architecture"><figcaption>Decoder Architecture</figcaption></figure></p><p><em><strong>Q: Wait, but do I see the output we need flowing into the decoder as input? What? Why?</strong></em> 😖</p><p>I am noticing that you are getting pretty good at asking questions. And that is a great question, something I even though myself a lot of times, and something that I hope will get much clearer by the time you reach the end of this post.</p><p>But to give an intuition, we can think of a transformer as a conditional language model in this case. A model that predicts the next word given an input word and an English sentence on which to condition upon or base its prediction on.</p><p>Such models are inherently sequential as in how would you train such a model? You start by giving the start token(<code>&lt;s></code>) and the model predicts the first word conditioned on the English sentence. You change the weights based on if the prediction is right or wrong. Then you give the start token and the first word (<code>&lt;s> der</code>) and the model predicts the second word. You change weights again. And so on.</p><p>The transformer decoder learns just like that but the beauty is that it doesn’t do that in a sequential manner. It uses masking to do this calculation and thus takes the whole output sentence (although shifted right by adding a <code>&lt;s></code> token to the front) while training. Also, please note that at prediction time we won’t give the output to the network</p><p><em><strong>Q: But, how does this masking exactly work?</strong></em></p><hr><h2 id=a-masked-multi-head-self-attention-layer>A) Masked Multi-Head Self Attention Layer</h2><p>It works, as usual, you wear it I mean <strong>😷</strong>. Kidding aside, as you can see that this time we have a <strong>Masked</strong> Multi-Head attention Layer in our decoder. This means that we will mask our shifted output (that is the input to the decoder) in a way that the network is never able to see the subsequent words since otherwise, it can easily copy that word while training.</p><p>So, how does the mask exactly work in the masked attention layer? If you remember, in the attention layer we multiplied the query(Q) and keys(K) and divided them by sqrt(d) before taking the softmax.</p><p>In a masked attention layer, though, we add the resultant matrix before the softmax(which will be of shape (TxT)) to a masking matrix.</p><p>So, In a masked layer, the function changes from:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/17_hub0f66815cbd700fa45e8a7a0a0ae93c2_43984_500x0_resize_box_2.png 500w
, /images/transformers/17_hub0f66815cbd700fa45e8a7a0a0ae93c2_43984_800x0_resize_box_2.png 800w
, /images/transformers/17_hub0f66815cbd700fa45e8a7a0a0ae93c2_43984_1200x0_resize_box_2.png 1200w
, /images/transformers/17_hub0f66815cbd700fa45e8a7a0a0ae93c2_43984_1500x0_resize_box_2.png 1500w" src=/images/transformers/17.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><em><strong>Q: I still don’t get it, what happens if we do that?</strong></em></p><p>That’s understandable actually. Let me break it in steps. So, our resultant matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers can be big as softmax not applied yet)</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/18.png alt="Schnelle currently attends to both Braune and Fuchs"><figcaption>Schnelle currently attends to both Braune and Fuchs</figcaption></figure></p><p>The word Schnelle will now be composed of both Braune and Fuchs if we take the above matrix’s softmax and multiply it with the value matrix V. But we don’t want that, so we add the mask matrix to it to give:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/19_hue1a990183c963952877e960b0d2df2d7_56805_500x0_resize_box_2.png 500w
, /images/transformers/19_hue1a990183c963952877e960b0d2df2d7_56805_800x0_resize_box_2.png 800w
, /images/transformers/19_hue1a990183c963952877e960b0d2df2d7_56805_1200x0_resize_box_2.png 1200w
, /images/transformers/19_hue1a990183c963952877e960b0d2df2d7_56805_1500x0_resize_box_2.png 1500w" src=/images/transformers/19.png alt="The mask operation applied to the matrix."><figcaption>The mask operation applied to the matrix</figcaption></figure></p><p>And, now what will happen after we do the softmax step?</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/20.png alt="Schnelle never attends to any word after Schnelle."><figcaption>Schnelle never attends to any word after Schnelle</figcaption></figure></p><p>Since $e^{-inf}$ = 0, all positions subsequent to Schnelle have been converted to 0. Now, if we multiply this matrix with the value matrix V, the vector corresponding to Schnelle’s position in the Z vector passing through the decoder would not contain any information of the subsequent words Braune and Fuchs just like we wanted.</p><p>And that is how the transformer takes the whole shifted output sentence at once and doesn’t learn in a sequential manner. Pretty neat I must say.</p><p><em><strong>Q: Are you kidding me? That’s actually awesome.</strong></em></p><p>So glad that you are still with me and you appreciate it. Now, coming back to the decoder. The next layer in the decoder is:</p><h3 id=b-multi-headed-attention-layer>B) Multi-Headed Attention Layer</h3><p>As you can see in the decoder architecture, a Z vector(Output of encoder) flows from the encoder to the multi-head attention layer in the Decoder. This Z output from the last encoder has a special name and is often called as memory. The attention layer takes as input both the encoder output and data flowing from below(shifted outputs) and uses attention. The Query vector Q is created from the data flowing in the decoder, while the Key(K) and value(V) vectors come from the encoder output.</p><p><em><strong>Q: Isn’t there any mask here?</strong></em></p><p>No, there is no mask here. The output coming from below is already masked and this allows every position in the decoder to attend over all the positions in the Value vector. So for every word position to be generated the decoder has access to the whole English sentence.</p><p>Here is a single attention layer(which will be part of a multi-head just like before):</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/21_hu8862062644998d3392260ec7496b119b_160822_500x0_resize_box_2.png 500w
, /images/transformers/21_hu8862062644998d3392260ec7496b119b_160822_800x0_resize_box_2.png 800w
, /images/transformers/21_hu8862062644998d3392260ec7496b119b_160822_1200x0_resize_box_2.png 1200w
, /images/transformers/21_hu8862062644998d3392260ec7496b119b_160822_1500x0_resize_box_2.png 1500w" src=/images/transformers/21.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><em><strong>Q: But won’t the shapes of Q, K, and V be different this time?</strong></em></p><p>You can look at the figure where I have done all the weights calculation. I would also ask you to see the shapes of the resultant Z vector and how our weight matrices until now never used the target or source sentence length in any of their dimensions. Normally, the shape cancels away in all our matrix calculations. For example, see how the S dimension cancels away in calculation 2 above. That is why while selecting the batches during training the authors talk about tight batches. That is in a batch all source sentences have similar lengths. And different batches could have different source lengths.</p><p>I will now talk about the skip level connections and the feed-forward layer. They are actually the same as in . . . .</p><p><strong><em>Q: Ok, I get it. We have the skip level connections and the FF layer and get a matrix of shape TxD after this whole decode operation.</em> <em>But where is the German translation?</em></strong></p><hr><h2 id=3-output-head>3. Output Head</h2><p>We are actually very much there now friend. Once, we are done with the transformer, the next thing is to add a task-specific output head on the top of the decoder output. This can be done by adding some linear layers and softmax on top to get the probability <em>across all the words in the german vocab</em>. We can do something like this:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/22_hu4496c7dff9a8d915378f6824e60a1512_53899_500x0_resize_box_2.png 500w
, /images/transformers/22_hu4496c7dff9a8d915378f6824e60a1512_53899_800x0_resize_box_2.png 800w
, /images/transformers/22_hu4496c7dff9a8d915378f6824e60a1512_53899_1200x0_resize_box_2.png 1200w
, /images/transformers/22_hu4496c7dff9a8d915378f6824e60a1512_53899_1500x0_resize_box_2.png 1500w" src=/images/transformers/22.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>As you can see we are able to generate probabilities. So far we know how to do a forward pass through this Transformer architecture. Let us see how we do the training of such a Neural Net Architecture.</p><hr><h2 id=training>Training:</h2><p>Till now, if we take a bird-eye view of the structure we have something like:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/23_hu8d6bababec466b4a851908ea3905d1be_50672_500x0_resize_box_2.png 500w
, /images/transformers/23_hu8d6bababec466b4a851908ea3905d1be_50672_800x0_resize_box_2.png 800w
, /images/transformers/23_hu8d6bababec466b4a851908ea3905d1be_50672_1200x0_resize_box_2.png 1200w
, /images/transformers/23_hu8d6bababec466b4a851908ea3905d1be_50672_1500x0_resize_box_2.png 1500w" src=/images/transformers/23.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We can give an English sentence and shifted output sentence and do a forward pass and get the probabilities over the German vocabulary. And thus we should be able to use a loss function like cross-entropy where the target could be the german word we want, and train the neural network using the Adam Optimizer. Just like any classification example. So, there is your German.</p><p>In the paper though, the authors use slight variations of optimizers and loss. You can choose to skip the below 2 sections on KL Divergence Loss and Learning rate schedule with Adam if you want as it is done only to churn out more performance out of the model and not an inherent part of the Transformer architecture as such.</p><p><em><strong>Q: I have been here for such a long time and have I complained?</strong></em> 😒</p><p>Okay. Okay. I get you. Let’s do it then.</p><h3 id=a-kl-divergence-with-label-smoothing>A) KL Divergence with Label Smoothing:</h3><p>KL Divergence is the information loss that happens when the distribution P is approximated by the distribution Q. When we use the KL Divergence loss, we try to estimate the target distribution(P) using the probabilities(Q) we generate from the model. And we try to minimize this information loss in the training.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/transformers/24.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>If you notice, in this form(without label smoothing which we will discuss) this is exactly the same as cross-entropy. Given two distributions like below.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/sbs1_hue1205c6444888a1c690f039ad48124ea_25365_500x0_resize_box_2.png 500w
, /images/transformers/sbs1_hue1205c6444888a1c690f039ad48124ea_25365_800x0_resize_box_2.png 800w
, /images/transformers/sbs1_hue1205c6444888a1c690f039ad48124ea_25365_1200x0_resize_box_2.png 1200w" src=/images/transformers/sbs1.png alt="Target distribution and probability distribution for a word(token)"><figcaption>Target distribution and probability distribution for a word(token)</figcaption></figure></p><p>The KL Divergence formula just plain gives <code>-logq(oder)</code> and that is the cross-entropy loss.</p><p>In the paper, though the authors used label smoothing with α = 0.1 and so the KL Divergence loss is not cross-entropy. What that means is that in the target distribution the output value is substituted by (1-α) and the remaining 0.1 is distributed across all the words. The authors say that this is so that the model is not too confident.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/sbs2_hu89a082fb3e6e153dfb1abc58128de5cf_25475_500x0_resize_box_2.png 500w
, /images/transformers/sbs2_hu89a082fb3e6e153dfb1abc58128de5cf_25475_800x0_resize_box_2.png 800w
, /images/transformers/sbs2_hu89a082fb3e6e153dfb1abc58128de5cf_25475_1200x0_resize_box_2.png 1200w" src=/images/transformers/sbs2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p><em><strong>Q: But, why do we make our models not confident? It seems absurd.</strong></em></p><p>Yes, it does but intuitively, you can think of it as when we give the target as 1 to our loss function, we have no doubts that the true label is True and others are not. But vocabulary is inherently a non-standardized target. For example, who is to say that you cannot use good in place of great? So we add some confusion in our labels so our model is not too rigid.</p><h3 id=b-a-particular-learning-rate-schedule-with-adam>B) A particular Learning Rate schedule with Adam</h3><p>The authors use a learning rate scheduler to increase the learning rate until warmup steps and then decrease it using the below function. And they used the Adam optimizer with $\beta_1$ = 0.9, $\beta_2$ = 0.98. Nothing too interesting here just some learning choices.</p><p>Source:<figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/transformers/29_hu9a562dd6fcb33460ffaf690089291135_16067_500x0_resize_box_2.png 500w" src=/images/transformers/29.png alt='<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="nofollow noopener">Paper</a>'><figcaption>Paper : https://arxiv.org/pdf/1706.03762.pdf</figcaption></figure></p><p><em><strong>Q: But wait I just remembered that we won’t have the shifted output at the prediction time, would we? How do we do predictions then?</strong></em></p><p>If you realize what we have at this point is a generative model and we will have to do the predictions in a generative way as we won’t know the output target vector when doing prediction. So predictions are still sequential.</p><hr><h2 id=prediction-time>Prediction Time</h2><p><figure><img src=/images/transformers/30.gif alt="Predicting with a greedy search using the Transformer"><figcaption>Predicting with a greedy search using the Transformer</figcaption></figure></p><p>This model does piece-wise predictions. In the original paper, they use the Beam Search to do prediction. But a greedy search would work fine as well for the purpose of explaining it. In the above example, I have shown how a greedy search would work exactly. The greedy search would start with:</p><ul><li><p>Passing the whole English sentence as encoder input and just the start token <code>&lt;st></code> as shifted output(input to the decoder) to the model and doing the forward pass.</p></li><li><p>The model will predict the next word — <code>der</code></p></li><li><p>Then, we pass the whole English sentence as encoder input and add the last predicted word to the shifted output(input to the decoder = <code>&lt;st> der</code>) and do the forward pass.</p></li><li><p>The model will predict the next word — <code>schnelle</code></p></li><li><p>Passing the whole English sentence as encoder input and <code>&lt;st> der schnelle</code> as shifted output(input to the decoder) to the model and doing the forward pass.</p></li><li><p>and so on, until the model predicts the end token <code>&lt;/s></code> or we generate some maximum number of tokens(something we can define) so the translation doesn’t run for an infinite duration in any case it breaks.</p></li></ul><h3 id=beam-search><strong>Beam Search:</strong></h3><p><em><strong>Q: Now I am greedy, Tell me about beam search as well.</strong></em></p><p>Okay, the beam search idea is inherently very similar to the above idea. In beam search, we don’t just look at the highest probability word generated but the top two words.</p><p>So, For example, when we gave the whole English sentence as encoder input and just the start token as shifted output, we get two best words as <code>i</code>(p=0.6) and <code>der</code>(p=0.3). We will now generate the output model for both output sequences,<code>&lt;s> i</code> and <code>&lt;s> der</code> and look at the probability of the next top word generated. For example, if <code>&lt;s> i</code> gave a probability of (p=0.05) for the next word and<code> &lt;s> der</code> gave (p=0.5) for the next predicted word, we discard the sequence <code>&lt;s> i</code> and go with <code>&lt;s> der</code> instead, as the sum of probability of sentence is maximized(<code>&lt;s> der next_word_to_der</code> p = 0.3+0.5 compared to <code>&lt;s> i next_word_to_i</code> p = 0.6+0.05). We then repeat this process to get the sentence with the highest probability.</p><p>Since we used the top 2 words, the beam size is 2 for this Beam Search. In the paper, they used beam search of size 4.</p><p><strong>PS</strong>: I showed that the English sentence is passed at every step for brevity, but in practice, the output of the encoder is saved and only the shifted output passes through the decoder at each time step.</p><p><em><strong>Q: Anything else you forgot to tell me? I will let you have your moment.</strong></em></p><p>Yes. Since you asked. Here it is:</p><h3 id=bpe-weight-sharing-and-checkpointing>BPE, Weight Sharing and Checkpointing</h3><p>In the paper, the authors used Byte pair encoding to create a common English German vocabulary. They then used shared weights across both the English and german embedding and pre-softmax linear transformation as the embedding weight matrix shape would work (Vocab Length X D).</p><p>Also, the authors average the last k checkpoints to create an ensembling effect to reach the performance*.* This is a pretty known technique where we average the weights in the last few epochs of the model to create a new model which is sort of an ensemble.</p><p><em><strong>Q: Can you show me some code?</strong></em></p><p>This post has already been so long, so I will do that in the
<a href=https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/>next post</a>
. Stay tuned.</p><p><em><strong>Now, finally, my turn to ask the question: Did you get how a transformer works? Yes, or No, you can answer in the comments. :)</strong></em></p><hr><h2 id=references>References</h2><ul><li><p><a href=https://arxiv.org/abs/1706.03762 target=_blank rel="nofollow noopener">Attention Is All You Need</a>
: The Paper which started it all.</p></li><li><p><a href=https://nlp.seas.harvard.edu/2018/04/03/attention.html target=_blank rel="nofollow noopener">The Annotated Transformer</a>
: This one has all the code. Although I will write a simple transformer in the next post too.</p></li><li><p><a href=http://jalammar.github.io/illustrated-transformer/ target=_blank rel="nofollow noopener">The Illustrated Transformer</a>
: This is one of the best posts on transformers.</p></li></ul><p>In this post, I covered how the Transformer architecture works. If you want to learn more about NLP, I would like to call out an excellent course on
<a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11503135394&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" target=_blank rel="nofollow noopener">**Natural Language Processing</a>
** from the Advanced Machine Learning Specialization. Do check it out.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at
<a href=https://medium.com/@rahul_agarwal target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><p>And, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><p>This post was first published
<a href=https://lionbridge.ai/articles/what-are-transformer-models-in-machine-learning/ target=_blank rel="nofollow noopener">here</a></p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>