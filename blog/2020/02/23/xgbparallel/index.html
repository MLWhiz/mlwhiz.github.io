<!doctype html><html lang=en-us><head><meta charset=utf-8><title>MLWhiz: Helping You Learn Data Science!</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This post is about running XGBoost on Multi-GPU machines."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.76.5"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Lightning Fast XGBoost on Multiple GPUs"><meta property="og:description" content="This post is about running XGBoost on Multi-GPU machines."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/02/23/xgbparallel/"><meta property="og:image" content="https://mlwhiz.com/images/xgbparallel/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/xgbparallel/main.png"><meta property="article:published_time" content="2020-02-23T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-26T16:23:35+05:30"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="programming"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/xgbparallel/main.png"><meta name=twitter:title content="Lightning Fast XGBoost on Multiple GPUs"><meta name=twitter:description content="This post is about running XGBoost on Multi-GPU machines."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/02/23/xgbparallel/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/02/23/xgbparallel/#primaryimage","url":"https://mlwhiz.com/images/xgbparallel/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/02/23/xgbparallel/#webpage","url":"https://mlwhiz.com/blog/2020/02/23/xgbparallel/","inLanguage":"en-US","name":"Lightning Fast XGBoost on Multiple GPUs - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/02/23/xgbparallel/#primaryimage"},"datePublished":"2020-02-23T00:00:00.00Z","dateModified":"2020-09-26T16:23:35.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"This post is about running XGBoost on Multi-GPU machines."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');fbq('init','402633927768628');fbq('track','PageView');</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="MLWhiz: Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/data-science class=categoryStyle>Data Science</a>
<a href=/categories/programming class=categoryStyle>Programming</a><h1>Lightning Fast XGBoost on Multiple GPUs</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>23 February 2020</span></div><img src=https://mlwhiz.com/images/xgbparallel/main.png class="img-fluid w-100 mb-4" alt="Lightning Fast XGBoost on Multiple GPUs"><div class="content mb-5"><p>XGBoost is one of the most used libraries fora data science.</p><p>At the time XGBoost came into existence, it was lightning fast compared to its nearest rival Python’s Scikit-learn GBM. But as the times have progressed, it has been rivaled by some awesome libraries like LightGBM and Catboost, both on speed as well as accuracy.</p><p>I, for one, use LightGBM for most of the use cases where I have just got CPU for training. But when I have a GPU or multiple GPUs at my disposal, I still love to train with XGBoost.</p><p>Why?</p><p>So I could make use of the excellent GPU Capabilities provided by XGBoost in conjunction with Dask to use XGBoost in both single and multi-GPU mode.</p><p>How?</p><p><em><strong>This post is about running XGBoost on Multi-GPU machines.</strong></em></p><hr><h2 id=dataset>Dataset:</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/xgbparallel/0_hue2eabb8d50b486a9453082344b292989_37246_500x0_resize_box_2.png 500w
, /images/xgbparallel/0_hue2eabb8d50b486a9453082344b292989_37246_800x0_resize_box_2.png 800w
, /images/xgbparallel/0_hue2eabb8d50b486a9453082344b292989_37246_1200x0_resize_box_2.png 1200w" src=/images/xgbparallel/0.png alt="UCI Higgs"></p><p>We are going to be using the
<a href=https://archive.ics.uci.edu/ml/datasets/HIGGS target=_blank rel="nofollow noopener">UCI Higgs dataset</a>
. This is a binary classification problem with 11M rows and 29 columns and can take a considerable time to solve.</p><p>From the UCI Site:</p><blockquote><p>The data has been produced using Monte Carlo simulations. The first 21 features (columns 2–22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.</p></blockquote><p>We can load this dataset into memory by using the nifty function that I borrow from
<a href=https://devblogs.nvidia.com/gradient-boosting-decision-trees-xgboost-cuda/ target=_blank rel="nofollow noopener">this NVidia post</a>
.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>if</span> sys<span style=color:#f92672>.</span>version_info[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>3</span>:
    <span style=color:#f92672>from</span> urllib.request <span style=color:#f92672>import</span> urlretrieve
<span style=color:#66d9ef>else</span>:
    <span style=color:#f92672>from</span> urllib <span style=color:#f92672>import</span> urlretrieve

data_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz&#34;</span>
dmatrix_train_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;higgs_train.dmatrix&#34;</span>
dmatrix_test_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;higgs_test.dmatrix&#34;</span>
csv_filename <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;HIGGS.csv.gz&#34;</span>
train_rows <span style=color:#f92672>=</span> <span style=color:#ae81ff>10500000</span>
test_rows <span style=color:#f92672>=</span> <span style=color:#ae81ff>500000</span>
num_round <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>

plot <span style=color:#f92672>=</span> True

<span style=color:#75715e># return xgboost dmatrix</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_higgs</span>():
    <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>isfile(dmatrix_train_filename) <span style=color:#f92672>and</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>isfile(dmatrix_test_filename):           
        dtrain <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(dmatrix_train_filename)
        dtest <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(dmatrix_test_filename)
        <span style=color:#66d9ef>if</span> dtrain<span style=color:#f92672>.</span>num_row() <span style=color:#f92672>==</span> train_rows <span style=color:#f92672>and</span> dtest<span style=color:#f92672>.</span>num_row() <span style=color:#f92672>==</span> test_rows:
            <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Loading cached dmatrix...&#34;</span>)
            <span style=color:#66d9ef>return</span> dtrain, dtest

    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>isfile(csv_filename):
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Downloading higgs file...&#34;</span>)
        urlretrieve(data_url, csv_filename)

    df_higgs_train <span style=color:#f92672>=</span> pandas<span style=color:#f92672>.</span>read_csv(csv_filename, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32,
                                     nrows<span style=color:#f92672>=</span>train_rows, header<span style=color:#f92672>=</span>None)
    dtrain <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(df_higgs_train<span style=color:#f92672>.</span>ix[:, <span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>29</span>], df_higgs_train[<span style=color:#ae81ff>0</span>])
    dtrain<span style=color:#f92672>.</span>save_binary(dmatrix_train_filename)
    df_higgs_test <span style=color:#f92672>=</span> pandas<span style=color:#f92672>.</span>read_csv(csv_filename, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32,
                                    skiprows<span style=color:#f92672>=</span>train_rows, nrows<span style=color:#f92672>=</span>test_rows,
                                    header<span style=color:#f92672>=</span>None)
    dtest <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(df_higgs_test<span style=color:#f92672>.</span>ix[:, <span style=color:#ae81ff>1</span>:<span style=color:#ae81ff>29</span>], df_higgs_test[<span style=color:#ae81ff>0</span>])
    dtest<span style=color:#f92672>.</span>save_binary(dmatrix_test_filename)

    <span style=color:#66d9ef>return</span> dtrain, dtest

dtrain, dtest <span style=color:#f92672>=</span> load_higgs()
</code></pre></div><p>This function downloads the Higgs dataset and creates Dmatrix objects for later XGBoost use.</p><hr><h2 id=xgboost-the-cpu-method>XGBoost: The CPU Method</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/xgbparallel/1_huf6ec92059a61361b00be5f1b38c3cce4_86964_500x0_resize_box_2.png 500w" src=/images/xgbparallel/1.png alt='<a href="https://pixabay.com/illustrations/processor-cpu-computer-chip-board-2217771/" target="_blank" rel="nofollow noopener">Source</a>'></p><p>As we have the data loaded, we can train the XGBoost model with CPU for benchmarking purposes.</p><pre><code>print(&quot;Training with CPU ...&quot;)
param = {}
param['objective'] = 'binary:logitraw'
param['eval_metric'] = 'error'
param['silent'] = 1
param['tree_method'] = 'hist'

tmp = time.time()
cpu_res = {}
xgb.train(param, dtrain, num_round, evals=[(dtest, &quot;test&quot;)],
          evals_result=cpu_res)
cpu_time = time.time() - tmp
print(&quot;CPU Training Time: %s seconds&quot; % (str(cpu_time)))

---------------------------------------------------------------

CPU Training Time: 717.6483490467072 seconds
</code></pre><p>This code takes 717 seconds, which is around 12 minutes to finish. That is great and commendable, but can we do better?</p><hr><h2 id=xgboost-the-single-gpu-method>XGBoost: The Single GPU Method</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/xgbparallel/2_hue71bc6b4f0da45625eab4d00571d9d01_555474_500x0_resize_box_2.png 500w
, /images/xgbparallel/2_hue71bc6b4f0da45625eab4d00571d9d01_555474_800x0_resize_box_2.png 800w
, /images/xgbparallel/2_hue71bc6b4f0da45625eab4d00571d9d01_555474_1200x0_resize_box_2.png 1200w" src=/images/xgbparallel/2.png alt='<a href="https://www.google.com/url?sa=i&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=images&amp;amp;cd=&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwiKwMPphI3nAhXtzzgGHeLNB1QQjhx6BAgBEAI&amp;amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;amp;psig=AOvVaw3QFPz_lTQrqraHJQz_ewwh&amp;amp;ust=1579433051897517" target="_blank" rel="nofollow noopener">Source</a>'></p><p>What is great is that we don’t have to change a lot in the above code to be able to use a single GPU for our model building.</p><blockquote><p>Why use CPU when we can use GPU?</p></blockquote><p>We change the tree_method to gpu_hist</p><pre><code>print(&quot;Training with Single GPU ...&quot;)

param = {}
param['objective'] = 'binary:logitraw'
param['eval_metric'] = 'error'
param['silent'] = 1
param['tree_method'] = 'gpu_hist'
tmp = time.time()
gpu_res = {}

xgb.train(param, dtrain, num_round, evals=[(dtest, &quot;test&quot;)],
          evals_result=gpu_res)
gpu_time = time.time() - tmp
print(&quot;GPU Training Time: %s seconds&quot; % (str(gpu_time)))

----------------------------------------------------------------
GPU Training Time: 78.2187008857727 seconds
</code></pre><p>And <em><strong>we achieve a 10x speedup</strong></em> with our model now finishing in 1.3 minutes. That is great, but can we do even better if we have multiple GPUs?</p><hr><h2 id=xgboost-the-multi-gpu-method>XGBoost: The Multi GPU Method</h2><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/xgbparallel/3_hu9f89ce971ee72c6b88af7b96b6bc4de0_30630_500x0_resize_box_2.png 500w" src=/images/xgbparallel/3.png alt='<a href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.nvidia.com%2Fcontent%2Fdam%2Fen-zz%2FSolutions%2Ftitan%2Ftitan-rtx%2Fnvidia-titan-rtx-nvlink-300-t%402x.jpg&amp;amp;imgrefurl=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdeep-learning-ai%2Fproducts%2Ftitan-rtx%2F&amp;amp;docid=LXUW4PiXbaOvoM&amp;amp;tbnid=2aobaeCTl1m0CM%3A&amp;amp;vet=10ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg..i&amp;amp;w=600&amp;amp;h=408&amp;amp;client=ubuntu&amp;amp;bih=2025&amp;amp;biw=1879&amp;amp;q=gpu%20titan%20rtx%20dual&amp;amp;ved=0ahUKEwi2uovhhI3nAhXVcn0KHf9pB_YQMwhLKAIwAg&amp;amp;iact=mrc&amp;amp;uact=8" target="_blank" rel="nofollow noopener">Source</a>'></p><p>I have, for example, 2 GPUs in my machine while the above code utilizes only 1 GPU. With GPU’s getting a lot cheaper now, it is not unusual for clusters to have more than 4 GPUs. So can we use multiple GPUs simultaneously?</p><blockquote><p>Two GPUs are always better than one</p></blockquote><p>To use MultiGPUs, the process is not so simple as to add a little argument as above, and there are a few steps involved.</p><p>The first is the difference in Data loading:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_higgs_for_dask</span>(client):
    <span style=color:#75715e># 1. read the CSV File using Pandas</span>
    df_higgs_train <span style=color:#f92672>=</span> pandas<span style=color:#f92672>.</span>read_csv(csv_filename, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32,
                                     nrows<span style=color:#f92672>=</span>train_rows, header<span style=color:#f92672>=</span>None)<span style=color:#f92672>.</span>ix[:, <span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>30</span>]
    df_higgs_test <span style=color:#f92672>=</span> pandas<span style=color:#f92672>.</span>read_csv(csv_filename, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32,
                                    skiprows<span style=color:#f92672>=</span>train_rows, nrows<span style=color:#f92672>=</span>test_rows,
                                    header<span style=color:#f92672>=</span>None)<span style=color:#f92672>.</span>ix[:, <span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>30</span>]

    <span style=color:#75715e># 2. Create a Dask Dataframe from Pandas Dataframe.</span>
    ddf_higgs_train <span style=color:#f92672>=</span> dask<span style=color:#f92672>.</span>dataframe<span style=color:#f92672>.</span>from_pandas(df_higgs_train, npartitions<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
    ddf_higgs_test <span style=color:#f92672>=</span> dask<span style=color:#f92672>.</span>dataframe<span style=color:#f92672>.</span>from_pandas(df_higgs_test, npartitions<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
    ddf_y_train <span style=color:#f92672>=</span> ddf_higgs_train[<span style=color:#ae81ff>0</span>]
    <span style=color:#66d9ef>del</span> ddf_higgs_train[<span style=color:#ae81ff>0</span>]
    ddf_y_test <span style=color:#f92672>=</span> ddf_higgs_test[<span style=color:#ae81ff>0</span>]
    <span style=color:#66d9ef>del</span> ddf_higgs_test[<span style=color:#ae81ff>0</span>]

    <span style=color:#75715e>#3. Create Dask DMatrix Object using dask dataframes</span>
    ddtrain <span style=color:#f92672>=</span> DaskDMatrix(client, ddf_higgs_train ,ddf_y_train)
    ddtest <span style=color:#f92672>=</span> DaskDMatrix(client, ddf_higgs_test ,ddf_y_test)

    <span style=color:#66d9ef>return</span> ddtrain, ddtest
</code></pre></div><p>There are multiple steps in data load as we need dask DMatrix objects to train XGBoost with multiple GPUs.</p><ol><li><p>Read the CSV File using Pandas.</p></li><li><p>Create a Dask Dataframe from Pandas Dataframe, and</p></li><li><p>Create Dask DMatrix Object using dask data frames.</p></li></ol><p>To use Multi-GPU for training XGBoost, we need to use Dask to create a GPU Cluster. This command creates a cluster of our GPUs that could be used by dask by using the client object later.</p><pre><code>cluster = LocalCUDACluster()
client = Client(cluster)
</code></pre><p>We can now load our Dask Dmatrix Objects and define the training parameters. Note nthread beings set to one and tree_method set to gpu_hist</p><pre><code>ddtrain, ddtest = load_higgs_for_dask(client)

param = {}
param['objective'] = 'binary:logitraw'
param['eval_metric'] = 'error'
param['silence'] = 1
param['tree_method'] = 'gpu_hist'
param['nthread'] = 1
</code></pre><p>We can now train on Multiple GPUs using:</p><pre><code>print(&quot;Training with Multiple GPUs ...&quot;)
tmp = time.time()
output = xgb.dask.train(client, param, ddtrain, num_boost_round=1000, evals=[(ddtest, 'test')])
multigpu_time = time.time() - tmp
bst = output['booster']
multigpu_res = output['history']
print(&quot;Multi GPU Training Time: %s seconds&quot; % (str(multigpu_time)))
---------------------------------------------------------------
Multi GPU Training Time: 50.08211898803711 seconds
</code></pre><p>Please note how the call to xgb.train changes to xgb.dask.train and how it also needs the dask client to work.</p><p>This took around 0.8 Minutes that is a 1.5x Speedup from Single GPU. I only had 2 GPUs at my disposal, so I can’t test it, but I believe that it increases linearly, i.e. more GPU and more reduction in time.</p><hr><h2 id=results>Results</h2><p>Here are the results of all three setups:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/xgbparallel/4_huf40ed266f394a1253cae7bd9663f6d2e_36799_500x0_resize_box_2.png 500w
, /images/xgbparallel/4_huf40ed266f394a1253cae7bd9663f6d2e_36799_800x0_resize_box_2.png 800w
, /images/xgbparallel/4_huf40ed266f394a1253cae7bd9663f6d2e_36799_1200x0_resize_box_2.png 1200w" src=/images/xgbparallel/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Although the difference between Multi and Single CPU looks redundant right now, it will be pretty considerable while running
<a href=https://towardsdatascience.com/automate-hyperparameter-tuning-for-your-models-71b18f819604 target=_blank rel="nofollow noopener">multiple hyperparameter tuning tasks</a>
at hand where one might need to run multiple GBM Models with different Hyperparams.</p><p>Also, this result can change when we scale it to many GPUs.</p><p>So keep scaling.</p><p>You can find the complete code for this post on
<a href=https://github.com/MLWhiz/data_science_blogs/blob/master/xgb_dask/XGB%20with%20Dask%2BGPU.ipynb target=_blank rel="nofollow noopener">Github</a>
.</p><hr><h2 id=continue-learning>Continue Learning</h2><p>If you are interested in Deep Learning and want to use your GPU for that, I would like to recommend this excellent course on
<a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Deep Learning in Computer Vision</a>
in the
<a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0" target=_blank rel="nofollow noopener">Advanced machine learning specialization</a>
.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" target=_blank rel="nofollow noopener">&lt;strong>Medium&lt;/strong></a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">&lt;strong>blog&lt;/strong></a></p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=759505.377&subid=0&type=4" rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mlwhiz"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD');kofiwidget2.draw();</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in Bangalore, where I am currently working with WalmartLabs .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>Sql</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://medium.com/@rahul_agarwal><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="MLWhiz: Helping You Learn Data Science!"></a></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Contact Me</h6><ul class=list-unstyled><li class=mb-3><i class="ti-location-pin mr-3 text-primary"></i>India, Bangalore</li><li class=mb-3><a class=text-dark href=mailto:rahul@mlwhiz.com><i class="ti-email mr-3 text-primary"></i>rahul@mlwhiz.com</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Social Contacts</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://www.linkedin.com/in/rahulagwl/>Linkedin</a></li><li class=mb-3><a class=text-dark href=https://medium.com/@rahul_agarwal>Medium</a></li><li class=mb-3><a class=text-dark href=https://twitter.com/MLWhiz>Twitter</a></li><li class=mb-3><a class=text-dark href=https://www.facebook.com/mlwhizblog>Facebook</a></li><li class=mb-3><a class=text-dark href=https://github.com/MLWhiz>Github</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Categories</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=/categories/awesome-guides>Awesome Guides</a></li><li class=mb-3><a class=text-dark href=/categories/big-data>Big Data</a></li><li class=mb-3><a class=text-dark href=/categories/computer-vision>Computer Vision</a></li><li class=mb-3><a class=text-dark href=/categories/data-science>Data Science</a></li><li class=mb-3><a class=text-dark href=/categories/deep-learning>Deep Learning</a></li><li class=mb-3><a class=text-dark href=/categories/learning-resources>Learning Resources</a></li><li class=mb-3><a class=text-dark href=/categories/natural-language-processing>Natural Language Processing</a></li><li class=mb-3><a class=text-dark href=/categories/programming>Programming</a></li></ul></div><div class="col-lg-3 col-sm-6 mb-5"><h6 class=mb-4>Quick Links</h6><ul class=list-unstyled><li class=mb-3><a class=text-dark href=https://mlwhiz.com/about>About</a></li><li class=mb-3><a class=text-dark href=https://mlwhiz.com/blog>Post</a></li></ul></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-54777926-1','auto');ga('send','pageview');</script></body></html>