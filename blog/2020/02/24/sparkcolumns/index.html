<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>5 Ways to add a new column in a PySpark Dataframe - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This post is going to be about Multiple ways to create a new column in Pyspark Dataframe"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="5 Ways to add a new column in a PySpark Dataframe - MLWhiz"><meta property="og:description" content="This post is going to be about Multiple ways to create a new column in Pyspark Dataframe"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2020/02/24/sparkcolumns/"><meta property="og:image" content="https://mlwhiz.com/images/sparkcolumns/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/sparkcolumns/main.png"><meta property="article:published_time" content="2020-02-24T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T16:18:05+01:00"><meta property="article:tag" content="Big Data"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/sparkcolumns/main.png"><meta name=twitter:title content="5 Ways to add a new column in a PySpark Dataframe - MLWhiz"><meta name=twitter:description content="This post is going to be about Multiple ways to create a new column in Pyspark Dataframe"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/favicon-200x200.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/favicon.png type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2020/02/24/sparkcolumns/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2020/02/24/sparkcolumns/#primaryimage","url":"https://mlwhiz.com/images/sparkcolumns/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2020/02/24/sparkcolumns/#webpage","url":"https://mlwhiz.com/blog/2020/02/24/sparkcolumns/","inLanguage":"en-US","name":"5 Ways to add a new column in a PySpark Dataframe - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2020/02/24/sparkcolumns/#primaryimage"},"datePublished":"2020-02-24T00:00:00.00Z","dateModified":"2023-07-07T16:18:05.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"This post is going to be about Multiple ways to create a new column in Pyspark Dataframe"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logo.png alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/big-data class=categoryStyle>Big Data</a>
<a href=/categories/data-science class=categoryStyle>Data Science</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>5 Ways to add a new column in a PySpark Dataframe</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>24 February 2020</span></div><img src=https://mlwhiz.com/images/sparkcolumns/main.png class="img-fluid w-100 mb-4" alt="5 Ways to add a new column in a PySpark Dataframe"><div class="content mb-5"><p><em><strong>Too much data is getting generated day by day.</strong></em></p><p>Although sometimes we can manage our big data using tools like
<a href="https://towardsdatascience.com/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=---------5------------------" target=_blank rel="nofollow noopener">Rapids</a>
or
<a href="https://towardsdatascience.com/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8?source=---------11------------------" target=_blank rel="nofollow noopener">Parallelization</a>
, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.</p><p>In my
<a href=https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a target=_blank rel="nofollow noopener">last post</a>
on Spark, I explained how to work with PySpark RDDs and Dataframes.</p><p>Although this post explains a lot on how to work with RDDs and basic Dataframe operations, I missed quite a lot when it comes to working with PySpark Dataframes.</p><p>And it is only when I required more functionality that I read up and came up with multiple solutions to do one single thing.</p><p><em><strong>How to create a new column in spark?</strong></em></p><p>Now, this might sound trivial, but believe me, it isn’t. With so much you might want to do with your data, I am pretty sure you will end up using most of these column creation processes in your workflow. Sometimes to utilize Pandas functionality, or occasionally to use RDDs based partitioning or sometimes to make use of the mature python ecosystem.</p><p><strong>This post is going to be about — “Multiple ways to create a new column in Pyspark Dataframe.”</strong></p><p>If you have PySpark installed, you can skip the Getting Started section below.</p><hr><h2 id=getting-started-with-spark>Getting Started with Spark</h2><p>I know that a lot of you won’t have spark installed in your system to try and learn. But installing Spark is a headache of its own.</p><p>Since we want to understand how it works and work with it, I would suggest that you use Spark on Databricks
<a href="https://databricks.com/try-databricks?utm_source=databricks&utm_medium=homev2tiletest" target=_blank rel="nofollow noopener">&lt;strong>here&lt;/strong></a>
online with the community edition. Don’t worry, it is free, albeit fewer resources, but that works for us right now for learning purposes.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/0_hu3cfe1af0b1e442ec633c1461f3c771d2_180736_500x0_resize_box_2.png 500w
, /images/sparkcolumns/0_hu3cfe1af0b1e442ec633c1461f3c771d2_180736_800x0_resize_box_2.png 800w
, /images/sparkcolumns/0_hu3cfe1af0b1e442ec633c1461f3c771d2_180736_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/0_hu3cfe1af0b1e442ec633c1461f3c771d2_180736_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/0.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Once you register and login will be presented with the following screen.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/1_hu71af21923c0723934f137c40b7a0ad14_150534_500x0_resize_box_2.png 500w
, /images/sparkcolumns/1_hu71af21923c0723934f137c40b7a0ad14_150534_800x0_resize_box_2.png 800w
, /images/sparkcolumns/1_hu71af21923c0723934f137c40b7a0ad14_150534_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/1_hu71af21923c0723934f137c40b7a0ad14_150534_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>You can start a new notebook here.</p><p>Select the Python notebook and give any name to your notebook.</p><p>Once you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.</p><p>The next step will be to check if the sparkcontext is present. To check if the sparkcontext is present, you have to run this command:</p><pre><code>sc
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/2_hu9054a8b2aaad7bd92bb6fb09737d2f8f_54326_500x0_resize_box_2.png 500w
, /images/sparkcolumns/2_hu9054a8b2aaad7bd92bb6fb09737d2f8f_54326_800x0_resize_box_2.png 800w
, /images/sparkcolumns/2_hu9054a8b2aaad7bd92bb6fb09737d2f8f_54326_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/2_hu9054a8b2aaad7bd92bb6fb09737d2f8f_54326_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>This means that we are set up with a notebook where we can run Spark.</p><hr><h2 id=data>Data</h2><p>Here, I will work on the Movielens
<a href=https://github.com/MLWhiz/data_science_blogs/tree/master/spark_post target=_blank rel="nofollow noopener">&lt;strong>ml-100k.zip&lt;/strong></a>
dataset. 100,000 ratings from 1000 users on 1700 movies. In this zipped folder, the file we will specifically work with is the rating file. This filename is kept as “u.data”</p><p>If you want to upload this data or any data, you can click on the Data tab in the left and then Add Data by using the GUI provided.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/3_hu9f65699aedb34fde43ccb91edcc0632f_80140_500x0_resize_box_2.png 500w
, /images/sparkcolumns/3_hu9f65699aedb34fde43ccb91edcc0632f_80140_800x0_resize_box_2.png 800w" src=/images/sparkcolumns/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We can then load the data using the following commands:</p><pre><code>ratings = spark.read.load(&quot;/FileStore/tables/u.data&quot;,format=&quot;csv&quot;, sep=&quot;\t&quot;, inferSchema=&quot;true&quot;, header=&quot;false&quot;)

ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])
</code></pre><p>Here is how it looks:</p><pre><code>ratings.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/4_huef5cd18fe015bca2567bc932ce288799_66108_500x0_resize_box_2.png 500w
, /images/sparkcolumns/4_huef5cd18fe015bca2567bc932ce288799_66108_800x0_resize_box_2.png 800w" src=/images/sparkcolumns/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Ok, so now we are set up to begin the part we are interested in finally. How to create a new column in PySpark Dataframe?</p><hr><h2 id=1-using-spark-native-functions>1. Using Spark Native Functions</h2><p>Photo by
<a href="https://unsplash.com/@andrewjamesphoto?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Andrew James</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/5_hub7ffba8203c9fc61ba5456f97b2ac0eb_4501599_500x0_resize_box_2.png 500w
, /images/sparkcolumns/5_hub7ffba8203c9fc61ba5456f97b2ac0eb_4501599_800x0_resize_box_2.png 800w
, /images/sparkcolumns/5_hub7ffba8203c9fc61ba5456f97b2ac0eb_4501599_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/5_hub7ffba8203c9fc61ba5456f97b2ac0eb_4501599_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/5.png alt='

<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>
'></p><p>The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.</p><p>We can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. We can import spark functions as:</p><pre><code>import pyspark.sql.functions as F
</code></pre><p>Our first function, the F.col function gives us access to the column. So if we wanted to multiply a column by 2, we could use F.col as:</p><pre><code>ratings_with_scale10 = ratings.withColumn(&quot;ScaledRating&quot;, 2*F.col(&quot;rating&quot;))

ratings_with_scale10.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/6_huee9e0f57c0e83dfe343f50dcde6a5afc_69218_500x0_resize_box_2.png 500w
, /images/sparkcolumns/6_huee9e0f57c0e83dfe343f50dcde6a5afc_69218_800x0_resize_box_2.png 800w" src=/images/sparkcolumns/6.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We can also use math functions like F.exp function:</p><pre><code>ratings_with_exp = ratings.withColumn(&quot;expRating&quot;, 2*F.exp(&quot;rating&quot;))

ratings_with_exp.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/7_hu83dca471c8ceea840ad2a52631e16d82_91232_500x0_resize_box_2.png 500w
, /images/sparkcolumns/7_hu83dca471c8ceea840ad2a52631e16d82_91232_800x0_resize_box_2.png 800w" src=/images/sparkcolumns/7.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list
<a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions target=_blank rel="nofollow noopener">here</a>
.</p><hr><h2 id=2-spark-udfs>2. Spark UDFs</h2><p>Photo by
<a href="https://unsplash.com/@divide_by_zero?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Divide By Zero</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/8_hu8721b3b3ac1e4b196e92b30de457281f_1154091_500x0_resize_box_2.png 500w
, /images/sparkcolumns/8_hu8721b3b3ac1e4b196e92b30de457281f_1154091_800x0_resize_box_2.png 800w
, /images/sparkcolumns/8_hu8721b3b3ac1e4b196e92b30de457281f_1154091_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/8_hu8721b3b3ac1e4b196e92b30de457281f_1154091_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/8.png alt='

<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>
'></p><p>Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I want to use the more matured Python functionality.</p><p>To use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()</p><pre><code>import pyspark.sql.functions as F
from pyspark.sql.types import *

def somefunc(value):
  if   value &lt; 3:
      return 'low'
  else:
      return 'high'

#convert to a UDF Function by passing in the function and return type of function

udfsomefunc = F.udf(somefunc, StringType())

ratings_with_high_low = ratings.withColumn(&quot;high_low&quot;, udfsomefunc(&quot;rating&quot;))

ratings_with_high_low.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/9_hu218fe8e49bef451519d1147cf7ace8b3_72302_500x0_resize_box_2.png 500w
, /images/sparkcolumns/9_hu218fe8e49bef451519d1147cf7ace8b3_72302_800x0_resize_box_2.png 800w" src=/images/sparkcolumns/9.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=3-using-rdds>3. Using RDDs</h2><p>Photo by
<a href="https://unsplash.com/@ryanquintal?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Ryan Quintal</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/10_hu1324e43b6a555132ed0b4b9a015ceb31_1798107_500x0_resize_box_2.png 500w
, /images/sparkcolumns/10_hu1324e43b6a555132ed0b4b9a015ceb31_1798107_800x0_resize_box_2.png 800w
, /images/sparkcolumns/10_hu1324e43b6a555132ed0b4b9a015ceb31_1798107_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/10_hu1324e43b6a555132ed0b4b9a015ceb31_1798107_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/10.png alt='

<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>
'></p><p>Sometimes both the spark UDFs and SQL Functions are not enough for a particular use-case. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs. You can use this one, mainly when you need access to all the columns in the spark data frame inside a python function.</p><p>Whatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem.</p><p>The process below makes use of the functionality to convert between Row and pythondict objects. We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again.</p><pre><code>import math
from pyspark.sql import Row

def rowwise_function(row):
  # convert row to dict:
  row_dict = row.asDict()
  # Add a new key in the dictionary with the new column name and value.
  row_dict['Newcol'] = math.exp(row_dict['rating'])
  # convert dict to row:
  newrow = Row(**row_dict)
  # return new row
  return newrow

# convert ratings dataframe to RDD
ratings_rdd = ratings.rdd
# apply our function to RDD
ratings_rdd_new = ratings_rdd.map(lambda row: rowwise_function(row))
# Convert RDD Back to DataFrame
ratings_new_df = sqlContext.createDataFrame(ratings_rdd_new)

ratings_new_df.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/11_hudd96e07eba2d9996043bf90795fc904a_108749_500x0_resize_box_2.png 500w
, /images/sparkcolumns/11_hudd96e07eba2d9996043bf90795fc904a_108749_800x0_resize_box_2.png 800w
, /images/sparkcolumns/11_hudd96e07eba2d9996043bf90795fc904a_108749_1200x0_resize_box_2.png 1200w" src=/images/sparkcolumns/11.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=4-pandas-udf>4. Pandas UDF</h2><p>Photo by
<a href="https://unsplash.com/@pbernardon?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Pascal Bernardon</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/12_hu23845d46ded0492e78581be49dabc52e_1670462_500x0_resize_box_2.png 500w
, /images/sparkcolumns/12_hu23845d46ded0492e78581be49dabc52e_1670462_800x0_resize_box_2.png 800w
, /images/sparkcolumns/12_hu23845d46ded0492e78581be49dabc52e_1670462_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/12_hu23845d46ded0492e78581be49dabc52e_1670462_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/12.png alt='

<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>
'></p><p>This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupby operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions.</p><p>The way we use it is by using the F.pandas_udf decorator. We assume here that the input to the function will be a pandas data frame. And we need to return a pandas dataframe in turn from this function.</p><p>The only complexity here is that we have to provide a schema for the output Dataframe. We can make that using the format below.</p><pre><code># Declare the schema for the output of our function
outSchema = StructType([StructField('user_id',IntegerType(),True),StructField('movie_id',IntegerType(),True),StructField('rating',IntegerType(),True),StructField('unix_timestamp',IntegerType(),True),StructField('normalized_rating',DoubleType(),True)])

# decorate our function with pandas_udf decorator
[@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)
def subtract_mean(pdf):
    # pdf is a pandas.DataFrame
    v = pdf.rating
    v = v - v.mean()
    pdf['normalized_rating'] =v
    return pdf

rating_groupwise_normalization = ratings.groupby(&quot;movie_id&quot;).apply(subtract_mean)

rating_groupwise_normalization.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/13_hu288dccabf0a92f30535802668f89ba2a_104327_500x0_resize_box_2.png 500w
, /images/sparkcolumns/13_hu288dccabf0a92f30535802668f89ba2a_104327_800x0_resize_box_2.png 800w
, /images/sparkcolumns/13_hu288dccabf0a92f30535802668f89ba2a_104327_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/13_hu288dccabf0a92f30535802668f89ba2a_104327_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/13.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>We can also make use of this to <em><strong>train multiple individual models on each spark node.</strong></em> For that, we replicate our data and give each replication a key and some training params like max_depth, etc. Our function then takes the pandas Dataframe, runs the required model, and returns the result. The structure would look something like below.</p><pre><code># 0. Declare the schema for the output of our function
outSchema = StructType([StructField('replication_id',IntegerType(),True),StructField('RMSE',DoubleType(),True)])

# decorate our function with pandas_udf decorator
[@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)
def run_model(pdf):
    # 1. Get hyperparam values
    num_trees = pdf.num_trees.values[0]
    depth = pdf.depth.values[0]
    replication_id = pdf.replication_id.values[0]
    # 2. Train test split
    Xtrain,Xcv,ytrain,ycv = train_test_split.....
    # 3. Create model using the pandas dataframe
    clf = RandomForestRegressor(max_depth = depth, num_trees=num_trees,....)
    clf.fit(Xtrain,ytrain)
    # 4. Evaluate the model
    rmse = RMSE(clf.predict(Xcv,ycv)
    # 5. return results as pandas DF
    res =pd.DataFrame({'replication_id':replication_id,'RMSE':rmse})
    return res

results = replicated_data.groupby(&quot;replication_id&quot;).apply(run_model)
</code></pre><p>Above is just an idea and not a working code. Though it should work with minor modifications.</p><hr><h2 id=5-using-sql>5. Using SQL</h2><p>For people who like
<a href="https://towardsdatascience.com/learning-sql-the-hard-way-4173f11b26f1?source=---------10------------------" target=_blank rel="nofollow noopener">SQL</a>
, there is a way even to create columns using SQL. For this, we need to register a temporary SQL table and then use simple select queries with an additional column. One might also use it to do joins.</p><pre><code>ratings.registerTempTable('ratings_table')
newDF = sqlContext.sql('select *, 2*rating as newCol from ratings_table')
newDF.show()
</code></pre><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/14_hu8eea9ccd0a6b509fd4cb258d69c89c1b_78393_500x0_resize_box_2.png 500w
, /images/sparkcolumns/14_hu8eea9ccd0a6b509fd4cb258d69c89c1b_78393_800x0_resize_box_2.png 800w
, /images/sparkcolumns/14_hu8eea9ccd0a6b509fd4cb258d69c89c1b_78393_1200x0_resize_box_2.png 1200w" src=/images/sparkcolumns/14.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=conclusion>Conclusion</h2><p>Photo by
<a href="https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral" target=_blank rel="nofollow noopener">Kelly Sikkema</a>
on
<img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/sparkcolumns/15_hu7cd08c310877446834a7e156ec547d7a_1247973_500x0_resize_box_2.png 500w
, /images/sparkcolumns/15_hu7cd08c310877446834a7e156ec547d7a_1247973_800x0_resize_box_2.png 800w
, /images/sparkcolumns/15_hu7cd08c310877446834a7e156ec547d7a_1247973_1200x0_resize_box_2.png 1200w
, /images/sparkcolumns/15_hu7cd08c310877446834a7e156ec547d7a_1247973_1500x0_resize_box_2.png 1500w" src=/images/sparkcolumns/15.png alt='

<a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral" target="_blank" rel="nofollow noopener">Unsplash</a>
'></p><p>And that is the end of this column(pun intended)</p><p>Hopefully, I’ve covered the column creation process well to help you with your Spark problems. If you need to learn more of spark basics, take a look at:</p><p><a href=https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a target=_blank rel="nofollow noopener">&lt;strong>The Hitchhikers guide to handle Big Data using Spark&lt;/strong></a></p><p><em><strong>You can find all the code for this post at the
<a href=https://github.com/MLWhiz/data_science_blogs/blob/master/spark_columns/Columns.ipynb target=_blank rel="nofollow noopener">GitHub repository</a>
or the
<a href=https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7664398068420572/312750581110937/3797400441762013/latest.html target=_blank rel="nofollow noopener">published notebook</a>
on databricks.</strong></em></p><p>Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on
<a href=https://coursera.pxf.io/4exq73 target=_blank rel="nofollow noopener">Big Data Essentials: HDFS, MapReduce and Spark RDD</a>
on Coursera.</p><p>Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
<a href="https://mlwhiz.medium.com/?source=post_page---------------------------" target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a></p><p>Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#00aaa1','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a data scientist consultant and big data engineer based in London, where I am currently working with Facebook .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class=categoryStyle style=color:#fff href=/categories/awesome-guides>Awesome Guides</a></li><li><a class=categoryStyle style=color:#fff href=/categories/bash>Bash</a></li><li><a class=categoryStyle style=color:#fff href=/categories/big-data>Big Data</a></li><li><a class=categoryStyle style=color:#fff href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class=categoryStyle style=color:#fff href=/categories/computer-vision>Computer Vision</a></li><li><a class=categoryStyle style=color:#fff href=/categories/data-science>Data Science</a></li><li><a class=categoryStyle style=color:#fff href=/categories/deep-learning>Deep Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/learning-resources>Learning Resources</a></li><li><a class=categoryStyle style=color:#fff href=/categories/machine-learning>Machine Learning</a></li><li><a class=categoryStyle style=color:#fff href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class=categoryStyle style=color:#fff href=/categories/opinion>Opinion</a></li><li><a class=categoryStyle style=color:#fff href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logo.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>