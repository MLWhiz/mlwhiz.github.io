<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Everything you need to learn about GPT — How Does ChatGPT Work? - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Everything you need to learn about GPT — How Does ChatGPT Work? - MLWhiz"><meta property="og:description" content="ChatGPT is what everyone is talking about nowadays. Would it take all the jobs?"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/"><meta property="og:image" content="https://mlwhiz.com/images/how_chatgpt_works/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/how_chatgpt_works/main.png"><meta property="article:published_time" content="2023-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-08T22:10:24+01:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Awesome Guides"><meta property="article:tag" content="ChatGPT Series"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/how_chatgpt_works/main.png"><meta name=twitter:title content="Everything you need to learn about GPT — How Does ChatGPT Work? - MLWhiz"><meta name=twitter:description content="ChatGPT is what everyone is talking about nowadays. Would it take all the jobs?"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/#primaryimage","url":"https://mlwhiz.com/images/how_chatgpt_works/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/#webpage","url":"https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/","inLanguage":"en-US","name":"Everything you need to learn about GPT — How Does ChatGPT Work? - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/#primaryimage"},"datePublished":"2023-07-07T00:00:00.00Z","dateModified":"2023-07-08T22:10:24.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"ChatGPT is what everyone is talking about nowadays. Would it take all the jobs?"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a>
<a href=/categories/chatgpt-series class=categoryStyle>Chat Gpt Series</a><h1>Everything you need to learn about GPT — How Does ChatGPT Work?</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>07 July 2023</span></div><div class="content mb-5"><p>ChatGPT is what everyone is talking about nowadays. Would it take all the jobs? Or would it result in misinformation on the web? There are just so many posts and articles that fill my inbox daily when it comes to GPTs. Add to that so many versions of GPTs and tools to use these GPTs; it is getting increasingly frustrating to keep track of everything in the GPT Landscape.</p><p>In this series of several posts about GPT, I intend to accumulate all the knowledge I have acquired over the months and arrange it logically into readable chunks for my readers. I have planned a few posts, and the first post, i.e., this one, will discuss the various advancements in the NLP space that made ChatGPT possible and talk specifically about <strong><em>how chatGPT Works</em></strong> . In the second post, I will take you through the <strong><em>ChatGPT API and how you could use the different ChatGPT endpoints</em></strong> . In the third post, we will delve deeper into <strong><em>Prompt Engineering</em></strong> , where I will take you through the different tactics you can use to create usable prompts. In the fourth Post, I will discuss <strong><em>Langchain</em></strong> and its various useful functions. And I will keep on adding new posts over here. So follow me up here on
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">Medium</a>
to be notified about these.</p><hr><h2 id=evolution-of-chatgpt>Evolution of ChatGPT</h2><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/how_chatgpt_works/1*huWymPGbq9WCV5utpoPIFg_hu6d5e91adf91c7d1708078f64b3155127_299232_500x0_resize_box_2.png 500w
, /images/how_chatgpt_works/1*huWymPGbq9WCV5utpoPIFg_hu6d5e91adf91c7d1708078f64b3155127_299232_800x0_resize_box_2.png 800w
, /images/how_chatgpt_works/1*huWymPGbq9WCV5utpoPIFg_hu6d5e91adf91c7d1708078f64b3155127_299232_1200x0_resize_box_2.png 1200w" src=/images/how_chatgpt_works/1*huWymPGbq9WCV5utpoPIFg.png alt="Author Image: How ChatGPT Came to be"><figcaption>Author Image: How ChatGPT Came to be</figcaption></figure></p><p>As with everything, ChatGPT didn’t come into existence without a cumulative effort. It had the shoulders of giants to climb upon. ChatGPT Website says, <em>“ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.”</em> While InstructGPT is itself based on GPT3, which in turn is based on the transformer architecture. So, if we try to look at the chronology, we will need to go through this series of papers to understand how the whole thing evolved to existence. You should go through all these papers if you want to, but I have tried summarizing each below:</p><h3 id=1-attention-is-all-you-needhttpsarxivorgpdf170603762pdf><a href=https://arxiv.org/pdf/1706.03762.pdf target=_blank rel="nofollow noopener">1. Attention Is All You Need:</a></h3><p>In the past, the LSTM and GRU architecture(as explained in my
<a href=https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566 target=_blank rel="nofollow noopener">post</a>
on NLP) and the attention mechanism used to be the State of Art Approach for Language modeling problems (put very simply, predict the next word) and Translation systems. But, the main problem with these architectures is that they are recurrent, and the runtime increases as the sequence length increases. These architectures take a sentence and sequentially process each word; hence, the whole runtime increases with the increase in sentence length. Transformer, a model architecture first explained in the paper Attention is all you need, lets go of this recurrence and instead relies entirely on <strong><em>an attention mechanism to draw global dependencies between input and output</em></strong> . Here are a few of my posts that delve deep into explaining Transformers. For now, you can think of them as the building blocks of all the other GPTs that would come after it.</p><ul><li><p><a href=https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076 target=_blank rel="nofollow noopener">Understanding Transformers the Data Science Way</a></p></li><li><p><a href=https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2 target=_blank rel="nofollow noopener">Understanding Transformers the Programming Way</a></p></li></ul><h3 id=2-improving-language-understanding-by-generative-pre-traininghttpscdnopenaicomresearch-coverslanguage-unsupervisedlanguage_understanding_paperpdf><a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf target=_blank rel="nofollow noopener">2. Improving Language Understanding by Generative Pre-Training</a></h3><p>This paper which introduced GPT-1, proposes pre-training a model on a large corpus of unlabeled text using a generative model, <strong><em>followed by fine-tuning on each specific task</em></strong> . While BERT used Transformer architecture as both encoder and decoder, GPT architecture uses only multi-layer Transformer decoder layers to create its network. The training consists of two stages.</p><ol><li><strong><em>Unsupervised pre-training</em></strong> : This stage trains a network on unsupervised data to predict the next word given a context window of k words.</li><li><strong>Supervised fine-tuning:</strong> This stage finetunes the network trained in stage 1 using a supervised dataset by adding a new layer on top of the stage 1 network and trying to minimize a weighted loss of classification loss and LM loss.</li></ol><h3 id=3-language-models-are-unsupervised-multitask-learnershttpsd4mucfpksywvcloudfrontnetbetter-language-modelslanguage_models_are_unsupervised_multitask_learnerspdf><a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf target=_blank rel="nofollow noopener">3. Language Models are Unsupervised Multitask Learners</a></h3><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/how_chatgpt_works/1*HdscdbNQWn4aEpNuPG0E4Q_hu63d8d73cc6c6ec30111af082e768e090_267285_500x0_resize_box_2.png 500w
, /images/how_chatgpt_works/1*HdscdbNQWn4aEpNuPG0E4Q_hu63d8d73cc6c6ec30111af082e768e090_267285_800x0_resize_box_2.png 800w
, /images/how_chatgpt_works/1*HdscdbNQWn4aEpNuPG0E4Q_hu63d8d73cc6c6ec30111af082e768e090_267285_1200x0_resize_box_2.png 1200w" src=/images/how_chatgpt_works/1*HdscdbNQWn4aEpNuPG0E4Q.png alt="Examples of naturally occurring demonstrations of English to French and French to English translation found throughout the WebText training set"><figcaption>Examples of naturally occurring demonstrations of English to French and French to English translation found throughout the WebText training set</figcaption></figure></p><p>In this paper, the authors introduce the concept of unsupervised multitask learning, where a single language model is trained on a wide range of language tasks simultaneously. The paper did two things:</p><ul><li>While the model&rsquo;s architecture remains similar to that of GPT-1 with few changes in normalization layers, this was a very big model with 1.5 Billion Parameters.</li><li>This model was not trained on the randomly chosen unsupervised corpus like Common Crawl but rather a dataset of millions of scraped webpages that emphasized document quality called WebText. This allowed the model to get trained on more intelligible text that correlated more with the tasks the model was expected to solve.</li></ul><h3 id=4-language-models-are-few-shot-learnershttpsarxivorgpdf200514165pdf><a href=https://arxiv.org/pdf/2005.14165.pdf target=_blank rel="nofollow noopener">4. Language Models are Few-Shot Learners</a></h3><p>In this paper, the researchers introduce GPT-3, an autoregressive language model with 175 billion parameters, and evaluate its performance in the few-shot setting without fine-tuning. Particularly,</p><ul><li>The model is similar to GPT-2 but with some modifications in the attention patterns of the transformer layers.</li><li>For training data, this model used a filtered version of CommonCrawl + Webtext they used for chatGPT + two internet-based books corpora.</li></ul><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/how_chatgpt_works/1*joMVvBgn0K7pqluidXbPhg_hu95e45d3a839909c2e7495ed06d402ecb_391793_500x0_resize_box_2.png 500w
, /images/how_chatgpt_works/1*joMVvBgn0K7pqluidXbPhg_hu95e45d3a839909c2e7495ed06d402ecb_391793_800x0_resize_box_2.png 800w
, /images/how_chatgpt_works/1*joMVvBgn0K7pqluidXbPhg_hu95e45d3a839909c2e7495ed06d402ecb_391793_1200x0_resize_box_2.png 1200w" src=/images/how_chatgpt_works/1*joMVvBgn0K7pqluidXbPhg.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><ul><li>This paper introduced the term “ <strong>Few-Shot Learning,</strong> ” or at least I heard it for the first time here. <strong><em>Few Shot</em></strong> is the term to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning but where no weight updates are allowed. This paper evaluated its big pretrained language model with the few-shot learning approach. At evaluation time, the model is given examples of tasks and then asked to finish the task. In a way, this was the start of moving from fine-tuning to prompt engineering.</li></ul><h3 id=5-deep-reinforcement-learning-from-human-preferenceshttpsarxivorgpdf170603741pdf><a href=https://arxiv.org/pdf/1706.03741.pdf target=_blank rel="nofollow noopener">5. Deep Reinforcement Learning from Human Preferences</a></h3><p>This paper talks about how the reward function of certain tasks is hard to construct and introduces a method for training reinforcement learning agents using human feedback in the form of preferences. The goal is to improve the performance and safety of AI systems by incorporating human values into the learning process. The paper presents a framework called Reinforcement Learning from Human Feedback (RLHF), which utilizes a reward model generated from human preferences to guide the agent’s learning. The process involves collecting pairwise comparisons from human evaluators to rank different agent behaviors. This ranking is used to construct a reward model that guides the agent toward behaviors humans prefer. The reinforcement model policy is then trained using the reward model.</p><h3 id=6-training-language-models-to-follow-instructions-with-human-feedbackhttpsarxivorgpdf220302155pdf><a href=https://arxiv.org/pdf/2203.02155.pdf target=_blank rel="nofollow noopener">6. Training language models to follow instructions with human feedback</a></h3><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/how_chatgpt_works/1*27sFm2tE81bqeb-nXfNOsw_hu4a6486a58006a5eb1227e0a7bb5ea72a_323251_500x0_resize_box_2.png 500w
, /images/how_chatgpt_works/1*27sFm2tE81bqeb-nXfNOsw_hu4a6486a58006a5eb1227e0a7bb5ea72a_323251_800x0_resize_box_2.png 800w
, /images/how_chatgpt_works/1*27sFm2tE81bqeb-nXfNOsw_hu4a6486a58006a5eb1227e0a7bb5ea72a_323251_1200x0_resize_box_2.png 1200w" src=/images/how_chatgpt_works/1*27sFm2tE81bqeb-nXfNOsw.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>The researchers at OpenAI trained this 80B parameter model using Reinforcement Learning from Human Feedback (RLHF) . The researchers:</p><p><strong>Step 1</strong> . The authors first trained a baseline model, which involved taking a pre-trained language model and fine-tuning it using a small set of data carefully curated by labelers. This fine-tuning process aims to train a supervised policy known as the SFT model, which generates outputs based on specific prompts.</p><p><strong>Step 2</strong> . To generate a reward model (RM), labelers are tasked with voting on many outputs generated by the SFT model. This voting process creates a dataset of comparison data. A new reward model is trained on this dataset, using the comparison data to guide be used in RLHF Step. As we can understand, the voting process is much easier than manually generating the prompts and responses for Step 1 and hence is more scalable.</p><p><strong>Step 3</strong> . In this step, the researchers used the SFT model as the baseline to initialize the PPO Model first and then used the rewards from the Reward model to change the weights of the PPO Model. Simply put, they sent a prompt to the model, observed the reward from the reward model, and changed model weight parameters to maximize the reward. In their objective, they also have certain terms that ensure that the SFT Model doesn’t diverge too much from the PPO Model for stability.</p><p><strong><em>Steps 2 and 3</em></strong> are then iterated continuously to create new, improved models.</p><blockquote><p><strong><em>And that is how ChatGPT came to be.</em></strong></p></blockquote><p>ChatGPT, when it launched, was based on the GPT3.5 Model, a model trained on GPT3 data along with the Open API data users interacted with. ChatGPT is now based on GPT-4 architecture as the baseline language model having a massive 170 trillion Parameters with RLHF used to train the PPO policy. Here is the
<a href=https://arxiv.org/pdf/2303.08774.pdf target=_blank rel="nofollow noopener">GPT-4 Technical Report</a>
, which specifies the various benchmarks this remarkable model has broken and how it compares with the past models.</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/how_chatgpt_works/1*tzBNEGV3HYL_Q-8JKXyAGw_hudefc578a24d104f62bb2e8dd0f508df2_113267_500x0_resize_box_2.png 500w
, /images/how_chatgpt_works/1*tzBNEGV3HYL_Q-8JKXyAGw_hudefc578a24d104f62bb2e8dd0f508df2_113267_800x0_resize_box_2.png 800w" src=/images/how_chatgpt_works/1*tzBNEGV3HYL_Q-8JKXyAGw.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=conclusion>Conclusion</h2><p>So, we were learning something each step of the way. The Attention paper taught us about transformers, the GPT-1 Paper about fine-tuning on different tasks, the GPT-2 Paper about using targeted datasets, the GPT-3 Paper about few-shot learning, the RLHF Paper about modeling rewards for RL using Human feedback, and the instructGPT Paper about using RLHF with GPT, which correspondingly came to be known as ChatGPT.</p><p>In this blog post, I aimed to provide an overview of the evolutionary journey that led to the development of the ChatGPT model. By examining various pivotal research papers, we have traced the path that brought us to the model&rsquo;s current state. And to see that the research is still going at a breakneck pace and we see new things every day is nothing short of extraordinary.</p><p>I will continue providing more info on these GPT models as we go through the GPT Series. Let me know what you think about them. Also, follow me up on
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
. Optionally, you may also
<a href=https://medium.com/@mlwhiz/membership target=_blank rel="nofollow noopener">sign up</a>
for a Medium membership to get full access to every story on Medium.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>