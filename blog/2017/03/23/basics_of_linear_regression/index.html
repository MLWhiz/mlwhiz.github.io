<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Basics Of Linear Regression - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This post provides a overview of Linear Regression"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Basics Of Linear Regression - MLWhiz"><meta property="og:description" content="This post provides a overview of Linear Regression"><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/"><meta property="og:image" content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta property="article:published_time" content="2017-03-23T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-13T22:36:49+01:00"><meta property="article:tag" content="Data Science"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta name=twitter:title content="Basics Of Linear Regression - MLWhiz"><meta name=twitter:description content="This post provides a overview of Linear Regression"><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/#primaryimage","url":"https://mlwhiz.com/images/category_bgs/default_bg.jpg","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/#webpage","url":"https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/","inLanguage":"en-US","name":"Basics Of Linear Regression - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/#primaryimage"},"datePublished":"2017-03-23T00:00:00.00Z","dateModified":"2023-08-13T22:36:49.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"This post provides a overview of Linear Regression"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/data-science class=categoryStyle>Data Science</a><h1>Basics Of Linear Regression</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>23 March 2017</span></div><img src=https://mlwhiz.com/images/category_bgs/default_bg.jpg class="img-fluid w-100 mb-4" alt="Basics Of Linear Regression"><div class="content mb-5"><p>Today we will look into the basics of linear regression. Here we go :</p><h2 id=contents>Contents</h2><ol><li>Simple Linear Regression (SLR)</li><li>Multiple Linear Regression (MLR)</li><li>Assumptions</li></ol><h2 id=1-simple-linear-regression>1. Simple Linear Regression</h2><p>Regression is the process of building a relationship between a dependent variable and set of independent variables. Linear Regression restricts this relationship to be linear in terms of coefficients. In SLR, we consider only one independent variable.</p><h3 id=example-the-waist-circumference--adipose-tissue-data>Example: The Waist Circumference – Adipose Tissue data</h3><ul><li><p>Studies have shown that individuals with excess Adipose tissue (AT) in the abdominal region have a higher risk of cardio-vascular diseases</p></li><li><p>Computed Tomography, commonly called the CT Scan is the only technique that allows for the precise and reliable measurement of the AT (at any site in the body)</p></li><li><p>The problems with using the CT scan are:</p><ul><li>Many physicians do not have access to this technology</li><li>Irradiation of the patient (suppresses the immune system)</li><li>Expensive</li></ul></li><li><p>Is there a simpler yet reasonably accurate way to predict the AT area? i.e.</p><ul><li>Easily available</li><li>Risk free</li><li>Inexpensive</li></ul></li><li><p>A group of researchers conducted a study with the aim of predicting abdominal AT area using simple anthropometric measurements i.e. measurements on the human body</p></li><li><p>The Waist Circumference – Adipose Tissue data is a part of this study wherein the aim is to study how well waist circumference(WC) predicts the AT area</p></li></ul><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Setting working directory</span>
filepath &lt;- <span style=color:#447fcf>c</span>(<span style=color:#ed9d13>&#34;/Users/nkaveti/Documents/Work_Material/Statistics Learning/&#34;</span>)
<span style=color:#447fcf>setwd</span>(filepath)

<span style=color:#999;font-style:italic># Reading data</span>
Waist_AT &lt;- <span style=color:#447fcf>read.csv</span>(<span style=color:#ed9d13>&#34;adipose_tissue.csv&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Number of rows: &#34;</span>, <span style=color:#447fcf>nrow</span>(Waist_AT), <span style=color:#ed9d13>&#34;\n&#34;</span>)
<span style=color:#447fcf>head</span>(Waist_AT)
</code></pre></div><pre><code>Number of rows:  109
</code></pre><table><thead><tr><th scope=col>Waist</th><th scope=col>AT</th></tr></thead><tbody><tr><td>74.75</td><td>25.72</td></tr><tr><td>72.60</td><td>25.89</td></tr><tr><td>81.80</td><td>42.60</td></tr><tr><td>83.95</td><td>42.80</td></tr><tr><td>74.65</td><td>29.84</td></tr><tr><td>71.85</td><td>21.68</td></tr></tbody></table><p>Let&rsquo;s start with a scatter plot of <strong>Waist</strong> Vs <strong>AT</strong>, to understand the relationship between these two variables.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#447fcf>plot</span>(AT ~ Waist, data = Waist_AT)
</code></pre></div><div style=margin-top:9px;margin-bottom:10px><center><img src=/images/output_6_0.png height=400 width=700></center></div><p>Any observations from above plot?</p><p>Now the objective is to find a linear relation between <code>Waist</code> and <code>AT</code>. In otherwords, finding the amount of change in <code>AT</code> per one unit change (increment/decrement) in <code>Waist</code>.</p><p>In SLR, it is equivalent to finding an optimal straight line equation such that the sum of squares of differences between straight line and the points will be minimum. This method of estimation is called as
<a href=https://en.wikipedia.org/wiki/Ordinary_least_squares target=_blank rel="nofollow noopener">Ordiany Least Squares (OLS)</a>
.</p><p>$$AT = \beta_0 + \beta_1 \ Waist + \epsilon$$</p><div>$$Min_{\beta_0 , \beta_1} \ \ \epsilon^\intercal \epsilon \implies Min_{\beta_0 , \beta_1} \ \ (AT - \beta_0 - \beta_1 \ Waist)^\intercal (AT - \beta_0 - \beta_1 \ Waist)$$</div><p>Where, $\beta_1$ represents the amount of change in <code>AT</code> per one unit change in <code>Waist</code>.</p><p>Now our problem becomes an unconstrained optimization problem. We can find optimal values for $\beta_0$ and $\beta_1$ using basic calculus.</p><p>Lets re-write above regression equation in matrix form</p><p>$$ AT = X \beta + \epsilon$$</p><p>Where, $ X = [1 \ \ Waist]$ 1 is a vector of ones and $\beta = (\beta_0, \ \beta_1)$</p><p>$$
\begin{equation}
\begin{split}
\epsilon^\intercal \epsilon & = {(AT - X \beta)}^\intercal {(AT - X \beta)} \<br>& = AT^\intercal AT - AT^\intercal X \beta - {(X \beta)}^\intercal AT + {(X \beta)}^\intercal (X \beta)
\end{split}
\end{equation}
$$</p><p>Now differentiate this w.r.t to $\beta$ and equate it to zero. Then we have,
$$\hat{\beta} = (X^\intercal X)^{-1} X^\intercal AT $$</p><p>Now we can find the fitted values of model by substituting $\hat{\beta}$ in above regression equation
$$\hat{AT} = X \hat{\beta}=X(X^\intercal X)^{-1} X^\intercal AT$$</p><p><strong>Note:</strong> We are arriving to above equation through an assumption
<a href=#Assumptions>$^1$</a>
of $E(\epsilon)=0$. What happens if this assumption violates?</p><p>Let, $X(X^\intercal X)^{-1} X^\intercal = H$
$$\hat{AT} = H \ AT$$</p><p>We call H as an hat matrix, because it transforms $AT$ into $\hat{AT}$ :D</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Lets compute the hat matrix</span>
X = <span style=color:#447fcf>cbind</span>(<span style=color:#3677a9>1</span>, Waist_AT$Waist)
temp = <span style=color:#447fcf>solve</span>(<span style=color:#447fcf>t</span>(X) %*% X) %*% <span style=color:#447fcf>t</span>(X)
betahat = temp %*% Waist_AT$AT <span style=color:#999;font-style:italic># Estimated coefficients</span>
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Let&#39;s compare the computed values with lm() output: \n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Computed Coefficients: \n \n&#34;</span>)
<span style=color:#447fcf>print</span>(<span style=color:#447fcf>data.frame</span>(Intercept = betahat[1], Waist = betahat[2]))
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n&#34;</span>)
<span style=color:#999;font-style:italic>#cat(&#34;Optimal value for beta_0 is: &#34;, betahat[1], &#34;and for beta_1 is: &#34;, betahat[2], &#34;\n \n&#34;)</span>
fit_lm = <span style=color:#447fcf>lm</span>(AT ~ Waist, data = Waist_AT)
<span style=color:#999;font-style:italic>#cat(&#34;Compare our computed estimates with lm() estimates&#34;, &#34;\n&#34;)</span>
<span style=color:#447fcf>print</span>(fit_lm)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n \n&#34;</span>)
H = X %*% temp <span style=color:#999;font-style:italic># Computing hat matrix</span>
AThat = H %*% Waist_AT$AT <span style=color:#999;font-style:italic># Computing predicted values</span>
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Therefore, there is a&#34;</span>, betahat[2], <span style=color:#ed9d13>&#34;increment in AT per one unit change in Waist \n&#34;</span> )
</code></pre></div><pre><code>Let's compare the computed values with lm() output:

Computed Coefficients:

  Intercept    Waist
1 -215.9815 3.458859
=======================================================================

Call:
lm(formula = AT ~ Waist, data = Waist_AT)

Coefficients:
(Intercept)        Waist
   -215.981        3.459

=======================================================================

Therefore, there is a 3.458859 increment in AT per one unit change in Waist
</code></pre><h2 id=whats-next>What&rsquo;s next?</h2><p>We succesfully computed estimates for regression coefficients and fitted values.</p><ol><li><p>We are working on only one sample, how can we generalise these results to population?</p></li><li><p>How to measure model&rsquo;s performance quantitatively?</p></li></ol><p>** We are working on only one sample, how can we generalise these results to population? **</p><p>Let&rsquo;s focus on question 1. Our regression coefficients are computed using only one sample and these values will change, if we change the sample. But how much they vary? We need to estimate the variation for each beta coefficient to check whether the corresponding regressor is consistently explaining the same behaviour even if we change the sample.</p><p>Now the big problem is collecting multiple samples to check the above hypothesis. Hence, we use distributions to check statistical significance of regressors.</p><p>For our example, we need to test below two hypotheses.</p><p>$$ Null \ Hypothesis: \beta_{0} = 0 $$</p><p>$$ Alternative \ Hypothesis: \beta_{0} \neq 0$$</p><p>$$ Null \ Hypothesis: \beta_{1} = 0 $$</p><p>$$ Alternative \ Hypothesis: \beta_{1} \neq 0$$</p><p>Test Statistic for these hypotheses is,</p><p>$$t = \frac{\hat{\beta_{i}}}{\sqrt{Var(\hat{\beta_{i}})}}$$</p><p>Test statistic <code>t</code> follows <code>t-distribution</code>, assuming
<a href=#Assumptions>$^2$</a>
dependent variable follows <code>normal distribution</code></p><p><strong>Suggestion:</strong> If your not aware of
<a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing target=_blank rel="nofollow noopener">testing of hypothesis</a>
,
<a href=https://en.wikipedia.org/wiki/Probability_distribution target=_blank rel="nofollow noopener">probability distributions</a>
and
<a href=https://en.wikipedia.org/wiki/P-value target=_blank rel="nofollow noopener">p-values</a>
please browse through the Google.</p><p>Let&rsquo;s recall that, $\hat{\beta} = (X^\intercal X)^{-1} X^\intercal AT$</p><p>$$\begin{equation}
\begin{split}
Var(\hat{\beta}) & = Var((X^\intercal X)^{-1} X^\intercal AT) \<br>& = (X^\intercal X)^{-1} X^\intercal \ Var(AT) \ X(X^\intercal X)^{-1} \<br>& = (X^\intercal X)^{-1} X^\intercal \ X(X^\intercal X)^{-1} \ \sigma^2 \<br>& = (X^\intercal X)^{-1} \sigma^2
\end{split}
\end{equation}
$$</p><p><strong>Note:</strong> In the above calculations we assumed
<a href=#Assumptions>$^3$</a>
$Var(AT) = \sigma^2$ (Constant). Where, $\sigma^2$ is variation in population AT.</p><p><strong>Suggestion:</strong> Try solving $(X^\intercal X)^{-1}$ with $X = [1, \ x]$ where $x = (x_1, x_2, x_3 &mldr; x_n)$. You will get the following expression.</p><div>$$
\
Var(\hat{\beta}) =
\frac{1}{n \sum x_i^2 - (\sum x_i)^2}
\begin{bmatrix}
\sum_{i=1}^n x_i^2 & -\sum x_i \\
    -\sum x_i & n
\end{bmatrix}
\sigma^2
\
$$</div><p>Diagonal elements of above matrix are varinaces of $\beta_0$ and $\beta_1$ respectively. Off-diagonal element is covariance between $\beta_0$ and $\beta_1$.</p><p>Hence,</p><div>$$Var(\hat{\beta_0}) = \frac{\sigma^2 \sum_{i = 1}^n x_i^2}{n \sum_{i = 1}^n (x_i - \bar{x})^2}$$</div><div>$$Var(\hat{\beta_1}) = \frac{\sigma^2}{\sum_{i = 1}^n (x_i - \bar{x})^2}$$</div><p>One important observation from $Var(\hat{\beta})$ expressions is, $Var(x)$ is inversely proportional to $Var(\hat{\beta})$. That is, we will get more consistent estimators if there is high variation in corresponding predictors.</p><p>Recall that, $\sigma^2$ in above expression is the population variance, not the sample. Hence, we need to estimate this using the sample that we have.</p><p>$$\hat{\sigma^2} = \frac{1}{n-2} \sum_{i = 1}^n e_i^2$$</p><p>Where, $e_i = AT_i - \hat{AT}_i$</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Let&#39;s compute variances of beta hat and test statistic &#39;t&#39;</span>
sigmasq = (<span style=color:#3677a9>1</span>/<span style=color:#447fcf>length</span>(AThat[-<span style=color:#447fcf>c</span>(<span style=color:#3677a9>1</span>:<span style=color:#3677a9>2</span>)]))*<span style=color:#447fcf>sum</span>((AThat - Waist_AT$AT)^2)
VarBeta0 = (sigmasq * <span style=color:#447fcf>sum</span>(Waist_AT$Waist^2))/(<span style=color:#447fcf>length</span>(AThat) * <span style=color:#447fcf>sum</span>((Waist_AT$Waist - <span style=color:#447fcf>mean</span>(Waist_AT$Waist))^2))
VarBeta1 = sigmasq/<span style=color:#447fcf>sum</span>((Waist_AT$Waist - <span style=color:#447fcf>mean</span>(Waist_AT$Waist))^2)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Let&#39;s compare the computed values with lm() output: \n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Computed Coefficients: \n \n&#34;</span>)
res = <span style=color:#447fcf>data.frame</span>(Estimate = betahat, Std.Error = <span style=color:#447fcf>c</span>(<span style=color:#447fcf>sqrt</span>(VarBeta0), <span style=color:#447fcf>sqrt</span>(VarBeta1)), t_value = <span style=color:#447fcf>c</span>(betahat[1]/<span style=color:#447fcf>sqrt</span>(VarBeta0), betahat[2]/<span style=color:#447fcf>sqrt</span>(VarBeta1)))
<span style=color:#447fcf>row.names</span>(res) = <span style=color:#447fcf>c</span>(<span style=color:#ed9d13>&#34;(Intercept)&#34;</span>, <span style=color:#ed9d13>&#34;Waist&#34;</span>)
res$p_value = <span style=color:#3677a9>2</span>*<span style=color:#447fcf>pt</span>(<span style=color:#447fcf>abs</span>(res$t_value), <span style=color:#447fcf>nrow</span>(Waist_AT)<span style=color:#3677a9>-1</span>, lower.tail = <span style=color:#6ab825;font-weight:700>FALSE</span>)
<span style=color:#447fcf>print</span>(res)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;=======================================================================&#34;</span>)
<span style=color:#447fcf>summary</span>(fit_lm)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;=======================================================================&#34;</span>)
</code></pre></div><pre><code>Let's compare the computed values with lm() output:

=======================================================================
Computed Coefficients:

               Estimate  Std.Error   t_value      p_value
(Intercept) -215.981488 21.7962708 -9.909103 7.507198e-17
Waist          3.458859  0.2346521 14.740376 1.297124e-27
=======================================================================



Call:
lm(formula = AT ~ Waist, data = Waist_AT)

Residuals:
     Min       1Q   Median       3Q      Max
-107.288  -19.143   -2.939   16.376   90.342

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) -215.9815    21.7963  -9.909   &lt;2e-16 ***
Waist          3.4589     0.2347  14.740   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.06 on 107 degrees of freedom
Multiple R-squared:   0.67, Adjusted R-squared:  0.667
F-statistic: 217.3 on 1 and 107 DF,  p-value: &lt; 2.2e-16



=======================================================================
</code></pre><p><strong>Note:</strong> Residual standard error = $\sqrt{sigmasq}$</p><p><strong>How to measure model&rsquo;s performance quantitatively?</strong></p><p>Let&rsquo;s focus on question 2 (How to measure model&rsquo;s performance quantitatively?). Recall that, our objective of building model is to explain the variation in <code>AT</code> using the variation in <code>Waist</code>.</p><p>Total variation in AT is, $\sum_{i=1}^n (AT - mean(AT))^2$ this can be splitted into two parts as follows:</p><div>$$
\begin{equation}
\begin{split}
\sum_{i=1}^n (AT_i - \bar{AT})^2 & = \sum_{i=1}^n (AT  - \hat{AT_i} + \hat{AT_i} - \bar{AT})^2 \\
& = \sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2 + \sum_{i=1}^n (AT_i - \hat{AT_i})^2
\end{split}
\end{equation}
$$</div><p>Where, $\sum_{i=1}^n (AT_i - \bar{AT})^2$ is the total variation in AT, $\sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2$ is the explained variation in AT, this is also called as <strong>Regression Sum of Squares</strong> and $\sum_{i=1}^n (AT_i - \hat{AT_i})^2$ is the unexplained variation in AT, this is also called as <strong>Error Sum of Squares</strong></p><p>We can measure our model using the proportion of total variation explained by independent variable(s). That is, $\frac{Regression \ Sum \ of \ Squares}{Total \ Sum \ of \ Squares}$</p><p>The above measure is called as Multiple R-squared:</p><div>$$Multiple \ R-squared = \frac{\sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2}{\sum_{i=1}^n (AT_i - \bar{AT})^2}$$</div><p><strong>Interesting facts:</strong> Multiple R-squared value in SLR is equals to $r^2$ and (1 - Multiple R-squared) is equals to the variance in residuals.</p><p>Where, r is
<a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient target=_blank rel="nofollow noopener">pearson&amp;rsquo;s correlation coefficient</a>
between dependent and independent variable.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Let&#39;s compute Multiple R-squared measure for our example</span>
SSR = <span style=color:#447fcf>sum</span>((AThat - <span style=color:#447fcf>mean</span>(Waist_AT$AT))^2)
SST = <span style=color:#447fcf>sum</span>((Waist_AT$AT - <span style=color:#447fcf>mean</span>(Waist_AT$AT))^2)
MulRSq = SSR/SST
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Compute Multiple R-squared: &#34;</span>, MulRSq, <span style=color:#ed9d13>&#34;\n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Note that computed R squared value is matching with lm() Multiple R-squared value in above output \n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n \n&#34;</span>)
</code></pre></div><pre><code>Compute Multiple R-squared:  0.6700369

Note that computed R squared value is matching with lm() Multiple R-squared value in above output

=======================================================================
</code></pre><p><strong>What happens to the Multiple R-squared value when you add an irrelevant variable to the model?</strong></p><p>In the below model, I am generating a random sample of uniform numbers between 1 to 100 and considering this as one of indepedent variable.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#447fcf>set.seed</span>(<span style=color:#3677a9>1234</span>)
fit_lm2 = <span style=color:#447fcf>lm</span>(AT ~ Waist + <span style=color:#447fcf>runif</span>(<span style=color:#447fcf>nrow</span>(Waist_AT), <span style=color:#3677a9>1</span>, <span style=color:#3677a9>100</span>), data = Waist_AT)
<span style=color:#447fcf>summary</span>(fit_lm2)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n \n&#34;</span>)
</code></pre></div><pre><code>Call:
lm(formula = AT ~ Waist + runif(nrow(Waist_AT), 1, 100), data = Waist_AT)

Residuals:
    Min      1Q  Median      3Q     Max
-106.06  -17.53   -3.63   13.70   91.36

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                   -226.2894    23.4350  -9.656 3.33e-16 ***
Waist                            3.5060     0.2376  14.757  &lt; 2e-16 ***
runif(nrow(Waist_AT), 1, 100)    0.1397     0.1181   1.183    0.239
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33 on 106 degrees of freedom
Multiple R-squared:  0.6743,  Adjusted R-squared:  0.6682
F-statistic: 109.7 on 2 and 106 DF,  p-value: &lt; 2.2e-16



=======================================================================
</code></pre><p>Multiple R-squared value increases irrespective of quality of explanation, which is incorrect. We should penalize our model performance if the quality of explanation is poor, that is why we need to adjust our R-squared value.</p><p>To penalize the explained part of AT, we inflate the unexplained part of AT with $\frac{Total \ degrees \ of \ freedom}{Error \ degrees \ of \ freedom}$. That is,</p><p>$$Adjusted \ R-squared = 1 - (1 - R^2) \frac{n-1}{n-p-1}$$</p><p>Where, n = Total number of observations; p = Total number of predictors (excluding intercept)</p><p>Adding a new independent variable will increase $\frac{n-1}{n-p-1}$ and $R^2$. If the amount of increment in $R^2$ is less than the amount of increment in $\frac{n-1}{n-p-1}$ than it will decrease the Adjusted R-squared value.</p><p>In <code>fit_lm2</code> model Adjusted R-squared decreases when we add randomly generated variable into the model.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Let&#39;s compute adjusted R-squared  for our example</span>
TDF = <span style=color:#447fcf>nrow</span>(Waist_AT[<span style=color:#3677a9>-1</span>, ]) <span style=color:#999;font-style:italic># Total degrees of freedom</span>
EDF = <span style=color:#447fcf>nrow</span>(Waist_AT[<span style=color:#3677a9>-1</span>, ]) - <span style=color:#3677a9>1</span> <span style=color:#999;font-style:italic># Error degrees of freedom, where 1 is the number of predictors</span>
AdjRSq = <span style=color:#3677a9>1</span> - (<span style=color:#3677a9>1</span> - MulRSq) * (TDF/EDF) <span style=color:#999;font-style:italic># Adjusted R square</span>
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Compute Multiple R-squared: &#34;</span>, AdjRSq, <span style=color:#ed9d13>&#34;\n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Note that computed Adjusted R-squared value is matching with lm() Adjusted R-squared value in the above output \n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Note: We are comparing with fit_lm model, not fit_lm2 \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n&#34;</span>)
</code></pre></div><pre><code>Compute Multiple R-squared:  0.6669531

Note that computed Adjusted R-squared value is matching with lm() Adjusted R-squared value in the above output

Note: We are comparing with fit_lm model, not fit_lm2
=======================================================================
</code></pre><p>Aforementioned measures (Multiple R-squared & Adjusted R-squared) for <strong>Goodness of fit</strong> are functions of sample and these will vary as sample changes. Similar to <code>t-test</code> for regression coefficeints we need some statistical test to test model&rsquo;s performance for population.</p><p>Objective is to compare the Mean sum of squares due to regression and Mean sum of squares due to error. <code>F-test</code> is very helpful to compare the variations.</p><div>$$ F-test = \frac{\frac{1}{p-1}\sum_{i=1}^n (\hat{AT_i} - \bar{AT})^2}{\frac{1}{n-p-1} \sum_{i=1}^n (\hat{AT_i} - AT_i)^2}$$</div><p><strong>Note:</strong> Above expression follows F distribution only if, AT follows Normal Distribution</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>RDF = TDF - EDF
SSE = SST - SSR
MSR = (<span style=color:#3677a9>1</span>/RDF)*SSR
MSE = (<span style=color:#3677a9>1</span>/EDF)*SSE
F_value = MSR/MSE
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Compute F statistic: &#34;</span>, F_value, <span style=color:#ed9d13>&#34;\n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Note that computed F-statistic is matching with lm() F-statistic value in the above output \n \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Note: We are comparing with fit_lm model, not fit_lm2 \n&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;======================================================================= \n&#34;</span>)
</code></pre></div><pre><code>Compute F statistic:  217.2787

Note that computed F-statistic is matching with lm() F-statistic value in the above output

Note: We are comparing with fit_lm model, not fit_lm2
=======================================================================
</code></pre><h2 id=2-multiple-linear-regression-mlr>2. Multiple Linear Regression (MLR)</h2><p>In multiple linear regression we consider more than one predictor and one dependent variable. Most of the above explanation is valid for MLR too.</p><h3 id=example-cars-mpg-miles-per-gallon-prediction>Example: Car&rsquo;s MPG (Miles Per Gallon) prediction</h3><p>Our interest is to model the MPG of a car based on the other variables.</p><p>Variable Description:</p><ul><li>VOL = cubic feet of cab space</li><li>HP = engine horsepower</li><li>MPG = average miles per gallon</li><li>SP = top speed, miles per hour</li><li>WT = vehicle weight, hundreds of pounds</li></ul><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Reading Boston housing prices data</span>
car = <span style=color:#447fcf>read.csv</span>(<span style=color:#ed9d13>&#34;Cars.csv&#34;</span>)
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Number of rows: &#34;</span>, <span style=color:#447fcf>nrow</span>(car), <span style=color:#ed9d13>&#34;\n&#34;</span>, <span style=color:#ed9d13>&#34;Number of variables: &#34;</span>, <span style=color:#447fcf>ncol</span>(car), <span style=color:#ed9d13>&#34;\n&#34;</span>)
<span style=color:#447fcf>head</span>(car)
</code></pre></div><pre><code>Number of rows:  81
 Number of variables:  5
</code></pre><table><thead><tr><th scope=col>HP</th><th scope=col>MPG</th><th scope=col>VOL</th><th scope=col>SP</th><th scope=col>WT</th></tr></thead><tbody><tr><td>49</td><td>53.70068</td><td>89</td><td>104.1854</td><td>28.76206</td></tr><tr><td>55</td><td>50.01340</td><td>92</td><td>105.4613</td><td>30.46683</td></tr><tr><td>55</td><td>50.01340</td><td>92</td><td>105.4613</td><td>30.19360</td></tr><tr><td>70</td><td>45.69632</td><td>92</td><td>113.4613</td><td>30.63211</td></tr><tr><td>53</td><td>50.50423</td><td>92</td><td>104.4613</td><td>29.88915</td></tr><tr><td>70</td><td>45.69632</td><td>89</td><td>113.1854</td><td>29.59177</td></tr></tbody></table><p>Our objective is to model the variation in <code>MPG</code> using other independent variables. That is,</p><p>$$MPG = \beta_0 + \beta_1 VOL + \beta_2 HP + \beta_3 SP + \beta_4 WT + \epsilon$$</p><p>Where, $\beta_1$ represents the amount of change in <code>MPG</code> per one unit change in <code>VOL</code> provided other variables are fixed. Let&rsquo;s consider below two cases,</p><p><strong>Case1:</strong> HP = 49; VOL = 89; SP = 104.1854; WT = 28.76206 => MPG = 104.1854</p><p><strong>Case2:</strong> HP = 49; VOL = 90; SP = 104.1854; WT = 28.76206 => MPG = 105.2453</p><p>then $\beta_1 = 105.2453 - 104.1854 = 1.0599$. Similarly, $\beta_2, \beta_3, \beta_4$</p><p>The above effect is called as
<a href=https://en.wikipedia.org/wiki/Ceteris_paribus target=_blank rel="nofollow noopener">&lt;code>Ceteris Paribus Effect&lt;/code></a>
.</p><p>But in real world it is very difficult to collect records in above manner. That&rsquo;s why we compute (function of) partial correlation coefficients to quantify the effect of one variable, keeping others constant.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Let&#39;s build MLR model to predict MPG based using other variables</span>
fit_mlr_actual = <span style=color:#447fcf>lm</span>(MPG ~ ., data = car)
<span style=color:#447fcf>summary</span>(fit_mlr_actual)
</code></pre></div><pre><code>Call:
lm(formula = MPG ~ ., data = car)

Residuals:
     Min       1Q   Median       3Q      Max
-0.94530 -0.32792 -0.04058  0.24256  1.71034

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  7.100e-17  5.461e-02   0.000   1.0000
HP          -1.285e+00  2.453e-01  -5.239  1.4e-06 ***
VOL         -8.207e-01  1.389e+00  -0.591   0.5563
SP           6.144e-01  2.458e-01   2.500   0.0146 *
WT           3.287e-01  1.390e+00   0.237   0.8136
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4915 on 76 degrees of freedom
Multiple R-squared:  0.7705,  Adjusted R-squared:  0.7585
F-statistic:  63.8 on 4 and 76 DF,  p-value: &lt; 2.2e-16
</code></pre><p>One key observation from above output is, Std. Error for <code>VOL</code> and <code>WT</code> is very huge comparing to others and this inflates <code>t values</code> and <code>p value</code>. Hence, these two variables becomes very insignificant for the model.</p><p>Let&rsquo;s go into deep, what happened to $Var(\hat{\beta_{VOL}})$ and $Var(\hat{\beta_{WT}})$?</p><p>Analogy for $Var(\hat{\beta})$ in MLR is as follows:</p><div>$$Var(\hat{\beta_{VOL}}) = \frac{\sigma^2}{n\sum_{i=1}^n (VOL_i - \bar{VOL})^2 (1 - R_{VOL}^2)}$$</div><p>Where, $R_{VOL}^2$ = Multiple R-squared value obtained by regressing VOL on all other independent variables</p><p><strong>Task:</strong> To understand it more clearly, take few random samples from cars data and run the MLR model and observe the variation in $\hat{\beta_{VOL}}$ and $\hat{\beta_{WT}}$.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Let&#39;s regress VOL on all other independent variables&#39;</span>
fit_mlr = <span style=color:#447fcf>lm</span>(VOL ~ HP + SP + WT, data = car)
<span style=color:#447fcf>summary</span>(fit_mlr)
</code></pre></div><pre><code>Call:
lm(formula = VOL ~ HP + SP + WT, data = car)

Residuals:
      Min        1Q    Median        3Q       Max
-0.068938 -0.031641 -0.008794  0.032018  0.077931

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) -6.155e-18  4.481e-03   0.000    1.000
HP           2.331e-02  1.995e-02   1.168    0.246
SP          -2.294e-02  2.000e-02  -1.147    0.255
WT           9.998e-01  4.557e-03 219.396   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.04033 on 77 degrees of freedom
Multiple R-squared:  0.9984,  Adjusted R-squared:  0.9984
F-statistic: 1.637e+04 on 3 and 77 DF,  p-value: &lt; 2.2e-16
</code></pre><p>It&rsquo;s surprising that, $R_{VOL}^2$ is 0.9984 and also only <code>WT</code> is significant. That is, these two predictors (<code>VOL</code> and <code>WT</code>) are highly correlated. This inflates $Var(\hat{\beta_{VOL}})$ and thus <code>t value</code>. We might be missing some of the important information because of high correlation between predictors. This problem is called as
<a href=https://en.wikipedia.org/wiki/Multicollinearity target=_blank rel="nofollow noopener">Multicollinearity</a>
.</p><p>One quick solution for this problem is to remove either <code>VOL</code> or <code>WT</code> from the model. Let&rsquo;s compute partial correlation coeficient between <code>MPG</code> and <code>VOL</code> by removing the effect of <code>WT</code> (say, $r_{MV.W}$) and partial correlation coeficient between <code>MPG</code> and <code>WT</code> by removing the effect of <code>VOL</code> (say, $r_{MW.V}$).</p><p>To compute $r_{MV.W}$ we need to compute the correlation between (a) part of <code>VOL</code> which cannot be explained by <code>WT</code> (regress <code>VOL</code> on <code>WT</code> and take the residuals) and (b) the part of <code>MPG</code> which cannot be explained by <code>WT</code> (regress <code>MPG</code> on <code>WT</code> and take the residuals)</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>fit_partial = <span style=color:#447fcf>lm</span>(VOL ~ WT, data = car)
fit_partial2 = <span style=color:#447fcf>lm</span>(MPG ~ WT, data = car)
res1 = fit_partial$residual
res2 = fit_partial2$residual
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Partial correlation coefficient between MPG and VOL by removing the effect of WT is: &#34;</span>, <span style=color:#447fcf>cor</span>(res1, res2))
</code></pre></div><pre><code>Partial correlation coefficient between MPG and VOL by removing the effect of WT is:  -0.08008873
</code></pre><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>fit_partial3 = <span style=color:#447fcf>lm</span>(WT ~ VOL, data = car)
fit_partial4 = <span style=color:#447fcf>lm</span>(MPG ~ VOL, data = car)
res1 = fit_partia3$residual
res2 = fit_partial4$residual
<span style=color:#447fcf>cat</span>(<span style=color:#ed9d13>&#34;Partial correlation coefficient between MPG and WT by removing the effect of VOL is: &#34;</span>, <span style=color:#447fcf>cor</span>(res1, res2))
</code></pre></div><pre><code>Partial correlation coefficient between MPG and WT by removing the effect of VOL is:  0.05538241
</code></pre><p>Since, $abs(r_{MV.W}) >= abs(r_{MW.V})$ we may remove <code>WT</code> from the model.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#999;font-style:italic># Remove WT and rerun the model</span>
fit_mlr_actual2 = <span style=color:#447fcf>lm</span>(MPG ~ .-WT, data = car)
<span style=color:#447fcf>summary</span>(fit_mlr_actual2)
</code></pre></div><pre><code>Call:
lm(formula = MPG ~ . - WT, data = car)

Residuals:
     Min       1Q   Median       3Q      Max
-0.94036 -0.31695 -0.03457  0.23316  1.71570

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  7.910e-17  5.427e-02   0.000   1.0000
HP          -1.293e+00  2.415e-01  -5.353 8.64e-07 ***
VOL         -4.925e-01  5.516e-02  -8.928 1.65e-13 ***
SP           6.222e-01  2.421e-01   2.571   0.0121 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4884 on 77 degrees of freedom
Multiple R-squared:  0.7704,  Adjusted R-squared:  0.7614
F-statistic: 86.11 on 3 and 77 DF,  p-value: &lt; 2.2e-16
</code></pre><p>After eliminating <code>WT</code> from the model there is an increment of ~0.3% in Adjusted R-squared and more importantly, <code>VOL</code> becomes significant at 0
<a href=https://en.wikipedia.org/wiki/Statistical_significance target=_blank rel="nofollow noopener">los</a>
(level of significance)</p><p><a id=Assumptions></a></p><h2 id=3-assumptions>3. Assumptions</h2><p><strong>Linear in Parameters:</strong> We assume that there is a linear relation between dependent and set of independent variables</p><p><strong>Zero conditional mean:</strong> $E(\epsilon \mid X) = 0$</p><p><strong>Homoskedasticity:</strong> $Var(\epsilon \mid X) = \sigma^2$ (Constant)</p><p><strong>No perfect Collinearity:</strong> All predecitors must be independent among themselves</p><p><strong>No serial correlation in errors:</strong> Erros must be uncorrelated among themselves. In otherwords, observations or records must be independent of each other.</p><p>We discussed first 4 assumptions in section 1 and 2.</p><p>Here is a book that I recommend to learn more about this:</p><p><a target=_blank href="https://www.amazon.com/gp/product/1111531048/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1111531048&linkCode=as2&tag=nkaveti-20&linkId=83f6e694209869322f8bfad406883d2f"><img border=0 src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=1111531048&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=nkaveti-20"></a><img src="//ir-na.amazon-adsystem.com/e/ir?t=nkaveti-20&l=am2&o=1&a=1111531048" width=1 height=1 border=0 alt style=border:none!important;margin:0!important></p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="MLWhiz - Your Home for DS, ML, AI!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>