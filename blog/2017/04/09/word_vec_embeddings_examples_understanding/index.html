<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-NMQD44T');</script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
  <meta name="author" content="Rahul Agarwal">
  <meta name="description" content="Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings. Till now whenever I heard the term word2vec I visualized it as ..." >
  <meta name="keywords" content="understand word2vec embeddings, word2vec layman, word2vec intuition, word2vec , today i learned this, today i learned this series, quora data science challenge, question similarity" >

<!-- OpenGraph protocol tags: http://ogp.me/ -->
<!-- originally adopted to be used for: https://blog.kmonsoor.com -->
<meta property="og:site_name" content="mlwhiz" />
<meta property="og:type" content="article" />
    
<meta property="og:title" content="Today I Learned This Part I: What are word2vec Embeddings?" />
<meta property="og:url" content="https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" />
<meta property="og:description" content="Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings. Till now whenever I heard the term word2vec I visualized it as ..." />

<meta name="twitter:image" content="https://mlwhiz.com" />

<meta property="article:published_time" content="2017-04-09 04:43:00" />
<!-- End of OpenGraph protocol tags -->


<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Today I Learned This Part I: What are word2vec Embeddings?" />
<meta property="twitter:description" content="Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings. Till now whenever I heard the term word2vec I visualized it as ..." />
<meta property="twitter:url" content="https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" />
<meta name="twitter:image" content="https://mlwhiz.com" />


  <link href="http://mlwhiz.com/atom.xml" type="application/atom+xml" rel="alternate"
        title="mlwhiz Atom Feed" />
  <link href="http://mlwhiz.com/feed.rss" type="application/rss+xml" rel="alternate"
        title="mlwhiz RSS Feed" />


<link rel="canonical" href="https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/"/>

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <link href="https://mlwhiz.com/favicon.png" rel="icon">
  <link href="https://mlwhiz.com/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">
      <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro" rel="stylesheet" type="text/css">
 <link href="https://mlwhiz.com/theme/css/custom.css"
        rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://mlwhiz.com/theme/highlight/styles/atelier-dune.dark.css">

  <script src="https://mlwhiz.com/theme/js/modernizr-2.0.js"></script>
  <script src="https://mlwhiz.com/theme/js/ender.js"></script>
  <script src="https://mlwhiz.com/theme/js/octopress.js" type="text/javascript"></script>

  <script src="https://mlwhiz.com/theme/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">

  <!--added buttons to share....-->

<!-- BEGIN SHAREAHOLIC CODE -->
<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>

<!-- END SHAREAHOLIC CODE -->
<!--
  <script src="//load.sumome.com/" data-sumo-site-id="22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48" async="async"></script>
 --> 
<!--
  <script type='text/javascript' src='//platform-api.sharethis.com/js/sharethis.js#property=5a1bdf9806d3310011e61348&product=sticky-share-buttons' async='async'></script>
-->


<!-- SUMO CODE FOR EMAILS-->
  <script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>


</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
  <header role="banner"><hgroup>
  <h1><a href="https://mlwhiz.com/">mlwhiz</a></h1>
    <h2>Deep Learning, Data Science and NLP Enthusiast</h2>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="http://mlwhiz.com/atom.xml" rel="subscribe-atom">Atom</a></li>

</ul>


<form onsubmit="this['q'].value = this['q_raw'].value + ' site:mlwhiz.com';" method="get" action="https://google.com/search">
  <fieldset role="search">
	<input type="hidden" value="" name="q">
	<input class="search" type="text" placeholder="Search" results="0" name="q_raw">
  </fieldset>

</form>

<ul class="main-navigation">
    <li><a href="http://mlwhiz.com">Blog</a></li>
    <li><a href="http://mlwhiz.com/archives.html">Archives</a></li>
    <li><a href="http://mlwhiz.com/pages/about.html">About</a></li>
    <li><a href="http://mlwhiz.com/pages/links.html">Great Links</a></li>
  
</ul></nav>
  <div id="main">
    <div id="content">

<div>
  <article class="hentry" role="article">

<header>
      <h1 class="entry-title">Today I Learned This Part I: What are word2vec Embeddings?</h1>
      <p class="meta"><time datetime="2017-04-09T04:43:00" pubdate>Apr 09, 2017</time></p>
</header>

  <div class="entry-content"><p>Recently Quora put out a <a href="https://www.kaggle.com/c/quora-question-pairs">Question similarity</a> competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.</p>
<p>Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.</p>
<p>For those who don't know <em>bag of words</em>:
If we have a series of sentences(documents)</p>
<div style="margin-left: 10px;margin-bottom:9px;color: green">
1. This is good       - [1,1,1,0,0]<br>
2. This is bad        - [1,1,0,1,0]<br>
3. This is awesome    - [1,1,0,0,1]<br>
</div>

<p>Bag of words would encode it using <em>0:This 1:is 2:good 3:bad 4:awesome</em></p>
<p>But it is much more powerful than that.</p>
<p>What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.</p>
<h2>How does that help?</h2>
<p>We can use this for multiple scenarios but the most common are:</p>
<p>A. <em>Using word2vec embeddings we can find out similarity between words</em>.
Assume you have to answer if these two statements signify the same thing:
<div style="margin-left: 10px;margin-bottom:9px;color: green">
1. President greets press in Chicago<br>
2. Obama speaks to media in Illinois.
</div>
If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.</p>
<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/word2vecembed.png"  height="400" width="700" ></center>
</div>

<p>But with a word encoding we can say that</p>
<div style="margin-left: 10px;margin-bottom:9px;color: green">
1. President is similar to Obama<br>
2. greets is similar to speaks<br>
3. press is similar to media<br>
4. Chicago is similar to Illinois<br>
</div>

<p>B. <em>Encode Sentences</em>: I read a <a href="https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur">post</a> from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.</p>
<p>His Approach: Lets say the sentence is "What is this"
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)
<div style="margin-left: 10px;margin-bottom:9px;color: green">
1. what : [.25 ,.25 ,.25 ,.25]<br>
2. is   : [  1 ,  0 ,  0 ,  0]<br>
3. this : [ .5 ,  0 ,  0 , .5]<br>
</div></p>
<p>Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.</p>
<div style="margin-bottom:9px;margin-left: 10px">
Elementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
<br>
divided by
<br>
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
<br>
gives:[1.16, .17, .17, 0.5]
</div>

<p>Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.</p>
<p>C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?<br>
Guess what it comes out to be : <em>Queen</em></p>
<h2>Application/Coding:</h2>
<p>Now lets get down to the coding part as we know a little bit of fundamentals.</p>
<p>First of all we download a custom word embedding from Google. There are many other embeddings too.</p>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="bash">wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
</code></pre>

<p>The above file is pretty big. Might take some time. Then moving on to coding.</p>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">from gensim.models import word2vec
model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)
</code></pre>

<p><br></p>
<h3>1. Starting simple, lets find out similar words. Want to find similar words to python?</h3>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">model.most_similar('python')
</code></pre>

<div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'pythons', 0.6688377261161804),<br>
 (u'Burmese_python', 0.6680364608764648),<br>
 (u'snake', 0.6606293320655823),<br>
 (u'crocodile', 0.6591362953186035),<br>
 (u'boa_constrictor', 0.6443519592285156),<br>
 (u'alligator', 0.6421656608581543),<br>
 (u'reptile', 0.6387745141983032),<br>
 (u'albino_python', 0.6158879995346069),<br>
 (u'croc', 0.6083582639694214),<br>
 (u'lizard', 0.601341724395752)]<br>
 </div>

<p><br></p>
<h3>2. Now we can use this model to find the solution to the equation:</h3>
<p>What is king - man + woman?</p>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">model.most_similar(positive = ['king','woman'],negative = ['man'])
</code></pre>

<div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'queen', 0.7118192315101624),<br>
 (u'monarch', 0.6189674139022827),<br>
 (u'princess', 0.5902431011199951),<br>
 (u'crown_prince', 0.5499460697174072),<br>
 (u'prince', 0.5377321839332581),<br>
 (u'kings', 0.5236844420433044),<br>
 (u'Queen_Consort', 0.5235946178436279),<br>
 (u'queens', 0.5181134343147278),<br>
 (u'sultan', 0.5098593235015869),<br>
 (u'monarchy', 0.5087412595748901)]<br>
</div>

<p>You can do plenty of freaky/cool things using this:</p>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"></script>

<h3>3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?</h3>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">model.most_similar(positive = ['emma','he','male','mr'],negative = ['she','mrs','female'])
</code></pre>

<div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'sanchez', 0.4920658469200134),<br>
 (u'kenny', 0.48300960659980774),<br>
 (u'alves', 0.4684845209121704),<br>
 (u'gareth', 0.4530612826347351),<br>
 (u'bellamy', 0.44884198904037476),<br>
 (u'gibbs', 0.445194810628891),<br>
 (u'dos_santos', 0.44508373737335205),<br>
 (u'gasol', 0.44387346506118774),<br>
 (u'silva', 0.4424275755882263),<br>
 (u'shaun', 0.44144102931022644)]<br><br>
</div>

<h3>4. Find which word doesn't belong to a <a href="https://github.com/dhammack/Word2VecExample/blob/master/main.py">list</a>?</h3>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">model.doesnt_match("math shopping reading science".split(" "))
</code></pre>

<p>I think staple doesnt belong in this list!</p>
<h2>Other Cool Things</h2>
<h3>1. Recommendations:</h3>
<div style="margin-top: 4px; margin-bottom: 10px;">
<center><img src="/images/recommendationpaper.png"  height="400" width="700" ></center>
</div>

<p>In this <a href="https://arxiv.org/abs/1603.04259">paper</a>, the authors have shown that itembased CF can be cast in the same framework of word embedding.</p>
<h3>2. Some other <a href="http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html">examples</a> that people have seen after using their own embeddings:</h3>
<p>Library - Books = Hall<br>
Obama + Russia - USA = Putin<br>
Iraq - Violence = Jordan<br>
President - Power = Prime Minister (Not in India Though)<br></p>
<h3>3.Seeing the above I started playing with it a little.</h3>
<p><strong>Is this model sexist?</strong></p>
<pre style="font-size:60%; padding:7px; margin:0em;">
<code class="python">model.most_similar(positive = ["donald_trump"],negative = ['brain'])
</code></pre>

<div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'novak', 0.40405112504959106),<br>
 (u'ozzie', 0.39440611004829407),<br>
 (u'democrate', 0.39187556505203247),<br>
 (u'clinton', 0.390536367893219),<br>
 (u'hillary_clinton', 0.3862358033657074),<br>
 (u'bnp', 0.38295692205429077),<br>
 (u'klaar', 0.38228923082351685),<br>
 (u'geithner', 0.380607008934021),<br>
 (u'bafana_bafana', 0.3801495432853699),<br>
 (u'whitman', 0.3790769875049591)]<br>
</div>

<p>Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.</p></div>

<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.415&subid=0&type=4"><IMG border="0"   alt="Deep Learning Specialization on Coursera" src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.415&subid=0&type=4&gridnum=16"></a>    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">Rahul Agarwal</span>
  </span>
<time datetime="2017-04-09T04:43:00" pubdate>Apr 09, 2017</time>  <span class="categories">
    <a class="category" href="https://mlwhiz.com/tag/python.html">Python</a>
    <a class="category" href="https://mlwhiz.com/tag/nlp.html">NLP</a>
    <a class="category" href="https://mlwhiz.com/tag/algorithms.html">Algorithms</a>
    <a class="category" href="https://mlwhiz.com/tag/kaggle.html">Kaggle</a>
  </span>
  <br>
  <span style="font-size:14px;color:green">
  Advertiser Disclosure: All Amazon links are affiliate links, which means I receive compensation for any purchases through them. You do not have to purchase via my links, but you support me if you do.
  </span>
</p><div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>

<!--
<div class="sharing">
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" data-via="MLWhiz" data-counturl="mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" >Tweet</a>
  <div class="g-plusone" data-size="medium"></div>
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
</div>
-->    </footer>
  </article>
  
  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
  </section>

</div>
<aside class="sidebar">
  <section>

     <!-- Bbuy coffee ko-fi Form -->
  <div style="text-align:center">    
  <a href='https://ko-fi.com/S6S3NPCD' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://az743702.vo.msecnd.net/cdn/kofi4.png?v=0' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>
  </div>

    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">NLP Learning Series: Text Preprocessing Methods for Deep Learning</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/">A Layman guide to moving from Keras to Pytorch</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2018/12/17/text_classification/">What Kagglers are using for Text Classification</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2018/12/07/connected_components/">To all Data Scientists - The one Graph Algorithm you need to know</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2018/09/22/object_detection/">Object Detection: An End to End Theoretical Perspective</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/">Hyperopt - A bayesian Parameter Tuning Framework</a>
      </li>
      <li class="post">
          <a href="https://mlwhiz.com/blog/2017/12/26/How_to_win_a_data_science_competition/">Using XGBoost for time series prediction tasks</a>
      </li>
    </ul>
  </section>
<!--
  <section>

    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="https://mlwhiz.com/category/ai-deep-learningkaggle.html">ai, deep learning,kaggle</a></li>
        <li><a href="https://mlwhiz.com/category/ai-deep-learningkaggle-nlp.html">ai, deep learning,kaggle, NLP</a></li>
        <li><a href="https://mlwhiz.com/category/big-data.html">Big Data</a></li>
        <li><a href="https://mlwhiz.com/category/data-science-statistics-resources-learning-books-python-distributions-statistical-inference-hadoop-spark-deep-learning.html">Data Science, Statistics, Resources, Learning, Books, Python, Distributions, Statistical Inference, hadoop, spark, deep learning</a></li>
        <li><a href="https://mlwhiz.com/category/deep-learning-image.html">deep learning, image</a></li>
        <li><a href="https://mlwhiz.com/category/hadoop.html">Hadoop</a></li>
        <li><a href="https://mlwhiz.com/category/machine-learning-statistics-linear-regression.html">Machine Learning, Statistics, Linear Regression</a></li>
        <li><a href="https://mlwhiz.com/category/pyspark-python.html">pyspark, python</a></li>
        <li><a href="https://mlwhiz.com/category/python.html">Python</a></li>
        <li><a href="https://mlwhiz.com/category/python-bash-tools.html">Python, bash, tools</a></li>
        <li><a href="https://mlwhiz.com/category/python-flask-ml.html">Python, Flask, ML</a></li>
        <li><a href="https://mlwhiz.com/category/python-kaggle-coursera.html">Python, kaggle, coursera,</a></li>
        <li><a href="https://mlwhiz.com/category/python-machine-learning.html">Python, machine learning</a></li>
        <li><a href="https://mlwhiz.com/category/python-machine-learning-hyperopt-bayesian-xgboost.html">Python, machine learning, hyperopt, bayesian, xgboost</a></li>
        <li><a href="https://mlwhiz.com/category/python-machine-learning-probability.html">Python, machine learning, probability</a></li>
        <li><a href="https://mlwhiz.com/category/python-nlp-algorithms-kaggle.html">Python, NLP, Algorithms, Kaggle</a></li>
        <li><a href="https://mlwhiz.com/category/python-nlp-algorithms-kaggle-tilt.html">Python, NLP, Algorithms, Kaggle ,TILT</a></li>
        <li><a href="https://mlwhiz.com/category/python-statistics.html">Python, Statistics</a></li>
        <li><a href="https://mlwhiz.com/category/python-visualization-statistics.html">Python, Visualization, Statistics</a></li>
        <li><a href="https://mlwhiz.com/category/statistics.html">Statistics</a></li>
        <li><a href="https://mlwhiz.com/category/statisticsdata-science.html">statistics,data science</a></li>
        <li><a href="https://mlwhiz.com/category/statisticsprobability.html">statistics,probability</a></li>
        <li><a href="https://mlwhiz.com/category/vw.html">VW</a></li>
    </ul>
  </section>


  <section>
  <h1>Tags</h1>
    <a href="https://mlwhiz.com/tag/.html"></a>,    <a href="https://mlwhiz.com/tag/nlp.html">NLP</a>,    <a href="https://mlwhiz.com/tag/data-munging.html">data munging</a>,    <a href="https://mlwhiz.com/tag/attention-models-for-text.html">Attention models for text</a>,    <a href="https://mlwhiz.com/tag/data-science.html">data science</a>,    <a href="https://mlwhiz.com/tag/probability.html">probability</a>,    <a href="https://mlwhiz.com/tag/big-data.html">Big Data</a>,    <a href="https://mlwhiz.com/tag/cdf.html">cdf</a>,    <a href="https://mlwhiz.com/tag/dataframe.html">dataframe</a>,    <a href="https://mlwhiz.com/tag/deep-learning.html">deep learning</a>,    <a href="https://mlwhiz.com/tag/rcnn.html">rcnn</a>,    <a href="https://mlwhiz.com/tag/books.html">Books</a>,    <a href="https://mlwhiz.com/tag/ggplot2.html">ggplot2</a>,    <a href="https://mlwhiz.com/tag/text-classification.html">text classification</a>,    <a href="https://mlwhiz.com/tag/learning.html">learning</a>,    <a href="https://mlwhiz.com/tag/statistic.html">Statistic</a>,    <a href="https://mlwhiz.com/tag/bidirectional-rnn.html">bidirectional RNN</a>,    <a href="https://mlwhiz.com/tag/graph-algorithms.html">graph algorithms</a>,    <a href="https://mlwhiz.com/tag/distributions.html">distributions</a>,    <a href="https://mlwhiz.com/tag/hyperparameter-tuning.html">hyperparameter tuning</a>,    <a href="https://mlwhiz.com/tag/stanford-software-seaborn.html">stanford software seaborn</a>,    <a href="https://mlwhiz.com/tag/apache-spark.html">Apache Spark</a>,    <a href="https://mlwhiz.com/tag/statistics.html">Statistics</a>,    <a href="https://mlwhiz.com/tag/vw.html">vw</a>,    <a href="https://mlwhiz.com/tag/pairplot-seaborn.html">pairplot seaborn</a>,    <a href="https://mlwhiz.com/tag/bidirectional-gru-for-text.html">bidirectional GRU for text</a>,    <a href="https://mlwhiz.com/tag/seaborn.html">Seaborn</a>,    <a href="https://mlwhiz.com/tag/artificial-intelligence.html">artificial intelligence</a>,    <a href="https://mlwhiz.com/tag/hadoop.html">hadoop</a>,    <a href="https://mlwhiz.com/tag/categorical-data.html">categorical data</a>,    <a href="https://mlwhiz.com/tag/birnn.html">birnn</a>,    <a href="https://mlwhiz.com/tag/machine-learning.html">machine learning</a>,    <a href="https://mlwhiz.com/tag/deeplearning.html">deeplearning</a>,    <a href="https://mlwhiz.com/tag/geometric.html">geometric</a>,    <a href="https://mlwhiz.com/tag/pandas.html">pandas</a>,    <a href="https://mlwhiz.com/tag/resources.html">Resources</a>,    <a href="https://mlwhiz.com/tag/linear-regression.html">Linear Regression</a>,    <a href="https://mlwhiz.com/tag/object-detection.html">object detection</a>,    <a href="https://mlwhiz.com/tag/pyspark.html">Pyspark</a>,    <a href="https://mlwhiz.com/tag/python.html">python</a>,    <a href="https://mlwhiz.com/tag/faster-rcnn.html">faster rcnn</a>,    <a href="https://mlwhiz.com/tag/kaggle.html">Kaggle</a>,    <a href="https://mlwhiz.com/tag/matplotlib.html">Matplotlib</a>,    <a href="https://mlwhiz.com/tag/binomial.html">binomial</a>,    <a href="https://mlwhiz.com/tag/variance.html">variance</a>,    <a href="https://mlwhiz.com/tag/regplot.html">regplot</a>,    <a href="https://mlwhiz.com/tag/python-visualizations.html">Python Visualizations</a>,    <a href="https://mlwhiz.com/tag/spark.html">spark</a>,    <a href="https://mlwhiz.com/tag/cs109.html">cs109</a>,    <a href="https://mlwhiz.com/tag/bash-for-data-science.html">bash for data science</a>,    <a href="https://mlwhiz.com/tag/web-scraping.html">web scraping</a>,    <a href="https://mlwhiz.com/tag/basics.html">Basics</a>,    <a href="https://mlwhiz.com/tag/ctr.html">ctr</a>,    <a href="https://mlwhiz.com/tag/statistical-inference.html">Statistical Inference</a>,    <a href="https://mlwhiz.com/tag/expected-value.html">expected value</a>,    <a href="https://mlwhiz.com/tag/bidirectional-lstm-for-text.html">bidirectional LSTM for text</a>,    <a href="https://mlwhiz.com/tag/layman-explanations.html">Layman Explanations</a>,    <a href="https://mlwhiz.com/tag/mapreduce.html">mapreduce</a>,    <a href="https://mlwhiz.com/tag/slr.html">SLR</a>,    <a href="https://mlwhiz.com/tag/openshift.html">Openshift</a>,    <a href="https://mlwhiz.com/tag/mlr.html">MLR</a>,    <a href="https://mlwhiz.com/tag/pretrained-models.html">pretrained models</a>,    <a href="https://mlwhiz.com/tag/algorithms.html">Algorithms</a>,    <a href="https://mlwhiz.com/tag/poisson.html">poisson</a>,    <a href="https://mlwhiz.com/tag/bayesian-optimization.html">bayesian optimization</a>,    <a href="https://mlwhiz.com/tag/bash-commands.html">bash commands</a>,    <a href="https://mlwhiz.com/tag/pdf.html">pdf</a>,    <a href="https://mlwhiz.com/tag/deploy-ml-models.html">Deploy ML Models</a>,    <a href="https://mlwhiz.com/tag/lmplot-seaborn.html">lmplot seaborn</a>,    <a href="https://mlwhiz.com/tag/flaskapp.html">FlaskApp</a>  </section>
-->
<!--

    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="http://mlwhiz.com/feed.rss" type="application/rss+xml" rel="alternate">RSS</a></li>
            <li><a href="http://mlwhiz.com/atom.xml" type="application/atom+xml" rel="alternate">Atom</a></li>
            <li><a href="http://twitter.com/mlwhiz" target="_blank">twitter</a></li>
        </ul>
    </section>


<section>
    <a href="http://twitter.com/MLWhiz" class="twitter-follow-button" data-show-count="true">Follow @MLWhiz</a>
</section>
-->

  <section>




 <!-- Begin shareaholic follow -->
   <div class="shareaholic-canvas" data-app="follow_buttons" data-app-id="28033293" style="white-space: inherit;"></div>
  <!-- Begin MailChimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
  #mc_embed_signup .button {background-color: #127edc;}
  #mc_embed_signup form {
    display: block;
    position: relative;
    text-align: left;
    padding: 10px -5px 10px 3%;}
  /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
     We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//mlwhiz.us15.list-manage.com/subscribe/post?u=4e9962f4ce4a94818bcc2f249&amp;id=87a48fafdd" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_4e9962f4ce4a94818bcc2f249_87a48fafdd" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
</section>
<!--End mc_embed_signup-->

</aside>    </div>
  </div>
  <footer role="contentinfo" style="margin-bottom:0em"><p>
  <script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
    Copyright &copy;  2014-2019  - Rahul Agarwal -
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p>  <script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
  </footer>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-54777926-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-54777926-1');
    
    ga('send', 'pageview');
</script>
	<script type="text/javascript">
	  var disqus_shortname = 'mlwhiz';
          var disqus_identifier = '/blog/2017/04/09/word_vec_embeddings_examples_understanding/';
          var disqus_url = 'https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/';
          var disqus_title = 'Today I Learned This Part I: What are word2vec Embeddings?';
	  (function() {
	    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
	    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	   })();
	</script>
  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
	  
    })();
  </script>
  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
</body>

</html>