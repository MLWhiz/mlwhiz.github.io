<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Today I Learned This Part I: What are word2vec Embeddings?</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Your Daily dose of data science.This is a series of post in which I write about the things I learn almost everyday. This post particularly provides a description,examples,code and practical use cases for word2vec embeddings in real world.">
	
	<link rel='preload' href='//apps.shareaholic.com/assets/pub/shareaholic.js' as='script' />
	<script type="text/javascript" data-cfasync="false" async src="//apps.shareaholic.com/assets/pub/shareaholic.js" data-shr-siteid="fd1ffa7fd7152e4e20568fbe49a489d0"></script>
	
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="Today I Learned This Part I: What are word2vec Embeddings?" />
<meta property="og:description" content="Your Daily dose of data science.This is a series of post in which I write about the things I learn almost everyday. This post particularly provides a description,examples,code and practical use cases for word2vec embeddings in real world." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" /><meta property="article:published_time" content="2017-04-09T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2017-04-09T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Today I Learned This Part I: What are word2vec Embeddings?"/>
<meta name="twitter:description" content="Your Daily dose of data science.This is a series of post in which I write about the things I learn almost everyday. This post particularly provides a description,examples,code and practical use cases for word2vec embeddings in real world."/>


	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	
	
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54777926-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
  	<script async>(function(s,u,m,o,j,v){j=u.createElement(m);v=u.getElementsByTagName(m)[0];j.async=1;j.src=o;j.dataset.sumoSiteId='22863fd8ad7ebbfab9b8ca60b7db8f65e9a15559f384f785f66903e365aa8f48';v.parentNode.insertBefore(j,v)})(window,document,'script','//load.sumo.com/');</script>
  	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</head>
<body class="body">
	  
	  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T"
	  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	  
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">

			<a class="logo__link" href="/" title="MLWhiz" rel="home">

				<div class="logo__title">MLWhiz</div>
				<div class="logo__tagline">Deep Learning, Data Science and NLP Enthusiast</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Blog</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/archive">Archive</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">About Me</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/atom.xml">RSS</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Today I Learned This Part I: What are word2vec Embeddings?</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2017-04-09T00:00:00">April 09, 2017</time>
</div>
</div>
		</header>
		<div class="content post__content clearfix">
			

<p>Recently Quora put out a <a href="https://www.kaggle.com/c/quora-question-pairs" rel="nofollow" target="_blank">Question similarity</a> competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.</p>

<p>Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.</p>

<p>For those who don&rsquo;t know <em>bag of words</em>:
If we have a series of sentences(documents)</p>

<ol>
<li>This is good       - [1,1,1,0,0]</li>
<li>This is bad        - [1,1,0,1,0]</li>
<li>This is awesome    - [1,1,0,0,1]</li>
</ol>

<p>Bag of words would encode it using <em>0:This 1:is 2:good 3:bad 4:awesome</em></p>

<p>But it is much more powerful than that.</p>

<p>What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.</p>

<h2 id="how-does-that-help">How does that help?</h2>

<p>We can use this for multiple scenarios but the most common are:</p>

<p>A. <em>Using word2vec embeddings we can find out similarity between words</em>.
Assume you have to answer if these two statements signify the same thing:</p>

<ol>
<li>President greets press in Chicago</li>
<li>Obama speaks to media in Illinois.</li>
</ol>

<p>If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.</p>

<div style="margin-top: 9px; margin-bottom: 10px;">
<center><img src="/images/word2vecembed.png"  height="400" width="700" ></center>
</div>

<p>But with a word encoding we can say that</p>

<ol>
<li>President is similar to Obama</li>
<li>greets is similar to speaks</li>
<li>press is similar to media</li>
<li>Chicago is similar to Illinois</li>
</ol>

<p>B. <em>Encode Sentences</em>: I read a <a href="https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur" rel="nofollow" target="_blank">post</a> from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.</p>

<p>His Approach: Lets say the sentence is &ldquo;What is this&rdquo;
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)</p>

<ol>
<li>what : [.25 ,.25 ,.25 ,.25]</li>
<li>is   : [  1 ,  0 ,  0 ,  0]</li>
<li>this : [ .5 ,  0 ,  0 , .5]</li>
</ol>

<p>Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.</p>

<pre><code>Elementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
divided by
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
gives:[1.16, .17, .17, 0.5]
</code></pre>

<p>Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.</p>

<p>C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?</p>

<p>Guess what it comes out to be : <em>Queen</em></p>

<h2 id="application-coding">Application/Coding:</h2>

<p>Now lets get down to the coding part as we know a little bit of fundamentals.</p>

<p>First of all we download a custom word embedding from Google. There are many other embeddings too.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</code></pre></div>
<p>The above file is pretty big. Might take some time. Then moving on to coding.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> word2vec
model <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>KeyedVectors<span style="color:#f92672">.</span>load_word2vec_format(<span style="color:#e6db74">&#39;data/GoogleNews-vectors-negative300.bin.gz&#39;</span>, binary<span style="color:#f92672">=</span>True)</code></pre></div>
<h3 id="1-starting-simple-lets-find-out-similar-words-want-to-find-similar-words-to-python">1. Starting simple, lets find out similar words. Want to find similar words to python?</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">model<span style="color:#f92672">.</span>most_similar(<span style="color:#e6db74">&#39;python&#39;</span>)</code></pre></div>
<div style="font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'pythons', 0.6688377261161804),<br>
 (u'Burmese_python', 0.6680364608764648),<br>
 (u'snake', 0.6606293320655823),<br>
 (u'crocodile', 0.6591362953186035),<br>
 (u'boa_constrictor', 0.6443519592285156),<br>
 (u'alligator', 0.6421656608581543),<br>
 (u'reptile', 0.6387745141983032),<br>
 (u'albino_python', 0.6158879995346069),<br>
 (u'croc', 0.6083582639694214),<br>
 (u'lizard', 0.601341724395752)]<br>
 </div>

<h3 id="2-now-we-can-use-this-model-to-find-the-solution-to-the-equation">2. Now we can use this model to find the solution to the equation:</h3>

<p>What is king - man + woman?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">model<span style="color:#f92672">.</span>most_similar(positive <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;king&#39;</span>,<span style="color:#e6db74">&#39;woman&#39;</span>],negative <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;man&#39;</span>])</code></pre></div>
<div style="font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'queen', 0.7118192315101624),<br>
 (u'monarch', 0.6189674139022827),<br>
 (u'princess', 0.5902431011199951),<br>
 (u'crown_prince', 0.5499460697174072),<br>
 (u'prince', 0.5377321839332581),<br>
 (u'kings', 0.5236844420433044),<br>
 (u'Queen_Consort', 0.5235946178436279),<br>
 (u'queens', 0.5181134343147278),<br>
 (u'sultan', 0.5098593235015869),<br>
 (u'monarchy', 0.5087412595748901)]<br>
</div>

<p>You can do plenty of freaky/cool things using this:</p>

<h3 id="3-lets-say-you-wanted-a-girl-and-had-a-girl-name-like-emma-in-mind-but-you-got-a-boy-so-what-is-the-male-version-for-emma">3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">model<span style="color:#f92672">.</span>most_similar(positive <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;emma&#39;</span>,<span style="color:#e6db74">&#39;he&#39;</span>,<span style="color:#e6db74">&#39;male&#39;</span>,<span style="color:#e6db74">&#39;mr&#39;</span>],negative <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;she&#39;</span>,<span style="color:#e6db74">&#39;mrs&#39;</span>,<span style="color:#e6db74">&#39;female&#39;</span>])</code></pre></div>
<div style="font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'sanchez', 0.4920658469200134),<br>
 (u'kenny', 0.48300960659980774),<br>
 (u'alves', 0.4684845209121704),<br>
 (u'gareth', 0.4530612826347351),<br>
 (u'bellamy', 0.44884198904037476),<br>
 (u'gibbs', 0.445194810628891),<br>
 (u'dos_santos', 0.44508373737335205),<br>
 (u'gasol', 0.44387346506118774),<br>
 (u'silva', 0.4424275755882263),<br>
 (u'shaun', 0.44144102931022644)]<br><br>
</div>

<h3 id="4-find-which-word-doesn-t-belong-to-a-list-https-github-com-dhammack-word2vecexample-blob-master-main-py">4. Find which word doesn&rsquo;t belong to a <a href="https://github.com/dhammack/Word2VecExample/blob/master/main.py" rel="nofollow" target="_blank">list</a>?</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">model<span style="color:#f92672">.</span>doesnt_match(<span style="color:#e6db74">&#34;math shopping reading science&#34;</span><span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>))</code></pre></div>
<p>I think staple doesn&rsquo;t belong in this list!</p>

<h2 id="other-cool-things">Other Cool Things</h2>

<h3 id="1-recommendations">1. Recommendations:</h3>

<div style="margin-top: 4px; margin-bottom: 10px;">
<center><img src="/images/recommendationpaper.png"  height="400" width="700" ></center>
</div>

<p>In this <a href="https://arxiv.org/abs/1603.04259" rel="nofollow" target="_blank">paper</a>, the authors have shown that itembased CF can be cast in the same framework of word embedding.</p>

<h3 id="2-some-other-examples-http-byterot-blogspot-in-2015-06-five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-nlp-gensim-html-that-people-have-seen-after-using-their-own-embeddings">2. Some other <a href="http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html" rel="nofollow" target="_blank">examples</a> that people have seen after using their own embeddings:</h3>

<p>Library - Books = Hall<br>
Obama + Russia - USA = Putin<br>
Iraq - Violence = Jordan<br>
President - Power = Prime Minister (Not in India Though)<br></p>

<h3 id="3-seeing-the-above-i-started-playing-with-it-a-little">3.Seeing the above I started playing with it a little.</h3>

<p><strong>Is this model sexist?</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">model<span style="color:#f92672">.</span>most_similar(positive <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;donald_trump&#34;</span>],negative <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;brain&#39;</span>])</code></pre></div>
<div style="font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px">
[(u'novak', 0.40405112504959106),<br>
 (u'ozzie', 0.39440611004829407),<br>
 (u'democrate', 0.39187556505203247),<br>
 (u'clinton', 0.390536367893219),<br>
 (u'hillary_clinton', 0.3862358033657074),<br>
 (u'bnp', 0.38295692205429077),<br>
 (u'klaar', 0.38228923082351685),<br>
 (u'geithner', 0.380607008934021),<br>
 (u'bafana_bafana', 0.3801495432853699),<br>
 (u'whitman', 0.3790769875049591)]<br>
</div>

<p>Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.</p>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/data-science/" rel="tag">Data Science</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/python/" rel="tag">Python</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/nlp/" rel="tag">NLP</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/algorithms/" rel="tag">Algorithms</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/kaggle/" rel="tag">Kaggle</a></li>
	</ul>
</div>
		

<div class="shareaholic-canvas" data-app="share_buttons" data-app-id="28372088"></div>
<a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&offerid=467035.415&subid=0&type=4"><IMG border="0"   alt="Deep Learning Specialization on Coursera" src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=467035.415&subid=0&type=4&gridnum=16"></a>


	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Top Data Science Resources on the Internet right now</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/blog/2017/04/16/maths_beats_intuition/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Maths Beats Intuition probably every damn time</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlwhiz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy;  2014-2019 Rahul Agarwal.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>



	</div>
<script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=93f2f4f9-cf51-415d-84af-08cbb74b178f"></script>
<script async defer src="/js/menu.js"></script></body>
</html>