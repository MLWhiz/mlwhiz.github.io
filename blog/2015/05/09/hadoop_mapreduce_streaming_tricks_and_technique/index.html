<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Hadoop Mapreduce Streaming Tricks and Techniques - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem."><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Hadoop Mapreduce Streaming Tricks and Techniques - MLWhiz"><meta property="og:description" content="I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/"><meta property="og:image" content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta property="article:published_time" content="2015-05-09T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-08T22:10:24+01:00"><meta property="article:tag" content="Big Data"><meta property="article:tag" content="Data science"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/category_bgs/default_bg.jpg"><meta name=twitter:title content="Hadoop Mapreduce Streaming Tricks and Techniques - MLWhiz"><meta name=twitter:description content="I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/#primaryimage","url":"https://mlwhiz.com/images/category_bgs/default_bg.jpg","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/#webpage","url":"https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/","inLanguage":"en-US","name":"Hadoop Mapreduce Streaming Tricks and Techniques - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/#primaryimage"},"datePublished":"2015-05-09T00:00:00.00Z","dateModified":"2023-07-08T22:10:24.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="Helping You Learn Data Science!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/big-data class=categoryStyle>Big Data</a>
<a href=/categories/data-science class=categoryStyle>Data Science</a><h1>Hadoop Mapreduce Streaming Tricks and Techniques</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>09 May 2015</span></div><img src=https://mlwhiz.com/images/category_bgs/default_bg.jpg class="img-fluid w-100 mb-4" alt="Hadoop Mapreduce Streaming Tricks and Techniques"><div class="content mb-5"><p>I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.</p><h2 id=using-shell-scripts-to-run-your-programs>Using Shell Scripts to run your Programs</h2><p><img src=/images/I-love-bash-1024x220.png></p><p>I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. <em>You can effectively organize your workflow by using shell scripts.</em> Now Shell scripts are not as formidable as they sound. We wont be doing programming perse using these shell scripts(Though they are pretty good at that too), we will just use them to store commands that we need to use sequentially.</p><p>Below is a sample of the shell script I use to run my Mapreduce Codes.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#cd2828;font-weight:700>#!/bin/bash
</span><span style=color:#cd2828;font-weight:700></span><span style=color:#999;font-style:italic>#Defining program variables</span>
<span style=color:#40ffff>IP</span>=<span style=color:#ed9d13>&#34;/data/input&#34;</span>
<span style=color:#40ffff>OP</span>=<span style=color:#ed9d13>&#34;/data/output&#34;</span>
<span style=color:#40ffff>HADOOP_JAR_PATH</span>=<span style=color:#ed9d13>&#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&#34;</span>
<span style=color:#40ffff>MAPPER</span>=<span style=color:#ed9d13>&#34;test_m.py&#34;</span>
<span style=color:#40ffff>REDUCER</span>=<span style=color:#ed9d13>&#34;test_r.py&#34;</span>

hadoop fs -rmr -skipTrash&amp;nbsp;<span style=color:#40ffff>$OP</span>
hadoop jar&amp;nbsp;<span style=color:#40ffff>$HADOOP_JAR_PATH</span> <span style=color:#ed9d13>\
</span><span style=color:#ed9d13></span>-file&amp;nbsp;<span style=color:#40ffff>$MAPPER</span> -mapper <span style=color:#ed9d13>&#34;python test_m.py&#34;</span> <span style=color:#ed9d13>\
</span><span style=color:#ed9d13></span>-file&amp;nbsp;<span style=color:#40ffff>$REDUCER</span> -reducer <span style=color:#ed9d13>&#34;python test_r.py&#34;</span> <span style=color:#ed9d13>\
</span><span style=color:#ed9d13></span>-input&amp;nbsp;<span style=color:#40ffff>$IP</span> -output&amp;nbsp;<span style=color:#40ffff>$OP</span>
</code></pre></div><p>I generally save them as test_s.sh and whenever i need to run them i simply type <code>sh test_s.sh</code>. This helps in three ways.</p><ul><li>It helps me to store hadoop commands in a manageable way.</li><li>It is easy to run the mapreduce code using the shell script.</li><li><em><strong>If the code fails, I do not have to manually delete the output directory</strong></em></li></ul><blockquote><em>The simplification of anything is always sensational.<br></em><small>Gilbert K. Chesterton</small></blockquote><h2 id=using-distributed-cache-to-provide-mapper-with-a-dictionary>Using Distributed Cache to provide mapper with a dictionary</h2><img src=/images/Game-Of-Thrones-Wallpaper-House-Sigils-1.png><p>Often times it happens that you want that your Hadoop Mapreduce program is able to access some static file. This static file could be a dictionary, could be parameters for the program or could be anything. What distributed cache does is that it provides this file to all the mapper nodes so that you can use that file in any way across all your mappers.
Now this concept although simple would help you to think about Mapreduce in a whole new light.
Lets start with an example.
Supppose you have to create a sample Mapreduce program that reads a big file containing the information about all the characters in <a href=http://www.hbo.com/game-of-thrones>Game of Thrones</a> stored as <strong><code>"/data/characters/"</code></strong>:</p><div style="width:50%;margin:0 auto"><table class=table><thead><tr><th>Cust_ID</th><th>User_Name</th><th>House</th></tr></thead><tbody><tr><td>1</td><td>Daenerys Targaryen</td><td>Targaryen</td></tr><tr><td>2</td><td>Tyrion Lannister</td><td>Lannister</td></tr><tr><td>3</td><td>Cersei Lannister</td><td>Lannister</td></tr><tr><td>4</td><td>Robert Baratheon</td><td>Baratheon</td></tr><tr><td>5</td><td>Robb Stark</td><td>Stark</td></tr></tbody></table></div><p>But you dont want to use the dead characters in the file for the analysis you want to do. <em>You want to count the number of living characters in Game of Thrones grouped by their House</em>. (I know its easy!!!!!)
One thing you could do is include an if statement in your Mapper Code which checks if the persons ID is 4 then exclude it from the mapper and such.
But the problem is that you would have to do it again and again for the same analysis as characters die like flies when it comes to George RR Martin.(Also where is the fun in that)
So you create a file which contains the Ids of all the dead characters at <strong><code>"/data/dead_characters.txt"</code></strong>:</p><div style="width:50%;margin:0 auto"><table class=table><thead><tr><th>Died</th></tr></thead><tbody><tr><td>4</td></tr><tr><td>5</td></tr></tbody></table></div><p>Whenever you have to run the analysis you can just add to this file and you wont have to change anything in the code.
Also sometimes this file would be long and you would not want to clutter your code with IDs and such.</p><p>So How Would we do it.
Let&rsquo;s go in a step by step way around this.
We will create a shell script, a mapper script and a reducer script for this task.</p><h2 id=1-shell-script>1) Shell Script</h2><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic>#!/bin/bash</span>
<span style=color:#999;font-style:italic>#Defining program variables</span>
DC=<span style=color:#ed9d13>&#34;/data/dead_characters.txt&#34;</span>
IP=<span style=color:#ed9d13>&#34;/data/characters&#34;</span>
OP=<span style=color:#ed9d13>&#34;/data/output&#34;</span>
HADOOP_JAR_PATH=<span style=color:#ed9d13>&#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&#34;</span>
MAPPER=<span style=color:#ed9d13>&#34;got_living_m.py&#34;</span>
REDUCER=<span style=color:#ed9d13>&#34;got_living_r.py&#34;</span>

hadoop jar&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>HADOOP_JAR_PATH \
-<span style=color:#24909d>file</span>&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>MAPPER -mapper <span style=color:#ed9d13>&#34;python got_living_m.py&#34;</span> \
-<span style=color:#24909d>file</span>&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>REDUCER -reducer <span style=color:#ed9d13>&#34;python got_living_r.py&#34;</span> \
-cacheFile&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>DC<span style=color:#999;font-style:italic>#ref \</span>
-<span style=color:#24909d>input</span>&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>IP -output&amp;nbsp;<span style=color:#a61717;background-color:#e3d2d2>$</span>OP
</code></pre></div><p>Note how we use the <code>"-cacheFile"</code> option here. We have specified that we will refer to the file that has been provided in the Distributed cache as <code>#ref</code>.</p><p>Next is our Mapper Script.</p><h2 id=2-mapper-script>2) Mapper Script</h2><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>sys</span>
dead_ids = <span style=color:#24909d>set</span>()

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>read_cache</span>():
	<span style=color:#6ab825;font-weight:700>for</span> line <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>open</span>(<span style=color:#ed9d13>&#39;ref&#39;</span>):
		<span style=color:#24909d>id</span> = line.strip()
		dead_ids.add(<span style=color:#24909d>id</span>)

read_cache()

<span style=color:#6ab825;font-weight:700>for</span> line <span style=color:#6ab825;font-weight:700>in</span> sys.stdin:
	rec = line.strip().split(<span style=color:#ed9d13>&#34;|&#34;</span>) <span style=color:#999;font-style:italic># Split using Delimiter &#34;|&#34;</span>
	<span style=color:#24909d>id</span> = rec[<span style=color:#3677a9>0</span>]
    house = rec[<span style=color:#3677a9>2</span>]
    <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#24909d>id</span> <span style=color:#6ab825;font-weight:700>not</span> <span style=color:#6ab825;font-weight:700>in</span> dead_ids:
    	<span style=color:#6ab825;font-weight:700>print</span> <span style=color:#ed9d13>&#34;</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>&#34;</span> % (house,<span style=color:#3677a9>1</span>)
</code></pre></div><p>And our Reducer Script.</p><h2 id=3-reducer-script>3) Reducer Script</h2><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>sys</span>
current_key = None
key = None
count = <span style=color:#3677a9>0</span>

<span style=color:#6ab825;font-weight:700>for</span> line <span style=color:#6ab825;font-weight:700>in</span> sys.stdin:
	line = line.strip()
	rec = line.split(<span style=color:#ed9d13>&#39;</span><span style=color:#ed9d13>\t</span><span style=color:#ed9d13>&#39;</span>)
	key = rec[<span style=color:#3677a9>0</span>]
	value = <span style=color:#24909d>int</span>(rec[<span style=color:#3677a9>1</span>])

	<span style=color:#6ab825;font-weight:700>if</span> current_key == key:
		count += value
	<span style=color:#6ab825;font-weight:700>else</span>:
		<span style=color:#6ab825;font-weight:700>if</span> current_key:
			<span style=color:#6ab825;font-weight:700>print</span> <span style=color:#ed9d13>&#34;</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>:</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>&#34;</span> %(key,<span style=color:#24909d>str</span>(count))		
		current_key = key
		count = value

<span style=color:#6ab825;font-weight:700>if</span> current_key == key:
    <span style=color:#6ab825;font-weight:700>print</span> <span style=color:#ed9d13>&#34;</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>:</span><span style=color:#ed9d13>%s</span><span style=color:#ed9d13>&#34;</span> %(key,<span style=color:#24909d>str</span>(count))
</code></pre></div><p>This was a simple program and the output will be just what you expected and not very exciting. <em><strong>But the Technique itself solves a variety of common problems. You can use it to pass any big dictionary to your Mapreduce Program</strong></em>. Atleast thats what I use this feature mostly for.
Hope You liked it. Will try to expand this post with more tricks.</p><p>The codes for this post are posted at github <a href=https://github.com/MLWhiz/Hadoop-Mapreduce-Tricks>here</a>.</p><p>Other Great Learning Resources For Hadoop:</p><ul><li><a href="http://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CB0QFjAA&url=http%3A%2F%2Fwww.michael-noll.com%2Ftutorials%2Fwriting-an-hadoop-mapreduce-program-in-python%2F&ei=8RRVVdP2IMe0uQShsYDYBg&usg=AFQjCNH3DqrlSIG8D-K8jgQWTALic1no5A&sig2=BivwTW6mdJs5c9w9VaSK2Q&bvm=bv.93112503,d.c2E">Michael Noll's Hadoop Mapreduce Tutorial</a></li><li><a href="http://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0CCMQFjAB&url=http%3A%2F%2Fhadoop.apache.org%2Fdocs%2Fr1.2.1%2Fstreaming.html&ei=8RRVVdP2IMe0uQShsYDYBg&usg=AFQjCNEIB4jmqcBs-GepHdn7DRxqTI9zXA&sig2=nYkAnDjjjaum5YVlYuMUJQ&bvm=bv.93112503,d.c2E">Apache's Hadoop Streaming Documentation</a></li></ul><p>Also I like these books a lot. Must have for a Hadooper&mldr;.</p><div style=margin-left:1em;text-align:center><a target=_blank href="https://www.amazon.com/gp/product/1785887211/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1785887211&linkCode=as2&tag=mlwhizcon-20&linkId=a0e7b4f0b2ea4a5146042890e1c04f7e"><img border=0 src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=1785887211&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=mlwhizcon-20"></a><img src="//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&l=am2&o=1&a=1785887211" width=1 height=1 border=0 alt style=border:none!important;margin:0!important><p></t></t></p><p><a target=_blank href="https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1491901632&linkCode=as2&tag=mlwhizcon-20&linkId=4122280e94f7bbd0ceebc9d13e60d103"><img border=0 src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=1491901632&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=mlwhizcon-20"></a><img src="//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&l=am2&o=1&a=1491901632" width=1 height=1 border=0 alt style=border:none!important;margin:0!important></p></div><p>The first book is a guide for using Hadoop as well as spark with Python. While the second one contains a detailed overview of all the things in Hadoop. Its the definitive guide.</p><script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"></script><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href=https://www.linkedin.com/in/rahulagwl/><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="Helping You Learn Data Science!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>