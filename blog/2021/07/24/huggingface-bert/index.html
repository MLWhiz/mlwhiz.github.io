<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-F34XSWQ5N4')</script><meta charset=utf-8><title>Understanding BERT with Huggingface - MLWhiz</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.82.0"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Understanding BERT with Huggingface - MLWhiz"><meta property="og:description" content="In my last post on BERT , I talked in quite a detail about BERT transformers and how they work on a basic level."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2021/07/24/huggingface-bert/"><meta property="og:image" content="https://mlwhiz.com/images/huggingface-bert/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/huggingface-bert/main.png"><meta property="article:published_time" content="2021-07-24T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T15:10:03+01:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/huggingface-bert/main.png"><meta name=twitter:title content="Understanding BERT with Huggingface - MLWhiz"><meta name=twitter:description content="In my last post on BERT , I talked in quite a detail about BERT transformers and how they work on a basic level."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2021/07/24/huggingface-bert/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2021/07/24/huggingface-bert/#primaryimage","url":"https://mlwhiz.com/images/huggingface-bert/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2021/07/24/huggingface-bert/#webpage","url":"https://mlwhiz.com/blog/2021/07/24/huggingface-bert/","inLanguage":"en-US","name":"Understanding BERT with Huggingface - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2021/07/24/huggingface-bert/#primaryimage"},"datePublished":"2021-07-24T00:00:00.00Z","dateModified":"2023-07-07T15:10:03.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"In my last post on BERT , I talked in quite a detail about BERT transformers and how they work on a basic level."},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(b,e,f,g,a,c,d){if(b.fbq)return;a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version='2.0',a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js'),fbq('init','402633927768628'),fbq('track','PageView')</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"><meta name=impact-site-verification value=1670148355></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Understanding BERT with Huggingface</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>24 July 2021</span></div><img src=https://mlwhiz.com/images/huggingface-bert/main.png class="img-fluid w-100 mb-4" alt="Understanding BERT with Huggingface"><div class="content mb-5"><p>In my last post on
<a href=https://mlwhiz.medium.com/explaining-bert-simply-using-sketches-ba30f6f0c8cb target=_blank rel="nofollow noopener">BERT</a>
, I talked in quite a detail about BERT transformers and how they work on a basic level. I went through the BERT Architecture, training data and training tasks.</p><p>But, as I like to say, we don’t really understand something before we implement it ourselves. So, in this post, we will implement a Question Answering Neural Network using BERT and HuggingFace Library.</p><hr><h2 id=what-is-a-question-answering-task><strong>What is a Question Answering Task?</strong></h2><p>In this task, we are given a question and a paragraph in which the answer lies to our BERT Architecture and the objective is to determine the start and end span for the answer in the paragraph.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/huggingface-bert/0_hua45328a19866c68052937d8fec303915_306691_500x0_resize_box_2.png 500w
, /images/huggingface-bert/0_hua45328a19866c68052937d8fec303915_306691_800x0_resize_box_2.png 800w
, /images/huggingface-bert/0_hua45328a19866c68052937d8fec303915_306691_1200x0_resize_box_2.png 1200w
, /images/huggingface-bert/0_hua45328a19866c68052937d8fec303915_306691_1500x0_resize_box_2.png 1500w" src=/images/huggingface-bert/0.png alt="Author Image: BERT Finetuning for Question-Answer Task"><figcaption>Author Image: BERT Finetuning for Question-Answer Task</figcaption></figure></p><p>As explained in the previous post, in the above example, we provide two inputs to the BERT architecture. The paragraph and the question separated by the <sep>token. The purple layers are the output of the BERT encoder. We now define two vectors S and E(which will be learned during fine-tuning) both having shapes(1x768). We then get some scores by taking the dot product of these vectors and the second sentence’s output vectors. Once we have the scores, we can just apply SoftMax over all these scores to get probabilities. The training objective is the sum of the log-likelihoods of the correct start and end positions. Mathematically, for the Probability vector for Start positions:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/huggingface-bert/1.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>where T_i is the word, we are focusing on. An analogous formula is for End positions.</p><p>To predict a span, we get all the scores — S.T and E.T and get the best span as the span having the maximum score, that is max(S.T_i + E.T_j) among all j≥i.</p><hr><h2 id=how-do-we-do-this-using-huggingface>How do we do this using Huggingface?</h2><p>Simply, Huggingface provides a pretty straightforward way to do this.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>datasets</span> <span style=color:#6ab825;font-weight:700>import</span> load_dataset, load_metric
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>random</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>transformers</span> <span style=color:#6ab825;font-weight:700>import</span> AutoTokenizer
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>transformers</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>transformers</span> <span style=color:#6ab825;font-weight:700>import</span> AutoModelForQuestionAnswering, TrainingArguments, Trainer
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch</span>
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>transformers</span> <span style=color:#6ab825;font-weight:700>import</span> default_data_collator
<span style=color:#6ab825;font-weight:700>from</span> <span style=color:#447fcf;text-decoration:underline>transformers</span> <span style=color:#6ab825;font-weight:700>import</span> AutoTokenizer, AutoModelForQuestionAnswering
<span style=color:#6ab825;font-weight:700>import</span> <span style=color:#447fcf;text-decoration:underline>torch</span>
tokenizer = AutoTokenizer.from_pretrained(<span style=color:#ed9d13>&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;</span>)
model = AutoModelForQuestionAnswering.from_pretrained(<span style=color:#ed9d13>&#34;bert-large-uncased-whole-word-masking-finetuned-squad&#34;</span>)
text = <span style=color:#ed9d13>r</span><span style=color:#ed9d13>&#34;&#34;&#34;
</span><span style=color:#ed9d13>🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
</span><span style=color:#ed9d13>architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural
</span><span style=color:#ed9d13>Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
</span><span style=color:#ed9d13>TensorFlow 2.0 and PyTorch.
</span><span style=color:#ed9d13>&#34;&#34;&#34;</span>
questions = [
    <span style=color:#ed9d13>&#34;How many pretrained models are available in Transformers?&#34;</span>,
    <span style=color:#ed9d13>&#34;What does Transformers provide?&#34;</span>,
    <span style=color:#ed9d13>&#34;Transformers provides interoperability between which frameworks?&#34;</span>,
]
<span style=color:#6ab825;font-weight:700>for</span> question <span style=color:#6ab825;font-weight:700>in</span> questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=<span style=color:#ed9d13>&#34;pt&#34;</span>)
    input_ids = inputs[<span style=color:#ed9d13>&#34;input_ids&#34;</span>].tolist()[<span style=color:#3677a9>0</span>]
text_tokens = tokenizer.convert_ids_to_tokens(input_ids)

    pred = model(**inputs)
    answer_start_scores, answer_end_scores = pred[<span style=color:#ed9d13>&#39;start_logits&#39;</span>][<span style=color:#3677a9>0</span>] ,pred[<span style=color:#ed9d13>&#39;end_logits&#39;</span>][<span style=color:#3677a9>0</span>]

    answer_start = torch.argmax(
        answer_start_scores
    )  <span style=color:#999;font-style:italic># Get the most likely beginning of answer with the argmax of the score</span>
    answer_end = torch.argmax(answer_end_scores) + <span style=color:#3677a9>1</span>  <span style=color:#999;font-style:italic># Get the most likely end of answer with the argmax of the score</span>
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
<span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Question: {question}&#34;</span>)
    <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Answer: {answer}</span><span style=color:#ed9d13>\n</span><span style=color:#ed9d13>&#34;</span>)

</code></pre></div><p>The output is:</p><pre><code>Question: How many pretrained models are available in Transformers?
Answer: over 32 +

Question: What does Transformers provide?
Answer: general - purpose architectures

Question: Transformers provides interoperability between which frameworks?
Answer: TensorFlow 2. 0 and pytorch
</code></pre><p>So, here we just used the pretrained tokenizer and model on SQUAD dataset provided by Huggingface to get this done.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>tokenizer = AutoTokenizer.from_pretrained(<span style=color:#a61717;background-color:#e3d2d2>“</span>bert-large-uncased-whole-word-masking-finetuned-squad<span style=color:#a61717;background-color:#e3d2d2>”</span>)
model = AutoModelForQuestionAnswering.from_pretrained(<span style=color:#a61717;background-color:#e3d2d2>“</span>bert-large-uncased-whole-word-masking-finetuned-squad<span style=color:#a61717;background-color:#e3d2d2>”</span>)
</code></pre></div><p>Once we have the model we just get the start and end probability scores and predict the span as the one that lies between the token that has the maximum start score and the token that has the maximum end score.</p><p>So for example, if the start scores for paragraph are:</p><pre><code>...
Transformers - 0.1
are - 0.2
general - 0.5
purpose - 0.1
architectures -0.01
(BERT - 0.001
...
</code></pre><p>And the end scores are:</p><pre><code>...
Transformers - 0.01
are - 0.02
general - 0.05
purpose - 0.01
architectures -0.01
(BERT - 0.8
...
</code></pre><p>We will get the output as input_ids[answer_start:answer_end]where answer_start is the index of word general(one with max start score) and answer_end is index of (BERT(One with max end score).And the answer would be “general purpose architectures”.</p><hr><h2 id=fine-tuning-our-own-model-using-a-question-answering-dataset>Fine-tuning Our Own Model using a Question-Answering dataset</h2><p>Almost all the time we might want to train our own QA model on our own datasets. In that example, we will start from the SQUAD dataset and the base BERT Model in the Huggingface library to finetune it.</p><p>Lets look at how the SQUAD Dataset looks before we start finetuning the model</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>datasets = load_dataset(<span style=color:#ed9d13>&#34;squad&#34;</span>)
<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>visualize</span>(datasets, datatype = <span style=color:#ed9d13>&#39;train&#39;</span>, n_questions=<span style=color:#3677a9>10</span>):
    n = <span style=color:#24909d>len</span>(datasets[datatype])
    random_questions=random.choices(<span style=color:#24909d>list</span>(<span style=color:#24909d>range</span>(n)),k=n_questions)
    <span style=color:#6ab825;font-weight:700>for</span> i <span style=color:#6ab825;font-weight:700>in</span> random_questions:
        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Context:{datasets[datatype][i][&#39;context&#39;]}&#34;</span>)
        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Question:{datasets[datatype][i][&#39;question&#39;]}&#34;</span>)
        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Answer:{datasets[datatype][i][&#39;answers&#39;][&#39;text&#39;]}&#34;</span>)
        <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Answer Start in Text:{datasets[datatype][i][&#39;answers&#39;][&#39;answer_start&#39;]}&#34;</span>)
        <span style=color:#6ab825;font-weight:700>print</span>(<span style=color:#ed9d13>&#34;-&#34;</span>*<span style=color:#3677a9>100</span>)
visualize(datasets)
</code></pre></div><pre><code>Context:Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.[citation needed]
Question:The confused deputy problem and the problem of not guaranteeing only one person has access are resolved by what?
Answer:['capabilities']
Answer Start in Text:[553]
--------------------------------------------------------------------
Context:In recent years, the nightclubs on West 27th Street have succumbed to stiff competition from Manhattan's Meatpacking District about fifteen blocks south, and other venues in downtown Manhattan.
Question:How many blocks south of 27th Street is Manhattan's Meatpacking District?
Answer:['fifteen blocks']
Answer Start in Text:[132]
--------------------------------------------------------------------
</code></pre><p>We can see each example contains the Context, Answer and the Start token for the Answer. We can use the script below to preprocess the data to the required format once we have the data in the above form. The script takes care of a lot of things amongst which the most important are the cases where the answer lies around <code>max_length</code> and calculating the span using the answer and the start token index.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#999;font-style:italic># Dealing with long docs:</span>
max_length = <span style=color:#3677a9>384</span> <span style=color:#999;font-style:italic># The maximum length of a feature (question and context)</span>
doc_stride = <span style=color:#3677a9>128</span> <span style=color:#999;font-style:italic># The authorized overlap between two part of the context when splitting it</span>

<span style=color:#6ab825;font-weight:700>def</span> <span style=color:#447fcf>prepare_train_features</span>(examples):
    <span style=color:#999;font-style:italic># Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results</span>
    <span style=color:#999;font-style:italic># in one example possible giving several features when a context is long, each of those features having a</span>
    <span style=color:#999;font-style:italic># context that overlaps a bit the context of the previous feature.</span>
    tokenized_examples = tokenizer(
        examples[<span style=color:#ed9d13>&#34;question&#34;</span> ],
        examples[<span style=color:#ed9d13>&#34;context&#34;</span> ],
        truncation=<span style=color:#ed9d13>&#34;only_second&#34;</span>,
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=<span style=color:#ed9d13>&#34;max_length&#34;</span>,
    )

    <span style=color:#999;font-style:italic># Since one example might give us several features if it has a long context, we need a map from a feature to</span>
    <span style=color:#999;font-style:italic># its corresponding example. This key gives us just that.</span>
    <span style=color:#999;font-style:italic># Looks like [0,1,2,2,2,3,4,5,5...] - Here 2nd input pair has been split in 3 parts</span>
    sample_mapping = tokenized_examples.pop(<span style=color:#ed9d13>&#34;overflow_to_sample_mapping&#34;</span>)
    <span style=color:#999;font-style:italic># The offset mappings will give us a map from token to character position in the original context. This will</span>
    <span style=color:#999;font-style:italic># help us compute the start_positions and end_positions.</span>
    <span style=color:#999;font-style:italic># Looks like [[(0,0),(0,3),(3,4)...] ] - Contains the actual start indices and end indices for each word in the input.</span>
    offset_mapping = tokenized_examples.pop(<span style=color:#ed9d13>&#34;offset_mapping&#34;</span>)

    <span style=color:#999;font-style:italic># Let&#39;s label those examples!</span>
    tokenized_examples[<span style=color:#ed9d13>&#34;start_positions&#34;</span>] = []
    tokenized_examples[<span style=color:#ed9d13>&#34;end_positions&#34;</span>] = []

    <span style=color:#6ab825;font-weight:700>for</span> i, offsets <span style=color:#6ab825;font-weight:700>in</span> <span style=color:#24909d>enumerate</span>(offset_mapping):
        <span style=color:#999;font-style:italic># We will label impossible answers with the index of the CLS token.</span>
        input_ids = tokenized_examples[<span style=color:#ed9d13>&#34;input_ids&#34;</span>][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        <span style=color:#999;font-style:italic># Grab the sequence corresponding to that example (to know what is the context and what is the question).</span>
        sequence_ids = tokenized_examples.sequence_ids(i)

        <span style=color:#999;font-style:italic># One example can give several spans, this is the index of the example containing this span of text.</span>
        sample_index = sample_mapping[i]
        answers = examples[<span style=color:#ed9d13>&#34;answers&#34;</span>][sample_index]
        <span style=color:#999;font-style:italic># If no answers are given, set the cls_index as answer.</span>
        <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#24909d>len</span>(answers[<span style=color:#ed9d13>&#34;answer_start&#34;</span>]) == <span style=color:#3677a9>0</span>:
            tokenized_examples[<span style=color:#ed9d13>&#34;start_positions&#34;</span>].append(cls_index)
            tokenized_examples[<span style=color:#ed9d13>&#34;end_positions&#34;</span>].append(cls_index)
        <span style=color:#6ab825;font-weight:700>else</span>:
            <span style=color:#999;font-style:italic># Start/end character index of the answer in the text.</span>
            start_char = answers[<span style=color:#ed9d13>&#34;answer_start&#34;</span>][<span style=color:#3677a9>0</span>]
            end_char = start_char + <span style=color:#24909d>len</span>(answers[<span style=color:#ed9d13>&#34;text&#34;</span>][<span style=color:#3677a9>0</span>])

            <span style=color:#999;font-style:italic># Start token index of the current span in the text.</span>
            token_start_index = <span style=color:#3677a9>0</span>
            <span style=color:#6ab825;font-weight:700>while</span> sequence_ids[token_start_index] != <span style=color:#3677a9>1</span>:
                token_start_index += <span style=color:#3677a9>1</span>

            <span style=color:#999;font-style:italic># End token index of the current span in the text.</span>
            token_end_index = <span style=color:#24909d>len</span>(input_ids) - <span style=color:#3677a9>1</span>
            <span style=color:#6ab825;font-weight:700>while</span> sequence_ids[token_end_index] != <span style=color:#3677a9>1</span>:
                token_end_index -= <span style=color:#3677a9>1</span>

            <span style=color:#999;font-style:italic># Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).</span>
            <span style=color:#6ab825;font-weight:700>if</span> <span style=color:#6ab825;font-weight:700>not</span> (offsets[token_start_index][<span style=color:#3677a9>0</span>] &lt;= start_char <span style=color:#6ab825;font-weight:700>and</span> offsets[token_end_index][<span style=color:#3677a9>1</span>] &gt;= end_char):
                tokenized_examples[<span style=color:#ed9d13>&#34;start_positions&#34;</span>].append(cls_index)
                tokenized_examples[<span style=color:#ed9d13>&#34;end_positions&#34;</span>].append(cls_index)
            <span style=color:#6ab825;font-weight:700>else</span>:
                <span style=color:#999;font-style:italic># Otherwise move the token_start_index and token_end_index to the two ends of the answer.</span>
                <span style=color:#999;font-style:italic># Note: we could go after the last offset if the answer is the last word (edge case).</span>
                <span style=color:#6ab825;font-weight:700>while</span> token_start_index &lt; <span style=color:#24909d>len</span>(offsets) <span style=color:#6ab825;font-weight:700>and</span> offsets[token_start_index][<span style=color:#3677a9>0</span>] &lt;= start_char:
                    token_start_index += <span style=color:#3677a9>1</span>
                tokenized_examples[<span style=color:#ed9d13>&#34;start_positions&#34;</span>].append(token_start_index - <span style=color:#3677a9>1</span>)
                <span style=color:#6ab825;font-weight:700>while</span> offsets[token_end_index][<span style=color:#3677a9>1</span>] &gt;= end_char:
                    token_end_index -= <span style=color:#3677a9>1</span>
                tokenized_examples[<span style=color:#ed9d13>&#34;end_positions&#34;</span>].append(token_end_index + <span style=color:#3677a9>1</span>)

    <span style=color:#6ab825;font-weight:700>return</span> tokenized_examples
</code></pre></div><p>Once we have data in required format we can just finetune our BERT base model.</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model_checkpoint = <span style=color:#ed9d13>&#34;bert-base-uncased&#34;</span>
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[<span style=color:#ed9d13>&#34;train&#34;</span>].column_names)

args = TrainingArguments(
    f<span style=color:#ed9d13>&#34;test-squad&#34;</span>,
    evaluation_strategy = <span style=color:#ed9d13>&#34;epoch&#34;</span>,
    learning_rate=<span style=color:#3677a9>2e-5</span>,
    per_device_train_batch_size=<span style=color:#3677a9>16</span>,
    per_device_eval_batch_size=<span style=color:#3677a9>16</span>,
    num_train_epochs=<span style=color:#3677a9>3</span>,
    weight_decay=<span style=color:#3677a9>0.01</span>,
)

data_collator = default_data_collator
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span style=color:#ed9d13>&#34;train&#34;</span>],
    eval_dataset=tokenized_datasets[<span style=color:#ed9d13>&#34;validation&#34;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
trainer.save_model(trainer.save_model(<span style=color:#ed9d13>&#34;test-squad-trained&#34;</span>))
</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/huggingface-bert/2_hub2256dc685c970439ce6f7ee545598a2_27084_500x0_resize_box_2.png 500w
, /images/huggingface-bert/2_hub2256dc685c970439ce6f7ee545598a2_27084_800x0_resize_box_2.png 800w" src=/images/huggingface-bert/2.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Once we train our model we can use it as:</p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model = AutoModelForQuestionAnswering.from_pretrained(<span style=color:#ed9d13>&#34;test-squad-trained&#34;</span>)
text = <span style=color:#ed9d13>r</span><span style=color:#ed9d13>&#34;&#34;&#34;
</span><span style=color:#ed9d13>🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
</span><span style=color:#ed9d13>architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural
</span><span style=color:#ed9d13>Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
</span><span style=color:#ed9d13>TensorFlow 2.0 and PyTorch
</span><span style=color:#ed9d13>&#34;&#34;&#34;</span>
questions = [
    <span style=color:#ed9d13>&#34;How many pretrained models are available in Transformers?&#34;</span>,
    <span style=color:#ed9d13>&#34;What does Transformers provide?&#34;</span>,
    <span style=color:#ed9d13>&#34;Transformers provides interoperability between which frameworks?&#34;</span>,
]
<span style=color:#6ab825;font-weight:700>for</span> question <span style=color:#6ab825;font-weight:700>in</span> questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=<span style=color:#ed9d13>&#34;pt&#34;</span>)
    input_ids = inputs[<span style=color:#ed9d13>&#34;input_ids&#34;</span>].tolist()[<span style=color:#3677a9>0</span>]
text_tokens = tokenizer.convert_ids_to_tokens(input_ids)

    pred = model(**inputs)
    answer_start_scores, answer_end_scores = pred[<span style=color:#ed9d13>&#39;start_logits&#39;</span>][<span style=color:#3677a9>0</span>] ,pred[<span style=color:#ed9d13>&#39;end_logits&#39;</span>][<span style=color:#3677a9>0</span>]

    answer_start = torch.argmax(
        answer_start_scores
    )  <span style=color:#999;font-style:italic># Get the most likely beginning of answer with the argmax of the score</span>
    answer_end = torch.argmax(answer_end_scores) + <span style=color:#3677a9>1</span>  <span style=color:#999;font-style:italic># Get the most likely end of answer with the argmax of the score</span>
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
<span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Question: {question}&#34;</span>)
    <span style=color:#6ab825;font-weight:700>print</span>(f<span style=color:#ed9d13>&#34;Answer: {answer}</span><span style=color:#ed9d13>\n</span><span style=color:#ed9d13>&#34;</span>)

</code></pre></div><p>In this case also, we take the index of max start scores and max end scores and predict the answer as the one that is between. If we want to get the exact implementation as provided in the BERT Paper we can tweek the above code a little and find out the indexes which maximize <code>(start_score + end_score)</code></p><div class=highlight><pre style=color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>text = <span style=color:#ed9d13>r</span><span style=color:#ed9d13>&#34;&#34;&#34;
</span><span style=color:#ed9d13>George Washington (February 22, 1732[b] – December 14, 1799) was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797. Previously, he led Patriot forces to victory in the nation&#39;s War for Independence. He presided at the Constitutional Convention of 1787, which established the U.S. Constitution and a federal government. Washington has been called the &#34;Father of His Country&#34; for his manifold leadership in the formative days of the new nation.
</span><span style=color:#ed9d13>&#34;&#34;&#34;</span>

question = <span style=color:#ed9d13>&#34;Who was the first president?&#34;</span>

inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=<span style=color:#ed9d13>&#34;pt&#34;</span>)
input_ids = inputs[<span style=color:#ed9d13>&#34;input_ids&#34;</span>].tolist()[<span style=color:#3677a9>0</span>]

text_tokens = tokenizer.convert_ids_to_tokens(input_ids)

pred = model(**inputs)
answer_start_scores, answer_end_scores = pred[<span style=color:#ed9d13>&#39;start_logits&#39;</span>][<span style=color:#3677a9>0</span>] ,pred[<span style=color:#ed9d13>&#39;end_logits&#39;</span>][<span style=color:#3677a9>0</span>]

<span style=color:#999;font-style:italic>#get the index of first highest 20 tokens only</span>
start_indexes = <span style=color:#24909d>list</span>(np.argsort(answer_start_scores.detach().numpy()))[::-<span style=color:#3677a9>1</span>][:<span style=color:#3677a9>20</span>]
end_indexes = <span style=color:#24909d>list</span>(np.argsort(answer_end_scores.detach().numpy()))[::-<span style=color:#3677a9>1</span>][:<span style=color:#3677a9>20</span>]

valid_answers = []
<span style=color:#6ab825;font-weight:700>for</span> start_index <span style=color:#6ab825;font-weight:700>in</span> start_indexes:
    <span style=color:#6ab825;font-weight:700>for</span> end_index <span style=color:#6ab825;font-weight:700>in</span> end_indexes:
        <span style=color:#6ab825;font-weight:700>if</span> start_index &lt;= end_index:
            valid_answers.append(
                {
                    <span style=color:#ed9d13>&#34;score&#34;</span>: answer_start_scores[start_index] + answer_end_scores[end_index],
                    <span style=color:#ed9d13>&#34;text&#34;</span>: tokenizer.convert_tokens_to_string(
                        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index+<span style=color:#3677a9>1</span>]))
                }
            )

valid_answers = <span style=color:#24909d>sorted</span>(valid_answers, key=<span style=color:#6ab825;font-weight:700>lambda</span> x: x[<span style=color:#ed9d13>&#34;score&#34;</span>], reverse=True)

</code></pre></div><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset="/images/huggingface-bert/3_huce63e4083df748862e118d8a221743f9_134553_500x0_resize_box_2.png 500w
, /images/huggingface-bert/3_huce63e4083df748862e118d8a221743f9_134553_800x0_resize_box_2.png 800w
, /images/huggingface-bert/3_huce63e4083df748862e118d8a221743f9_134553_1200x0_resize_box_2.png 1200w
, /images/huggingface-bert/3_huce63e4083df748862e118d8a221743f9_134553_1500x0_resize_box_2.png 1500w" src=/images/huggingface-bert/3.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><hr><h2 id=references>References</h2><ul><li><p><a href=https://arxiv.org/abs/1706.03762 target=_blank rel="nofollow noopener">Attention Is All You Need</a>
: The Paper which introduced Transformers.</p></li><li><p><a href=https://arxiv.org/abs/1810.04805 target=_blank rel="nofollow noopener">BERT Paper</a>
: Do read this paper.</p></li><li><p><a href=https://huggingface.co/transformers/usage.html target=_blank rel="nofollow noopener">Huggingface</a></p></li></ul><p>In this post, I covered how we can create a Question Answering Model from scratch using BERT. I hope it would have been useful both for understanding BERT as well as Huggingface library.</p><p>If you want to look at other post in this series please take a look at:</p><ul><li><p><a href=https://mlwhiz.com/blog/2020/09/20/transformers/>Understanding Transformers, the Data Science Way</a></p></li><li><p><a href=https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/>Understanding Transformers, the Programming Way</a></p></li><li><p><a href=https://mlwhiz.medium.com/explaining-bert-simply-using-sketches-ba30f6f0c8cb target=_blank rel="nofollow noopener">Explaining BERT Simply Using Sketches</a></p></li></ul><p>If you want to know more about NLP, I would like to recommend this awesome
<a href=https://coursera.pxf.io/9WjZo0 target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><p>And, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//mlwhiz.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support Me on Ko-fi','#972EB4','S6S3NPCD'),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p>I’m a Machine Learning Engineer based in London, where I am currently working with Roku .</p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="MLWhiz - Your Home for DS, ML, AI!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-54777926-1','auto'),ga('send','pageview')</script></body></html>