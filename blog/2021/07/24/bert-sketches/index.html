<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-F34XSWQ5N4"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F34XSWQ5N4")</script><meta charset=utf-8><title>Explaining BERT Simply Using Sketches - MLWhiz</title>
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Want to Learn Computer Vision and NLP? - MLWhiz"><meta name=author content="Rahul Agarwal"><meta name=generator content="Hugo 0.139.2"><link rel=stylesheet href=https://mlwhiz.com/plugins/compressjscss/main.css><meta property="og:title" content="Explaining BERT Simply Using Sketches - MLWhiz"><meta property="og:description" content="In my last series of posts on Transformers, I talked about how a 

transformer 
works and how to 

implement
 one yourself for a translation task."><meta property="og:type" content="article"><meta property="og:url" content="https://mlwhiz.com/blog/2021/07/24/bert-sketches/"><meta property="og:image" content="https://mlwhiz.com/images/bert_sketches/main.png"><meta property="og:image:secure_url" content="https://mlwhiz.com/images/bert_sketches/main.png"><meta property="article:published_time" content="2021-07-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-13T13:34:49+01:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Awesome Guides"><meta name=twitter:card content="summary"><meta name=twitter:image content="https://mlwhiz.com/images/bert_sketches/main.png"><meta name=twitter:title content="Explaining BERT Simply Using Sketches - MLWhiz"><meta name=twitter:description content="In my last series of posts on Transformers, I talked about how a 

transformer 
works and how to 

implement
 one yourself for a translation task."><meta name=twitter:site content="@mlwhiz"><meta name=twitter:creator content="@mlwhiz"><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href=https://mlwhiz.com/scss/style.min.css media=screen><link rel=stylesheet href=/css/style.css><link rel=stylesheet type=text/css href=/css/font/flaticon.css><link rel="shortcut icon" href=https://mlwhiz.com/images/logos/favicon-32x32.png type=image/x-icon><link rel=icon href=https://mlwhiz.com/images/logos/favicon.ico type=image/x-icon><link rel=canonical href=https://mlwhiz.com/blog/2021/07/24/bert-sketches/><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://www.mlwhiz.com/#website","url":"https://www.mlwhiz.com/","name":"MLWhiz","description":"Want to Learn Computer Vision and NLP? - MLWhiz","potentialAction":{"@type":"SearchAction","target":"https://www.mlwhiz.com/search?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://mlwhiz.com/blog/2021/07/24/bert-sketches/#primaryimage","url":"https://mlwhiz.com/images/bert_sketches/main.png","width":700,"height":450},{"@type":"WebPage","@id":"https://mlwhiz.com/blog/2021/07/24/bert-sketches/#webpage","url":"https://mlwhiz.com/blog/2021/07/24/bert-sketches/","inLanguage":"en-US","name":"Explaining BERT Simply Using Sketches - MLWhiz","isPartOf":{"@id":"https://www.mlwhiz.com/#website"},"primaryImageOfPage":{"@id":"https://mlwhiz.com/blog/2021/07/24/bert-sketches/#primaryimage"},"datePublished":"2021-07-24T00:00:00.00Z","dateModified":"2022-04-13T13:34:49.00Z","author":{"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca"},"description":"In my last series of posts on Transformers, I talked about how a transformer works and how to implement one yourself for a translation task.\n"},{"@type":["Person"],"@id":"https://mlwhiz.com/about/#/schema/person/76376876bchxkzbchjsdjcca","name":"Rahul Agarwal","image":{"@type":"ImageObject","@id":"https://www.mlwhiz.com/#authorlogo","url":"https://mlwhiz.com/images/author.jpg","caption":"Rahul Agarwal"},"description":"Hi there, I\u2019m Rahul Agarwal. I\u2019m a data scientist consultant and big data engineer based in Bangalore. I see a lot of times  students and even professionals wasting their time and struggling to get started with Computer Vision, Deep Learning, and NLP. I Started this Site with a purpose to augment my own understanding about new things while helping others learn about them in the best possible way.","sameAs":["https://www.linkedin.com/in/rahulagwl/","https://medium.com/@rahul_agarwal","https://twitter.com/MLWhiz","https://www.facebook.com/mlwhizblog","https://github.com/MLWhiz","https://www.instagram.com/itsmlwhiz"]}]}</script><script async data-uid=a0ebaf958d src=https://mlwhiz.ck.page/a0ebaf958d/index.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,skipTags:["script","noscript","style","textarea","pre"],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var e,t=MathJax.Hub.getAllJax();for(e=0;e<t.length;e+=1)t[e].SourceElement().parentNode.className+=" has-jax"}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link href=//apps.shareaholic.com/assets/pub/shareaholic.js as=script><script type=text/javascript data-cfasync=false async src=//apps.shareaholic.com/assets/pub/shareaholic.js data-shr-siteid=fd1ffa7fd7152e4e20568fbe49a489d0></script><script>!function(e,t,n,s,o,i,a){if(e.fbq)return;o=e.fbq=function(){o.callMethod?o.callMethod.apply(o,arguments):o.queue.push(arguments)},e._fbq||(e._fbq=o),o.push=o,o.loaded=!0,o.version="2.0",o.queue=[],i=t.createElement(n),i.async=!0,i.src=s,a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(i,a)}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js"),fbq("init","402633927768628"),fbq("track","PageView")</script><noscript><img height=1 width=1 style=display:none src="https://www.facebook.com/tr?id=402633927768628&ev=PageView&noscript=1"></noscript><meta property="fb:pages" content="213104036293742"><meta name=facebook-domain-verification content="qciidcy7mm137sewruizlvh8zbfnv4"><meta name=impact-site-verification value=1670148355></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NMQD44T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class=preloader></div><header class=navigation><div class=container><nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0"><a class="navbar-brand mobile-view" href=https://mlwhiz.com/><img class=img-fluid src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><div class=desktop-view><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&amp;followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=nav-item><a class=nav-link href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=nav-item><a class=nav-link href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><a class="navbar-brand mx-auto desktop-view" href=https://mlwhiz.com/><img class=img-fluid-custom src=https://mlwhiz.com/images/logos/logo.svg alt="MLWhiz - Your Home for DS, ML, AI!"></a><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://mlwhiz.com/about>About</a></li><li class=nav-item><a class=nav-link href=https://mlwhiz.com/blog>Blog</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Topics</a><div class=dropdown-menu><a class=dropdown-item href=https://mlwhiz.com/categories/natural-language-processing>NLP</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/computer-vision>Computer Vision</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/deep-learning>Deep Learning</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/data-science>DS/ML</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/big-data>Big Data</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/awesome-guides>My Best Content</a>
<a class=dropdown-item href=https://mlwhiz.com/categories/learning-resources>Learning Resources</a></div></li></ul><div class="search pl-lg-4"><button id=searchOpen class=search-btn><i class=ti-search></i></button><div class=search-wrapper><form action=https://mlwhiz.com//search class=h-100><input class="search-box px-4" id=search-query name=s type=search placeholder="Type & Hit Enter..."></form><button id=searchClose class=search-close><i class="ti-close text-dark"></i></button></div></div></div></nav></div></header><section class=section-sm><div class=container><div class=row><div class="col-lg-8 mb-5 mb-lg-0"><a href=/categories/deep-learning class=categoryStyle>Deep Learning</a>
<a href=/categories/natural-language-processing class=categoryStyle>Natural Language Processing</a>
<a href=/categories/awesome-guides class=categoryStyle>Awesome Guides</a><h1>Explaining BERT Simply Using Sketches</h1><div class="mb-3 post-meta"><span>By Rahul Agarwal</span>
<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
<span>24 July 2021</span></div><img src=https://mlwhiz.com/images/bert_sketches/main.png class="img-fluid w-100 mb-4" alt="Explaining BERT Simply Using Sketches"><div class="content mb-5"><p>In my last series of posts on Transformers, I talked about how a
<a href=https://mlwhiz.com/blog/2020/09/20/transformers/>transformer </a>works and how to
<a href=https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/>implement</a>
one yourself for a translation task.</p><p>In this post, I will go a step further and try to explain BERT, one of the most popular NLP models that utilize a Transformer at its core and which achieved State of the Art performance on many NLP tasks including Classification, Question Answering, and NER Tagging when it was first introduced.</p><p>Specifically, unlike other posts on the same topic, I will try to go through the highly influential BERT
<a href=https://arxiv.org/abs/1810.04805 target=_blank rel="nofollow noopener">paper</a>
— Pre-training of Deep Bidirectional Transformers for Language Understanding while keeping the jargon to a minimum and try to explain how BERT works through sketches.</p><hr><h2 id=so-what-is-bert>So, what is BERT?</h2><p>In simple words, BERT is an architecture that can be used for a lot of downstream tasks such as question answering, Classification, NER etc. <em><strong>One can assume a pre-trained BERT as a black box that provides us with H = 768 shaped vectors for each input token(word) in a sequence</strong></em>. Here, the sequence can be a single sentence or a pair of sentences separated by the separator [SEP] and starting with a token [CLS]. We will get into explaining these tokens in more detail in later stages in this post.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/0_hu6221069953458131578.png 500w
, /images/bert_sketches/0_hu5139869761019386175.png 800w
, /images/bert_sketches/0_hu6113079040466611642.png 1200w
, /images/bert_sketches/0_hu12142214195600170619.png 1500w' src=/images/bert_sketches/0.png alt="High Level View of BERT"><figcaption>A very High view of BERT, We get a 768 sized vector for all words in our input sentence.</figcaption></figure></p><hr><h2 id=but-what-is-the-use-of-such-a-blackbox>But, What is the use of such a Blackbox?</h2><p><em><strong>A BERT model essentially works like how most Deep Learning models for Imagenet work</strong></em>. First, we train the BERT model on a large corpus (Masked LM Task), and then we finetune the model for our own task which could be Classification, Question Answering or NER, etc. by adding a few extra layers at the end.</p><p><strong>For example,</strong> we would train BERT first on a corpus like Wikipedia(Masked LM Task) and then Finetune the model on our own data to do a classification task like classifying reviews as negative or positive or neutral by adding a few extra layers. In practice, we just use the output from the [CLS] token for the classification task. So, the whole architecture for fine-tuning looks like this:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/13_hu6340962316004127131.png 500w
, /images/bert_sketches/13_hu13941963701828393864.png 800w
, /images/bert_sketches/13_hu1535901337697411405.png 1200w
, /images/bert_sketches/13_hu9549473356857403948.png 1500w' src=/images/bert_sketches/13.png alt="We just use the [CLS] Token output for classification along with some added Linear and Softmax Layers"><figcaption>We just use the [CLS] Token output for classification along with some added Linear and Softmax Layers</figcaption></figure></p><p>So, as I said in my previous post on transformers that <em><strong>all Deep Learning is just Matrix Multiplication</strong></em>, we just introduce a new W layer having a shape of (H x num_classes = 768 x 3) and train the whole architecture using our training data and Cross-Entropy loss on the classification.</p><p>One could also have just gotten the sentence features through the last layer and then just run a Logistic Regression Classifier on top or take an average of all the outputs and then run a logistic regression on top. There are many possibilities, and what works best will depend on the data for the task.</p><p>In the above example, I explained how you could do Classification using BERT. In pretty much similar ways, one can also use BERT for Question Answering and NER based Tasks. I will get to the architectures used for various tasks by the end of this post.</p><hr><h2 id=but-how-is-it-different-from-an-embedding-then><strong>But, How is it different from an Embedding then?</strong></h2><p>You are getting the Gist. Essentially, BERT just provides us with contextual-bidirectional embeddings.</p><ul><li><p><strong>Contextual:</strong> The embeddings of a word are not static. That is, they depend on the context of words around it. So in a sentence like “one bird was flying below another bird”, the two embeddings of the word “bird” will be different.</p></li><li><p><strong>Bi-Directional:</strong> While directional models in the past like LSTM’s read the text input sequentially (left-to-right or right-to-left), the Transformer actually reads the entire sequence of words at once and thus is considered bidirectional.</p></li></ul><p>So, for a sentence like “BERT model is awesome.” the embeddings for the word “model” will have context from all the words “BERT”, “Awesome”, and “is”.</p><hr><h2 id=enough-just-tell-me-about-how-it-works>Enough, Just tell me about how it works?</h2><p>Now we understand the basics; I will divide this section into three major parts — Architecture, Inputs, and Training.</p><hr><h2 id=1-architecture>1. Architecture</h2><p>This is the most simple part if you have read my
<a href=https://mlwhiz.com/blog/2020/09/20/transformers/>post</a>
on Transformers. BERT is essentially just made up of stacked up encoder layers.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/2_hu14208069130331733204.png 500w
, /images/bert_sketches/2_hu18416294172784039901.png 800w
, /images/bert_sketches/2_hu14305968106194190744.png 1200w
, /images/bert_sketches/2_hu15838903238145755774.png 1500w' src=/images/bert_sketches/2.png alt="Author Image: BERT is just a stack of encoders"><figcaption>Author Image: BERT is just a stack of encoders</figcaption></figure></p><p>In the paper, the authors have experimented with two models:</p><ul><li><p><strong>BERT Base</strong>: Number of Layers L=12, Size of the hidden layer, H=768, and Self-attention heads, A=12 with Total Parameters=110M</p></li><li><p><strong>BERT Large</strong>: Number of Layers L=24, Size of the hidden layer, H=1024, and Self-attention heads, A=16 with Total Parameters=340M</p></li></ul><hr><h2 id=2-training-inputs>2. Training Inputs</h2><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/3_hu17295704053829868617.png 500w
, /images/bert_sketches/3_hu2665570877371574138.png 800w
, /images/bert_sketches/3_hu2560409661533142063.png 1200w
, /images/bert_sketches/3_hu10505330021342571765.png 1500w' src=/images/bert_sketches/3.png alt="Inputs to BERT"><figcaption>Inputs to BERT</figcaption></figure></p><p>We give inputs to BERT using the above structure. The input consists of a pair of sentences, called sequences, and two special tokens — [CLS] and [SEP].</p><p>So, in this example, for two sentences “my dog is cute” and “he likes playing”, BERT First uses
<a href=https://paperswithcode.com/method/wordpiece target=_blank rel="nofollow noopener">wordpiece tokenization</a>
to convert the sequence into tokens and adds the [CLS] token in the start and the [SEP] token in the beginning and end of the second sentence, so the input is:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/4_hu11270811104158848563.png 500w
, /images/bert_sketches/4_hu13027613664278025425.png 800w' src=/images/bert_sketches/4.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>The wordpiece tokenization used in BERT necessarily breaks words like playing into “play” and “##ing”. This helps in two ways —</p><ul><li><p>It helps limit the size of Vocabulary as we don’t have to keep the various form of words like playing, plays, player etc. in our vocabulary.</p></li><li><p>It helps us with out-of-vocab words. For example, if plays don’t occur in the vocabulary, we might still have embeddings for play and ##s</p></li></ul><p><strong>Token Embeddings:</strong> We then get the Token embeddings by indexing a Matrix of size 30000x768(H). Here, 30000 is the Vocab length after wordpiece tokenization. The weights of this matrix would be learned while training.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/5_hu9455756022250789810.png 500w
, /images/bert_sketches/5_hu5822693896658762131.png 800w
, /images/bert_sketches/5_hu12580543958678130996.png 1200w
, /images/bert_sketches/5_hu15796428640230022557.png 1500w' src=/images/bert_sketches/5.png alt="Author Image: Token Embeddings come by indexing a matrix of size VocabxH"><figcaption>Author Image: Token Embeddings come by indexing a matrix of size VocabxH</figcaption></figure></p><p><strong>Segment Embeddings:</strong> For tasks such as question answering, we should specify which segment this sentence is from. These are either all 0 vectors of H length if the embedding is from sentence 1, or a vector of 1’s if the embedding is from sentence 2.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/6_hu1638430990475803363.png 500w
, /images/bert_sketches/6_hu17961900688688226126.png 800w
, /images/bert_sketches/6_hu16935753381441588857.png 1200w
, /images/bert_sketches/6_hu13444391289749137811.png 1500w' src=/images/bert_sketches/6.png alt="Author Image: Segment embedding are all 0&rsquo;s or all 1’s vector specifying 1st Sentence or 2nd Sentence."><figcaption>Author Image: Segment embedding are all 0's or all 1’s vector specifying 1st Sentence or 2nd Sentence</figcaption></figure></p><p><strong>Position Embeddings</strong>: These are the embeddings used to specify the position of words in the sequence, the same as we did in the transformer architecture. So we essentially have a constant matrix with some preset pattern. This matrix has the number of columns as 768. The first row of this matrix is the embedding for token [CLS], the second row as embedding for the word “my”, the third row is embedding for the word “dog” and so on.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/7_hu656137255704999652.png 500w
, /images/bert_sketches/7_hu8952956543531876793.png 800w' src=/images/bert_sketches/7.png alt="Author Image: Patterns are used to specify word position"><figcaption>Author Image: Patterns are used to specify word position</figcaption></figure></p><p>So the Final Input given to BERT is <strong>Token Embeddings + Segment Embeddings + Position Embeddings.</strong></p><hr><h2 id=3-training-masked-lm>3. Training Masked LM:</h2><p>We finally reach the most interesting part of BERT here as this is where most of the novel concepts are introduced.<em><strong>I will try to explain these concepts by going through various architecture attempts and finding faults with each of them to arrive at the final BERT architecture.</strong></em></p><p><strong>Attempt 1:</strong> So, for example, if we set up BERT training as below:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/8_hu12331903014769276395.png 500w
, /images/bert_sketches/8_hu109216179265580130.png 800w
, /images/bert_sketches/8_hu661013467349971592.png 1200w
, /images/bert_sketches/8_hu2242695054419384881.png 1500w' src=/images/bert_sketches/8.png alt="Author Image: Attempt 1- Predict all words."><figcaption>Author Image: Attempt 1- Predict all words.</figcaption></figure></p><p>We try to predict each word of the input sequence using our training data with Cross-Entropy loss. Can you guess the problem with this approach?</p><p><em><strong>The problem is that the learning task is trivial.</strong></em> The network knows beforehand what it has to predict, and it can thus easily learn weights to reach a 100% classification accuracy.</p><p><strong>Attempt 2 — Masked LM:</strong> This starts the beginning of the paper&rsquo;s approach to overcoming the previous approach&rsquo;s problem. We mask 15% random words in each training input sequence and just predict output for those words. In Pictorial terms:</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/9_hu13265946708852269411.png 500w
, /images/bert_sketches/9_hu9514843133205851921.png 800w
, /images/bert_sketches/9_hu4885387588465140691.png 1200w
, /images/bert_sketches/9_hu9378786326547023257.png 1500w' src=/images/bert_sketches/9.png alt="Author Image: Attempt 2- Predict only masked words"><figcaption>Author Image: Attempt 2- Predict only masked words</figcaption></figure></p><p>Thus the loss gets calculated only over masked words. <em><strong>So now the model learns to predict words it hasn’t seen while seeing all the context around those words.</strong></em></p><p>Please note, I have masked 3 words here even when I should mask just 1 word as 15% of 8 is 1 to explain in this toy example.</p><p><em><strong>Can you find the problem with this approach?</strong></em></p><p>This model has essentially learned that it should predict good probabilities for only the [MASK] token. That is at the prediction time or at the fine-tuning time when this model will not get [MASK] as input; the model won’t predict good contextual embeddings.</p><p><strong>Attempt 3 — Masked LM with random Words:</strong></p><p>In this attempt, we will still mask 15% of the positions. But we will replace any word in 20% of those masked tokens by some random word. We do this because we want to let the model know that we still want some output when the word is not a [MASK] token. So if we have a sequence of length 500, we will mask 75 tokens(15% of 500), and in those 75 tokens, 15 tokens(20 % of 75) would be replaced by random words. Pictorially, here we replace some of the masks by random words.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/10_hu18124234797046747456.png 500w
, /images/bert_sketches/10_hu14151520749067539521.png 800w
, /images/bert_sketches/10_hu17221483166457440085.png 1200w
, /images/bert_sketches/10_hu4679297007707007208.png 1500w' src=/images/bert_sketches/10.png alt="Author Image: Attempt 3- Predict masked words and Random Words"><figcaption>Author Image: Attempt 3- Predict masked words and Random Words</figcaption></figure></p><p>Advantage: The network will still work with any word now.</p><p>Problem: The network has learned that Input Word is never equal to the Output word. That is the output vector at the position of “random word” would never be “random word.”</p><p><strong>Attempt 4: Masked LM with Random Words and Unmasked Words</strong></p><p>To solve this problem, the authors suggest the below training setup.</p><blockquote><p>The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time</p></blockquote><p>So if we have a sequence of length 500, we will mask 75 tokens(15% of 500), and in those 75 tokens, 7 tokens(10 % of 75) would be replaced by random words, and 7 tokens (10% of 75) will be used as it is. Pictorially, we replace some of the masks with random words and replace some of the masks by actual words.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/11_hu11707080172034754138.png 500w
, /images/bert_sketches/11_hu15390385083473792818.png 800w
, /images/bert_sketches/11_hu16049243542008341797.png 1200w
, /images/bert_sketches/11_hu928117456963296918.png 1500w' src=/images/bert_sketches/11.png alt="Author Image: Attempt 4- Predict masked words, Random Words and Unmasked Words"><figcaption>Author Image: Attempt 4- Predict masked words, Random Words and Unmasked Words</figcaption></figure></p><p>So, now we have the best setup where our model doesn’t learn any unsavory patterns.</p><p><strong>But what if I keep only Mask + Unmask Setup?</strong> The model will learn that whenever the word is present, just predict that word.</p><hr><h2 id=4-training-additional-nsp-task>4. Training Additional NSP Task</h2><p><em>From the BERT paper:</em></p><blockquote><p>Many important downstream tasks such as **Question Answering (QA) **and <strong>Natural Language Inference (NLI)</strong> are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.</p></blockquote><p>So, now we understand the Masked LM task, BERT Model also has one more training task which goes in parallel while **Training Masked LM **task. This task is called Next Sentence Prediction(NSP). So while creating the training data, we choose the sentences A and B for each training example such that 50% of the time B is the actual next sentence that follows A (labelled as IsNext), and 50% of the time it is a random sentence from the corpus (labelled as NotNext). We then use the CLS token output to get the binary loss which is also backpropagated through the network to learn the weights.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/12_hu16820754939419885203.png 500w
, /images/bert_sketches/12_hu12284458890867147651.png 800w
, /images/bert_sketches/12_hu187085845452795023.png 1200w
, /images/bert_sketches/12_hu6924404227099007433.png 1500w' src=/images/bert_sketches/12.png alt="Author Image: Training BERT with NSP task"><figcaption>Author Image: Training BERT with NSP task</figcaption></figure></p><p>So we have got the BERT model now that can provide us with contextual embeddings. How can I use it for various tasks?</p><hr><h2 id=finetuning-for-relevant-task>Finetuning for Relevant Task</h2><p>We already have seen how we can use BERT for the classification task by adding a few layers on top of the [CLS] output and fine-tuning the weights.</p><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/13_hu6340962316004127131.png 500w
, /images/bert_sketches/13_hu13941963701828393864.png 800w
, /images/bert_sketches/13_hu1535901337697411405.png 1200w
, /images/bert_sketches/13_hu9549473356857403948.png 1500w' src=/images/bert_sketches/13.png alt="Author Image: BERT Finetuning for Classification"><figcaption>Author Image: BERT Finetuning for Classification</figcaption></figure></p><p>Here is how we can use BERT for other tasks, from the paper:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/14_hu8841617055692765779.png 500w
, /images/bert_sketches/14_hu9098861259366754382.png 800w' src=/images/bert_sketches/14.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Let’s go through each of them one by one.</p><ol><li><p><strong>Sentence Pair Classification tasks</strong> — This is pretty similar to the classification task. That is add a Linear + Softmax layer on top of the 768 sized CLS output.</p></li><li><p><strong>Single Sentence Classification Task</strong> — Same as above.</p></li><li><p><strong>Single Sentence Tagging Task</strong> —This is pretty similar to the setup we use while training BERT, just that we need to predict some tags for each token rather than the word itself. For example, for a POS Tagging task like predicting Noun, Verb, or Adjective, we will just add a Linear layer of size (768 x n_outputs) and add a softmax layer on top to predict.</p></li><li><p><strong>Question Answering Tasks</strong> — This is the most interesting task and would need some more context to understand how BERT is used to solve it. In this task, we are given a question and a paragraph in which the answer lies. The objective is to determine the start and end span for the answer in the paragraph.</p></li></ol><p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset='/images/bert_sketches/15_hu2773388370685113423.png 500w
, /images/bert_sketches/15_hu4801676540080967189.png 800w
, /images/bert_sketches/15_hu13714017326382716557.png 1200w
, /images/bert_sketches/15_hu16163049309734851704.png 1500w' src=/images/bert_sketches/15.png alt="Author Image: BERT Finetuning for Question-Answer Task"><figcaption>Author Image: BERT Finetuning for Question-Answer Task</figcaption></figure></p><p>So, in the above example, we define two vectors S and E(which will be learned during fine-tuning) both having shapes(1x768). We then take a dot product of these vectors with the second sentence’s output vectors from BERT, giving us some scores. We then apply Softmax over these scores to get probabilities. The training objective is the sum of the log-likelihoods of the correct start and end positions. Mathematically, for the Probability vector for Start positions:</p><p><img sizes="(min-width: 35em) 1200px, 100vw" srcset src=/images/bert_sketches/16.png alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence"></p><p>Where T_i is the word we are focussing on. An analogous formula is for End positions.</p><p>To predict a span, we get all the scores — S.T and E.T and get the best span as the span having the maximum Score, that is max(S.T_i + E.T_j) among all j≥i.</p><hr><h2 id=references>References</h2><ul><li><p><a href=https://arxiv.org/abs/1706.03762 target=_blank rel="nofollow noopener">Attention Is All You Need</a>
: The Paper which introduced Transformers.</p></li><li><p><a href=https://arxiv.org/abs/1810.04805 target=_blank rel="nofollow noopener">BERT Paper</a>
: Do read this paper.</p></li></ul><p>In this post, I covered how BERT works in a simple to understand manner. If you just want to remember a few ideas that BERT introduced just remember these:</p><ul><li><p>BERT provides contextual Embeddings.</p></li><li><p>It introduced the Masked LM task to provide Bidirectional Embeddings.</p></li></ul><p><em><strong>Now, finally, my turn to ask the question: Did you get how BERT works? Yes, or No, you can answer in the comments. :)</strong></em></p><p>If you want to know more about NLP, I would like to recommend this awesome
<a href=https://coursera.pxf.io/9WjZo0 target=_blank rel="nofollow noopener">Natural Language Processing Specialization</a>
. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.</p><p>I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at
<a href=https://mlwhiz.medium.com/ target=_blank rel="nofollow noopener">Medium</a>
or Subscribe to my
<a href=https://mlwhiz.ck.page/a9b8bda70c target=_blank rel="nofollow noopener">blog</a>
.</p><p>And, finally a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.</p><script async data-uid=8d7942551b src=https://mlwhiz.ck.page/8d7942551b/index.js></script><div class=shareaholic-canvas data-app=share_buttons data-app-id=28372088 style=margin-bottom:1px></div><a href=https://coursera.pxf.io/coursera rel=nofollow><img border=0 alt="Start your future with a Data Analysis Certificate." src="https://ad.linksynergy.com/fs-bin/show?id=lVarvwc5BD0&bids=759505.377&subid=0&type=4&gridnum=16"></a><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mlwhiz.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><div class=col-lg-4><div class=widget><script type=text/javascript src=https://ko-fi.com/widgets/widget_2.js></script><script type=text/javascript>kofiwidget2.init("Support Me on Ko-fi","#972EB4","S6S3NPCD"),kofiwidget2.draw()</script></div><div class=widget><h4 class=widget-title>About Me</h4><img src=https://mlwhiz.com/images/author.jpg alt class="img-fluid author-thumb-sm d-block mx-auto rounded-circle mb-4"><p><p>I’m a Machine Learning Engineer based in London, where I am currently working with <strong>Roku</strong> .</p></p><a href=https://mlwhiz.com/about/ class="btn btn-outline-primary">Know More</a></div><div class=widget><h4 class=widget-title>Topics</h4><ul class=list-unstyled><li><a class="categoryStyle text-white" href=/categories/awesome-guides>Awesome Guides</a></li><li><a class="categoryStyle text-white" href=/categories/bash>Bash</a></li><li><a class="categoryStyle text-white" href=/categories/big-data>Big Data</a></li><li><a class="categoryStyle text-white" href=/categories/chatgpt-series>Chatgpt Series</a></li><li><a class="categoryStyle text-white" href=/categories/computer-vision>Computer Vision</a></li><li><a class="categoryStyle text-white" href=/categories/data-science>Data Science</a></li><li><a class="categoryStyle text-white" href=/categories/deep-learning>Deep Learning</a></li><li><a class="categoryStyle text-white" href=/categories/learning-resources>Learning Resources</a></li><li><a class="categoryStyle text-white" href=/categories/machine-learning>Machine Learning</a></li><li><a class="categoryStyle text-white" href=/categories/natural-language-processing>Natural Language Processing</a></li><li><a class="categoryStyle text-white" href=/categories/opinion>Opinion</a></li><li><a class="categoryStyle text-white" href=/categories/programming>Programming</a></li></ul></div><div class=widget><h4 class=widget-title>Tags</h4><ul class=list-inline><li class=list-inline-item><a class="tagStyle text-white" href=/tags/algorithms>Algorithms</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/artificial-intelligence>Artificial Intelligence</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/awesome-guides>Awesome Guides</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/best-content>Best Content</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/big-data>Big Data</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/chatgpt>Chatgpt</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/computer-vision>Computer Vision</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/curated-resources>Curated Resources</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/dask>Dask</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/data-science>Data Science</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deep-learning>Deep Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/deployment>Deployment</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/ec2>Ec2</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/generative-adversarial-networks>Generative Adversarial Networks</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/graphs>Graphs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/image-classification>Image Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/instance-segmentation>Instance Segmentation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/interpretability>Interpretability</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/jobs>Jobs</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/kaggle>Kaggle</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/language-modeling>Language Modeling</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/learning-resources>Learning Resources</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/machine-learning>Machine Learning</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/math>Math</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/multiprocessing>Multiprocessing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/natural-language-processing>Natural Language Processing</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/object-detection>Object Detection</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/oop>Oop</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/opinion>Opinion</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pandas>Pandas</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/production>Production</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/productivity>Productivity</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/python>Python</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/pytorch>Pytorch</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/spark>Spark</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/sql>SQL</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/statistics>Statistics</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/streamlit>Streamlit</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/text-classification>Text Classification</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/timeseries>Timeseries</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/tools>Tools</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/transformers>Transformers</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/translation>Translation</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/visualization>Visualization</a></li><li class=list-inline-item><a class="tagStyle text-white" href=/tags/xgboost>Xgboost</a></li></ul></div><div class=widget><h4 class=widget-title>Connect With Me</h4><ul class="list-inline social-links"><li class=list-inline-item><a href="https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&amp;followMember=rahulagwl"><i class=ti-linkedin></i></a></li><li class=list-inline-item><a href=https://mlwhiz.medium.com/><i class=ti-book></i></a></li><li class=list-inline-item><a href=https://twitter.com/MLWhiz><i class=ti-twitter-alt></i></a></li><li class=list-inline-item><a href=https://www.facebook.com/mlwhizblog><i class=ti-facebook></i></a></li><li class=list-inline-item><a href=https://github.com/MLWhiz><i class=ti-github></i></a></li></ul></div><script async data-uid=bfe9f82f10 src=https://mlwhiz.ck.page/bfe9f82f10/index.js></script><script async data-uid=3452d924e2 src=https://mlwhiz.ck.page/3452d924e2/index.js></script></div></div></div></section><footer><div class=container><div class=row><div class="col-12 text-center mb-5"><a href=https://mlwhiz.com/><img src=https://mlwhiz.com/images/logos/mlwhiz_black.png class=img-fluid-custom-bottom alt="MLWhiz - Your Home for DS, ML, AI!"></a></div><div class="col-12 border-top py-4 text-center">Copyright © 2020 <a href=https://mlwhiz.com style=color:#972eb4>MLWhiz</a> All Rights Reserved</div></div></div></footer><script>var indexURL="https://mlwhiz.com/index.json"</script><script src=https://mlwhiz.com/plugins/compressjscss/main.js></script><script src=https://mlwhiz.com/js/script.min.js></script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-54777926-1","auto"),ga("send","pageview")</script></body></html>