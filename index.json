[{"categories":["Deep Learning","Natural Language Processing","Awesome Guides"],"contents":"Transformers have become the defacto standard for NLP tasks nowadays.\nWhile the Transformer architecture was introduced with NLP, they are now being used in Computer Vision and to generate music as well. I am sure you would all have heard about the GPT3 Transformer and its applications thereof.\nBut all these things aside, they are still hard to understand as ever.\nIt has taken me multiple readings through the Google research paper that first introduced transformers along with just so many blog posts to really understand how a transformer works.\nSo, I thought of putting the whole idea down in as simple words as possible and with some very basic Math and some puns as I am a proponent of having some fun while learning. I will try to keep both the jargon and the technicality to a minimum, yet it is such a topic that I could only do so much. And my goal is to make the reader understand even the most gory details of Transformer by the end of this post.\nAlso, this is officially my longest post both in terms of time taken to write it as well as length of the post. Hence, I will advice you to Grab A Coffee. ‚òïÔ∏è\nSo, here goes ‚Äî This post will be a highly conversational one and it is about ‚ÄúDecoding The Transformer‚Äù.\n Q: So, Why should I even understand Transformer?\nIn the past, the LSTM and GRU architecture(as explained here in my past post on NLP) along with attention mechanism used to be the State of the Art Approach for Language modeling problems (put very simply, predict the next word) and Translation systems. But, the main problem with these architectures is that they are recurrent in nature, and the runtime increases as the sequence length increases. That is, these architectures take a sentence and process each word in a sequential way, and hence with the increase in sentence length the whole runtime increases.\nTransformer, a model architecture first explained in the paper Attention is all you need, lets go of this recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. And that makes it FAST.\n  From the Paper  This is the picture of the full transformer as taken from the paper. And, it surely is intimidating. So, I will aim to demystify it in this post by going through each individual piece. So read ahead.\n The Big Picture Q: That sounds interesting. So, what does a transformer do exactly?\nEssentially, a transformer can perform almost any NLP task. It can be used for language modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one language to another, or for a classification problem will provide the class probability using an appropriate output layer.\nIt all will depend on the final outputs layer for the network but, the Transformer basic structure will remain quite the same for any task. For this particular post, I will be continuing with the machine translation example.\nSo from a very high place, this is how the transformer looks for a translation task. It takes as input an English sentence and returns a German sentence.\n  Transformer for Translation   The Building Blocks Q: That was too basic. üòé Can you expand on it?\nOkay, just remember in the end, you asked for it. Let‚Äôs go a little deeper and try to understand what a transformer is composed of.\nSo, a transformer is essentially composed of a stack of encoder and decoder layers. The role of an encoder layer is to encode the English sentence into a numerical form using the attention mechanism, while the decoder aims to use the encoded information from the encoder layers to give the German translation for the particular English sentence.\nIn the figure below, the transformer is given as input an English sentence, which gets encoded using 6 encoder layers. The output from the final encoder layer then goes to each decoder layer to translate English to German.\n  Data Flow in a Transformer   1. Encoder Architecture Q: That‚Äôs alright but, how does an encoder stack encode an English sentence exactly?\nPatience, I am getting to it. So, as I said the encoder stack contains six encoder layers on top of each other(As given in the paper, but the future versions of transformers use even more layers). And each encoder in the stack has essentially two main layers:\n  a multi-head self-attention Layer, and\n  a position-wise fully connected feed-forward network\n    Very basic encoder Layer  They are a mouthful. Right? Don‚Äôt lose me yet as I will explain both of them in the coming sections. Right now, just remember that the encoder layer incorporates attention and a position-wise feed-forward network.\nQ: But, how does this layer expect its inputs to be?\nThis layer expects its inputs to be of the shape SxD (as shown in the figure below) where S is the source sentence(English Sentence) length, and D is the dimension of the embedding whose weights can be trained with the network. In this post, we will be using D as 512 by default throughout. While S will be the maximum length of sentence in a batch. So it normally changes with batches.\n  Encoder ‚Äî Input and Output shapes are the same  And what about the outputs of this layer? Remember that the encoder layers are stacked on top of each other. So, we want to be able to have an output of the same dimension as the input so that the output can flow easily into the next encoder. So the output is also of the shape, SxD.\nQ: Enough about the sizes talk, I understand what goes in and what goes out but what actually happens in the Encoder layer?\nOkay, let‚Äôs go through the attention layer and the feedforward layer one by one:\nA) Self-attention layer   How Self-Attention Works  The above figure must look daunting but it is easy to understand. So just stay with me here.\nDeep Learning is essentially nothing but a lot of matrix calculations and what we are essentially doing in this layer is a lot of matrix calculations intelligently. The self-attention layer initializes with 3 weight matrices ‚Äî Query($W_q$), Key($W_k$), and Value($W_v$). Each of these matrices has a size of (Dxd) where d is taken as 64 in the paper. The weights for these matrices will be trained when we train the model.\nIn the first calculation(Calc 1 in the figure), we create matrices Q, K, and V by multiplying the input with the respective Query, Key, and Value matrix.\nTill now it is trivial and shouldn‚Äôt make any sense, but it is at the second calculation where it gets interesting. Let‚Äôs try to understand the output of the softmax function. We start by multiplying the Q and K·µÄ matrix to get a matrix of size (SxS) and divide it by the scalar ‚àöd. We then take a softmax to make the rows sum to one.\nIntuitively, we can think of the resultant SxS matrix as the contribution of each word in another word. For example, it might look like this:\n  After Softmax  As you can see the diagonal entries are big. This is because the word contribution to itself is high. That is reasonable. But we can see here that the word ‚Äúquick‚Äù devolves into ‚Äúquick‚Äù and ‚Äúfox‚Äù and the word ‚Äúbrown‚Äù also devolves into ‚Äúbrown‚Äù and ‚Äúfox‚Äù. That intuitively helps us to say that both the words ‚Äî ‚Äúquick‚Äù and ‚Äúbrown‚Äù each refers to the ‚Äúfox‚Äù.\nOnce we have this SxS matrix with contributions we multiply this matrix by the Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64). So, what the operation actually does is that it replaces the embedding vector of a word like ‚Äúquick‚Äù with say .75 x (quick embedding) and .2x(fox embedding) and thus now the resultant output for the word ‚Äúquick‚Äù has attention embedded in itself.\nNote that the output of this layer has the dimension (Sxd) and before we get done with the whole encoder we need to change it back to D=512 as we need the output of this encoder as the input of another encoder.\nQ: But, you called this layer Multi-head self-attention Layer. What is the multi-head?\nOkay, my bad but in my defense, I was just getting to that.\nIt‚Äôs called a multi-head because we use many such self-attention layers in parallel. That is, we have many self-attention layers stacked on top of each other. The number of attention layers,h, is kept as 8 in the paper. So the input X goes through many self-attention layers parallelly, each of which gives a z matrix of shape (Sxd) = 4x64. We concatenate these 8(h) matrices and again apply a final output linear layer, $W_o$, of size DxD.\nWhat size do we get? For the concatenate operation we get a size of SxD(4x(64x8) = 4x512). And multiplying this output by $W_o$, we get the final output Z with the shape of SxD(4x512) as desired.\nAlso, note the relation between h,d, and D i.e. h x d = D\n  The Full multi-headed self-attention Layer  Thus, we finally get the output Z of shape 4x512 as intended. But before it goes into another encoder we pass it through a Feed-Forward Network.\nB) Position-wise feed-forward network Once we understand the multi-headed attention layer, the Feed-forward network is actually pretty easy to understand. It is just a combination of various linear and dropout layers on the output Z. Consequentially, it is again just a lot of Matrix multiplication here.\n  Each word goes into the feed-forward network  The feed-forward network applies itself to each position in the output Z parallelly(Each position can be thought of as a word) and hence the name Position-wise feed-forward network. The feed-forward network also shares weight, so that the length of the source sentence doesn‚Äôt matter(Also, if it didn‚Äôt share weights, we would have to initialize a lot of such networks based on max source sentence length and that is not feasible)\n  It is actually just a linear layer that gets applied to each position(or word)  With this, we near an okayish understanding of the encoder part of the Transformer.\nQ: Hey, I was just going through the picture in the paper, and the encoder stack has something called ‚Äúpositional encoding‚Äù and ‚ÄúAdd \u0026amp; Norm‚Äù also. What are these?\n  I am back again here so you don‚Äôt have to scroll  Okay, These two concepts are pretty essential to this particular architecture. And I am glad you asked this one. So, we will discuss these steps before moving further to the decoder stack.\nC. Positional Encodings Since, our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add ‚Äúpositional encodings‚Äù to the input embeddings at the bottoms of both the encoder and decoder stacks(as we will see later). The positional encodings need to have the same dimension, D as the embeddings have so that the two can be summed.\n  Add a static positional pattern to X  In the paper, the authors used sine and cosine functions to create positional embeddings for different positions.\n This particular mathematical thing actually generates a 2d matrix which is added to the embedding vector that goes into the first encoder step.\nPut simply, it‚Äôs just a constant matrix that we add to the sentence so that the network could get the position of the word.\n   Positional encoding matrix for the first 300 and 3000 positions  Above is the heatmap of the position encoding matrix that we will add to the input that is to be given to the first encoder. I am showing the heatmap for the first 300 positions and the first 3000 positions. We can see that there is a distinct pattern that we provide to our Transformer to understand the position of each word. And since we are using a function comprised of sin and cos, we are able to embed positional embeddings for very high positions also pretty well as we can see in the second picture.\nInteresting Fact: The authors also let the Transformer learn these encodings too and didn‚Äôt see any difference in performance as such. So, they went with the above idea as it doesn‚Äôt depend on sentence length and so even if the test sentence is bigger than train samples, we would be fine.\nD. Add and Normalize Another thing, that I didn‚Äôt mention for the sake of simplicity while explaining the encoder is that the encoder(the decoder architecture too) architecture has skip level residual connections(something akin to resnet50) also. So, the exact encoder architecture in the paper looks like below. Simply put, it helps traverse information for a much greater length in a Deep Neural Network. This can be thought of as akin(intuitively) to information passing in an organization where you have access to your manager as well as to your manager‚Äôs manager.\n  The Skip level connections help information flow in the network   2. Decoder Architecture Q: Okay, so till now we have learned that an encoder takes an input sentence and encodes its information in a matrix of size SxD(4x512). That‚Äôs all great but how does it help the decoder decode it to German?\nGood things come to those who wait. So, before understanding how the decoder does that, let us understand the decoder stack.\nThe decoder stack contains 6 decoder layers in a stack (As given in the paper again) and each decoder in the stack is comprised of these main three layers:\n  Masked multi-head self-attention Layer\n  multi-head self-attention Layer, and\n  a position-wise fully connected feed-forward network\n  It also has the same positional encoding as well as the skip level connection as well. We already know how the multi-head attention and feed-forward network layers work, so we will get straight into what is different in the decoder as compared to the encoder.\n  Decoder Architecture  Q: Wait, but do I see the output we need flowing into the decoder as input? What? Why? üòñ\nI am noticing that you are getting pretty good at asking questions. And that is a great question, something I even though myself a lot of times, and something that I hope will get much clearer by the time you reach the end of this post.\nBut to give an intuition, we can think of a transformer as a conditional language model in this case. A model that predicts the next word given an input word and an English sentence on which to condition upon or base its prediction on.\nSuch models are inherently sequential as in how would you train such a model? You start by giving the start token(\u0026lt;s\u0026gt;) and the model predicts the first word conditioned on the English sentence. You change the weights based on if the prediction is right or wrong. Then you give the start token and the first word (\u0026lt;s\u0026gt; der) and the model predicts the second word. You change weights again. And so on.\nThe transformer decoder learns just like that but the beauty is that it doesn‚Äôt do that in a sequential manner. It uses masking to do this calculation and thus takes the whole output sentence (although shifted right by adding a \u0026lt;s\u0026gt; token to the front) while training. Also, please note that at prediction time we won‚Äôt give the output to the network\nQ: But, how does this masking exactly work?\n A) Masked Multi-Head Self Attention Layer It works, as usual, you wear it I mean üò∑. Kidding aside, as you can see that this time we have a Masked Multi-Head attention Layer in our decoder. This means that we will mask our shifted output (that is the input to the decoder) in a way that the network is never able to see the subsequent words since otherwise, it can easily copy that word while training.\nSo, how does the mask exactly work in the masked attention layer? If you remember, in the attention layer we multiplied the query(Q) and keys(K) and divided them by sqrt(d) before taking the softmax.\nIn a masked attention layer, though, we add the resultant matrix before the softmax(which will be of shape (TxT)) to a masking matrix.\nSo, In a masked layer, the function changes from:\n Q: I still don‚Äôt get it, what happens if we do that?\nThat‚Äôs understandable actually. Let me break it in steps. So, our resultant matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers can be big as softmax not applied yet)\n  Schnelle currently attends to both Braune and Fuchs  The word Schnelle will now be composed of both Braune and Fuchs if we take the above matrix‚Äôs softmax and multiply it with the value matrix V. But we don‚Äôt want that, so we add the mask matrix to it to give:\n  The mask operation applied to the matrix  And, now what will happen after we do the softmax step?\n  Schnelle never attends to any word after Schnelle  Since $e^{-inf}$ = 0, all positions subsequent to Schnelle have been converted to 0. Now, if we multiply this matrix with the value matrix V, the vector corresponding to Schnelle‚Äôs position in the Z vector passing through the decoder would not contain any information of the subsequent words Braune and Fuchs just like we wanted.\nAnd that is how the transformer takes the whole shifted output sentence at once and doesn‚Äôt learn in a sequential manner. Pretty neat I must say.\nQ: Are you kidding me? That‚Äôs actually awesome.\nSo glad that you are still with me and you appreciate it. Now, coming back to the decoder. The next layer in the decoder is:\nB) Multi-Headed Attention Layer As you can see in the decoder architecture, a Z vector(Output of encoder) flows from the encoder to the multi-head attention layer in the Decoder. This Z output from the last encoder has a special name and is often called as memory. The attention layer takes as input both the encoder output and data flowing from below(shifted outputs) and uses attention. The Query vector Q is created from the data flowing in the decoder, while the Key(K) and value(V) vectors come from the encoder output.\nQ: Isn‚Äôt there any mask here?\nNo, there is no mask here. The output coming from below is already masked and this allows every position in the decoder to attend over all the positions in the Value vector. So for every word position to be generated the decoder has access to the whole English sentence.\nHere is a single attention layer(which will be part of a multi-head just like before):\n Q: But won‚Äôt the shapes of Q, K, and V be different this time?\nYou can look at the figure where I have done all the weights calculation. I would also ask you to see the shapes of the resultant Z vector and how our weight matrices until now never used the target or source sentence length in any of their dimensions. Normally, the shape cancels away in all our matrix calculations. For example, see how the S dimension cancels away in calculation 2 above. That is why while selecting the batches during training the authors talk about tight batches. That is in a batch all source sentences have similar lengths. And different batches could have different source lengths.\nI will now talk about the skip level connections and the feed-forward layer. They are actually the same as in . . . .\nQ: Ok, I get it. We have the skip level connections and the FF layer and get a matrix of shape TxD after this whole decode operation. But where is the German translation?\n 3. Output Head We are actually very much there now friend. Once, we are done with the transformer, the next thing is to add a task-specific output head on the top of the decoder output. This can be done by adding some linear layers and softmax on top to get the probability across all the words in the german vocab. We can do something like this:\n As you can see we are able to generate probabilities. So far we know how to do a forward pass through this Transformer architecture. Let us see how we do the training of such a Neural Net Architecture.\n Training: Till now, if we take a bird-eye view of the structure we have something like:\n We can give an English sentence and shifted output sentence and do a forward pass and get the probabilities over the German vocabulary. And thus we should be able to use a loss function like cross-entropy where the target could be the german word we want, and train the neural network using the Adam Optimizer. Just like any classification example. So, there is your German.\nIn the paper though, the authors use slight variations of optimizers and loss. You can choose to skip the below 2 sections on KL Divergence Loss and Learning rate schedule with Adam if you want as it is done only to churn out more performance out of the model and not an inherent part of the Transformer architecture as such.\nQ: I have been here for such a long time and have I complained? üòí\nOkay. Okay. I get you. Let‚Äôs do it then.\nA) KL Divergence with Label Smoothing: KL Divergence is the information loss that happens when the distribution P is approximated by the distribution Q. When we use the KL Divergence loss, we try to estimate the target distribution(P) using the probabilities(Q) we generate from the model. And we try to minimize this information loss in the training.\n If you notice, in this form(without label smoothing which we will discuss) this is exactly the same as cross-entropy. Given two distributions like below.\n  Target distribution and probability distribution for a word(token)  The KL Divergence formula just plain gives -logq(oder) and that is the cross-entropy loss.\nIn the paper, though the authors used label smoothing with Œ± = 0.1 and so the KL Divergence loss is not cross-entropy. What that means is that in the target distribution the output value is substituted by (1-Œ±) and the remaining 0.1 is distributed across all the words. The authors say that this is so that the model is not too confident.\n Q: But, why do we make our models not confident? It seems absurd.\nYes, it does but intuitively, you can think of it as when we give the target as 1 to our loss function, we have no doubts that the true label is True and others are not. But vocabulary is inherently a non-standardized target. For example, who is to say that you cannot use good in place of great? So we add some confusion in our labels so our model is not too rigid.\nB) A particular Learning Rate schedule with Adam The authors use a learning rate scheduler to increase the learning rate until warmup steps and then decrease it using the below function. And they used the Adam optimizer with $\\beta_1$ = 0.9, $\\beta_2$ = 0.98. Nothing too interesting here just some learning choices.\nSource:  Paper : https://arxiv.org/pdf/1706.03762.pdf  Q: But wait I just remembered that we won‚Äôt have the shifted output at the prediction time, would we? How do we do predictions then?\nIf you realize what we have at this point is a generative model and we will have to do the predictions in a generative way as we won‚Äôt know the output target vector when doing prediction. So predictions are still sequential.\n Prediction Time   Predicting with a greedy search using the Transformer  This model does piece-wise predictions. In the original paper, they use the Beam Search to do prediction. But a greedy search would work fine as well for the purpose of explaining it. In the above example, I have shown how a greedy search would work exactly. The greedy search would start with:\n  Passing the whole English sentence as encoder input and just the start token \u0026lt;st\u0026gt; as shifted output(input to the decoder) to the model and doing the forward pass.\n  The model will predict the next word ‚Äî der\n  Then, we pass the whole English sentence as encoder input and add the last predicted word to the shifted output(input to the decoder = \u0026lt;st\u0026gt; der) and do the forward pass.\n  The model will predict the next word ‚Äî schnelle\n  Passing the whole English sentence as encoder input and \u0026lt;st\u0026gt; der schnelle as shifted output(input to the decoder) to the model and doing the forward pass.\n  and so on, until the model predicts the end token \u0026lt;/s\u0026gt; or we generate some maximum number of tokens(something we can define) so the translation doesn‚Äôt run for an infinite duration in any case it breaks.\n  Beam Search: Q: Now I am greedy, Tell me about beam search as well.\nOkay, the beam search idea is inherently very similar to the above idea. In beam search, we don‚Äôt just look at the highest probability word generated but the top two words.\nSo, For example, when we gave the whole English sentence as encoder input and just the start token as shifted output, we get two best words as i(p=0.6) and der(p=0.3). We will now generate the output model for both output sequences,\u0026lt;s\u0026gt; i and \u0026lt;s\u0026gt; der and look at the probability of the next top word generated. For example, if \u0026lt;s\u0026gt; i gave a probability of (p=0.05) for the next word and \u0026lt;s\u0026gt; der gave (p=0.5) for the next predicted word, we discard the sequence \u0026lt;s\u0026gt; i and go with \u0026lt;s\u0026gt; der instead, as the sum of probability of sentence is maximized(\u0026lt;s\u0026gt; der next_word_to_der p = 0.3+0.5 compared to \u0026lt;s\u0026gt; i next_word_to_i p = 0.6+0.05). We then repeat this process to get the sentence with the highest probability.\nSince we used the top 2 words, the beam size is 2 for this Beam Search. In the paper, they used beam search of size 4.\nPS: I showed that the English sentence is passed at every step for brevity, but in practice, the output of the encoder is saved and only the shifted output passes through the decoder at each time step.\nQ: Anything else you forgot to tell me? I will let you have your moment.\nYes. Since you asked. Here it is:\nBPE, Weight Sharing and Checkpointing In the paper, the authors used Byte pair encoding to create a common English German vocabulary. They then used shared weights across both the English and german embedding and pre-softmax linear transformation as the embedding weight matrix shape would work (Vocab Length X D).\nAlso, the authors average the last k checkpoints to create an ensembling effect to reach the performance*.* This is a pretty known technique where we average the weights in the last few epochs of the model to create a new model which is sort of an ensemble.\nQ: Can you show me some code?\nThis post has already been so long, so I will do that in the next post. Stay tuned.\nNow, finally, my turn to ask the question: Did you get how a transformer works? Yes, or No, you can answer in the comments. :)\n References    Attention Is All You Need : The Paper which started it all.\n   The Annotated Transformer : This one has all the code. Although I will write a simple transformer in the next post too.\n   The Illustrated Transformer : This is one of the best posts on transformers.\n  In this post, I covered how the Transformer architecture works. If you‚Äôre interested in more technical machine learning articles, you can check out my other articles in the related resources section below. And if you‚Äôd like machine learning articles delivered directly to your inbox, you can subscribe to the Lionbridge AI newsletter here .\n","permalink":"https://mlwhiz.com/blog/2020/09/20/transformers/","tags":["Artificial Intelligence","Translation","Language Modeling"],"title":"Understanding Transformers, the Data Science Way"},{"categories":["Deep Learning","Natural Language Processing","Computer Vision","Awesome Guides"],"contents":"PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.\nI remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch . With its high customizability and pythonic syntax,PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.\nSo, in this PyTorch guide, I will try to ease some of the pain with PyTorch for starters and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.\nBut, that is not to say that this is aimed at beginners only as I will also talk about the high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.\nSo let‚Äôs get some coffee ‚òï Ô∏èand start it up.\n Tensors Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.\n1. Create a Tensor We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of more things with tensors just like you can do with NumPy arrays.\n# Using torch.Tensor t = torch.Tensor([[1,2,3],[3,4,5]]) print(f\u0026#34;Created Tensor Using torch.Tensor:\\n{t}\u0026#34;) # Using torch.randn t = torch.randn(3, 5) print(f\u0026#34;Created Tensor Using torch.randn:\\n{t}\u0026#34;) # using torch.[ones|zeros](*size) t = torch.ones(3, 5) print(f\u0026#34;Created Tensor Using torch.ones:\\n{t}\u0026#34;) t = torch.zeros(3, 5) print(f\u0026#34;Created Tensor Using torch.zeros:\\n{t}\u0026#34;) # using torch.randint - a tensor of size 4,5 with entries between 0 and 10(excluded) t = torch.randint(low = 0,high = 10,size = (4,5)) print(f\u0026#34;Created Tensor Using torch.randint:\\n{t}\u0026#34;) # Using from_numpy to convert from Numpy Array to Tensor a = np.array([[1,2,3],[3,4,5]]) t = torch.from_numpy(a) print(f\u0026#34;Convert to Tensor From Numpy Array:\\n{t}\u0026#34;) # Using .numpy() to convert from Tensor to Numpy array t = t.numpy() print(f\u0026#34;Convert to Numpy Array From Tensor:\\n{t}\u0026#34;)  2. Tensor Operations Again, there are a lot of operations you can do on these tensors. The full list of functions can be found here .\nA = torch.randn(3,4) W = torch.randn(4,2) # Multiply Matrix A and W t = A.mm(W) print(f\u0026#34;Created Tensor t by Multiplying A and W:\\n{t}\u0026#34;) # Transpose Tensor t t = t.t() print(f\u0026#34;Transpose of Tensor t:\\n{t}\u0026#34;) # Square each element of t t = t**2 print(f\u0026#34;Square each element of Tensor t:\\n{t}\u0026#34;) # return the size of a tensor print(f\u0026#34;Size of Tensor t using .size():\\n{t.size()}\u0026#34;)  Note: What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is deprecated , and all methods for variables work with Tensors. So, if you don‚Äôt know about them, it‚Äôs fine as they re not needed, and if you know them, you can forget about them.\n The nn.Module Photo by Fernand De Canne on Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Moduleto define any model as a class. Every model class necessarily contains an __init__ procedure block and a block for the forward pass.\n  In the __init__ part, the user can define all the layers the network is going to have but doesn\u0026rsquo;t yet define how those layers would be connected to each other.\n  In the forward pass block, the user defines how data flows from one layer to another inside the network.\n  So, put simply, any network we define will look like:\nclass myNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) x = self.lin2(x) return x Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don‚Äôt know what that will do).\nclass myCrazyNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 784) self.lin3 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x_lin1 = self.lin1(x) x_lin2 = x + self.lin2(x_lin1) x_lin2 = self.lin1(x_lin2) x = self.lin3(x_lin2) return x We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.\nx = torch.randn((100,784)) model = myCrazyNeuralNet() model(x).size() -------------------------- torch.Size([100, 10])  A word about Layers Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using nn.Module. For example, rather than using the predefined Linear Layer nn.Linear from Pytorch above, we could have created our custom linear layer.\nclass myCustomLinearLayer(nn.Module): def __init__(self,in_size,out_size): super().__init__() self.weights = nn.Parameter(torch.randn(in_size, out_size)) self.bias = nn.Parameter(torch.zeros(out_size)) def forward(self, x): return x.mm(self.weights) + self.bias You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch docs :\n Parameters are \u0026lt;code\u0026gt;*Tensor*\u0026lt;/code\u0026gt; subclasses, that have a very special property when used with Module - when they‚Äôre assigned as Module attributes they are automatically added to the list of its parameters, and will appear in parameters() iterator\n As you will later see, the model.parameters() iterator will be an input to the optimizer. But more on that later.\nRight now, we can now use this custom layer in any PyTorch network, just like any other layer.\nclass myCustomNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = myCustomLinearLayer(784,10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) return x x = torch.randn((100,784)) model = myCustomNeuralNet() model(x).size() ------------------------------------------ torch.Size([100, 10]) But then again, Pytorch would not be so widely used if it didn‚Äôt provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are: nn.Linear , nn.Conv2d , nn.MaxPool2d , nn.ReLU , nn.BatchNorm2d , nn.Dropout , nn.Embedding , nn.GRU / nn.LSTM , nn.Softmax , nn.LogSoftmax , nn.MultiheadAttention , nn.TransformerEncoder , nn.TransformerDecoder I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.\n So, a Conv2d Layer needs as input an Image of height H and width W, with Cin channels. Now, for the first layer in a convnet, the number of in_channels would be 3(RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.\nTo check a new layer which I don‚Äôt know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:\nconv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1) And then pass some random input through it. Here 100 is the batch size.\nx = torch.randn((100,3,24,24)) conv_layer(x).size() -------------------------------- torch.Size([100, 64, 24, 24]) So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.\n Datasets and DataLoaders How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at torchvision.datasets and torchtext.datasets . But, to give a concrete example for datasets, let‚Äôs say we had to pass images to an Image Neural net using a folder which has images in this structure:\ndata train sailboat kayak . .  We can use torchvision.datasets.ImageFolder dataset to get an example image like below:\nfrom torchvision import transforms from torchvision.datasets import ImageFolder traindir = \u0026#34;data/train/\u0026#34; t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = ImageFolder(root=traindir,transform=t) print(\u0026#34;Num Images in Dataset:\u0026#34;, len(train_dataset)) print(\u0026#34;Example Image and Label:\u0026#34;, train_dataset[2])  This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:\nfor i in range(0,len(train_dataset)): image ,label = train_dataset[i] pred = model(image) But that is not optimal. We want to do batching. We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) We can simply iterate with batches using:\nfor image_batch, label_batch in train_dataloader: print(image_batch.size(),label_batch.size()) break ------------------------------------------------- torch.Size([64, 3, 224, 224]) torch.Size([64]) So actually, the whole process of using datasets and Dataloaders becomes:\nt = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = torchvision.datasets.ImageFolder(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch) You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning here .\nThis is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don‚Äôt fit our use case.\n Understanding Custom Datasets To write our custom datasets, we can make use of the abstract class torch.utils.data.Dataset provided by Pytorch. We need to inherit this Dataset class and need to define two methods to create a custom Dataset.\n  __len__ : a function that returns the size of the dataset. This one is pretty simple to write in most cases.\n  __getitem__: a function that takes as input an index i and returns the sample at index i.\n  For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in __init__ part where we use glob.glob to get image names and do some general preprocessing.\nfrom glob import glob from PIL import Image from torch.utils.data import Dataset class customImageFolderDataset(Dataset): \u0026#34;\u0026#34;\u0026#34;Custom Image Loader dataset.\u0026#34;\u0026#34;\u0026#34; def __init__(self, root, transform=None): \u0026#34;\u0026#34;\u0026#34; Args: root (string): Path to the images organized in a particular folder structure. transform: Any Pytorch transform to be applied \u0026#34;\u0026#34;\u0026#34; # Get all image paths from a directory self.image_paths = glob(f\u0026#34;{root}/*/*\u0026#34;) # Get the labels from the image paths self.labels = [x.split(\u0026#34;/\u0026#34;)[-2] for x in self.image_paths] # Create a dictionary mapping each label to a index from 0 to len(classes). self.label_to_idx = {x:i for i,x in enumerate(set(self.labels))} self.transform = transform def __len__(self): # return length of dataset return len(self.image_paths) def __getitem__(self, idx): # open and send one image and label img_name = self.image_paths[idx] label = self.labels[idx] image = Image.open(img_name) if self.transform: image = self.transform(image) return image,self.label_to_idx[label] Also, note that we open our images one at a time in the __getitem__ method and not while initializing. This is not done in __init__ because we don\u0026rsquo;t want to load all our images in the memory and just need to load the required ones.\nWe can now use this dataset with the utility Dataloader just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.\nt = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = customImageFolderDataset(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch)  Understanding Custom DataLoaders This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.\nSo let‚Äôs say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It‚Äôs alright if you don‚Äôt understand the layers used in it right now; just know that it can process sequences with variable sizes.\nclass BiLSTM(nn.Module): def __init__(self): super().__init__() self.hidden_size = 64 drp = 0.1 max_features, embed_size = 10000,300 self.embedding = nn.Embedding(max_features, embed_size) self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True) self.linear = nn.Linear(self.hidden_size*4 , 64) self.relu = nn.ReLU() self.dropout = nn.Dropout(drp) self.out = nn.Linear(64, 1) def forward(self, x): h_embedding = self.embedding(x) h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)) h_lstm, _ = self.lstm(h_embedding) avg_pool = torch.mean(h_lstm, 1) max_pool, _ = torch.max(h_lstm, 1) conc = torch.cat(( avg_pool, max_pool), 1) conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) return out This network expects its input to be of shape (batch_size, seq_length) and works with any seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).\nmodel = BiLSTM() input_batch_1 = torch.randint(low = 0,high = 10000, size = (100,**10**)) input_batch_2 = torch.randint(low = 0,high = 10000, size = (100,**25**)) print(model(input_batch_1).size()) print(model(input_batch_2).size()) ------------------------------------------------------------------ torch.Size([100, 1]) torch.Size([100, 1]) Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.\nSo, how do we do this? Let‚Äôs write a very simple custom dataset class first.\nclass CustomTextDataset(Dataset): \u0026#39;\u0026#39;\u0026#39; Simple Dataset initializes with X and y vectors We start by sorting our X and y vectors by sequence lengths \u0026#39;\u0026#39;\u0026#39; def __init__(self,X,y=None): self.data = list(zip(X,y)) # Sort by length of first element in tuple self.data = sorted(self.data, key=lambda x: len(x[0])) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] Also, let‚Äôs generate some random data which we will use with this custom Dataset.\nimport numpy as np train_data_size = 1024 sizes = np.random.randint(low=50,high=300,size=(train_data_size,)) X = [np.random.randint(0,10000, (sizes[i])) for i in range(train_data_size)] y = np.random.rand(train_data_size).round() #checking one example in dataset print((X[0],y[0]))  Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.\nWe can use the custom dataset now using:\ntrain_dataset = CustomTextDataset(X,y) If we now try to use the Dataloader on this dataset with batch_size\u0026gt;1, we will get an error. Why is that?\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10) for xb,yb in train_dataloader: print(xb.size(),yb.size())  This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn‚Äôt face this error.\nSo, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?\nWe can use collate_fn parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch, y_batch ) with padded sequence lengths based on max_sequence_length in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.\ndef collate_text(batch): # get text sequences in batch data = [item[0] for item in batch] # get labels in batch target = [item[1] for item in batch] # get max_seq_length in batch max_seq_len = max([len(x) for x in data]) # pad text sequences based on max_seq_len data = [np.pad(p, (0, max_seq_len - len(p)), \u0026#39;constant\u0026#39;) for p in data] # convert data and target to tensor data = torch.LongTensor(data) target = torch.LongTensor(target) return [data, target] We can now use this collate_fn with our Dataloader as:\ntrain_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,collate_fn = collate_text) for xb,yb in train_dataloader: print(xb.size(),yb.size())  It will work this time as we have provided a custom collate_fn. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.\n Training a Neural Network We know how to create a neural network using nn.Module. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:\nnum_epochs = 5 for epoch in range(num_epochs): # Set model to train mode model.train() for x_batch,y_batch in train_dataloader: # Clear gradients optimizer.zero_grad() # Forward pass - Predicted outputs pred = model(x_batch) # Find Loss and backpropagation of gradients loss = loss_criterion(pred, y_batch) loss.backward() # Update the parameters optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: pred = model(x_batch) val_loss = loss_criterion(pred, y_batch) In the above code, we are running five epochs and in each epoch:\n  We iterate through the dataset using a data loader.\n  In each iteration, we do a forward pass using model(x_batch)\n  We calculate the Loss using a loss_criterion\n  We back-propagate that loss using loss.backward() call. We don\u0026rsquo;t have to worry about the calculation of the gradients at all, as this simple call does it all for us.\n  Take an optimizer step to change the weights in the whole network using optimizer.step(). This is where weights of the network get modified using the gradients calculated in loss.backward() call.\n  We go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using model.eval().Please note we don\u0026rsquo;t back-propagate losses in eval mode.\n  Till now, we have talked about how to use nn.Module to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let\u0026rsquo;s talk about the various options available for Loss Functions and Optimizers.\n Loss functions Pytorch provides us with a variety of loss functions for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss , nn.NLLLoss , nn.KLDivLoss and nn.MSELoss . You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of nn.NLLLoss  The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:\n  the input expected needs to be of size (batch_size x Num_Classes ) ‚Äî These are the predictions from the Neural Network we have created.\n  We need to have the log-probabilities of each class in the input ‚Äî To get log-probabilities from a Neural Network, we can add a LogSoftmax Layer as the last layer of our network.\n  The target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.\n  So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don\u0026rsquo;t want to use this LogSoftmax layer, you could have just used \u0026lt;code\u0026gt;nn.CrossEntropyLoss\u0026lt;/code\u0026gt; class myClassificationNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin = nn.Linear(784, 10) self.logsoftmax = nn.LogSoftmax(dim=1) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin(x) x = self.logsoftmax(x) return x Let‚Äôs define a random input to pass to our network to test it:\n# some random input: X = torch.randn(100,784) y = torch.randint(low = 0,high = 10,size = (100,)) And pass it through the model to get predictions:\nmodel = myClassificationNet() preds = model(X) We can now get the loss as:\ncriterion = nn.NLLLoss() loss = criterion(preds,y) loss ------------------------------------------ tensor(2.4852, grad_fn=\u0026lt;NllLossBackward\u0026gt;)  Custom Loss Function Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss\ndef customMseLoss(output,target): loss = torch.mean((output - target)**2) return loss You can use this custom loss just like before. But note that we don‚Äôt instantiate the loss using criterion this time as we have defined it as a function.\noutput = model(x) loss = customMseLoss(output, target) loss.backward() If we wanted, we could have also written it as a class using nn.Module , and then we would have been able to use it as an object. Here is an NLLLoss custom example:\nclass CustomNLLLoss(nn.Module): def __init__(self): super().__init__() def forward(self, x, y): # x should be output from LogSoftmax Layer log_prob = -1.0 * x # Get log_prob based on y class_index as loss=-mean(ylogp) loss = log_prob.gather(1, y.unsqueeze(1)) loss = loss.mean() return loss criterion = CustomNLLLoss() loss = criterion(preds,y)  Optimizers Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta , torch.optim.Adagrad , torch.optim.RMSprop and the most widely used torch.optim.Adam .\nTo use the most used Adam optimizer from PyTorch, we can simply instantiate it with:\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999)) And then use optimizer.zero_grad() and optimizer.step() while training the model.\nI am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the pytorch-optimizer library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in PyTorch or pytorch-optimizers .\nOther optimizers from Other optimizers from pytorch-optimizer library\n Using GPU/Multiple GPUs Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using model.to('cuda'). Or if you want to use multiple GPUs, you can use nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using DataParallel if needed.\n# Whether to train on a gpu train_on_gpu = torch.cuda.is_available() print(f\u0026#39;Train on gpu: {train_on_gpu}\u0026#39;)# Number of gpus if train_on_gpu: gpu_count = torch.cuda.device_count() print(f\u0026#39;{gpu_count} gpus detected.\u0026#39;) if gpu_count \u0026gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to(\u0026#39;cuda\u0026#39;) if multi_gpu: model = nn.DataParallel(model) The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It‚Äôs as simple as adding a few lines of code to our training loop.\nnum_epochs = 5 for epoch in range(num_epochs): model.train() for x_batch,y_batch in train_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() optimizer.zero_grad() pred = model(x_batch) loss = loss_criterion(pred, y_batch) loss.backward() optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() pred = model(x_batch) val_loss = loss_criterion(pred, y_batch)  Conclusion Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.\nYou can find the code for this post here on my GitHub repo, where I keep codes for all my blogs.\nIf you want to learn more about Pytorch using a course based structure, take a look at the Deep Neural Networks with PyTorch course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization .\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/09/09/pytorch_guide/","tags":["Artificial Intelligence","Pytorch"],"title":"The Most Complete Guide to PyTorch for Data Scientists"},{"categories":["Data Science","Awesome Guides"],"contents":"Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of ‚ÄúCoding one code block at a time,‚Äù as I like to call it, and it helps me to think more clearly while helping me make my code more modular.\nYet, Jupyter is not suitable for some tasks in its present form. And the most prominent is when I have to work with .py files. And one will need to work with .py files whenever they want to push your code to production or change other people‚Äôs code. So, until now, I used sublime text to edit Python files, and I found it excellent. But recently, when I looked at the Atom editor, my loyalties seemed to shift when I saw the multiple out of the box options provided by it.\nNow, the real power to Atom comes from the various packages you can install. In this post, I will talk about the packages that help make Atom just the most hackable and wholesome development environment ever.\n Installing Atom and Some Starting Tweaks Before we even begin, we need to install Atom. You can do it from the main website here . The installation process is pretty simple, whatever your platform is. For Linux, I just downloaded the .deb file and double-clicked it. Once you have installed Atom, You can look at doing some tweaks:\n Open Core settings in Atom using Ctrl+Shift+P and typing settings therein. This Ctrl+Shift+P command is going to be one of the most important commands in Atom as it lets you navigate and run a lot of commands.   Accessing the Settings window using Ctrl+Shift+P\n Now go to the Editor menu and Uncheck ‚ÄúSoft Tabs‚Äù. This is done so that TAB key registers as a TAB and not two spaces. If you want you can also activate ‚ÄúSoft Wrap‚Äù which wraps the text if the text exceeds the window width.   My preferred settings for soft-wrap and soft-tabs.\nNow, as we have Atom installed, we can look at some of the most awesome packages it provides. And the most important of them is GitHub.\n 1. Commit to Github without leaving Editor Are you fed up with leaving your text editor to use terminal every time you push a commit to Github? If your answer is yes, Atom solves this very problem by letting you push commits without you ever leaving the text editor window.\nThis is one of the main features that pushed me towards Atom from Sublime Text. I like how this functionality comes preloaded with Atom and it doesn‚Äôt take much time to set it up.\nTo start using it, click on the GitHub link in the right bottom of the Atom screen, and the Atom screen will prompt you to log in to your Github to provide access. It is a one-time setup, and once you log in and give the token generated to Atom, you will be able to push your commits from the Atom screen itself without navigating to the terminal window.\n  ![]  ![]   The process to push a commit is:\n  Change any file or multiple files.\n  Click on Git on the bottom right corner.\n  Stage the Changes\n  Write a commit message.\n  Click on Push in the bottom right corner.\n  And we are done:)\n  Below, I am pushing a very simple commit to Github, where I add a title to my Markdown file. Its a GIF file, so it might take some time to load.\n Committing in Atom\n 2. Write Markdown with real-time preview I am always torn between the medium editor vs. Markdown whenever I write blog posts for my site. For one, I prefer using Markdown when I have to use Math symbols for my post or have to use custom HTML. But, I also like the Medium editor as it is WYSIWYG(What You See Is What You Get). And with Atom, I have finally found the perfect markdown editor for me, which provides me with Markdown as well as WYSIWYG. And it has now become a default option for me to create any README.md files for GitHub.\nUsing Markdown in Atom is again a piece of cake and is activated by default. To see a live preview with Markdown in Atom:\n  Use Ctrl+Shift+M to open Markdown Preview Pane.\n  Whatever changes you do in the document will reflect near real-time in the preview window.\n   Markdown Split Screen editor\n3. Minimap ‚Äî A navigation map for Large code files Till now, we haven‚Äôt installed any new package to Atom, so let‚Äôs install an elementary package as our first package. This package is called minimap , and it is something that I like to have from my Sublime Text days. It lets you have a side panel where you can click and reach any part of the code. Pretty useful for large files.\nTo install a package, you can go to settings and click on Install Packages. Ctrl+Shift+P \u0026gt; Settings \u0026gt; + Install \u0026gt; Minimap\u0026gt; Install\n Installing Minimap or any package\nOnce you install the package, you can see the minimap on the side of your screen.\n Sidebar to navigate large files with ease\n4. Python Autocomplete with function definitions in Text Editor An editor is never really complete until it provides you with some autocomplete options for your favorite language. Atom integrates well with Kite, which tries to integrate AI and autocomplete.\nSo, to enable autocomplete with Kite, we can use the package named autocomplete-python in Atom. The install steps remain the same as before. i.e.\nCtrl+Shift+P \u0026gt; Settings \u0026gt; + Install \u0026gt; autocomplete-python\u0026gt; Install\nYou will also see the option of using Kite along with it. I usually end up using Kite instead of Jedi(Another autocomplete option). This is how it looks when you work on a Python document with Kite autocompletion.\n Autocomplete with Kite lets you see function definitions too.\n5. Hydrogen ‚Äî Run Python code in Jupyter environment Want to run Python also in your Atom Editor with any Jupyter Kernel? There is a way for that too. We just need to install ‚Äú Hydrogen ‚Äù using the same method as before. Once Hydrogen is installed you can use it by:\n  Run the command on which your cursor is on using Ctrl+Enter.\n  Select any Kernel from the Kernel Selection Screen. I select pyt kernel from the list.\n  Now I can continue working in pyt kernel.\n   Runnin command using Ctrl+Enter will ask you which environment to use.\nSometimes it might happen that you don‚Äôt see an environment/kernel in Atom. In such cases, you can install ipykernel to make that kernel visible to Jupyter as well as Atom.\nHere is how to make a new kernel and make it visible in Jupyter/Atom:\nconda create -n exampleenv python=3.7 conda activate exampleenv conda install -c anaconda ipykernel python -m ipykernel install --user --name=exampleenv  Once you run these commands, your kernel will be installed. You can now update the Atom‚Äôs kernel list by using:\nCtrl+Shift+P \u0026gt;Hydrogen: Update Kernels\n And your kernel should now be available in your Atom editor.\n6. Search Stack Overflow from your Text Editor Stack Overflow is an integral part of any developer‚Äôs life. But you know what the hassle is? To leave the coding environment and go to Chrome to search for every simple thing you need to do. And we end up doing it back and forth throughout the day. So, what if we can access Stack Overflow from Atom? You can do precisely that through the ‚Äú ask-stack ‚Äù package, which lets one search for questions on SO. We can access it using Ctrl+Alt+A\n Access Stack Overflow in Atom using Ctrl+Alt+A.\nSome other honorable mentions of packages you could use are:\n   Teletype : Do Pair Coding.\n  Linter: Checks code for Stylistic and Programmatic errors. To enable linting in Python, You can use ‚Äú linter ‚Äù and ‚Äú python-linters ‚Äù.\n   Highlight Selected : Highlight all occurrences of a text by double-clicking or selecting the text with a cursor.\n   Atom-File-Icons : Provides you with file icons in the left side tree view. Looks much better than before, right?\n   Icons for files\n Conclusion In this post, I talked about how I use Atom in my Python Development flow.\nThere are a plethora of other packages in Atom which you may like, and you can look at them to make your environment even more customizable. Or one can even write their own packages as well as Atom is called as the ‚ÄúMost Hackable Editor‚Äù.\nIf you want to learn about Python and not exactly a Python editor, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out. Also, here are my course recommendations to become a Data Scientist in 2020.\nI am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/","tags":["Machine Learning","Data Science","Tools","Productivity","Awesome Guides"],"title":"Create an Awesome Development Setup for Data Science using Atom"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We‚Äôve reached a stage where it‚Äôs becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence . However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.\nIn my view, GANs will change the way we generate video games and special effects. Using this approach, we could create realistic textures or characters on demand.\nSo in this post, we‚Äôre going to look at the generative adversarial networks behind AI-generated images, and help you to understand how to create and build your similar application with PyTorch. We‚Äôll try to keep the post as intuitive as possible for those of you just starting out, but we‚Äôll try not to dumb it down too much.\nAt the end of this article, you‚Äôll have a solid understanding of how General Adversarial Networks (GANs) work, and how to build your own.\n Task Overview In this post, we will create unique anime characters using the Anime Face Dataset . It is a dataset consisting of 63,632 high-quality anime faces in a number of styles. It‚Äôs a good starter dataset because it‚Äôs perfect for our goal.\nWe will be using Deep Convolutional Generative Adversarial Networks (DC-GANs) for our project. Though we‚Äôll be using it to generate the faces of new anime characters, DC-GANs can also be used to create modern fashion styles , general content creation, and sometimes for data augmentation as well.\nBut before we get into the coding, let‚Äôs take a quick look at how GANs work.\n INTUITION: Brief Intro to GANs for Generating Fake Images GANs typically employ two dueling neural networks to train a computer to learn the nature of a dataset well enough to generate convincing fakes. One of these Neural Networks generates fakes (the generator), and the other tries to classify which images are fake (the discriminator). These networks improve over time by competing against each other.\nPerhaps imagine the generator as a robber and the discriminator as a police officer. The more the robber steals, the better he gets at stealing things. But at the same time, the police officer also gets better at catching the thief. Well, in an ideal world, anyway.\nThe losses in these neural networks are primarily a function of how the other network performs:\n  Discriminator network loss is a function of generator network quality: Loss is high for the discriminator if it gets fooled by the generator‚Äôs fake images.\n  Generator network loss is a function of discriminator network quality: Loss is high if the generator is not able to fool the discriminator.\n  In the training phase, we train our discriminator and generator networks sequentially, intending to improve performance for both. The end goal is to end up with weights that help the generator to create realistic-looking images. In the end, we‚Äôll use the generator neural network to generate high-quality fake images from random noise***.***\n The Generator architecture One of the main problems we face when working with GANs is that the training is not very stable. So we have to come up with a generator architecture that solves our problem and also results in stable training. The diagram below is taken from the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks , which explains the DC-GAN generator architecture.\n Though it might look a little bit confusing, essentially you can think of a generator neural network as a black box which takes as input a 100 dimension normally generated vector of numbers and gives us an image:\n So how do we create such an architecture? Below, we use a dense layer of size 4x4x1024 to create a dense vector out of the 100-d vector. We then reshape the dense vector in the shape of an image of 4√ó4 with 1024 filters, as shown in the following figure:\n  Note that we don‚Äôt have to worry about any weights right now as the network itself will learn those during training.\nOnce we have the 1024 4√ó4 maps, we do upsampling using a series of transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, however, we don‚Äôt halve the number of maps. We reduce the maps to 3 for each RGB channel since we need three channels for the output image.\n Now, What are Transpose convolutions? Put simply, transposing convolutions provides us with a way to upsample images. In a convolution operation, we try to go from a 4√ó4 image to a 2√ó2 image. But when we transpose convolutions, we convolve from 2√ó2 to 4√ó4 as shown in the following figure:\n Some of you may already know that unpooling is commonly used for upsampling input feature maps in convolutional neural networks (CNN). So why don‚Äôt we use unpooling here?\nThe reason comes down to the fact that unpooling does not involve any learning. However, transposed convolution is learnable, so it‚Äôs preferred. Later in the article, we‚Äôll see how the parameters can be learned by the generator.\n The Discriminator architecture Now that we‚Äôve covered the generator architecture, let‚Äôs look at the discriminator as a black box. In practice, it contains a series of convolutional layers with a dense layer at the end to predict if an image is fake or not. You can see an example in the figure below:\n Every image convolutional neural network works by taking an image as input, and predicting if it is real or fake using a sequence of convolutional layers.\n Data preprocessing and visualization Before going any further with our training, we preprocess our images to a standard size of 64x64x3. We will also need to normalize the image pixels before we train our GAN. You can see the process in the code below, which I‚Äôve commented on for clarity.\n# Root directory for dataset dataroot = \u0026#34;anime_images/\u0026#34; # Number of workers for dataloader workers = 2 # Batch size during training batch_size = 128 # Spatial size of training images. All images will be resized to this size using a transformer. image_size = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # We can use an image folder dataset the way we have it setup. # Create the dataset dataset = datasets.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # Create the dataloader dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # Decide which device we want to run on device = torch.device(\u0026#34;cuda:0\u0026#34; if (torch.cuda.is_available() and ngpu \u0026gt; 0) else \u0026#34;cpu\u0026#34;) # Plot some training images real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(\u0026#34;off\u0026#34;) plt.title(\u0026#34;Training Images\u0026#34;) plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) The resultant output of the code is as follows:\n  So Many different Characters ‚Äî Can our Generator understand the patterns?   Implementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.\n Generating noise vector for Generator We need to generate the noise which we want to convert to an image using our generator architecture.\nWe use a normal distribution\n  to generate the noise vector:\nnz = 100 noise = torch.randn(64, nz, 1, 1, device=device)   Generator architecture The generator is the most crucial part of the GAN. Here, we‚Äôll create a generator by adding some transposed convolution layers to upsample the noise vector to an image. You‚Äôll notice that this generator architecture is not the same as the one given in the DC-GAN paper I linked above.\nIn order to make it a better fit for our data, I had to make some architectural changes. I added a convolution layer in the middle and removed all dense layers from the generator architecture to make it fully convolutional.\nI also used a lot of Batchnorm layers and leaky ReLU activation. The following code block is the function I will use to create the generator:\n# Size of feature maps in generator ngf = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input noise) nz = 100 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is noise, going into a convolution # Transpose 2D conv layer 1. nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # Resulting state size - (ngf*8) x 4 x 4 i.e. if ngf= 64 the size is 512 maps of 4x4 # Transpose 2D conv layer 2. nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # Resulting state size -(ngf*4) x 8 x 8 i.e 8x8 maps # Transpose 2D conv layer 3. nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # Resulting state size. (ngf*2) x 16 x 16 # Transpose 2D conv layer 4. nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # Resulting state size. (ngf) x 32 x 32 # Final Transpose 2D conv layer 5 to generate final image. # nc is number of channels - 3 for 3 image channel nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), # Tanh activation to get final normalized image nn.Tanh() # Resulting state size. (nc) x 64 x 64 ) def forward(self, input): \u0026#39;\u0026#39;\u0026#39; This function takes as input the noise vector\u0026#39;\u0026#39;\u0026#39; return self.main(input) Now we can instantiate the model using the generator class. We are keeping the default weight initializer for PyTorch even though the paper says to initialize the weights using a mean of 0 and std dev of 0.2. The default weights initializer from Pytorch is more than good enough for our project.\n# Create the generator netG = Generator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == 'cuda') and (ngpu \u0026gt; 1): netG = nn.DataParallel(netG, list(range(ngpu))) # Print the model print(netG)  We can see the final generator model:\n  The Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.\n# Number of channels in the training images. For color images this is 3 nc = 3 # Size of feature maps in discriminator ndf = 64 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) Now we can instantiate the discriminator exactly as we did the generator.\n# Create the Discriminator netD = Discriminator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == 'cuda') and (ngpu \u0026gt; 1): netD = nn.DataParallel(netD, list(range(ngpu))) # Print the model print(netD)  Here is the architecture of the discriminator:\n  Training Understanding how the training works in GAN is essential. It‚Äôs interesting, too; we can see how training the generator and discriminator together improves them both at the same time.\nNow that we have our discriminator and generator models, next we need to initialize separate optimizers for them.\n# Initialize BCELoss function criterion = nn.BCELoss() # Create batch of latent vectors that we will use to visualize # the progression of the generator fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Establish convention for real and fake labels during training real_label = 1. fake_label = 0. # Setup Adam optimizers for both G and D # Learning rate for optimizers lr = 0.0002 # Beta1 hyperparam for Adam optimizers beta1 = 0.5 optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))  The Training Loop This is the main area where we need to understand how the blocks we‚Äôve created will assemble and work together.\n# Lists to keep track of progress/Losses img_list = [] G_losses = [] D_losses = [] iters = 0 # Number of training epochs num_epochs = 50 # Batch size during training batch_size = 128 print(\u0026#34;Starting Training Loop...\u0026#34;) # For each epoch for epoch in range(num_epochs): # For each batch in the dataloader for i, data in enumerate(dataloader, 0): ############################ # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z))) # Here we: # A. train the discriminator on real data # B. Create some fake images from Generator using Noise # C. train the discriminator on fake data ########################### # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images using generator # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D\u0026#39;s loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real + errD_fake # Update D optimizerD.step() ############################ # (2) Update G network: maximize log(D(G(z))) # Here we: # A. Find the discriminator output on Fake images # B. Calculate Generators loss based on this output. Note that the label is 1 for generator. # C. Update Generator ########################### netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost # Since we just updated D, perform another forward pass of all-fake batch through D output = netD(fake).view(-1) # Calculate G\u0026#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() # Output training stats every 50th Iteration in an epoch if i % 1000 == 0: print(\u0026#39;[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f/ %.4f\u0026#39; % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # Save Losses for plotting later G_losses.append(errG.item()) D_losses.append(errD.item()) # Check how the generator is doing by saving G\u0026#39;s output on a fixed_noise vector if (iters % 250 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): #print(iters) with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters += 1 It may seem complicated, but I‚Äôll break down the code above step by step in this section. The main steps in every training iteration are:\nStep 1: Sample a batch of normalized images from the dataset\nfor i, data in enumerate(dataloader, 0):  Step 2: Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their labels.\n # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D's loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real + errD_fake # Update D optimizerD.step()  Step 3: Backpropagate the errors through the generator by computing the loss gathered from discriminator output on fake images as the input and 1‚Äôs as the target while keeping the discriminator as untrainable ‚Äî This ensures that the loss is higher when the generator is not able to fool the discriminator. You can check it yourself like so: if the discriminator gives 0 on the fake image, the loss will be high i.e., BCELoss(0,1).\n netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost output = netD(fake).view(-1) # Calculate G's loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step()  We repeat the steps using the for loop to end up with a good discriminator and generator.\n Results The final output of our generator can be seen below. The GAN generates pretty good images for our content editor friends to work with.\nThe images might be a little crude, but still, this project was a starter for our GAN journey. The field is constantly advancing with better and more complex GAN architectures, so we‚Äôll likely see further increases in image quality from these architectures. Also, keep in mind that these images are generated from a noise vector only: this means the input is some noise, and the output is an image. It‚Äôs quite incredible.\n ALL THESE IMAGES ARE FAKE\n 1. Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average, and the variance is also decreasing as we do more steps. It‚Äôs possible that training for even more iterations would give us even better results.\nplt.figure(figsize=(10,5)) plt.title(\u0026quot;Generator and Discriminator Loss During Training\u0026quot;) plt.plot(G_losses,label=\u0026quot;G\u0026quot;) plt.plot(D_losses,label=\u0026quot;D\u0026quot;) plt.xlabel(\u0026quot;iterations\u0026quot;) plt.ylabel(\u0026quot;Loss\u0026quot;) plt.legend() plt.show()    2. Image Animation at every 250th Iteration in Jupyter Notebook We can choose to see the output as an animation using the below code:\n#%%capture fig = plt.figure(figsize=(8,8)) plt.axis(\u0026quot;off\u0026quot;) ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml())   You can choose to save an animation object as a gif as well if you want to send them to some friends.\nani.save('animation.gif', writer='imagemagick',fps=5) Image(url='animation.gif')    3. Image generated at every 200 Iter Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases, the images are getting better.\n# create a list of 16 images to show every_nth_image = np.ceil(len(img_list)/16) ims = [np.transpose(img,(1,2,0)) for i,img in enumerate(img_list)if i%every_nth_image==0] print(\u0026quot;Displaying generated images\u0026quot;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(20,20)) gs1 = gridspec.GridSpec(4, 4) gs1.update(wspace=0, hspace=0) step = 0 for i,image in enumerate(ims): ax1 = plt.subplot(gs1[i]) ax1.set_aspect('equal') fig = plt.imshow(image) # you might need to change some params here fig = plt.text(7,30,\u0026quot;Step: \u0026quot;+str(step),bbox=dict(facecolor='red', alpha=0.5),fontsize=12) plt.axis('off') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) step+=int(250*every_nth_image) #plt.tight_layout() plt.savefig(\u0026quot;GENERATEDimage.png\u0026quot;,bbox_inches='tight',pad_inches=0) plt.show()  Given below is the result of the GAN at different time steps:\n  Conclusion In this post, we covered the basics of GANs for creating fairly believable fake images. We hope you now have an understanding of generator and discriminator architecture for DC-GANs, and how to build a simple DC-GAN to generate anime images from scratch.\nThough this model is not the most perfect anime face generator, using it as a base helps us to understand the basics of generative adversarial networks, which in turn can be used as a stepping stone to more exciting and complex GANs as we move forward.\nLook at it this way, as long as we have the training data at hand, we now have the ability to conjure up realistic textures or characters on demand. That is no small feat.\nFor a closer look at the code for this post, please visit my GitHub repository where you can find the code for this post as well as all my posts.\nIf you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew Ng. Andrew is a great instructor, and this course is excellent too.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\nThis post was first published here \n","permalink":"https://mlwhiz.com/blog/2020/08/27/pyt_gan/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Awesome Guides","Generative Adversarial Networks"],"title":"A Layman‚Äôs Introduction to GANs for Data Scientists using PyTorch"},{"categories":["Data Science","Learning Resources"],"contents":"With ML Engineer job roles in all the vogue and a lot of people preparing for them, I get asked a lot of times by my readers to recommend courses for the ML engineer roles particularly and not for the Data Science roles.\nNow, while both ML and Data Science pretty much have a high degree of overlap and I could very well argue that ML engineers do need to know many of the Data Science skills , there is a special place in hell reserved for ML engineers and that is the production and deployment part of the Data Science modeling process.\nSo, it doesn‚Äôt hurt to look at what two of the world‚Äôs biggest companies are looking at when it comes to ML engineering. These courses taught by Google and Amazon are rather popular and typically beginner level. So these are the best bet for people who are looking to start their journey to become an ML Engineer.\nMy main criteria for selecting these particular courses is the practical utility they provide as well as the pedigree they bring. Also, note that you don‚Äôt need to take these courses in any order or even take all of them. Just focus on one or two of them based on your particular requirements and you should be fine.\n Google  1. Google IT Automation with Python Professional Certificate \nPhoto by Mitchell Luo on Have you ever been asked to implement an API for your machine learning model, write some bash scripts to run a CRON job or some auto-scheduler and felt lost? This beginner-level professional certificate looks at Python from an engineering and production point of view rather than just Data Science. Something that we Data Scientists learn only after a few years of experience in the field and what is essentially a day to day thing for an MLE role.\nThe professional certificate from Google essentially talks about Python, Regex, Bash Scripting, automation testing, Github, debugging, Scaling up, and Programming Interfaces(APIs).\nMost of these might not look so beginner-friendly right now, but they are some super cool skills to have in your portfolio and actually not that hard once you start understanding about the whole coding ecosystem.\n 2. Cloud Engineering with Google Cloud Professional Certificate \nA lot of companies have started using Google Cloud Platform nowadays. If yours is such a company, this particular professional certificate might provide a lot of value. This specialization particularly focusses on the GCP platform and its various services like Google App Engine, Google Compute Engine, Google Kubernetes Engine, Google Cloud Storage, and BigQuery. You will also learn about other engineering concepts like load balancing, autoscaling, infrastructure automation, and managed services.\nAll of these services are pretty much becoming a standard for cloud computing at a lot of companies and it helps to learn about those if you are using the GCP infrastructure for building and deploying your models.\n Amazon:  1. AWS Fundamentals \nFor people who aim to work at Amazon or the companies that use Amazon Web Services (AWS), this specialization teaches the AWS fundamentals and provides an overview of the features, benefits, and capabilities of AWS.\nThe main services you learn in this specialization are AWS Lambda(serverless compute), Amazon API Gateway(create, publish, maintain, monitor, and secure APIs at any scale), Amazon DynamoDB(Fast, flexible and scalable NoSQL database service), and Amazon Lex(Conversational AI for Chatbots), along with taking your application to the cloud.\nThis particular Amazon specialization is engineering heavy and by the end, you would understand how to build and deploy serverless applications with AWS.\n 2. Getting Started with AWS Machine Learning \nPhoto by Pietro Jeng on There are a lot of things to consider while building a great machine learning system. We, as data scientists, only worry about the data and modeling part of the project but do we ever think about how we will deploy our models once we have them?\nI have seen a lot of ML projects, and a lot of them are doomed to fail as they don‚Äôt have a set plan for production from the onset. Having a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.\n This course will teach you:\n How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance.\n How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.\nSource     3. AWS Computer Vision: Getting Started with GluonCV \nThis course also provides an overview of Machine Learning with Amazon Web Services but with a specific emphasis on Computer Vision applications.\nIn this course, you will learn how to build and train a CV model using the Apache MXNet and GluonCV toolkit. The instructors start by discussing artificial neural networks and other deep learning concepts and then walk through how to combine neural network building blocks into complete computer vision models and train them efficiently.\nThis course covers AWS services and frameworks including Amazon Rekognition, Amazon SageMaker, Amazon SageMaker GroundTruth, Amazon SageMaker Neo, AWS Deep Learning AMIs via Amazon EC2, AWS Deep Learning Containers, and Apache MXNet on AWS.\nThis course will provide a lot of value to learners who want to train a deep learning vision model from scratch and deploy it efficiently using the AWS computing platform.\n Conclusion In this post, I talked about the best courses I would recommend for people who are trying to get into ML engineering in this not so much return to school session.\nAlso, here are my course recommendations to become a Data Scientist in 2020.\nI am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/08/27/mlengineercourses/","tags":["Machine Learning","Data Science","Production"],"title":"Become an ML Engineer with these courses from Amazon and Google"},{"categories":["Data Science"],"contents":"Pandas is one of the best data manipulation libraries in recent times. It lets you slice and dice, groupby, join and do any arbitrary data transformation. You can take a look at this post , which talks about handling most of the data manipulation cases using a straightforward, simple, and matter of fact way using Pandas.\nBut even with how awesome pandas generally is, there sometimes are moments when you would like to have just a bit more. Say you come from a SQL background in which the same operation was too easy. Or you wanted to have more readable code. Or you just wanted to run an ad-hoc SQL query on your data frame. Or, maybe you come from R and want a replacement for sqldf.\nFor example, one of the operations that Pandas doesn‚Äôt have an alternative for is non-equi joins, which are quite trivial in SQL.\nIn this series of posts named Python Shorts , I will explain some simple but very useful constructs provided by Python, some essential tips, and some use cases I come up with regularly in my Data Science work.\nThis post is essentially about using SQL with pandas Dataframes.\n But, what are non-equi joins, and why would I need them? Let‚Äôs say you have to join two data frames. One shows us the periods where we offer some promotions on some items. And the second one is our transaction Dataframe. I want to know the sales that were driven by promotions, i.e., the sales that happen for an item in the promotion period.\nWe can do this by doing a join on the item column as well as a join condition (TransactionDt‚â•StartDt and TransactionDt‚â§EndDt). Since now our join conditions have a greater than and less than signs as well, such joins are called non-equi joins. Do think about how you will do such a thing in Pandas before moving on.\n  The Pandas Solution So how will you do it in Pandas? Yes, a Pandas based solution exists, though I don‚Äôt find it readable enough.\nLet‚Äôs start by generating some random data to work with.\nimport pandas as pd import random import datetime def random_dt_bw(start_date,end_date): days_between = (end_date - start_date).days random_num_days = random.randrange(days_between) random_dt = start_date + datetime.timedelta(days=random_num_days) return random_dt def generate_data(n=1000): items = [f\u0026#34;i_{x}\u0026#34; for x in range(n)] start_dates = [random_dt_bw(datetime.date(2020,1,1),datetime.date(2020,9,1)) for x in range(n)] end_dates = [x + datetime.timedelta(days=random.randint(1,10)) for x in start_dates] offerDf = pd.DataFrame({\u0026#34;Item\u0026#34;:items, \u0026#34;StartDt\u0026#34;:start_dates, \u0026#34;EndDt\u0026#34;:end_dates}) transaction_items = [f\u0026#34;i_{random.randint(0,n)}\u0026#34; for x in range(5*n)] transaction_dt = [random_dt_bw(datetime.date(2020,1,1),datetime.date(2020,9,1)) for x in range(5*n)] sales_amt = [random.randint(0,1000) for x in range(5*n)] transactionDf = pd.DataFrame({\u0026#34;Item\u0026#34;:transaction_items,\u0026#34;TransactionDt\u0026#34;:transaction_dt,\u0026#34;Sales\u0026#34;:sales_amt}) return offerDf,transactionDf offerDf,transactionDf = generate_data(n=100000)  You don‚Äôt need to worry about the random data generation code above. Just know how our random data looks like:\n      Once we have the data, we can do the non-equi join by merging the data on the column item and then filtering by the required condition.\nmerged_df = pd.merge(offerDf,transactionDf,on='Item') pandas_solution = merged_df[(merged_df['TransactionDt']\u0026gt;=merged_df['StartDt']) \u0026amp; (merged_df['TransactionDt']\u0026lt;=merged_df['EndDt'])]  The result is below just as we wanted:\n  The PandaSQL solution The Pandas solution is alright, and it does what we want, but we could also have used PandaSQL to get the same thing done in a much more readable way.\nWhat is PandaSQL ?\nPandaSQL provides us with a way to write SQL on Pandas Dataframes. So if you have got some SQL queries already written, it might make more sense to use pandaSQL rather than converting them to pandas syntax. To get started with PandaSQL we install it simply with:\npip install -U pandasql  Once we have pandaSQL installed, we can use it by creating a pysqldf function that takes a query as an input and runs the query to return a Pandas DF. Don‚Äôt worry about the syntax; it remains more or less constant.\nfrom pandasql import sqldf pysqldf = lambda q: sqldf(q, globals())  We can now run any SQL query on our Pandas data frames using this function. And, below is the non-equi join, we want to do in the much more readable SQL format.\nq = \u0026quot;\u0026quot;\u0026quot; SELECT A.*,B.TransactionDt,B.Sales FROM offerDf A INNER JOIN transactionDf B ON A.Item = B.Item AND A.StartDt \u0026lt;= B.TransactionDt AND A.EndDt \u0026gt;= B.TransactionDt; \u0026quot;\u0026quot;\u0026quot; pandaSQL_solution = pysqldf(q)  The result is a pandas Dataframe as we would expect. The index is already reset for us, unlike before.\n  Caveats: While the PandaSQL function lets us run SQL queries on our Pandas data frames and is an excellent tool to be aware of in certain situations, it is not as performant as pure pandas syntax.\n  When we time Pandas against the more readable PandaSQL, we find that the PandaSQL takes around 10x the time of native Pandas.\n Conclusion In this post of the Python Shorts series, we learned about pandaSQL, which lets us use SQL queries on our Dataframes. We also looked at how to do non-equi joins using both native pandas as well as pandaSQL.\nWhile the PandaSQL library is not as performant as native pandas, it is a great addition to our data analytics toolbox when we want to do ad-hoc analysis and to people who feel much more comfortable with using SQL queries.\nFor a closer look at the code for this post, please visit my GitHub repository, where you can find the code for this post as well as all my posts.\nContinue Learning If you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/08/27/pandasql/","tags":["Machine Learning","Data Science","Python"],"title":"How to use SQL with Pandas?"},{"categories":["Data Science"],"contents":"As Alexander Pope said, to err is human. By that metric, who is more human than us data scientists? We devise wrong hypotheses constantly and then spend time working on them just to find out how wrong we were.\nWhen looking at mistakes from an experiment, a data scientist needs to be critical, always on the lookout for something that others may have missed. But sometimes, in our day-to-day routine, we can easily get lost in little details. When this happens, we often fail to look at the overall picture, ultimately failing to deliver what the business wants.\nOur business partners have hired us to generate value. We won‚Äôt be able to generate that value unless we develop business-oriented critical thinking, including having a more holistic perspective of the business at hand. So here is some practical advice for your day-to-day work as a data scientist. These recommendations will help you to be more diligent and more impactful at the same time.\n 1. Beware of the Clean Data Syndrome Tell me how many times this has happened to you: You get a data set and start working on it straight away. You create neat visualizations and start building models. Maybe you even present automatically generated descriptive analytics to your business counterparts!\nBut do you ever ask, ‚ÄúDoes this data actually make sense?‚Äù Incorrectly assuming that the data is clean could lead you toward very wrong hypotheses. Not only that, but you‚Äôre also missing an important analytical opportunity with this assumption.\nYou can actually discern a lot of important patterns by looking at discrepancies in the data. For example, if you notice that a particular column has more than 50 percent of values missing, you might think about dropping the column. But what if the missing column is because the data collection instrument has some error? By calling attention to this, you could have helped the business to improve its processes.\nOr what if you‚Äôre given a distribution of customers that shows a ratio of 90 percent men versus 10 percent women, but the business is a cosmetics company that predominantly markets its products to women? You could assume you have clean data and show the results as is, or you can use common sense and ask the business partner if the labels are switched.\nSuch errors are widespread. Catching them not only helps the future data collection processes but also prevents the company from making wrong decisions by preventing various other teams from using bad data.\n 2. Be Aware of the business   Source : Fab.com Beginnings\nYou probably know fab.com. If you don‚Äôt, it‚Äôs a website that sells selected health and fitness items. But the site‚Äôs origins weren‚Äôt in e-commerce. Fab.com started as Fabulis.com, a social networking site for gay men. One of the site‚Äôs most popular features was called the ‚ÄúGay Deal of the Day.‚Äù\nOne day, the deal was for hamburgers. Half of the deal‚Äôs buyers were women, despite the fact that they weren‚Äôt the site‚Äôs target users. This fact caused the data team to realize that they had an untapped market for selling goods to women. So Fabulis.com changed its business model to serve this newfound market.\nBe on the lookout for something out of the ordinary. Be ready to ask questions. If you see something in the data, you may have hit gold. Data can help a business to optimize revenue, but sometimes it has the power to change the direction of the company as well.\n  Source : Flickr Origins as ‚ÄúGame Neverending‚Äù\nAnother famous example of this is Flickr, which started out as a multiplayer game . Only when the founders noticed that people were using it as a photo upload service did the company pivot to the photo-sharing app we know it as today.\nTry to see patterns that others would miss. Do you see a discrepancy in some buying patterns or maybe something you can‚Äôt seem to explain? That might be an opportunity in disguise when you look through a wider lens.\n 3. Focus on the right metrics What do we want to optimize for? Most businesses fail to answer this simple question.\nEvery business problem is a little different and should, therefore, be optimized differently. For example, a website owner might ask you to optimize for daily active users. Daily active users is a metric defined as the number of people who open a product on a given day. But is that the right metric ? Probably not! In reality, it‚Äôs just a vanity metric, meaning one that makes you look good but doesn‚Äôt serve any purpose when it comes to actionability. This metric will always increase if you are spending marketing dollars across various channels to bring more and more customers to your site.\nInstead, I would recommend optimizing the percentage of users that are active to get a better idea of how my product is performing. A big marketing campaign might bring a lot of users to my site, but if only a few of them convert to active, the marketing campaign was a failure and my site stickiness factor is very low. You can measure the stickiness by the second metric and not the first one. If the percentage of active users is increasing, that must mean that they like my website.\nAnother example of looking at the wrong metric happens when we create classification models. We often try to increase accuracy for such models. But do we really want accuracy as a metric of our model performance?\n Imagine that we‚Äôre predicting the number of asteroids that will hit the Earth. If we want to optimize for accuracy, we can just say zero all the time, and we will be 99.99 percent accurate. That 0.01 percent error could be hugely impactful, though. What if that 0.01 percent is a planet-killing-sized asteroid? A model can be reasonably accurate but not at all valuable. A better metric would be the F score, which would be zero in this case, because the recall of such a model is zero as it never predicts an asteroid hitting the Earth.\nWhen it comes to data science, designing a project and the metrics we want to use for evaluation is much more important than modeling itself. The metrics themselves need to specify the business goal and aiming for a wrong goal effectively destroys the whole purpose of modeling. For example, F1 or PRAUC is a better metric in terms of asteroid prediction as they take into consideration both the precision and recall of the model. If we optimize for accuracy, our whole modeling effort could just be in vain.\n 4. Statistics Lie sometimes Be skeptical of any statistics that get quoted to you. Statistics have been used to lie in advertisements, in workplaces, and in a lot of other arenas in the past. People will do anything to get sales or promotions.\n For example, do you remember Colgate‚Äôs claim that 80 percent of dentists recommended their brand? This statistic seems pretty good at first. If so many dentists use Colgate, I should too, right? It turns out that during the survey, the dentists could choose multiple brands rather than just one. So other brands could be just as popular as Colgate.\n Marketing departments are just myth creation machines. We often see such examples in our daily lives. Take, for example, this 1992 ad from Chevrolet . Just looking at just the graph and not at the axis labels, it looks like Nissan/Datsun must be dreadful truck manufacturers. In fact, the graph indicates that more than 95 percent of the Nissan and Datsun trucks sold in the previous 10 years were still running. And the small difference might just be due to sample sizes and the types of trucks sold by each of the companies. As a general rule, never trust a chart that doesn‚Äôt label the Y-axis.\nAs a part of the ongoing pandemic, we‚Äôre seeing even more such examples with a lot of studies promoting cures for COVID-19. This past June in India, a man claimed to have made medicine for coronavirus that cured 100 percent of patients in seven days. This news predictably caused a big stir, but only after he was asked about the sample size did we understand what was actually happening here. With a sample size of 100, the claim was utterly ridiculous on its face. Worse, the way the sample was selected was hugely flawed. His organization selected asymptomatic and mildly symptomatic users with a mean age between 35 and 45 with no pre-existing conditions, I was dumbfounded ‚Äî this was not even a random sample. So not only was the study useless, it was actually unethical.\nWhen you see charts and statistics, remember to evaluate them carefully. Make sure the statistics were sampled correctly and are being used in an ethical, honest way.\n 5. Don‚Äôt Give in to Fallacies Photo by Jonathan Petersson on During the summer of 1913 in a casino in Monaco, gamblers watched in amazement as the roulette wheel landed on black an astonishing 26 times in a row. And since the probability of red versus black is precisely half, they were confident that red was ‚Äúdue.‚Äù It was a field day for the casino and a perfect example of gambler‚Äôs fallacy , a.k.a. the Monte Carlo fallacy.\nThis happens in everyday life outside of casinos too. People tend to avoid long strings of the same answer . Sometimes they do so while sacrificing accuracy of judgment for the sake of getting a pattern of decisions that look fairer or more probable. For example, an admissions office may reject the next application they see if they have approved three applications in a row, even if the application should have been accepted on merit.\nThe world works on probabilities. We are seven billion people, each doing an event every second of our lives. Because of that sheer volume, rare events are bound to happen. But we shouldn‚Äôt put our money on them.\nThink also of the spurious correlations we end up seeing regularly. This particular graph shows that organic food sales cause autism. Or is it the opposite? Just because two variables move together in tandem doesn‚Äôt necessarily mean that one causes the other. Correlation does not imply causation and as data scientists, it is our job to be on a lookout for such fallacies, biases, and spurious correlations. We can‚Äôt allow oversimplified conclusions to cloud our work.\nData scientists have a big role to play in any organization. A good data scientist must be both technical as well as business-driven to perform the job‚Äôs requirements well. Thus, we need to make a conscious effort to understand the business‚Äô needs while also polishing our technical skills.\n Continue Learning If you want to learn more about how to apply Data Science in a business context, I would like to call out the AI for Everyone course by Andrew Ng which focusses on spotting opportunities to apply AI to problems in your own organization, working with an AI team and build an AI strategy in your company.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nThis post was first published here ","permalink":"https://mlwhiz.com/blog/2020/08/12/ctskills/","tags":["Machine Learning","Data Science","Opinion"],"title":"5 Essential Business-Oriented Critical Thinking Skills For Data Scientists"},{"categories":["Deep Learning","Computer Vision"],"contents":"Creating my workstation has been a dream for me, if nothing else.\nI knew the process involved, yet I somehow never got to it. It might have been time or money. Mostly Money.\nBut this time I just had to do it. I was just fed up with setting up a server on AWS for any small personal project and fiddling with all the installations. Or I had to work on Google Collab notebooks, which have a lot of limitations on running times and network connections. So, I found out some time to create a Deep Learning Rig with some assistance from NVIDIA folks.\nThe whole process involved a lot of reading up and watching a lot of Youtube videos from Linus Tech Tips . And as it was the first time I was assembling a computer from scratch, it was sort of special too.\nBuilding the DL rig as per your requirements takes up a lot of research. I researched on individual parts, their performance, reviews, and even the aesthetics.\nNow, most of the workstation builds I researched were focussed on gaming, so I thought of putting down a Deep Learning Rig Spec as well.\nI will try to put all the components I used along with the reasons why I went with those particular parts as well.\n***Also, if you want to see how I set up the Deep Learning libraries after setting up the system to use Ubuntu 18.04, you can view ***this definitive guide for Setting up a Deep Learning Workstation .\n So why the need for a workstation? The very first answer that comes to my mind is, why not?\nI work a lot on deep learning and machine learning applications, and it always has been such a massive headache to churn up a new server and installing all the dependencies every time I start to work on a new project.\nAlso, it looks great, sits on your desk, is available all the time, and is open to significant customization as per your requirements.\nAdding to this the financial aspects of using the GCP or AWS, and I was pretty much sold on the idea of building my rig.\n My Build It took me a couple of weeks to come up with the final build.\nI knew from the start that I want to have a lot of computing power and also something that would be upgradable in the coming years. Currently, my main priorities were to get a system that could support two NVIDIA RTX Titan cards with NVLink. That would allow me to have 48GB GPU memory at my disposal. Simply awesome.\nPS:* The below build might not be the best build, and there may be cheaper alternatives present, but I know for sure that it is the build with the minimal future headache. So I went with it. I also contacted Nvidia to get a lot of suggestions about this particular build and only went forward after they approved of it.\n1. Intel i9 9920x 3.5 GHz 12 core Processor   Yes, I went with an Intel processor and not an AMD one. My reason for this (though people may differ with me on this) is because Intel has more compatible and related software like Intel‚Äôs MKL, which benefits most of the Python libraries I use.\nAnother and maybe a more important reason, at least for me, was that it was suggested by the people at NVIDIA to go for i9 if I wanted to have a dual RTX Titan configuration. Again zero headaches in the future.\nSo why this particular one from the Intel range?\nI started with 9820X with its ten cores and 9980XE with 18 cores, but the latter stretched my budget a lot. I found that i9‚Äì9920X , with its 12 cores and 3.5 GHz processor, fit my budget just fine, and as it is always better to go for the mid-range solution, I went with it.\nNow a CPU is the component that decides a lot of other components you are going to end up using.\nFor example, if you choose an i9 9900X range of CPU, you will have to select an X299 motherboard, or if you are going to use an AMD Threadripper CPU , you will need an X399 Motherboard. So be mindful of choosing the right CPU and motherboard.\n2. MSI X299 SLI PLUS ATX LGA2066 Motherboard   This was a particularly difficult choice. There are just too many options here. I wanted a Motherboard that could support at least 96GB RAM (again as per the specifications by the NVIDIA Folks for supporting 2 Titans). That meant that I had to have at least six slots if I were to use 16GB RAM Modules as 16x6=96. I got 8 in this one, so it is expandable till 128 GB RAM.\nI also wanted to be able to have 2 TB NVMe SSD in my system(in the future), and that meant I needed 2 M.2 ports, which this board has. Or else I would have to go for a much expensive 2TB Single NVMe SSD.\nI looked into a lot of options, and based on the ATX Form factor, 4 PCI-E x16 slots, and the reasonable pricing of the board, I ended up choosing this one .\n3. Noctua NH-D15 chromax.BLACK 82.52 CFM CPU Cooler   Liquid cooling is in rage right now. And initially, I also wanted to go for an AIO cooler, i.e., liquid cooling.\nBut after talking to a couple of people at NVIDIA as well as scrouging through the internet forums on the pro and cons of both options, I realized that Air cooling is better suited to my needs. So I went for the Noctua NH-D15 , which is one of the best Air coolers in the market. So, I went with the best air cooling instead of a mediocre water cooling. And this cooler is SILENT. More on this later.\n4. Phanteks Enthoo Pro Tempered Glass Case  The next thing to think was a case that is going to be big enough to handle all these components and also be able to provide the required cooling. It was where I spent most of my time while researching.\nI mean, we are going to keep 2 Titan RTX, 9920x CPU, 128 GB RAM. It‚Äôs going to be a hellish lot of heat in there.\nAdd to that the space requirements for the Noctua air cooler and the capability to add a lot of fans, and I was left with two options based on my poor aesthetic sense as well as the availability in my country. The options were ‚Äî Corsair Air 540 ATX and the Phanteks Enthoo Pro Tempered Glass PH-ES614PTG_SWT .\nBoth of them are exceptional cases, but I went through with the Enthoo Pro as it is a more recently launched case and has a bigger form factor(Full Tower) offers options for more customizable build in the future too.\n5. Dual Titan RTX with 3 Slot NVLink  These 2 Titan RTX are by far the most important and expensive part of the whole build. These alone take up 80% of the cost, but aren‚Äôt they awesome?\nI wanted to have a high-performance GPU in my build, and the good folks at NVIDIA were generous enough to send me two of these to test out.\nI just love them. The design. The way they look in the build and the fact that they can be combined using a 3 Slot NVLink to provide 48 GB of GPU RAM effectively. Just awesome. If money is an issue, 2 x RTX 2080 Ti would also work fine as well. Only a problem will be that you might need smaller batch sizes training on RTX 2080 Ti, and in some cases, you might not be able to train large models as RTX2080Ti has 11GB RAM only. Also, you won‚Äôt be able to use NVLink, which combines the VRAM of multiple GPUs in Titans.\n6. Samsung 970 Evo Plus 1 TB NVME Solid State Drive   What about storage? NVMe SSD, of course, and the Samsung Evo Plus is the unanimous and most popular winner in this SSD race.\nI bought 1 of them till now, but as I have 2 M.2 ports in my motherboard, I will get total storage of 2TB SSD in the future.\nYou can also get a couple of 2.5\u0026quot; SSD for more storage space.\n7. Corsair Vengeance LPX 128GB (8x16GB) DDR4 3200 MHz  I wanted to have a minimum of 96GB RAM, as suggested by the NVIDIA team. So I said what the heck and went with the full 128 GB RAM without cheaping out.\nAs you can see, these RAM sticks are not RGB lit, and that is a conscious decision as the Noctua Air Cooler doesn‚Äôt provide a lot of clearance for RAM Slots and the RGB ones had a slightly higher height. So keep that in mind. Also, I was never trying to go for an RGB Build anyway as I want to focus on those lit up Titans in my build.\n8. Corsair 1200W Power Supply   A 1200W power supply is a pretty big one, but that is needed realizing that the estimated wattage of our components at full wattage is going to be ~965W.\nI had a couple of options for the power supply from other manufacturers also but went with this one because of Corsair‚Äôs name. I would have gone with HX1200i , but it was not available, and AX1200i was much more expensive than this one at my location. But both of them are excellent options apart from this one.\n9. Even More Fans  The Phanteks case comes up with three fans, but I was recommended to upgrade the intake, and exhaust fans of the case to BeQuiet BL071 PWM Fans as Dual Titans can put out a lot of heat. I have noticed that the temperature of my room is almost 2‚Äì3 degrees higher than the outside temperature, as I generally keep the machine on.\nTo get the best possible airflow, I bought 5 of these. I have put two at the top of the case along with a Phanteks case fan, 2 of them in the front, and one fan at the back of the case.\n10. Peripherals  The Essentials ‚Äî A cup of tea and those speakers\nThis section is not necessary but wanted to put it in for completion.\nGiven all the power we have got, I didn‚Äôt want to cheap out on the peripherals. So I got myself an LG 27UK650 4k monitor for content creation, \u0026lt;strong\u0026gt;BenQ\u0026lt;/strong\u0026gt; EX2780Q 1440p 144hz Gaming Monitor for a little bit of gaming, a Mechanical Cherry MX Red Corsair K68 Keyboard and a Corsair M65 Pro Mouse.\nAnd my build is complete.\n Pricing üí∞üí∞üí∞ I will put the price as per the PCPartPicker site as I have gotten my components from different countries and sources. You can also check the part list at the PCPartPicker site: https://pcpartpicker.com/list/zLVjZf  As you can see, this is pretty expensive by any means (even after getting the GPUs from NVIDIA), but that is the price you pay for certain afflictions, I guess.\n Finally  In this post, I talked about all the parts you are going to need to assemble your deep learning rig and my reasons for getting these in particular.\nYou might try to look out for better components or a different design, but this one has been working pretty well for me for quite some time now, and is it fast.\nIf you want to see how I set up the Deep Learning libraries after setting up the system with these components, you can view this definitive guide for Setting up a Deep Learning Workstation with Ubuntu 18.04\nLet me know what you think in the comments.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog ","permalink":"https://mlwhiz.com/blog/2020/08/09/owndlrig/","tags":["Artificial Intelligence","Deep Learning","Computer Vision","Object Detection"],"title":"Creating my First Deep Learning + Data Science Workstation"},{"categories":["Big Data","Data Science"],"contents":"Data Exploration is a key part of Data Science. And does it take long? Ahh. Don‚Äôt even ask. Preparing a data set for ML not only requires understanding the data set, cleaning, and creating new features, it also involves doing these steps repeatedly until we have a fine-tuned system.\nAs we moved towards bigger datasets, Apache Spark came as a ray of hope. It gave us a scalable and distributed in-memory system to work with Big Data. By the by, we also saw frameworks like Pytorch and Tensorflow that inherently parallelized matrix computations using thousands of GPU cores.\nBut never did we see these two systems working in tandem in the past. We continued to use Spark for Big Data ETL tasks and GPUs for matrix intensive problems in Deep Learning .\n And that is where Spark 3.0 comes. It provides us with a way to add NVIDIA GPUs to our Spark cluster nodes. The work done by these nodes can now be parallelized using both the CPU+GPU using the software platform for GPU computing, RAPIDS .\n Spark + GPU + RAPIDS = Spark 3.0  As per NVIDIA , the early adopters of Spark 3.0 already see a significantly faster performance with their current data loads. Such reductions in processing times can allow Data Scientists to perform more iterations on much bigger datasets, allowing Retailers to improve their forecasting, finance companies to enhance their credit models, and ad tech firms to improve their ability to predict click-through rates.\nExcited yet. So how can you start using Spark 3.0? Luckily, Google Cloud, Spark, and NVIDIA have come together and simplified the cluster creation process for us. With Dataproc on Google Cloud, we can have a fully-managed Apache Spark cluster with GPUs in a few minutes.\nThis post is about setting up your own Dataproc Spark Cluster with NVIDIA GPUs on Google Cloud.\n 1. Create a New GCP Project After the initial signup on the Google Cloud Platform , we can start a new project. Here I begin by creating a new project namedSparkDataProc.\n Create a New Project\n 2. Enable the APIs in the GCP Project Once we add this project, we can go to our new project and start a Cloud Shell instance by clicking the ‚ÄúActivate Cloud Shell‚Äù button at the top right corner. Doing so will open up a terminal window at the bottom of our screen where we can run our next commands to set up a data proc cluster:\n After this, we will need to run some commands to set up our project in the cloud shell. We start by enabling dataproc services within your project. Enable the Compute and Dataproc APIs to access Dataproc, and enable the Storage API as you‚Äôll need a Google Cloud Storage bucket to house your data. We also set our default region. This may take several minutes:\ngcloud services enable compute.googleapis.com gcloud services enable dataproc.googleapis.com gcloud services enable storage-api.googleapis.com gcloud config set dataproc/region us-central1   3. Create and Put some data in GCS Bucket Once done, we can create a new Google Cloud Storage Bucket, where we will keep all our data in the Cloud Shell:\n#You might need to change this name as this needs to be unique across all the users export BUCKET_NAME=rahulsparktest #Create the Bucket gsutil mb gs://${BUCKET_NAME}  We can also put some data in the bucket for later run purposes when we are running our spark cluster.\n# Get data in cloudshell terminal git clone https://github.com/caroljmcdonald/spark3-book mkdir -p ~/data/cal_housing tar -xzf spark3-book/data/cal_housing.tgz -C ~/data # Put data into Bucket using gsutil gsutil cp ~/data/CaliforniaHousing/cal_housing.data gs://${BUCKET_NAME}/data/cal_housing/cal_housing.csv   4. Setup the DataProc Rapids Cluster To create a DataProc RAPIDS cluster that uses NVIDIA T4 GPUs, we need to get some initialization scripts that are used to instantiate our cluster. These scripts will install the GPU drivers( install_gpu_driver.sh ) and create the Rapids conda environment( rapids.sh ) automatically for us. Since these scripts are in the development phase, the best way is to get the scripts from the GitHub source. We can do this using the below commands in our cloud shell in which we get the initialization scripts and copy them into our GS Bucket:\nwget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/rapids/rapids.sh wget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/gpu/install_gpu_driver.sh gsutil cp rapids.sh gs://$BUCKET_NAME gsutil cp install_gpu_driver.sh gs://$BUCKET_NAME  We can now create our cluster using the below command in the Cloud Shell. In the below command, we are using a predefined image version(2.0.0-RC2-ubuntu18) which has Spark 3.0 and python 3.7 to create our dataproc cluster. I am using a previous version of this image since the newest version has some issues with running Jupyter and Jupyter Lab. You can get a list of all versions here .\nCLUSTER_NAME=sparktestcluster REGION=us-central1 gcloud beta dataproc clusters create ${CLUSTER_NAME} \\ --image-version 2.0.0-RC2-ubuntu18 \\ --master-machine-type n1-standard-8 \\ --worker-machine-type n1-highmem-32 \\ --worker-accelerator type=nvidia-tesla-t4,count=2 \\ --optional-components ANACONDA,JUPYTER,ZEPPELIN \\ --initialization-actions gs://$BUCKET_NAME/install_gpu_driver.sh,gs://$BUCKET_NAME/rapids.sh \\ --metadata rapids-runtime=SPARK \\ --metadata gpu-driver-provider=NVIDIA \\ --bucket ${BUCKET_NAME} \\ --subnet default \\ --enable-component-gateway \\ --properties=\u0026quot;^#^spark:spark.task.resource.gpu.amount=0.125#spark:spark.executor. cores=8#spark:spark.task.cpus=1#spark:spark.yarn.unmanagedAM.enabled=false\u0026quot;   Our resulting Dataproc cluster has:\n  One 8-core master node and two 32-core worker nodes\n  Two NVIDIA T4 GPUs attached to each worker node\n  Anaconda, Jupyter, and Zeppelin enabled\n  Component gateway enabled for accessing Web UIs hosted on the cluster\n  Extra Spark config tuning suitable for a notebook environment set using the properties flag. Specifically, we set spark.executor.cores=8 for improved parallelization and spark.yarn.unmanagedAM.enabled=false since it currently breaks SparkUI.\n  Troubleshooting: If you get errors regarding limits after this command, you might want to change some of the quotas in your default Google Console Quotas Page . The limits I ended up changing were:\n  GPUs (all regions) to 12 (Minimum:4)\n  CPUs (all regions) to 164 (Minimum:72)\n  NVIDIA T4 GPUs in us-central1 to 12 (Minimum:4)\n  CPUs in us-central1 to 164 (Minimum:72)\n  I actually requested more limits than I required as the limit increase process might take a little longer and I will spin up some larger clusters later.\n 5. Run JupyterLab on DataProc Rapids Cluster Once your command succeeds(It might take 10‚Äì15 mins) you will be able to see your Dataproc cluster at https://console.cloud.google.com/dataproc/clusters . Or you can go to the Google Cloud Platform console on your browser and search for ‚ÄúDataproc‚Äù and click on the ‚ÄúDataproc‚Äù icon(It looks like three connected circles). This will navigate you to the Dataproc clusters page.\n Now, you would be able to open a web interface(Jupyter/JupyterLab/Zeppelin) if you click on the sparktestcluster and then ‚ÄúWeb Interfaces‚Äù.\n After opening up your Jupyter Pyspark Notebook, here is some example code for you to run if you are following along with this tutorial. In this code, we load a small dataset, and we see that the df.count() function ran in 252ms which is indeed fast for Spark, but I would do a much detailed benchmarking post later so keep tuned.\nfile = \u0026quot;gs://rahulsparktest/data/cal_housing/cal_housing.csv\u0026quot; df = spark.read.load(file,format=\u0026quot;csv\u0026quot;, sep=\u0026quot;,\u0026quot;, inferSchema=\u0026quot;true\u0026quot;, header=\u0026quot;false\u0026quot;) colnames = [\u0026quot;longitude\u0026quot;,\u0026quot;latitude\u0026quot;,\u0026quot;medage\u0026quot;,\u0026quot;totalrooms\u0026quot;,\u0026quot;totalbdrms\u0026quot;,\u0026quot;population\u0026quot;,\u0026quot;houshlds\u0026quot;,\u0026quot;medincome\u0026quot;,\u0026quot;medhvalue\u0026quot;] df = df.toDF(*colnames) df.count()   6. Access the Spark UI That is all well and done, but one major problem I faced was that I was not able to access the Spark UI using the link provided in the notebook. I found out that there were two ways to access the Spark UI for debugging purposes:\nA. Using the Web Interface option:\nWe can access Spark UI by clicking first on Yarn Resource Manager Link on the Web Interface and then on Application Master on the corresponding page:\n And, you will arrive at the Spark UI Page:\n B. Using the SSH Tunneling option:\nAnother option to access the Spark UI is using Tunneling. To do this, you need to go to the Web Interface Page and click on ‚ÄúCreate an SSH tunnel to connect to a web interface‚Äù.\n This will give you two commands that you want to run on your local machine and not on Cloud shell. But before running them, you need to install google cloud SDK to your machine and set it up for your current project:\nsudo snap install google-cloud-sdk --classic # This Below command will open the browser where you can authenticate by selecting your own google account. gcloud auth login # Set up the project as sparkdataproc (project ID) gcloud config set project sparkdataproc  Once done with this, we can simply run the first command:\ngcloud compute ssh sparktestcluster-m --project=sparkdataproc --zone=us-central1-b -- -D 1080 -N  And then the second one in another tab/window. This command will open up a new chrome window where you can access the Spark UI by clicking on Application Master the same as before.\n/usr/bin/google-chrome --proxy-server=\u0026quot;socks5://localhost:1080\u0026quot; --user-data-dir=\u0026quot;/tmp/sparktestcluster-m\u0026quot; [http://sparktestcluster-m:8088](http://sparktestcluster-m:8088)  And that is it for setting up a Spark3.0 Cluster accelerated by GPUs.\nIt took me around 30 mins to go through all these steps if I don‚Äôt count the debugging time and the quota increase requests.\nI am totally amazed by the concept of using a GPU on Spark and the different streams of experiments it opens up. Will be working on a lot of these in the coming weeks not only to benchmark but also because it is fun. So stay tuned.\n Continue Learning Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on Big Data Essentials: HDFS, MapReduce, and Spark RDD and Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames by Yandex on Coursera.\nI am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at Medium or Subscribe to my blog .\n","permalink":"https://mlwhiz.com/blog/2020/08/04/spark_dataproc/","tags":["Spark","Machine Learning","Data Science","Artificial Intelligence","Production"],"title":"Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps"},{"categories":["Programming","Computer Vision","Awesome Guides"],"contents":"Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.\nThat post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.\nI scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS. None of those seemed particularly great or complete to me.\nSo, I tried to do this myself using some help from FastAPI documentation . In this post, we will look at predominantly four things:\n  Setting Up an Amazon Instance\n  Creating a FastAPI API for Object Detection\n  Deploying FastAPI using Docker\n  An End to End App with UI\n  So, without further ado, let‚Äôs get started.\nYou can skip any part you feel you are versed with though I would expect you to go through the whole post, long as it may be, as there‚Äôs a lot of interconnection between concepts.\n 1. Setting Up Amazon Instance Before we start with using the Amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website . Works just like a single sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.\n  Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console .\n  On the AWS Management Console, you can select ‚ÄúLaunch a Virtual Machine.‚Äù Here we are trying to set up the machine where we will deploy our FastAPI API.\n  In the first step, you need to choose the AMI template for the machine. I am selecting the 18.04 Ubuntu Server since Ubuntu.\n    In the second step, I select the t2.xlarge machine, which has 4 CPUs and 16GB RAM rather than the free tier since I want to use an Object Detection model and will need some resources.    Keep pressing Next until you reach the ‚Äú6. Configure Security Group‚Äù tab. This is the most crucial step here. You will need to add a rule with Type: ‚ÄúHTTP‚Äù and Port Range:80.    You can click on ‚ÄúReview and Launch‚Äù and finally on the ‚ÄúLaunch‚Äù button to launch the instance. Once you click on Launch, you might need to create a new key pair. Here I am creating a new key pair named fastapi and downloading that using the ‚ÄúDownload Key Pair‚Äù button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on ‚ÄúLaunch Instance‚Äù after downloading the key pair    You can now go to your instances to see if your instance has started. Hint: See the Instance state; it should be showing ‚ÄúRunning.‚Äù    Also, to note here are the Public DNS(IPv4) address and the IPv4 public IP. We will need it to connect to this machine. For me, they are:  Public DNS (IPv4): ec2-18-237-28-174.us-west-2.compute.amazonaws.com IPv4 Public IP: 18.237.28.174  Once you have that run the following commands in the folder, you saved the fastapi.pem file. If the file is named fastapi.txt you might need to rename it to fastapi.pem.  # run fist command if fastapi.txt gets downloaded. # mv fastapi.txt fastapi.pem chmod 400 fastapi.pem ssh -i \u0026quot;fastapi.pem\u0026quot; ubuntu@\u0026lt;Your Public DNS(IPv4) Address\u0026gt;  Now we have got our Amazon instance up and running. We can move on here to the real part of the post.\n 2. Creating a FastAPI API for Object Detection Before we deploy an API, we need to have an API with us, right? In one of my last posts, I had written a simple tutorial to understand FastAPI and API basics. Do read the post if you want to understand FastAPI basics.\nSo, here I will try to create an Image detection API. As for how to pass the Image data to the API? The idea is ‚Äî What is an image but a string? An image is just made up of bytes, and we can encode these bytes as a string. We will use the base64 string representation, which is a popular way to get binary data to ASCII characters. And, we will pass this string representation to give an image to our API.\nA. Some Image Basics: What is Image, But a String? So, let us first see how we can convert an Image to a String. We read the binary data from an image file using the ‚Äòrb‚Äô flag and turn it into a base64 encoded data representation using the base64.b64encode function. We then use the decode to utf-8 function to get the base encoded data into human-readable characters. Don‚Äôt worry if it doesn‚Äôt make a lot of sense right now. Just understand that any data is binary, and we can convert binary data to its string representation using a series of steps.\nAs a simple example, if I have a simple image like below, we can convert it to a string using:\n import base64 with open(\u0026#34;sample_images/dog_with_ball.jpg\u0026#34;, \u0026#34;rb\u0026#34;) as image_file: base64str = base64.b64encode(image_file.read()).decode(\u0026#34;utf-8\u0026#34;)  Here I have got a string representation of a file named dog_with_ball.png on my laptop.\nGreat, we now have a string representation of an image. And, we can send this string representation to our FastAPI. But we also need to have a way to read an image back from its string representation. After all, our image detection API using PyTorch and any other package needs to have an image object that they can predict, and those methods don‚Äôt work on a string.\nSo here is a way to create a PIL image back from an image‚Äôs base64 string. Mostly we just do the reverse steps in the same order. We encode in ‚Äòutf-8‚Äô using .encode. We then use base64.b64decode to decode to bytes. We use these bytes to create a bytes object using io.BytesIO and use Image.open to open this bytes IO object as a PIL image, which can easily be used as an input to my PyTorch prediction code.*** Again simply, it is just a way to convert base64 image string to an actual image.***\nimport base64 import io from PIL import Image def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(\u0026#39;utf-8\u0026#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img So does this function work? Let‚Äôs see for ourselves. We can use just the string to get back the image.\n And we have our happy dog back again. Looks better than the string.\nB. Writing the Actual FastAPI code So, as now we understand that our API can get an image as a string from our user, let‚Äôs create an object detection API that makes use of this image as a string and outputs the bounding boxes for the object with the object classes as well.\nHere, I will be using a Pytorch pre-trained fasterrcnn_resnet50_fpn detection model from the torchvision.models for object detection, which is trained on the COCO dataset to keep the code simple, but one can use any model. You can look at these posts if you want to train your custom image classification or image detection model using Pytorch.\nBelow is the full code for the FastAPI. Although it may look long, we already know all the parts. In this code, we essentially do the following steps:\n  Create our fast API app using the FastAPI() constructor.\n  Load our model and the classes it was trained on. I got the list of classes from the PyTorch docs .\n  We also defined a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user. Here the end-user gives the base64str and some score threshold for object detection prediction.\n  We add a function called base64str_to_PILImage which does just what it is named.\n  And we write a predict function called get_predictionbase64 which returns a dict of bounding boxes and classes using a base64 string representation of an image and a threshold as an input. We also add @app .put(‚Äú/predict‚Äù) on top of this function to define our endpoint. If you need to understand put and endpoint refer to my previous post on FastAPI.\n  from fastapi import FastAPI from pydantic import BaseModel import torchvision from torchvision import transforms import torch from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from PIL import Image import numpy as np import cv2 import io, json import base64 app = FastAPI() # load a pre-trained Model and convert it to eval mode. # This model loads just once when we start the API. model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) COCO_INSTANCE_CATEGORY_NAMES = [ \u0026#39;__background__\u0026#39;, \u0026#39;person\u0026#39;, \u0026#39;bicycle\u0026#39;, \u0026#39;car\u0026#39;, \u0026#39;motorcycle\u0026#39;, \u0026#39;airplane\u0026#39;, \u0026#39;bus\u0026#39;, \u0026#39;train\u0026#39;, \u0026#39;truck\u0026#39;, \u0026#39;boat\u0026#39;, \u0026#39;traffic light\u0026#39;, \u0026#39;fire hydrant\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;stop sign\u0026#39;, \u0026#39;parking meter\u0026#39;, \u0026#39;bench\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;sheep\u0026#39;, \u0026#39;cow\u0026#39;, \u0026#39;elephant\u0026#39;, \u0026#39;bear\u0026#39;, \u0026#39;zebra\u0026#39;, \u0026#39;giraffe\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;backpack\u0026#39;, \u0026#39;umbrella\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;handbag\u0026#39;, \u0026#39;tie\u0026#39;, \u0026#39;suitcase\u0026#39;, \u0026#39;frisbee\u0026#39;, \u0026#39;skis\u0026#39;, \u0026#39;snowboard\u0026#39;, \u0026#39;sports ball\u0026#39;, \u0026#39;kite\u0026#39;, \u0026#39;baseball bat\u0026#39;, \u0026#39;baseball glove\u0026#39;, \u0026#39;skateboard\u0026#39;, \u0026#39;surfboard\u0026#39;, \u0026#39;tennis racket\u0026#39;, \u0026#39;bottle\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;wine glass\u0026#39;, \u0026#39;cup\u0026#39;, \u0026#39;fork\u0026#39;, \u0026#39;knife\u0026#39;, \u0026#39;spoon\u0026#39;, \u0026#39;bowl\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;sandwich\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;broccoli\u0026#39;, \u0026#39;carrot\u0026#39;, \u0026#39;hot dog\u0026#39;, \u0026#39;pizza\u0026#39;, \u0026#39;donut\u0026#39;, \u0026#39;cake\u0026#39;, \u0026#39;chair\u0026#39;, \u0026#39;couch\u0026#39;, \u0026#39;potted plant\u0026#39;, \u0026#39;bed\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;dining table\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;toilet\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;tv\u0026#39;, \u0026#39;laptop\u0026#39;, \u0026#39;mouse\u0026#39;, \u0026#39;remote\u0026#39;, \u0026#39;keyboard\u0026#39;, \u0026#39;cell phone\u0026#39;, \u0026#39;microwave\u0026#39;, \u0026#39;oven\u0026#39;, \u0026#39;toaster\u0026#39;, \u0026#39;sink\u0026#39;, \u0026#39;refrigerator\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;book\u0026#39;, \u0026#39;clock\u0026#39;, \u0026#39;vase\u0026#39;, \u0026#39;scissors\u0026#39;, \u0026#39;teddy bear\u0026#39;, \u0026#39;hair drier\u0026#39;, \u0026#39;toothbrush\u0026#39; ] model.eval() # define the Input class class Input(BaseModel): base64str : str threshold : float def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(\u0026#39;utf-8\u0026#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img @app.put(\u0026#34;/predict\u0026#34;) def get_predictionbase64(d:Input): \u0026#39;\u0026#39;\u0026#39; FastAPI API will take a base 64 image as input and return a json object \u0026#39;\u0026#39;\u0026#39; # Load the image img = base64str_to_PILImage(d.base64str) # Convert image to tensor transform = transforms.Compose([transforms.ToTensor()]) img = transform(img) # get prediction on image pred = model([img]) pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][\u0026#39;labels\u0026#39;].numpy())] pred_boxes = [[(float(i[0]), float(i[1])), (float(i[2]), float(i[3]))] for i in list(pred[0][\u0026#39;boxes\u0026#39;].detach().numpy())] pred_score = list(pred[0][\u0026#39;scores\u0026#39;].detach().numpy()) pred_t = [pred_score.index(x) for x in pred_score if x \u0026gt; d.threshold][-1] pred_boxes = pred_boxes[:pred_t+1] pred_class = pred_class[:pred_t+1] return {\u0026#39;boxes\u0026#39;: pred_boxes, \u0026#39;classes\u0026#39; : pred_class} C. Local Before Global: Test the FastAPI code locally Before we move on to AWS, let us check if the code works on our local machine. We can start the API on our laptop using:\nuvicorn fastapiapp:app --reload  The above means that your API is now running on your local server, and the \u0026ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this \u0026ndash;reload flag when you put the API in production.\nYou should see something like:\n You can now try to access this API and see if it works using the requests module:\nimport requests,json payload = json.dumps({ \u0026#34;base64str\u0026#34;: base64str, \u0026#34;threshold\u0026#34;: 0.5 }) response = requests.put(\u0026#34;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)\u0026#34;,data = payload) data_dict = response.json()  And so we get our results using the API. This image contains a dog and a sports ball. We also have corner 1 (x1,y1) and corner 2 (x2,y2) coordinates of our bounding boxes.\nD. Lets Visualize Although not strictly necessary, we can visualize how the results look in our Jupyter notebook:\nfrom PIL import Image import numpy as np import cv2 import matplotlib.pyplot as plt def PILImage_to_cv2(img): return np.asarray(img) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) # Write the prediction class plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() img = Image.open(\u0026#34;sample_images/dog_with_ball.jpg\u0026#34;) drawboundingbox(img, data_dict[\u0026#39;boxes\u0026#39;], data_dict[\u0026#39;classes\u0026#39;]) Here is the output:\n Here you will note that I got the image from the local file system, and that sort of can be considered as cheating as we don‚Äôt want to save every file that the user sends to us through a web UI. We should have been able to use the same base64string object that we also had to create this image. Right?\nNot to worry, we could do that too. Remember our base64str_to_PILImage function? We could have used that also.\nimg = base64str_to_PILImage(base64str) drawboundingbox(img, data_dict['boxes'], data_dict['classes'])   That looks great. We have our working FastAPI, and we also have our amazon instance. We can now move on to Deployment.\n 3. Deployment on Amazon ec2 Till now, we have created an AWS instance and, we have also created a FastAPI that takes as input a base64 string representation of an image and returns bounding boxes and the associated class. But all the FastAPI code still resides in our local machine. How do we put it on the ec2 server? And run predictions on the cloud.\nA. Install Docker We will deploy our app using docker, as is suggested by the fastAPI creator himself. I will try to explain how docker works as we go. The below part may look daunting but it just is a series of commands and steps. So stay with me.\nWe can start by installing docker using:\nsudo apt-get update sudo apt install docker.io  We then start the docker service using:\nsudo service docker start  B. Creating the folder structure for docker ‚îî‚îÄ‚îÄ dockerfastapi ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ app ‚îÇ ‚îî‚îÄ‚îÄ main.py ‚îî‚îÄ‚îÄ requirements.txt  Here dockerfastapi is our project‚Äôs main folder. And here are the different files in this folder:\ni. requirements.txt: Docker needs a file, which tells it which all libraries are required for our app to run. Here I have listed all the libraries I used in my Fastapi API.\nnumpy opencv-python matplotlib torchvision torch fastapi pydantic  ii. Dockerfile: The second file is Dockerfile.\nFROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app COPY requirements.txt . RUN pip --no-cache-dir install -r requirements.txt  How Docker works?: You can skip this section, but it will help to get some understanding of how docker works.\n The dockerfile can be thought of something like a sh file,which contains commands to create a docker image that can be run in a container. One can think of a docker image as an environment where everything like Python and Python libraries is installed. A container is a unit which is just an isolated box in our system that uses a dockerimage. The advantage of using docker is that we can create multiple docker images and use them in multiple containers. For example, one image might contain python36, and another can contain python37. And we can spawn multiple containers in a single Linux server.\nOur Dockerfile contains a few things:\n  FROM command: Here the first line FROM specifies that we start with tiangolo‚Äôs (FastAPI creator) Docker image. As per his site: ‚ÄúThis image has an ‚Äúauto-tuning‚Äù mechanism included so that you can just add your code and get that same high performance automatically. And without making sacrifices‚Äù. What we are doing is just starting from an image that installs python3.7 for us along with some added configurations for uvicorn and gunicorn ASGI servers and a start.sh file for ASGI servers automatically. For adventurous souls, particularly commandset 1 and commandset2 get executed through a sort of a daisy-chaining of commands.\n  COPY command: We can think of a docker image also as a folder that contains files and such. Here we copy our app folder and the requirements.txt file, which we created earlier to our docker image.\n  RUN Command: We run pip install command to install all our python dependencies using the requirements.txt file that is now on the docker image.\n  iii. main.py: This file contains the fastapiapp.py code we created earlier. Remember to keep the name of the file main.py only.\nC. Docker Build We have got all our files in the required structure, but we haven‚Äôt yet used any docker command. We will first need to build an image containing all dependencies using Dockerfile.\nWe can do this simply by:\nsudo docker build -t myimage .  This downloads, copies and installs some files and libraries from tiangolo‚Äôs image and creates an image called myimage. This myimage has python37 and some python packages as specified by requirements.txt file.\n We will then just need to start a container that runs this image. We can do this using:\nsudo docker run -d --name mycontainer -p 80:80 myimage  This will create a container named mycontainer which runs our docker image myimage. The part 80:80 connects our docker container port 80 to our Linux machine port 80.\n And actually that‚Äôs it. At this point, you should be able to open the below URL in your browser.\n# \u0026lt;IPV4 public IP\u0026gt;/docs URL: 18.237.28.174/docs   And we can check our app programmatically using:\npayload = json.dumps({ \u0026#34;base64str\u0026#34;: base64str, \u0026#34;threshold\u0026#34;: 0.5 }) response = requests.put(\u0026#34;[http://18.237.28.174/predict](http://18.237.28.174/predict)\u0026#34;,data = payload) data_dict = response.json() print(data_dict)   Yup, finally our API is deployed.  D. Troubleshooting as the real world is not perfect All the above was good and will just work out of the box if you follow the exact instructions, but the real world doesn‚Äôt work like that. You will surely get some errors along the way and would need to debug your code. So to help you with that, some docker commands may come handy:\n Logs: When we ran our container using sudo docker run we don‚Äôt get a lot of info, and that is a big problem when you are debugging. You can see the real-time logs using¬†the¬†below¬†command. If you see an error here, you will need to change your code and build the image again.   sudo docker logs -f mycontainer   Starting and Stopping Docker: Sometimes, it might help just to restart your docker. In that case, you can use:   sudo service docker stop sudo service docker start  Listing images and containers: Working with docker, you will end up creating images and containers, but you won‚Äôt be able to see them in the working directory. You can list your images and containers using:   sudo docker container ls sudo docker image ls   Deleting unused docker images or containers: You might need to remove some images or containers as these take up a lot of space on the system. Here is how you do that.   # the prune command removes the unused containers and images sudo docker system prune # delete a particular container sudo docker rm mycontainer # remove myimage sudo docker image rm myimage # remove all images sudo docker image prune ‚Äî all  **Checking localhost:**The Linux server doesn‚Äôt have a browser, but we can still see the browser output though it‚Äôs a little ugly:   curl localhost   Develop without reloading image again and again: For development, it‚Äôs useful to be able just to change the contents of the code on our machine and test it live, without having to build the image every time. In that case, it‚Äôs also useful to run the server with live auto-reload automatically at every code change. Here, we use our app directory on our Linux machine, and we replace the default (/start.sh) with the development alternative /start-reload.sh during development. After everything looks fine, we can build our image again run it inside the container.   sudo docker run -d -p 80:80 -v $(pwd):/app myimage /start-reload.sh If this doesn‚Äôt seem sufficient, adding here a docker cheat sheet containing useful docker commands:\n  4. An End to End App with UI We are done here with our API creation, but we can also create a UI based app using Streamlit using our FastAPI API. This is not how you will do it in a production setting (where you might have developers making apps using react, node.js or javascript)but is mostly here to check the end-to-end flow of how to use an image API. I will host this barebones Streamlit app on local rather than the ec2 server, and it will get the bounding box info and classes from the FastAPI API hosted on ec2.\nIf you need to learn more about how streamlit works, you can check out this post . Also, if you would want to deploy this streamlit app also to ec2, here is a tutorial again.\nHere is the flow of the whole app with UI and FastAPI API on ec2:\n Project Architecture\nThe most important problems we need to solve in our streamlit app are:\nHow to get an image file from the user using Streamlit? A. Using File uploader: We can use the file uploader using:\nbytesObj = st.file_uploader(‚ÄúChoose an image file‚Äù) The next problem is, what is this bytesObj we get from the streamlit file uploader? In streamlit, we will get a bytesIO object from the file_uploader and we will need to convert it to base64str for our FastAPI app input. This can be done using:\ndef bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(\u0026#34;utf-8\u0026#34;) base64str = bytesioObj_to_base64str(bytesObj) B. Using URL: We can also get an image URL from the user using text_input.\nurl = st.text_input(‚ÄòEnter URL‚Äô) We can then get image from URL in base64 string format using the requests module and base64 encode and utf-8 decode:\ndef ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(\u0026#34;utf-8\u0026#34;) base64str = ImgURL_to_base64str(url) And here is the complete code of our Streamlit app. You have seen most of the code in this post already.\nimport streamlit as st import base64 import io import requests,json from PIL import Image import cv2 import numpy as np import matplotlib.pyplot as plt import requests import random # use file uploader object to recieve image # Remember that this bytes object can be used only once def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(\u0026#34;utf-8\u0026#34;) # Image conversion functions def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(\u0026#39;utf-8\u0026#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img def PILImage_to_cv2(img): return np.asarray(img) def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(\u0026#34;utf-8\u0026#34;) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() st.markdown(\u0026#34;\u0026lt;h1\u0026gt;Our Object Detector App using FastAPI\u0026lt;/h1\u0026gt;\u0026lt;br\u0026gt;\u0026#34;, unsafe_allow_html=True) bytesObj = st.file_uploader(\u0026#34;Choose an image file\u0026#34;) st.markdown(\u0026#34;\u0026lt;center\u0026gt;\u0026lt;h2\u0026gt;or\u0026lt;/h2\u0026gt;\u0026lt;/center\u0026gt;\u0026#34;, unsafe_allow_html=True) url = st.text_input(\u0026#39;Enter URL\u0026#39;) if bytesObj or url: # In streamlit we will get a bytesIO object from the file_uploader # and we convert it to base64str for our FastAPI if bytesObj: base64str = bytesioObj_to_base64str(bytesObj) elif url: base64str = ImgURL_to_base64str(url) # We will also create the image in PIL Image format using this base64 str # Will use this image to show in matplotlib in streamlit img = base64str_to_PILImage(base64str) # Run FastAPI payload = json.dumps({ \u0026#34;base64str\u0026#34;: base64str, \u0026#34;threshold\u0026#34;: 0.5 }) response = requests.put(\u0026#34;http://18.237.28.174/predict\u0026#34;,data = payload) data_dict = response.json() st.markdown(\u0026#34;\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;App Result\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\u0026#34;, unsafe_allow_html=True) drawboundingbox(img, data_dict[\u0026#39;boxes\u0026#39;], data_dict[\u0026#39;classes\u0026#39;]) st.pyplot() st.markdown(\u0026#34;\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;FastAPI Response\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\u0026lt;br\u0026gt;\u0026#34;, unsafe_allow_html=True) st.write(data_dict) We can run this streamlit app in local using:\nstreamlit run streamlitapp.py  And we can see our app running on our localhost:8501. Works well with user-uploaded images as well as URL based images. Here is a cat image for some of you cat enthusiasts as well.\n So that‚Äôs it. We have created a whole workflow here to deploy image detection models through FastAPI on ec2 and utilizing those results in Streamlit. I hope this helps your woes around deploying models in production. You can find the code for this post as well as all my posts at my GitHub repository.\nLet me know if you like this post and if you would like to include Docker or FastAPI or Streamlit in your day to day deployment needs. I am also looking to create a much detailed post on Docker so follow me up to stay tuned with my writing as well. Details below.\n Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/","tags":["Machine Learning","Data Science","Production","Productivity","Tools","Artificial Intelligence","Computer Vision"],"title":"Deployment could be easy‚Ää‚Äî‚ÄäA Data Scientist‚Äôs Guide to deploy an Image detection FastAPI API using Amazon ec2"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"Ultralytics recently launched YOLOv5 amid controversy surrounding its name. For context, the first three versions of YOLO (You Only Look Once) were created by Joseph Redmon. Following this, Alexey Bochkovskiy created YOLOv4 on darknet, which boasted higher Average Precision (AP) and faster results than previous iterations.\nNow, Ultralytics has released YOLOv5, with comparable AP and faster inference times than YOLOv4. This has left many asking: is a new version warranted given similar accuracy to YOLOv4? Whatever the answer may be, it‚Äôs definitely a sign of how quickly the detection community is evolving.\n Since they first ported YOLOv3 , Ultralytics has made it very simple to create and deploy models using Pytorch, so I was eager to try out YOLOv5. As it turns out, Ultralytics has further simplified the process, and the results speak for themselves.\nIn this article, we‚Äôll create a detection model using YOLOv5, from creating our dataset and annotating it to training and inferencing using their remarkable library. This post focuses on the implementation of YOLOv5, including:\n  Creating a toy dataset\n  Annotating the image data\n  Creating the project structure\n  Training YOLOv5\n   Creating Custom Dataset You can forgo the first step if you have your image Dataset. Since I don‚Äôt have images, I am downloading data from the Open Image Dataset(OID), which is an excellent resource for getting annotated image data that can be used for classification as well as detection. Note that we won‚Äôt be using the provided annotations from OID and create our own for the sake of learning.\n1. OIDv4 Download Images: To download images from the Open Image dataset, we start by cloning the OIDv4_ToolKit and installing all requirements.\ngit clone [https://github.com/EscVM/OIDv4_ToolKit](https://github.com/EscVM/OIDv4_ToolKit) cd [OIDv4_ToolKit](https://github.com/EscVM/OIDv4_ToolKit) pip install -r requirements.txt  We can now use the main.py script within this folder to download images as well as labels for multiple classes.\nBelow I am downloading the data for Cricketball and Football to create our Custom Dataset. That is, we will be creating a dataset with footballs and cricket balls, and the learning task is to detect these balls.\npython3 main.py downloader --classes Cricket_ball Football --type_csv all -y --limit 500  The below command creates a directory named ‚ÄúOID‚Äù with the following structure:\n OID directory structure. We will take only the image files(.jpgs) from here and not the labels as we will annotate manually to create our Custom Dataset, though we can use them if required for a different project.\nBefore we continue, we will need to copy all the images in the same folder to start our labeling exercise from Scratch. You can choose to do this manually, but this can also be quickly done programmatically using recursive glob function:\nimport os from glob import glob os.system(\u0026quot;mkdir Images\u0026quot;) images = glob(r'OID/**/*.jpg', recursive=True) for img in images: os.system(f\u0026quot;cp {img} Images/\u0026quot;)  2. Label Images with HyperLabel We will use a tool called Hyperlabel to label our images. In the past, I have used many tools to create annotations like labelimg, labelbox, etc. but never came across a tool so straightforward and that too open source. The only downside is that you cannot get this tool for Linux and only for Mac and Windows, but I guess that is fine for most of us.\n    The best part of this tool is the variety of output formats it provides. Since we want to get the data for Yolo, we will close Yolo Format and export it after being done with our annotations. But you can choose to use this tool if you want to get annotations in JSON format(COCO) or XML format(Pascal VOC) too.\n Exporting in Yolo format essentially creates a .txt file for each of our images, which contains the class_id, x_center, y_center, width, and the height of the image. It also creates a file named obj.names , which helps map the class_id to the class name. For example:\n       Notice that the coordinates are scaled from 0 to 1 in the annotation file. Also, note that the class_id is 0 for Cricketball and 1 for football as per obj.names file, which starts from 0. There are a few other files we create using this, but we won‚Äôt be using them in this example.\nOnce we have done this, we are mostly set up with our custom dataset and would only need to rearrange some of these files for subsequent training and validation splits later when we train our model. The dataset currently will be a single folder like below containing both the images as well as annotations:\ndataset - 0027773a6d54b960.jpg - 0027773a6d54b960.txt - 2bded1f9cb587843.jpg - 2bded1f9cb587843.txt -- --   Setting up the project To train our custom object detector, we will be using Yolov5 from Ultralytics. We start by cloning the repository and installing the dependencies:\n# clone repo git clone [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) cd yolov5 pip install -U -r requirements.txt  We then start with creating our own folder named training in which we will keep our custom dataset.\n!mkdir training  We start by copying our custom dataset folder in this folder and creating the train validation folders using the simple train_val_folder_split.ipynb notebook. This code below just creates some train and validation folders and populates them with images.\nimport glob, os import random # put your own path here dataset_path = 'dataset' # Percentage of images to be used for the validation set percentage_test = 20 !mkdir data !mkdir data/images !mkdir data/labels !mkdir data/images/train !mkdir data/images/valid !mkdir data/labels/train !mkdir data/labels/valid # Populate the folders p = percentage_test/100 for pathAndFilename in glob.iglob(os.path.join(dataset_path, \u0026quot;*.jpg\u0026quot;)): title, ext = os.path.splitext(os.path.basename(pathAndFilename)) if random.random() \u0026lt;=p : os.system(f\u0026quot;cp {dataset_path}/{title}.jpg data/images/valid\u0026quot;) os.system(f\u0026quot;cp {dataset_path}/{title}.txt data/labels/valid\u0026quot;) else: os.system(f\u0026quot;cp {dataset_path}/{title}.jpg data/images/train\u0026quot;) os.system(f\u0026quot;cp {dataset_path}/{title}.txt data/labels/train\u0026quot;)  After running this, your data folder structure should look like below. It should have two directories images and labels.\n We now have to add two configuration files to training folder:\n1. Dataset.yaml: We create a file ‚Äúdataset.yaml‚Äù that contains the path of training and validation images and also the classes.\n# train and val datasets (image directory or *.txt file with image paths) train: training/data/images/train/ val: training/data/images/valid/ # number of classes nc: 2 # class names names: ['Cricketball', 'Football']  2. Model.yaml: We can use multiple models ranging from small to large while creating our network. For example, yolov5s.yaml file in the yolov5/models directory is the small Yolo model with 7M parameters, while the yolov5x.yaml is the largest Yolo model with 96M Params. For this project, I will use the yolov5l.yaml which has 50M params. We start by copying the file from yolov5/models/yolov5l.yaml to the training folder and changing nc , which is the number of classes to 2 as per our project requirements.\n# parameters nc: 2 # change number of classes depth_multiple: 1.0 # model depth multiple width_multiple: 1.0 # layer channel multiple   Train At this point our training folder looks like:\n Once we are done with the above steps, we can start training our model. This is as simple as running the below command, where we provide the locations of our config files and various other params. You can check out the different other options in train.py file, but these are the ones I found noteworthy.\n# Train yolov5l on custom dataset for 300 epochs $ python train.py --img 640 --batch 16 --epochs 300--data training/dataset.yaml --cfg training/yolov5l.yaml --weights ''  Sometimes you might get an error with PyTorch version 1.5 in that case run on a single GPU using:\n# Train yolov5l on custom dataset for 300 epochs $ python train.py --img 640 --batch 16 --epochs 300--data training/dataset.yaml --cfg training/yolov5l.yaml --weights '' --device 0  Once you start the training, you can check whether the training has been set up by checking the automatically created filetrain_batch0.jpg , which contains the training labels for the first batch and test_batch0_gt.jpg which includes the ground truth for test images. This is how they look for me.\n      Left: train_batch0.jpg, Right: test_batch0_gt.jpg\n Results To see the results for the training at localhost:6006 in your browser using tensorboard, run this command in another terminal tab\ntensorboard --logdir=runs  Here are the various validation metrics. These metrics also get saved in a file results.png at the end of the training run.\n  Predict Ultralytics Yolov5 provides a lot of different ways to check the results on new data.\nTo detect some images you can simply put them in the folder named inference/images and run the inference using the best weights as per validation AP:\npython detect.py --weights weights/best.pt   You can also detect in a video using the detect.py file:\npython detect.py --weights weights/best.pt --source inference/videos/messi.mp4 --view-img --output inference/output  Here I specify that I want to see the output using the ‚Äî view-img flag, and we store the output at the location inference/output. This will create a .mp4 file in this location. It\u0026rsquo;s impressive that the network can see the ball, the speed at which inference is made here, and also the mindblowing accuracy on never observed data.\n You can also use the webcam as a source by specifying the \u0026ndash;source as 0. You can check out the various other options in detect.py file.\n Conclusion In this post, I talked about how to create a Yolov5 object detection model using a Custom Dataset. I love the way Ultralytics has made it so easy to create an object detection model.\nAdditionally, the various ways that they have provided to see the model results make it a complete package I have seen in a long time.\nIf you would like to experiment with the custom dataset yourself, you can download the annotated data on Kaggle and the code at Github .\nIf you want to know more about various Object Detection techniques, motion estimation, object tracking in video, etc., I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization . If you wish to know more about how the object detection field has evolved over the years, you can also take a look at my last post on Object detection.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog ","permalink":"https://mlwhiz.com/blog/2020/08/08/yolov5/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Object Detection"],"title":"How to Create an End to End Object Detector using Yolov5"},{"categories":["Natural Language Processing","Deep Learning","Computer Vision","Awesome Guides"],"contents":"Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.\nBut this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.\nNow, every time I create the whole deep learning setup from an installation viewpoint, I end up facing similar challenges. It‚Äôs like running around in circles with all these various dependencies and errors. This time also I had to try many things before the whole configuration came to life without errors.\nSo this time, I made it a point to document everything while installing all the requirements and their dependencies in my own system.\nThis post is about setting up your own Linux Ubuntu 18.04 system for deep learning with everything you might need.\nIf a pre-built deep learning system is preferred, I can recommend Exxact‚Äôs line of workstations and servers.\nI assume that you have a fresh Ubuntu 18.04 installation. I am taking inspiration from Slav Ivanov‚Äôs excellent post in 2017 on creating a Deep Learning box. You can call it the 2020 version for the same post from a setup perspective, but a lot of the things have changed from then, and there are a lot of caveats with specific CUDA versions not supported by Tensorflow and Pytorch.\nStarting up  Before we do anything with our installation, we need to update our Linux system to the latest packages. We can do this simply by using:\nsudo apt-get update sudo apt-get --assume-yes upgrade sudo apt-get --assume-yes install tmux build-essential gcc g++ make binutils sudo apt-get --assume-yes install software-properties-common sudo apt-get --assume-yes install git  The Process So now we have everything set up we want to install the following four things:\n  GPU Drivers: Why is your PC not supporting high graphic resolutions? Or how would your graphics cards talk to your python interfaces?\n  CUDA: A layer to provide access to the GPU‚Äôs instruction set and parallel computation units. In simple words, it allows us a way to write code for GPUs\n  CuDNN: a library that provides Primitives for Deep Learning Network\n  Pytorch, Tensorflow, and Rapids: higher-level APIs to code Deep Neural Networks\n  1. GPU Drivers The first step is to add the latest NVIDIA drivers. You can choose the GPU product type, Linux 64 bit, and download Type as ‚ÄúLinux Long-Lived‚Äù for the 18.04 version.\n Clicking on search will take you to a downloads page:\n From where you can download the driver file NVIDIA-Linux-x86_64‚Äì440.44.run and run it using:\nchmod +x NVIDIA-Linux-x86_64‚Äì440.44.run sudo sh NVIDIA-Linux-x86_64‚Äì440.44.run  For you, the file may be named differently, depending on the latest version.\n2. CUDA We will now need to install the CUDA toolkit. Somehow the CUDA toolkit 10.2 is still not supported by Pytorch and Tensorflow, so we will go with CUDA Toolkit 10.1, which is supported by both.\nAlso, the commands on the product page for CUDA 10.1 didn‚Äôt work for me and the commands I ended up using are:\nsudo apt-key adv --fetch-keys [http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub](http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub) \u0026amp;\u0026amp; echo \u0026quot;deb [https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64) /\u0026quot; | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get -o Dpkg::Options::=\u0026quot;--force-overwrite\u0026quot; install cuda-10-1 cuda-drivers  The next step is to create the LD_LIBRARY_PATH and append to the PATH variable the path where CUDA got installed. Just run this below command on your terminal.\necho 'export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}' \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; echo 'export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' \u0026gt;\u0026gt; ~/.bashrc \u0026amp;\u0026amp; source ~/.bashrc \u0026amp;\u0026amp; sudo ldconfig  After this, one can check if CUDA is installed correctly by using:\nnvcc --version   As you can see, the CUDA Version is 10.1 as we wanted. Also, check if you can use the command:\nnvidia-smi  For me, it showed an error when I used it the first time, but a simple reboot solved the issue. And both my NVIDIA graphic cards show up in all their awesome glory. Don‚Äôt worry that the display says the CUDA version supported is 10.2. I was also confused , but it is just the maximum CUDA version supported by the graphics driver that is shown in nvidia-smi.\n 3.CuDNN What is the use of all these libraries if we are not going to train neural nets? CuDNN provides various primitives for Deep Learning, which are later used by PyTorch/TensorFlow.\nBut we first need to get a developer account first to install CuDNN. Once you fill-up the signup form, you will see the screen below. Select the cuDNN version that applies to your CUDA version. For me, the CUDA version is 10.1, so I select the second one.\n Once you select the appropriate CuDNN version the screen expands:\n For my use case, I needed to download three files for Ubuntu 18.04:\n[cuDNN Runtime Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Developer Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-dev_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-doc_7.6.5.32-1%2Bcuda10.1_amd64.deb)  After downloading these files, you can install using these commands. You can also see the exact commands if anything changes in the future:\n# Install the runtime library: sudo dpkg -i libcudnn7_7.6.5.32-1+cuda10.1_amd64.deb #Install the developer library: sudo dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.1_amd64.deb #Install the code samples and cuDNN User Guide(Optional): sudo dpkg -i libcudnn7-doc_7.6.5.32-1+cuda10.1_amd64.deb  4. Anaconda, Pytorch, Tensorflow, and Rapids And finally, we reach the crux. We will install the software which we will interface with most of the times.\nWe need to install Python with virtual environments. I have downloaded python3 as it is the most stable version as of now, and it is time to say goodbye to Python 2.7. It was great while it lasted. And we will also install Pytorch and Tensorflow. I prefer them both for specific tasks as applicable.\nYou can go to the anaconda distribution page and download the package.\n Once downloaded you can simply run the shell script:\nsudo sh Anaconda3-2019.10-Linux-x86_64.sh  You will also need to run these commands on your shell to add some commands to your ~/.bashrc file, and update the conda distribution with the latest libraries versions.\ncat \u0026gt;\u0026gt; ~/.bashrc \u0026lt;\u0026lt; 'EOF' export PATH=$HOME/anaconda3/bin:${PATH} EOF source .bashrc conda upgrade -y --all  The next step is creating a new environment for your deep learning pursuits or using an existing one. I created a new Conda environment using:\nconda create --name py37  Here py37 is the name we provide to this new conda environment. You can activate this conda environment using:\nconda activate py37  You should now be able to see something like:\n Notice the py37 at the start of command in terminal\nWe can now add all our required packages to this environment using pip or conda. The latest version 1.3, as seen from the pytorch site , is not yet available for CUDA 10.2, as I already mentioned, so we are in luck with CUDA 10.1. Also, we will need to specify the version of TensorFlow as 2.1.0, as this version was built using 10.1 CUDA.\nI also install RAPIDS, which is a library to get your various data science workloads to GPUs. Why use GPUs only for deep learning and not for Data processing? You can get the command to install rapids from the rapids release selector :\n sudo apt install python3-pip conda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.11 python=3.7 cudatoolkit=10.1 pip install torchvision  Since PyTorch installation interfered with TensorFlow, I installed TensorFlow in another environment.\nconda create --name tf conda activate tf pip install --upgrade tensorflow  Now we can check if the TF and Pytorch installations are correctly done by using the below commands in their own environments:\n# Should print True python3 -c \u0026quot;import tensorflow as tf; print(tf.test.is_gpu_available())\u0026quot; # should print cuda python3 -c \u0026quot;import torch; print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\u0026quot;  If the install is showing some errors for TensorFlow or the GPU test is failing, you might want to add these two additional lines at the end of your bashrc file and restart the terminal:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 export CUDA_HOME=/usr/local/cuda  You might also want to install jupyter lab or jupyter notebook. Thanks to the developers, the process is as easy as just running jupyter labor jupyter notebook in your terminal, whichever you do prefer. I personally like notebook better without all the unnecessary clutter.\nConclusion In this post, I talked about all the software you are going to need to install in your deep learning rig without hassle.\nYou might still need some help and face some problems for which my best advice would be to check out the different NVIDIA and Stack Overflow forums.\nSo we have got our deep learning rig setup, and its time for some tests now. In the next few posts, I am going to do some benchmarking on the GPUs and will try to write more on various deep Learning libraries one can include in their workflow. So stay tuned.\nContinue Learning If you want to learn more about Deep Learning, here is an excellent course. You can start for free with the 7-day Free Trial.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/06/06/dlrig/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Computer Vision","Productivity"],"title":"A definitive guide for Setting up a Deep Learning Workstation with Ubuntu"},{"categories":null,"contents":"Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.\nAs Data Science and Web developers try to collaborate, API‚Äôs become an essential piece of the puzzle to make codes as well as skills more modular. In fact, in the same way, that a data scientist can‚Äôt be expected to know much about Javascript or nodeJS, a frontend developer should be able to get by without knowing any Data Science Language. And APIs do play a considerable role in this abstraction.\nBut, APIs are confusing. I myself have been confused a lot while creating and sharing them with my development teams who talk in their API terminology like GET request, PUT request, endpoint, Payloads, etc.\nThis post will be about simplifying and understanding how APIs work, explaining some of the above terms, and creating an API using the excellent API building framework called FastAPI, which makes creating APIs a breeze.\n What is an API? Before we go any further, we need to understand what an API is. According to Wikipedia:\n An application programming interface (API) is a computing interface which defines interactions between multiple software intermediaries. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow, etc.\n The way I like to understand an API is that it‚Äôs an ‚Äúonline function,‚Äù a function that I can call online.\nFor example:\nI can have a movie API, which returns me names of drama movies when I pass the ‚Äúanimation‚Äù genre as input.\n The advantage of using such a sort of mechanism is that the API user doesn‚Äôt get access to the whole dataset or source code and yet they can get all the information they need. This is how many services on the internet like Amazon Rekognition , which is an image and video API, or Google Natural Language API , which is an NLP API works. They provide us access to some great functions without letting us have the source code, which is often valuable and kept hidden. For example, I can send an image to Amazon Rekognition API, and it can provide me with Face detection and Analysis.\n For example, here is a free API floated by Open Movie DB, which lets us search for movies using parameters:\n[http://www.omdbapi.com/?i=tt3896198\u0026amp;apikey=9ea43e94](http://www.omdbapi.com/?i=tt3896198\u0026amp;apikey=9ea43e94)  Here I provided the IMDB id for the movie Guardians of the Galaxy 2, using the i parameter for the API. If you open this link in your browser, you will see the whole information of the movie as per the Open Movie Database\n Output from OMDB\nBut before we go any further, let‚Äôs understand some terms:\n  Endpoint: In the above API call, the endpoint is : http://www.omdbapi.com/ . Simply this is the location of where the function code is running.\n  API Access Key: Most of the public APIs will have some access key, which you can request. For OMDB API, I had to register and get the API key which is 9ea43e94 .\n  **? Operator:**This operator is used to specify the parameters we want to send to the API or our ‚Äúonline function.‚Äù Here we give two params to our API i.e., IMDB movie ID and API Access Key using the ? operator. Since there are multiple inputs, we use \u0026amp; operator also.\n   Why FastAPI? ‚ÄúIf you‚Äôre looking to learn one modern framework for building REST APIs, check out FastAPI [‚Ä¶] It‚Äôs fast, easy to use and easy to learn [‚Ä¶]‚Äù ‚Äî spaCy creators\nWhile Python has many frameworks to build APIs, the most common being Flask and Tornado, FastAPI is much better than available alternatives in its ease of usage as it seems much more pythonic in comparison with Flask.\nAlso, FastAPI is fast. As the Github docs say, ‚ÄúVery high performance, on par with NodeJS and Go.‚Äù We can also check the latency benchmarks for ourselves.\n That is around a speedup by a factor of 2 when compared to Flask and that too without a lot of code change. This means a huge deal when it comes to building an API that can serve millions of customers as it can reduce production efforts and also use less expensive hardware to serve.\nSo enough of comparison and talk, let‚Äôs try to use FastAPI to create our API.\n How to write an API with FastAPI? One of the most common use cases for Data Science is how to create an API for getting a model‚Äôs prediction? Let us assume that we have a Titanic Survival model in place that predicts if a person will survive or not. And, it needs a person‚Äôs age and sex as input params to predict. We will create this API using FastAPI in two ways: GET and PUT. Don‚Äôt worry; I will explain each as we go.\nWhat is GET? ‚Äî In a GET request, we usually try to retrieve data using query parameters that are embedded in the query string itself. For example, in the OMDB API, we use the GET request to specify the movie id and access key as part of the query itself.\nWhat is PUT? ‚Äî An alternative to the GET request is the PUT request, where we send parameters using a payload, as we will see in the second method. The payload is not part of the query string, and thus PUT is more secure. It will become more clear when you see the second part.\nBut before we go any further, we need to install FastAPI and uvicorn ASGI server with:\npip install fastapi pip install uvicorn  1. The GET Way: A simple FastAPI method to writing a GET API for our titanic model use case is as follows:\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026quot;/predict\u0026quot;) def predict_complex_model(age: int,sex:str): # Assume a big and complex model here. For this test I am using a simple rule based model if age\u0026lt;10 or sex=='F': return {'survived':1} else: return {'survived':0}  Save the above code in a file named fastapiapp.py and then you can run it using the below command on terminal.\n$ uvicorn fastapiapp:app --reload  The above means that your API is now running on your server, and the \u0026ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this \u0026ndash;reload flag when you put the API in production. Now you can visit the below path in your browser, and you will get the prediction results:\n[http://127.0.0.1:8000/predict?age=10\u0026amp;sex=M](http://127.0.0.1:8000/predict?age=10\u0026amp;sex=M)   What happens is as you hit the command in your browser, it calls the http://127.0.0.1:8000/ predict endpoint which in turn calls the associated method predict_complex_model with the with params age=10 and sex=\u0026lsquo;M\u0026rsquo;\nSo, it allows us to use our function from a browser, but that‚Äôs still not very helpful. Your developer friend needs to use your predict function to show output on a frontend website. How can you provide him with access to this function?\nIt is pretty simple. If your developer friend also uses Python, for example, he can use the requests module like below:\nimport requests age = 15 sex = \u0026quot;F\u0026quot; response = requests.get(f\u0026quot;[http://127.0.0.1:8000/predict?age={age}\u0026amp;sex={](http://127.0.0.1:8000/predict?age=10\u0026amp;sex=M)sex}\u0026quot;) output = response.json()   So we can get the output from the running API(on the server) into our Python Program. A Javascript user would use Javascript Request Library, and a nodeJS developer will use something similar to do this in nodeJS. We will just need to provide them with the endpoint and parameters required.\nTo test your API, you could also go to the:\n[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  Where you will find a GUI way to test your API.\n But as we said earlier, THIS IS NOT SECURE as GET parameters are passed via URL. This means that parameters get stored in server logs and browser history. This is not intended. Further, this toy example just had two input parameters, so we were able to do it this way, think of a case where we need to provide many parameters to our predict function.\nIn such a case or I dare say in most of the cases, we use the PUT API.\n2. The PUT Way Using the PUT API, we can call any function by providing a payload to the function. A payload is nothing but a JSON dictionary of input parameters that doesn‚Äôt get appended to the query string and is thus much more secure than GET.\nHere is the minimal example where we do that same thing as before using PUT. We just change the content of fastapiapp.py to:\nfrom fastapi import FastAPI from pydantic import BaseModel class Input(BaseModel): age : int sex : str app = FastAPI() [@app](http://twitter.com/app).put(\u0026quot;/predict\u0026quot;) def predict_complex_model(d:Input): if d.age\u0026lt;10 or d.sex=='F': return {'survived':1} else: return {'survived':0}  note that we use app.put here in place of app.get previously. We also needed to provide a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user while previously in GET, we validated the inputs using the function parameter list. Also, this time you won‚Äôt be able to see your content using a URL on the web. For example, using the browser to point to the endpoint location gives:\n So, we can check using the programmatic way using requests in Python again:\nimport requests,json payload = json.dumps({ \u0026quot;age\u0026quot;: 10, \u0026quot;sex\u0026quot;: \u0026quot;F\u0026quot; }) response = requests.put(\u0026quot;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)\u0026quot;,data = payload) response.json()   Notice that we use requests.put here and we provide the payload using the data param in the requests.put function and we also make use of json library to convert our payload to JSON from a dict object.\nWe could also have used the GUI way as before using:\n[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)   And, we are done with creating our API. It was simple for a change.\nFastAPI makes the API creation, which used to be one of the dreaded parts of the Data Science process, much more intuitive, easy, and Fast.\nYou can find the code for this post as well as all my posts at my GitHub repository.\n Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/","tags":["Machine Learning","Data Science","Production","Productivity","Tools","Programming","deployment"],"title":"A Layman‚Äôs Guide for Data Scientists to create APIs in minutes"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook‚Äôs tagging feature works? Or how Google Lens recognizes products through images?\nAll of the above are examples of image classification in different settings. Multiclass image classification is a common task in computer vision, where we categorize an image into three or more classes.\nIn the past, I always used Keras for computer vision projects. However, recently when the opportunity to work on multiclass image classification presented itself, I decided to use PyTorch. I have already moved from Keras to PyTorch for all NLP tasks , so why not vision, too?\n  PyTorch is powerful, and I also like its more pythonic structure.\n In this post, we‚Äôll create an end to end pipeline for image multiclass classification using Pytorch. This will include training the model, putting the model‚Äôs results in a form that can be shown to business partners, and functions to help deploy the model easily. As an added feature we will look at Test Time Augmentation using Pytorch also.\nBut before we learn how to do image classification, let‚Äôs first look at transfer learning, the most common method for dealing with such problems.\n What is Transfer Learning? Transfer learning is the process of repurposing knowledge from one task to another. From a modelling perspective, this means using a model trained on one dataset and fine-tuning it for use with another. But why does it work?\nLet‚Äôs start with some background. Every year the visual recognition community comes together for a very particular challenge: The Imagenet Challenge . The task in this challenge is to classify 1,000,000 images into 1,000 categories.\nThis challenge has already resulted in researchers training big convolutional deep learning models. The results have included great models like Resnet50 and Inception.\nBut, what does it mean to train a neural model? Essentially, it means the researchers have learned the weights for a neural network after training the model on a million images.\nSo, what if we could get those weights? We could then use them and load them into our own neural networks model to predict on the test dataset, right? Actually, we can go even further than that; we can add an extra layer on top of the neural network these researchers have prepared to classify our own dataset.\n While the exact workings of these complex models is still a mystery, we do know that the lower convolutional layers capture low-level image features like edges and gradients. In comparison, higher convolutional layers capture more and more intricate details, such as body parts, faces, and other compositional features.\n Source: Source: Visualizing and Understanding Convolutional Networks . You can see how the first few layers capture basic shapes, and the shapes become more and more complex in the later layers.\nIn the example above from ZFNet (a variant of Alexnet), one of the first convolutional neural networks to achieve success on the Imagenet task, you can see how the lower layers capture lines and edges, and the later layers capture more complex features. The final fully-connected layers are generally assumed to capture information that is relevant for solving the respective task, e.g. ZFNet‚Äôs fully-connected layers indicate which features are relevant for classifying an image into one of 1,000 object categories.\nFor a new vision task, it is possible for us to simply use the off-the-shelf features of a state-of-the-art CNN pre-trained on ImageNet, and train a new model on these extracted features.\nThe intuition behind this idea is that a model trained to recognize animals might also be used to recognize cats vs dogs. In our case,\n a model that has been trained on 1000 different categories has seen a lot of real-world information, and we can use this information to create our own custom classifier.\n So that‚Äôs the theory and intuition. How do we get it to actually work? Let‚Äôs look at some code. You can find the complete code for this post on Github .\n Data Exploration We will start with the Boat Dataset from Kaggle to understand the multiclass image classification problem. This dataset contains about 1,500 pictures of boats of different types: buoys, cruise ships, ferry boats, freight boats, gondolas, inflatable boats, kayaks, paper boats, and sailboats. Our goal is to create a model that looks at a boat image and classifies it into the correct category.\nHere‚Äôs a sample of images from the dataset:\n And here are the category counts:\n Since the categories ‚Äúfreight boats‚Äù, ‚Äúinflatable boats‚Äù , and ‚Äúboats‚Äù don‚Äôt have a lot of images; we will be removing these categories when we train our model.\n Creating the required Directory Structure Before we can go through with training our deep learning models, we need to create the required directory structure for our images. Right now, our data directory structure looks like:\nimages sailboat kayak . .  We need our images to be contained in 3 folders train, val and test. We will then train on the images in train dataset, validate on the ones in the val dataset and finally test them on images in the test dataset.\ndata train sailboat kayak . . val sailboat kayak . . test sailboat kayak . .  You might have your data in a different format, but I have found that apart from the usual libraries, the glob.glob and os.system functions are very helpful. Here you can find the complete data preparation code . Now let‚Äôs take a quick look at some of the not-so-used libraries that I found useful while doing data prep.\nWhat is glob.glob? Simply, glob lets you get names of files or folders in a directory using a regex. For example, you can do something like:\nfrom glob import glob categories = glob(‚Äúimages/*‚Äù) print(categories) ------------------------------------------------------------------ ['images/kayak', 'images/boats', 'images/gondola', 'images/sailboat', 'images/inflatable boat', 'images/paper boat', 'images/buoy', 'images/cruise ship', 'images/freight boat', 'images/ferry boat']  What is os.system? os.system is a function in os library which lets you run any command-line function in python itself. I generally use it to run Linux functions, but it can also be used to run R scripts within python as shown here . For example, I use it in my data preparation to copy files from one directory to another after getting the information from a pandas data frame. I also use f string formatting .\nimport os for i,row in fulldf.iterrows(): # Boat category cat = row['category'] # section is train,val or test section = row['type'] # input filepath to copy ipath = row['filepath'] # output filepath to paste opath = ipath.replace(f\u0026quot;images/\u0026quot;,f\u0026quot;data/{section}/\u0026quot;) # running the cp command os.system(f\u0026quot;cp '{ipath}' '{opath}'\u0026quot;)  Now since we have our data in the required folder structure, we can move on to more exciting parts.\n Data Preprocessing Transforms: 1. Imagenet Preprocessing\nIn order to use our images with a network trained on the Imagenet dataset, we need to preprocess our images in the same way as the Imagenet network. For that, we need to rescale the images to 224√ó224 and normalize them as per Imagenet standards. We can use the torchvision transforms library to do that. Here we take a CenterCrop of 224√ó224 and normalize as per Imagenet standards. The operations defined below happen sequentially. You can find a list of all transforms provided by PyTorch here .\ntransforms.Compose([ transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])  2. Data Augmentations\nWe can do a lot more preprocessing for data augmentations. Neural networks work better with a lot of data. Data augmentation is a strategy which we use at training time to increase the amount of data we have.\nFor example, we can flip the image of a boat horizontally, and it will still be a boat. Or we can randomly crop images or add color jitters. Here is the image transforms dictionary I have used that applies to both the Imagenet preprocessing as well as augmentations. This dictionary contains the various transforms we have for the train, test and validation data as used in this great post . As you‚Äôd expect, we don‚Äôt apply the horizontal flips or other data augmentation transforms to the test data and validation data because we don‚Äôt want to get predictions on an augmented image.\n# Image transformations image_transforms = { # Train uses data augmentation 'train': transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]), # Validation does not use augmentation 'valid': transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), # Test does not use augmentation 'test': transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), }  Here is an example of the train transforms applied to an image in the training dataset. Not only do we get a lot of different images from a single image, but it also helps our network become invariant to the object orientation.\nex_img = Image.open('/home/rahul/projects/compvisblog/data/train/cruise ship/cruise-ship-oasis-of-the-seas-boat-water-482183.jpg') t = image_transforms['train'] plt.figure(figsize=(24, 24)) for i in range(16): ax = plt.subplot(4, 4, i + 1) _ = imshow_tensor(t(ex_img), ax=ax) plt.tight_layout()   DataLoaders The next step is to provide the training, validation, and test dataset locations to PyTorch. We can do this by using the PyTorch datasets and DataLoader class. This part of the code will mostly remain the same if we have our data in the required directory structures.\n# Datasets from folders traindir = \u0026quot;data/train\u0026quot; validdir = \u0026quot;data/val\u0026quot; testdir = \u0026quot;data/test\u0026quot; data = { 'train': datasets.ImageFolder(root=traindir, transform=image_transforms['train']), 'valid': datasets.ImageFolder(root=validdir, transform=image_transforms['valid']), 'test': datasets.ImageFolder(root=testdir, transform=image_transforms['test']) } # Dataloader iterators, make sure to shuffle dataloaders = { 'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True,num_workers=10), 'val': DataLoader(data['valid'], batch_size=batch_size, shuffle=True,num_workers=10), 'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True,num_workers=10) }  These dataloaders help us to iterate through the dataset. For example, we will use the dataloader below in our model training. The data variable will contain data in the form (batch_size, color_channels, height, width) while the target is of shape (batch_size) and hold the label information.\ntrain_loader = dataloaders['train'] for ii, (data, target) in enumerate(train_loader):   Modeling 1. Create the model using a pre-trained model Right now these following pre-trained models are available to use in the torchvision library:\n   AlexNet    VGG    ResNet    SqueezeNet    DenseNet    Inception v3\n   GoogLeNet    ShuffleNet v2\n   MobileNet v2\n   ResNeXt    Wide ResNet    MNASNet   Here I will be using resnet50 on our dataset, but you can effectively use any other model too as per your choice.\nfrom torchvision import models model = models.resnet50(pretrained=True)  We start by freezing our model weights since we don‚Äôt want to change the weights for the renet50 models.\n# Freeze model weights for param in model.parameters(): param.requires_grad = False  The next thing we need to do is to replace the linear classification layer in the model by our custom classifier. I have found that to do this, it is better first to see the model structure to determine what is the final linear layer. We can do this simply by printing the model object:\nprint(model) ------------------------------------------------------------------ ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) . . . . (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) **(fc): Linear(in_features=2048, out_features=1000, bias=True)** )  Here we find that the final linear layer that takes the input from the convolutional layers is named fc\nWe can now simply replace the fc layer using our custom neural network. This neural network takes input from the previous layer to fc and gives the log softmax output of shape (batch_size x n_classes).\nn_inputs = model.fc.in_features model.fc = nn.Sequential( nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))  Please note that the new layers added now are fully trainable by default.\n2. Load the model on GPU We can use a single GPU or multiple GPU(if we have them) using DataParallel from PyTorch. Here is what we can use to detect the GPU as well as the number of GPUs to load the model on GPU. Right now I am training my models on dual NVIDIA Titan RTX GPUs.\n# Whether to train on a gpu train_on_gpu = cuda.is_available() print(f'Train on gpu: {train_on_gpu}') # Number of gpus if train_on_gpu: gpu_count = cuda.device_count() print(f'{gpu_count} gpus detected.') if gpu_count \u0026gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to('cuda') if multi_gpu: model = nn.DataParallel(model)  3. Define criterion and optimizers One of the most important things to notice when you are training any model is the choice of loss-function and the optimizer used. Here we want to use categorical cross-entropy as we have got a multiclass classification problem and the Adam optimizer, which is the most commonly used optimizer. But since we are applying a LogSoftmax operation on the output of our model, we will be using the NLL loss.\nfrom torch import optim criteration = nn.NLLLoss() optimizer = optim.Adam(model.parameters())  4. Training the model Given below is the full code used to train the model. It might look pretty big on its own, but essentially what we are doing is as follows:\n  Start running epochs. In each epoch-\n  Set the model mode to train using model.train().\n  Loop through the data using the train dataloader.\n  Load your data to the GPU using the data, target = data.cuda(), target.cuda() command\n  Set the existing gradients in the optimizer to zero using optimizer.zero_grad()\n  Run the forward pass through the batch using output = model(data)\n  Compute loss using loss = criterion(output, target)\n  Backpropagate the losses through the network using loss.backward()\n  Take an optimizer step to change the weights in the whole network using optimizer.step()\n  All the other steps in the training loop are just to maintain the history and calculate accuracy.\n  Set the model mode to eval using model.eval().\n  Get predictions for the validation data using valid_loader and calculate valid_loss and valid_acc\n  Print the validation loss and validation accuracy results every print_every epoch.\n  Save the best model based on validation loss.\n  Early Stopping: If the cross-validation loss doesn‚Äôt improve for max_epochs_stop stop the training and load the best available model with the minimum validation loss.\n  def train(model, criterion, optimizer, train_loader, valid_loader, save_file_name, max_epochs_stop=3, n_epochs=20, print_every=1): \u0026#34;\u0026#34;\u0026#34;Train a PyTorch Model Params -------- model (PyTorch model): cnn to train criterion (PyTorch loss): objective to minimize optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters train_loader (PyTorch dataloader): training dataloader to iterate through valid_loader (PyTorch dataloader): validation dataloader used for early stopping save_file_name (str ending in \u0026#39;.pt\u0026#39;): file path to save the model state dict max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping n_epochs (int): maximum number of training epochs print_every (int): frequency of epochs to print training stats Returns -------- model (PyTorch model): trained cnn with best weights history (DataFrame): history of train and validation loss and accuracy \u0026#34;\u0026#34;\u0026#34; # Early stopping intialization epochs_no_improve = 0 valid_loss_min = np.Inf valid_max_acc = 0 history = [] # Number of epochs already trained (if using loaded in model weights) try: print(f\u0026#39;Model has been trained for: {model.epochs} epochs.\\n\u0026#39;) except: model.epochs = 0 print(f\u0026#39;Starting Training from Scratch.\\n\u0026#39;) overall_start = timer() # Main loop for epoch in range(n_epochs): # keep track of training and validation loss each epoch train_loss = 0.0 valid_loss = 0.0 train_acc = 0 valid_acc = 0 # Set to training model.train() start = timer() # Training loop for ii, (data, target) in enumerate(train_loader): # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Clear gradients optimizer.zero_grad() # Predicted outputs are log probabilities output = model(data) # Loss and backpropagation of gradients loss = criterion(output, target) loss.backward() # Update the parameters optimizer.step() # Track train loss by multiplying average loss by number of examples in batch train_loss += loss.item() * data.size(0) # Calculate accuracy by finding max log probability _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) # Need to convert correct tensor from int to float to average accuracy = torch.mean(correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples in batch train_acc += accuracy.item() * data.size(0) # Track training progress print( f\u0026#39;Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.\u0026#39;, end=\u0026#39;\\r\u0026#39;) # After training loops ends, start validation else: model.epochs += 1 # Don\u0026#39;t need to keep track of gradients with torch.no_grad(): # Set to evaluation mode model.eval() # Validation loop for data, target in valid_loader: # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Forward pass output = model(data) # Validation loss loss = criterion(output, target) # Multiply average loss times the number of examples in batch valid_loss += loss.item() * data.size(0) # Calculate validation accuracy _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) accuracy = torch.mean( correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples valid_acc += accuracy.item() * data.size(0) # Calculate average losses train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) # Calculate average accuracy train_acc = train_acc / len(train_loader.dataset) valid_acc = valid_acc / len(valid_loader.dataset) history.append([train_loss, valid_loss, train_acc, valid_acc]) # Print training and validation results if (epoch + 1) % print_every == 0: print( f\u0026#39;\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}\u0026#39; ) print( f\u0026#39;\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\tValidation Accuracy: {100 * valid_acc:.2f}%\u0026#39; ) # Save the model if validation loss decreases if valid_loss \u0026lt; valid_loss_min: # Save model torch.save(model.state_dict(), save_file_name) # Track improvement epochs_no_improve = 0 valid_loss_min = valid_loss valid_best_acc = valid_acc best_epoch = epoch # Otherwise increment count of epochs with no improvement else: epochs_no_improve += 1 # Trigger early stopping if epochs_no_improve \u0026gt;= max_epochs_stop: print( f\u0026#39;\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%\u0026#39; ) total_time = timer() - overall_start print( f\u0026#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.\u0026#39; ) # Load the best state dict model.load_state_dict(torch.load(save_file_name)) # Attach the optimizer model.optimizer = optimizer # Format history history = pd.DataFrame( history, columns=[ \u0026#39;train_loss\u0026#39;, \u0026#39;valid_loss\u0026#39;, \u0026#39;train_acc\u0026#39;, \u0026#39;valid_acc\u0026#39; ]) return model, history # Attach the optimizer model.optimizer = optimizer # Record overall time and print out stats total_time = timer() - overall_start print( f\u0026#39;\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%\u0026#39; ) print( f\u0026#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.\u0026#39; ) # Format history history = pd.DataFrame( history, columns=[\u0026#39;train_loss\u0026#39;, \u0026#39;valid_loss\u0026#39;, \u0026#39;train_acc\u0026#39;, \u0026#39;valid_acc\u0026#39;]) return model, history # Running the model model, history = train( model, criterion, optimizer, dataloaders[\u0026#39;train\u0026#39;], dataloaders[\u0026#39;val\u0026#39;], save_file_name=save_file_name, max_epochs_stop=3, n_epochs=100, print_every=1) Here is the output from running the above code. Just showing the last few epochs. The validation accuracy started at ~55% in the first epoch, and we ended up with a validation accuracy of ~90%.\n And here are the training curves showing the loss and accuracy metrics:\n   Inference and Model Results We want our results in different ways to use our model. For one, we require test accuracies and confusion matrices. All of the code for creating these results is in the code notebook.\n1. Test Results The overall accuracy of the test model is:\nOverall Accuracy: 88.65 %  Here is the confusion matrix for results on the test dataset.\n We can also look at the category wise accuracies. I have also added the train counts to see the results from a new perspective.\n 2. Visualizing Predictions for Single Image For deployment purposes, it helps to be able to get predictions for a single image. You can get the code from the notebook.\n 3. Visualizing Predictions for a Category We can also see the category wise results for debugging purposes and presentations.\n 4. Test results with Test Time Augmentation We can also do test time augmentation to increase our test accuracy. Here I am using a new test data loader and transforms:\n# Image transformations tta_random_image_transforms = transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]) # Datasets from folders ttadata = { 'test': datasets.ImageFolder(root=testdir, transform=tta_random_image_transforms) } # Dataloader iterators ttadataloader = { 'test': DataLoader(ttadata['test'], batch_size=512, shuffle=False,num_workers=10) }  We can then get the predictions on the test set using the below function:\ndef tta_preds_n_averaged(model, test_loader,n=5): \u0026#34;\u0026#34;\u0026#34;Returns the TTA preds from a trained PyTorch model Params -------- model (PyTorch model): trained cnn for inference test_loader (PyTorch DataLoader): test dataloader Returns -------- results (array): results for each category \u0026#34;\u0026#34;\u0026#34; # Hold results results = np.zeros((len(test_loader.dataset), n_classes)) bs = test_loader.batch_size model.eval() with torch.no_grad(): #aug loop: for _ in range(n): # Testing loop tmp_pred = np.zeros((len(test_loader.dataset), n_classes)) for i,(data, targets) in enumerate(tqdm.tqdm(test_loader)): # Tensors to gpu if train_on_gpu: data, targets = data.to(\u0026#39;cuda\u0026#39;), targets.to(\u0026#39;cuda\u0026#39;) # Raw model output out = model(data) tmp_pred[i*bs:(i+1)*bs] = np.array(out.cpu()) results+=tmp_pred return results/n In the function above, I am applying the tta_random_image_transforms to each image 5 times before getting its prediction. The final prediction is the average of all five predictions. When we use TTA over the whole test dataset, we noticed that the accuracy increased by around 1%\nTTA Accuracy: 89.71%  Also, here is the results for TTA compared to normal results category wise:\n In this small dataset, the TTA might not seem to add much value, but I have noticed that it adds value with big datasets.\n Conclusion In this post, I talked about the end to end pipeline for working on a multiclass image classification project using PyTorch. We worked on creating some readymade code to train a model using transfer learning, visualize the results, use Test time augmentation, and got predictions for a single image so that we can deploy our model when needed using any tool like Streamlit .\nYou can find the complete code for this post on Github .\nIf you would like to learn more about Image Classification and Convolutional Neural Networks take a look at the Deep Learning Specialization from Andrew Ng. Also, to learn more about PyTorch and start from the basics, you can take a look at the Deep Neural Networks with PyTorch course offered by IBM.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\nThis post was first published here .\n","permalink":"https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Awesome Guides","Image classification"],"title":"End to End Pipeline for setting up Multiclass Image Classification for Data Scientists"},{"categories":["Deep Learning","Data science"],"contents":"With the advent of so many computing and serving frameworks, it is getting stressful day by day for the developers to put a model into production . If the question of what model performs best on my data was not enough, now the question is what framework to choose for serving a model trained with Sklearn or LightGBM or PyTorch . And new frameworks are being added as each day passes.\nSo is it imperative for a Data Scientist to learn a different framework because a Data Engineer is comfortable with that, or conversely, does a Data Engineer need to learn a new platform that the Data Scientist favors?\nAdd to that the factor of speed and performance that these various frameworks offer, and the question suddenly becomes even more complicated.\nSo, I was pleasantly surprised when I came across the Hummingbird project on Github recently, which aims to answer this question or at least takes a positive step in the right direction.\n So, What is HummingBird?  As per their documentation:\n Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch ) to accelerate traditional ML models. Thanks to Hummingbird, users can benefit from:\n  (1) all the current and future optimizations implemented in neural network frameworks;\n  (2) native hardware acceleration;\n  (3) having a unique platform to support both traditional and neural network models, and have all of this\n  (4) without having to re-engineer their models.\n Put even more simply; you can now convert your models written in Scikit-learn or Xgboost or LightGBM into PyTorch models and gain the performance benefits of Pytorch while inferencing.\nAs of right now, Here is the list of operators Hummingbird supports with more on the way.\n A Simple Example We can start by installing Hummingbird, which is as simple as:\npip install hummingbird-ml  To use hummingbird, I will begin with a minimal example on a small random classification Dataset. We start by creating a sample dataset with 100,000 rows and using a RandomForestClassifier on top of that.\nimport numpy as np from sklearn.ensemble import RandomForestClassifier from hummingbird.ml import convert # Create some random data for binary classification from sklearn import datasets X, y = datasets.make_classification(n_samples=100000, n_features=28) # Create and train a model (scikit-learn RandomForestClassifier in this case) skl_model = RandomForestClassifier(n_estimators=1000, max_depth=10) skl_model.fit(X, y)  What hummingbird helps us with is to convert this sklearn model into a PyTorch model by just using the simple command:\n# Using Hummingbird to convert the model to PyTorch model = convert(skl_model, 'pytorch') print(type(model)) -------------------------------------------------------- hummingbird.ml._container.PyTorchBackendModel  We can now load our new Pytorch model to GPU using:\nmodel.to('cuda')  This is great. So, we can convert from sklearn model to a PyTorch model, which should run faster on a GPU. But by how much?\nLet us see a simple performance comparison.\n Comparison 1. Batch Mode We will start by using the sklearn model to predict the whole train dataset and check out the time it takes.\n We can do the same with our new PyTorch model:\n That is a speedup of 9580/195 ~ 50x.\n2. Single Example Prediction We predict a single example here to see how the model would perform in a real-time setting. The sklearn model:\n vs. Pytorch model\n That is again a speedup of 79.6/1.6 ~50x.\n Small Caveat A small caveat I experienced is that the predictions from the sklearn model and the hummingbird PyTorch model were not exactly the same.\nFor example, here are the predictions I got from both models:\n Yes, sometimes, they differ in the 7th digit, which might be a function of the conversion process. I think that it won‚Äôt change the final 1 or 0 predictions much. We can also check that:\nscikit_1_0 = scikit_preds[:,1]\u0026gt;0.5 hb_1_0 = hb_preds[:,1]\u0026gt;0.5 print(len(scikit_1_0) == sum(scikit_1_0==hb_1_0)) ------------------------------------------------------------ True  So, for this case, both the models exactly gave the same 1 or 0 predictions for the whole dataset of 100,000 rows.\nSo I guess it is okay.\n Conclusion The developers at Microsoft are still working on adding many more operators which range from models to feature engineering like MinMaxScaler or LabelEncoder to the code, and I am hopeful that they will further develop and improve this project. Here is the roadmap to development if you are interested.\nAlthough Hummingbird is not perfect yet, it is the first system able to run classical ML inference DNN frameworks and proves them mature enough to be used as generic compilers. I will try to include it in my development workflow when it comes to making predictions at high throughput or latency.\nYou can find the code for this post as well as all my posts at my GitHub repository.\nContinue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/","tags":["Deep Learning","Artificial Intelligence","Productivity","Python","Machine Learning"],"title":"How to run your ML model Predictions 50 times faster?"},{"categories":["Big Data","Data Science","Awesome Guides"],"contents":"Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.\nReason: Too much data is getting generated day by day\nAnd that brings us to Spark which is one of the most used tools when it comes to working with Big Data.\nWhile once upon a time Spark used to be heavily reliant on RDD manipulations , Spark has now provided a DataFrame API for us Data Scientists to work with. Here is the documentation for the adventurous folks. But while the documentation is good, it does not explain it from the perspective of a Data Scientist. Neither does it properly document the most common use cases for Data Science.\nIn this post, I will talk about installing Spark, standard Spark functionalities you will need to work with DataFrames, and finally some tips to handle the inevitable errors you will face.\nThis post is going to be quite long. Actually one of my longest posts on medium, so go on and pick up a Coffee.\nAlso here is the Table of Contents, if you want to skip to a specific section:\n Installation Data   Basic Functions   Read See a few rows in the file Change Column Names Select Columns Sort Cast Filter GroupBy Joins    Broadcast/Map Side Joins    Use SQL with DataFrames    Create New Columns   Using Spark Native Functions Using Spark UDFs Using RDDs Using Pandas UDF    Spark Window Functions   Ranking Lag Variables Rolling Aggregations    Pivot Dataframes    Unpivot/Stack Dataframes    Salting   Some More Tips and Tricks  Caching Save and Load from an intermediate step Repartitioning Reading Parquet File in Local   Conclusion   Installation I am working on installing Spark on Ubuntu 18.04, but the steps should remain the same for MAC too. I am assuming that you already have Anaconda and Python3 installed. After that, you can just go through these steps:\n Download the Spark Binary from Apache Spark Website . And click on the Download Spark link to download Spark.   Once you have downloaded the above file, you can start with unzipping the file in your home directory. Just Open up the terminal and put these commands in.\ncd ~ cp Downloads/spark-2.4.5-bin-hadoop2.7.tgz ~ tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz  Check your Java Version. As of version 2.4 Spark works with Java 8. You can check your Java Version using the command java -version on the terminal window.\nI had Java 11 in my machine, so I had to run the following commands on my terminal to install and change default Java to Java 8:\nsudo apt install openjdk-8-jdk sudo update-alternatives --config java  You will need to manually select the Java version 8 by typing the selection number.\n Rechecking Java version should give something like:\n Edit your ~/.bashrc file and add the following lines at the end of the file:\nfunction pysparknb () { #Spark path SPARK_PATH=~/spark-2.4.5-bin-hadoop2.7 export PYSPARK_DRIVER_PYTHON=\u0026quot;jupyter\u0026quot; export PYSPARK_DRIVER_PYTHON_OPTS=\u0026quot;notebook\u0026quot; # For pyarrow 0.15 users, you have to add the line below or you will get an error while using pandas_udf export ARROW_PRE_0_15_IPC_FORMAT=1 **# Change the local[10] to local[numCores in your machine]** $SPARK_PATH/bin/pyspark --master **local[10]** }  Source ~/.bashrc\nsource ~/.bashrc  Run the pysparknb function in the terminal and you will be able to access the notebook. You will be able to open a new notebook as well as the sparkcontext will be loaded automatically.\npysparknb    Data With the installation out of the way, we can move to the more interesting part of this post. I will be working with the Data Science for COVID-19 in South Korea , which is one of the most detailed datasets on the internet for COVID.\nPlease note that I will be using this dataset to showcase some of the most useful functionalities of Spark, but this should not be in any way considered a data exploration exercise for this amazing dataset.\nSource: I will mainly work with the following three tables only in this post:\n  Cases\n  Region\n  TimeProvince\n  You can find all the code at the GitHub repository.\n 1. Basic Functions Read We can start by loading the files in our dataset using the spark.read.load command. This command reads parquet files, which is the default file format for spark, but you can add the parameter format to read .csv files using it.\ncases = spark.read.load(\u0026quot;/home/rahul/projects/sparkdf/coronavirusdataset/Case.csv\u0026quot;,format=\u0026quot;csv\u0026quot;, sep=\u0026quot;,\u0026quot;, inferSchema=\u0026quot;true\u0026quot;, header=\u0026quot;true\u0026quot;)  See a few rows in the file cases.show()   This file contains the cases grouped by way of the infection spread. This might have helped in the rigorous tracking of Corona Cases in South Korea.\nThe way this file looks is great right now, but sometimes as we increase the number of columns, the formatting becomes not too great. I have noticed that the following trick helps in displaying in pandas format in my Jupyter Notebook. The .toPandas() function converts a spark dataframe into a pandas Dataframe which is easier to show.\ncases.limit(10).toPandas()   Change Column Names Sometimes we would like to change the name of columns in our Spark Dataframes. We can do this simply using the below command to change a single column:\ncases = cases.withColumnRenamed(\u0026quot;infection_case\u0026quot;,\u0026quot;infection_source\u0026quot;)  Or for all columns:\ncases = cases.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed', 'latitude', 'longitude'])  Select Columns We can select a subset of columns using the select keyword.\ncases = cases.select('province','city','infection_case','confirmed') cases.show()   Sort We can sort by the number of confirmed cases. Here note that the cases data frame will not change after performing this command as we don‚Äôt assign it to any variable.\ncases.sort(\u0026quot;confirmed\u0026quot;).show()   But that is inverted. We want to see the most cases at the top. We can do this using the F.desc function:\n# descending Sort from pyspark.sql import functions as F cases.sort(F.desc(\u0026quot;confirmed\u0026quot;)).show()   We can see the most cases in a logical area in South Korea originated from Shincheonji Church.\nCast Though we don‚Äôt face it in this dataset, there might be scenarios where Pyspark reads a double as integer or string, In such cases, you can use the cast function to convert types.\nfrom pyspark.sql.types import DoubleType, IntegerType, StringType cases = cases.withColumn('confirmed', F.col('confirmed').cast(IntegerType())) cases = cases.withColumn('city', F.col('city').cast(StringType()))  Filter We can filter a data frame using multiple conditions using AND(\u0026amp;), OR(|) and NOT(~) conditions. For example, we may want to find out all the different infection_case in Daegu Province with more than 10 confirmed cases.\ncases.filter((cases.confirmed\u0026gt;10) \u0026amp; (cases.province=='Daegu')).show()   GroupBy We can use groupBy function with a spark DataFrame too. Pretty much same as the pandas groupBy with the exception that you will need to import pyspark.sql.functions. Here is the list of functions you can use with this function module.\nfrom pyspark.sql import functions as F cases.groupBy([\u0026quot;province\u0026quot;,\u0026quot;city\u0026quot;]).agg(F.sum(\u0026quot;confirmed\u0026quot;) ,F.max(\u0026quot;confirmed\u0026quot;)).show()   If you don‚Äôt like the new column names, you can use the alias keyword to rename columns in the agg command itself.\ncases.groupBy([\u0026quot;province\u0026quot;,\u0026quot;city\u0026quot;]).agg( F.sum(\u0026quot;confirmed\u0026quot;).alias(\u0026quot;TotalConfirmed\u0026quot;),\\ F.max(\u0026quot;confirmed\u0026quot;).alias(\u0026quot;MaxFromOneConfirmedCase\u0026quot;)\\ ).show()   Joins To Start with Joins we will need to introduce one more CSV file. We will go with the region file which contains region information such as elementary_school_count, elderly_population_ratio, etc.\nregions = spark.read.load(\u0026quot;/home/rahul/projects/sparkdf/coronavirusdataset/Region.csv\u0026quot;,format=\u0026quot;csv\u0026quot;, sep=\u0026quot;,\u0026quot;, inferSchema=\u0026quot;true\u0026quot;, header=\u0026quot;true\u0026quot;) regions.limit(10).toPandas()   We want to get this information in our cases file by joining the two DataFrames. We can do this by using:\ncases = cases.join(regions, ['province','city'],how='left') cases.limit(10).toPandas()    2. Broadcast/Map Side Joins Sometimes you might face a scenario where you need to join a very big table(~1B Rows) with a very small table(~100‚Äì200 rows). The scenario might also involve increasing the size of your database like in the example below.\n Such sort of operations is aplenty in Spark where you might want to apply multiple operations to a particular key. But assuming that the data for each key in the Big table is large, it will involve a lot of data movement. And sometimes so much that the application itself breaks. A small optimization then you can do when joining on such big tables(assuming the other table is small) is to broadcast the small table to each machine/node when you perform a join. You can do this easily using the broadcast keyword. This has been a lifesaver many times with Spark when everything else fails.\nfrom pyspark.sql.functions import broadcast cases = cases.join(broadcast(regions), ['province','city'],how='left')   3. Use SQL with DataFrames If you want, you can also use SQL with data frames. Let us try to run some SQL on the cases table.\nWe first register the cases dataframe to a temporary table cases_table on which we can run SQL operations. As you can see, the result of the SQL select statement is again a Spark Dataframe.\ncases.registerTempTable('cases_table') newDF = sqlContext.sql('select * from cases_table where confirmed\u0026gt;100') newDF.show()   I have shown a minimal example above, but you can use pretty much complex SQL queries involving GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query.\n 4. Create New Columns There are many ways that you can use to create a column in a PySpark Dataframe. I will try to show the most usable of them.\nUsing Spark Native Functions The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.\nWe can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. Our first function, the F.col function gives us access to the column. So if we wanted to add 100 to a column, we could use F.col as:\nimport pyspark.sql.functions as F casesWithNewConfirmed = cases.withColumn(\u0026quot;NewConfirmed\u0026quot;, 100 + F.col(\u0026quot;confirmed\u0026quot;)) casesWithNewConfirmed.show()   We can also use math functions like F.exp function:\ncasesWithExpConfirmed = cases.withColumn(\u0026quot;ExpConfirmed\u0026quot;, F.exp(\u0026quot;confirmed\u0026quot;)) casesWithExpConfirmed.show()   There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list here .\nUsing Spark UDFs Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I need more matured Python functionality.\nTo use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()\nimport pyspark.sql.functions as F from pyspark.sql.types import * def casesHighLow(confirmed): if confirmed \u0026lt; 50: return 'low' else: return 'high' #convert to a UDF Function by passing in the function and return type of function casesHighLowUDF = F.udf(casesHighLow, StringType()) CasesWithHighLow = cases.withColumn(\u0026quot;HighLow\u0026quot;, casesHighLowUDF(\u0026quot;confirmed\u0026quot;)) CasesWithHighLow.show()   Using RDDs This might seem a little odd, but sometimes both the spark UDFs and SQL functions are not enough for a particular use-case. I have observed the RDDs being much more performant in some use-cases in real life. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs.\nWhatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem. Don‚Äôt worry much if you don‚Äôt understand it. It is just here for completion.\nThe process below makes use of the functionality to convert between Row and pythondict objects. We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again. This might come in handy in a lot of situations.\nimport math from pyspark.sql import Row def rowwise_function(row): # convert row to python dictionary: row_dict = row.asDict() # Add a new key in the dictionary with the new column name and value. # This might be a big complex function. row_dict['expConfirmed'] = float(np.exp(row_dict['confirmed'])) # convert dict to row back again: newrow = Row(**row_dict) # return new row return newrow # convert cases dataframe to RDD cases_rdd = cases.rdd # apply our function to RDD cases_rdd_new = cases_rdd.map(lambda row: rowwise_function(row)) # Convert RDD Back to DataFrame casesNewDf = sqlContext.createDataFrame(cases_rdd_new) casesNewDf.show()   Using Pandas UDF This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupBy operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions rather than Spark window functions which we will go through later in this post.\nThe way we use it is by using the F.pandas_udf decorator. We assume here that the input to the function will be a pandas data frame. And we need to return a pandas dataframe in turn from this function.\nThe only complexity here is that we have to provide a schema for the output Dataframe. We can use the original schema of a dataframe to create the outSchema.\ncases.printSchema()   Here I am using Pandas UDF to get normalized confirmed cases grouped by infection_case. The main advantage here is that I get to work with pandas dataframes in Spark.\nfrom pyspark.sql.types import IntegerType, StringType, DoubleType, BooleanType from pyspark.sql.types import StructType, StructField # Declare the schema for the output of our function outSchema = StructType([StructField('case_id',IntegerType(),True), StructField('province',StringType(),True), StructField('city',StringType(),True), StructField('group',BooleanType(),True), StructField('infection_case',StringType(),True), StructField('confirmed',IntegerType(),True), StructField('latitude',StringType(),True), StructField('longitude',StringType(),True), StructField('normalized_confirmed',DoubleType(),True) ]) # decorate our function with pandas_udf decorator @F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def subtract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.confirmed v = v - v.mean() pdf['normalized_confirmed'] = v return pdf confirmed_groupwise_normalization = cases.groupby(\u0026quot;infection_case\u0026quot;).apply(subtract_mean) confirmed_groupwise_normalization.limit(10).toPandas()    5. Spark Window Functions Window functions may make a whole blog post in itself. Here I will talk about some of the most important window functions available in spark.\nFor this, I will also use one more data CSV, which has dates present as that will help with understanding Window functions much better. I will use the TimeProvince dataframe which contains daily case information for each province.\n Ranking You can get rank as well as dense_rank on a group using this function. For example, you may want to have a column in your cases table that provides the rank of infection_case based on the number of infection_case in a province. We can do this by:\nfrom pyspark.sql.window import Window windowSpec = Window().partitionBy(['province']).orderBy(F.desc('confirmed')) cases.withColumn(\u0026quot;rank\u0026quot;,F.rank().over(windowSpec)).show()   Lag Variables Sometimes our data science models may need lag based features. For example, a model might have variables like the price last week or sales quantity the previous day. We can create such features using the lag function with window functions. Here I am trying to get the confirmed cases 7 days before. I am filtering to show the results as the first few days of corona cases were zeros. You can see here that the lag_7 day feature is shifted by 7 days.\nfrom pyspark.sql.window import Window windowSpec = Window().partitionBy(['province']).orderBy('date') timeprovinceWithLag = timeprovince.withColumn(\u0026quot;lag_7\u0026quot;,F.lag(\u0026quot;confirmed\u0026quot;, 7).over(windowSpec)) timeprovinceWithLag.filter(timeprovinceWithLag.date\u0026gt;'2020-03-10').show()   Rolling Aggregations Sometimes it helps to provide rolling averages to our models. For example, we might want to have a rolling 7-day sales sum/mean as a feature for our sales regression model. Let us calculate the rolling mean of confirmed cases for the last 7 days here. This is what a lot of the people are already doing with this dataset to see the real trends.\nfrom pyspark.sql.window import Window windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(-6,0) timeprovinceWithRoll = timeprovince.withColumn(\u0026quot;roll_7_confirmed\u0026quot;,F.mean(\u0026quot;confirmed\u0026quot;).over(windowSpec)) timeprovinceWithRoll.filter(timeprovinceWithLag.date\u0026gt;'2020-03-10').show()   There are a few things here to understand. First is the rowsBetween(-6,0) function that we are using here. This function has a form of rowsBetween(start,end) with both start and end inclusive. Using this we only look at the past 7 days in a particular window including the current_day. Here 0 specifies the current_row and -6 specifies the seventh row previous to current_row. Remember we count starting from 0.\nSo to get roll_7_confirmed for date 2020‚Äì03‚Äì22 we look at the confirmed cases for dates 2020‚Äì03‚Äì22 to 2020‚Äì03‚Äì16 and take their mean.\nIf we had used rowsBetween(-7,-1) we would just have looked at past 7 days of data and not the current_day.\nOne could also find a use for rowsBetween(Window.unboundedPreceding, Window.currentRow) where we take the rows between the first row in a window and the current_row to get running totals. I am calculating cumulative_confirmed here.\nfrom pyspark.sql.window import Window windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow) timeprovinceWithRoll = timeprovince.withColumn(\u0026quot;cumulative_confirmed\u0026quot;,F.sum(\u0026quot;confirmed\u0026quot;).over(windowSpec)) timeprovinceWithRoll.filter(timeprovinceWithLag.date\u0026gt;'2020-03-10').show()    6. Pivot Dataframes Sometimes we may need to have the dataframe in flat format. This happens frequently in movie data where we may want to show genres as columns instead of rows. We can use pivot to do this. Here I am trying to get one row for each date and getting the province names as columns.\npivotedTimeprovince = timeprovince.groupBy('date').pivot('province').agg(F.sum('confirmed').alias('confirmed') , F.sum('released').alias('released')) pivotedTimeprovince.limit(10).toPandas()   One thing to note here is that we need to provide an aggregation always with the pivot function even if the data has a single row for a date.\n 7. Unpivot/Stack Dataframes This is just the opposite of the pivot. Given a pivoted dataframe like above, can we go back to the original?\nYes, we can. But the way is not that straightforward. For one we will need to replace - with _ in the column names as it interferes with what we are about to do. We can simply rename the columns:\nnewColnames = [x.replace(\u0026quot;-\u0026quot;,\u0026quot;_\u0026quot;) for x in pivotedTimeprovince.columns] pivotedTimeprovince = pivotedTimeprovince.toDF(*newColnames)  Now we will need to create an expression which looks like the below:\n\u0026quot;stack(34, 'Busan_confirmed' , Busan_confirmed,'Busan_released' , Busan_released,'Chungcheongbuk_do_confirmed' , . . . 'Seoul_released' , Seoul_released,'Ulsan_confirmed' , Ulsan_confirmed,'Ulsan_released' , Ulsan_released) as (Type,Value)\u0026quot;  The general format is as follows:\n\u0026quot;stack(\u0026lt;cnt of columns you want to put in one column\u0026gt;, 'firstcolname', firstcolname , 'secondcolname' ,secondcolname ......) as (Type, Value)\u0026quot;  It may seem daunting, but we can create such an expression using our programming skills.\nexpression = \u0026quot;\u0026quot; cnt=0 for column in pivotedTimeprovince.columns: if column!='date': cnt +=1 expression += f\u0026quot;'{column}' , {column},\u0026quot; expression = f\u0026quot;stack({cnt}, {expression[:-1]}) as (Type,Value)\u0026quot;  And we can unpivot using:\nunpivotedTimeprovince = pivotedTimeprovince.select('date',F.expr(exprs))   And voila! we have got our dataframe in a vertical format. There are quite a few column creations, filters, and join operations needed to get exactly the same format as before, but I will not get into those.\n 8. Salting Sometimes it might happen that a lot of data goes to a single executor since the same key is assigned for a lot of rows in our data. Salting is another way that helps you to manage data skewness.\nSo assuming we want to do the sum operation when we have skewed keys. We can start by creating the Salted Key and then doing a double aggregation on that key as the sum of a sum still equals sum. To understand this assume we need the sum of confirmed infection_cases on the cases table and assume that the key infection_cases is skewed. We can do the required operation in two steps.\n1. Create a Salting Key\nWe first create a salting key using a concatenation of infection_case column and a random_number between 0 to 9. In case your key is even more skewed, you can split it in even more than 10 parts.\ncases = cases.withColumn(\u0026quot;salt_key\u0026quot;, F.concat(F.col(\u0026quot;infection_case\u0026quot;), F.lit(\u0026quot;_\u0026quot;), F.monotonically_increasing_id() % 10))  This is how the table looks after the operation:\n 2. First Groupby on salt key\ncases_temp = cases.groupBy([\u0026quot;infection_case\u0026quot;,\u0026quot;salt_key\u0026quot;]).agg(F.sum(\u0026quot;confirmed\u0026quot;)).show()   3. Second Group On the original Key\n Here we saw how the sum of sum can be used to get the final sum. You can also make use of facts like:\n  min of min is min\n  max of max is max\n  sum of count is count\n  You can think about ways in which salting as an idea could be applied to joins too.\n Some More Tips and Tricks Caching Spark works on the lazy execution principle. What that means is that nothing really gets executed until you use an action function like the .count() on a dataframe. And if you do a .count function, it generally helps to cache at this step. So I have made it a point to cache() my dataframes whenever I do a .count() operation.\ndf.cache().count()  Save and Load from an intermediate step df.write.parquet(\u0026quot;data/df.parquet\u0026quot;) df.unpersist() spark.read.load(\u0026quot;data/df.parquet\u0026quot;)  When you work with Spark you will frequently run with memory and storage issues. While in some cases such issues might be resolved using techniques like broadcasting, salting or cache, sometimes just interrupting the workflow and saving and reloading the whole dataframe at a crucial step has helped me a lot. This helps spark to let go of a lot of memory that gets utilized for storing intermediate shuffle data and unused caches.\nRepartitioning You might want to repartition your data if you feel your data has been skewed while working with all the transformations and joins. The simplest way to do it is by using:\ndf = df.repartition(1000)  Sometimes you might also want to repartition by a known scheme as this scheme might be used by a certain join or aggregation operation later on. You can use multiple columns to repartition using:\ndf = df.repartition('cola', 'colb','colc','cold')  You can get the number of partitions in a data frame using:\ndf.rdd.getNumPartitions()  You can also check out the distribution of records in a partition by using the glom function. This helps in understanding the skew in the data that happens while working with various transformations.\ndf.glom().map(len).collect()  Reading Parquet File in Local Sometimes you might want to read the parquet files in a system where Spark is not available. In such cases, I normally use the below code:\nfrom glob import glob def load_df_from_parquet(parquet_directory): df = pd.DataFrame() for file in glob(f\u0026quot;{parquet_directory}/*\u0026quot;): df = pd.concat([df,pd.read_parquet(file)]) return df   Conclusion Source: This was a big post and congratulations on you reaching the end. These are the most common functionalities I end up using in my day to day job.\nHopefully, I‚Äôve covered the Dataframe basics well enough to pique your interest and help you get started with Spark. If you want to learn more about how Spark Started or RDD basics take a look at this post You can find all the code at this GitHub repository where I keep code for all my posts.\nContinue Learning Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on Big Data Essentials: HDFS, MapReduce and Spark RDD and Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames by Yandex on Coursera.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/","tags":["Big Data","Machine Learning","Data Science","Statistics","Awesome Guides","Best Content"],"title":"The Most Complete Guide to pySpark DataFrames"},{"categories":["Data Science"],"contents":"Every few years, some academic and professional field gets a lot of cachet in the popular imagination. Right now, that field is data science. As a result, a lot of people are looking to get into it. Add to that the news outlets calling data science sexy and various academic institutes promising to make a data scientist out of you in just a few months, and you‚Äôve got the perfect recipe for disaster.\nOf course, as a data scientist myself, I don‚Äôt think the problem lies in people choosing data science as a profession. If you‚Äôre interested in working with data, understanding business problems, grappling with math, and you love coding, you‚Äôre probably going to thrive in data science. You‚Äôll get a lot of opportunities to use math and coding to develop innovative solutions to problems and will likely find the work rewarding. The main issue here is that the motivations people have for entering the field are often flawed.\nFor some, the appeal is money, while others like the way the title sounds. Even worse, some people are probably just responding to the herd mentality that our society has instilled. For instance, not long ago, every graduate aspired to do an MBA. And I myself am guilty of the same. It took me a GMAT test and a couple of rejections to realize that I didn‚Äôt really want the degree. Ultimately, those rejections were the best thing that happened to me because, after that, I finally looked at data science as an option. Once I got into it, I found that I love the math involved and all the different ways in which I get to use data science to help solve problems for businesses.\n Today, I see that data science has somehow acquired the same stature that the MBA once had.\n A lot of people want to do it, but they don‚Äôt know what the job really entails. And this has resulted in a lot of people calling themselves data scientists and a lot of bad decision making. In fact, a lot of people considering entering the profession probably don‚Äôt even know what data science is.\nToday, the whole field has been democratized by the availability of so much material. A plethora of MOOCs from the best instructors cover concepts from basic to advanced, and you can easily find packages that let you create models with just a few lines of code.\nI genuinely love the fact that there are so many resources to learn and practice data science. But this democratization has created a few problems of its own. In this piece, I want to briefly look at some of these problems and the adverse effect they could have on the field.\n Automated Data Science? A lot of AutoML packages aim at democratizing data science. They provide a repository of models, automate the hyperparameter tuning process, and sometimes offer a way to put these models into production. The availability of such packages has led a lot of people to think that data science could be fully automated, eliminating the need for data scientists altogether. Or, if the processes can‚Äôt be automated, these tools will allow anyone to become a data scientist.\nI sincerely disagree. I have found such codebases useful at times, but they look at data science purely from a coding perspective.\n In my view, data science involves a lot of work apart from modeling.\n The work of data science includes understanding and identifying the problem at hand and setting up the right evaluation metrics. You also have to analyze the profitability of the project: most businesses don‚Äôt want to spend money on projects that don‚Äôt positively affect the bottom line. You can work with existing data, but sometimes you might need to come up with ways to set up new data pipelines to gather data to solve the problem. This requires talking to the stakeholders and gaining a holistic understanding of the problem. A data scientist also needs to be able to carry out data munging and feature creation to churn more performance out of existing models. In the end, model testing and setting the feedback loop require endless hours of discussions with the business and are pretty specific to each project. Someone who just runs code might not be able to add value to such discussions as they don‚Äôt really understand what goes behind the models they have used in AutoML.\nThen there comes the issue of domain knowledge. Processes that are acceptable in a retail domain are not applicable in the finance domain where a small change could result in your customers losing a lot of money. Some things just can‚Äôt be automated since they require domain knowledge and an understanding of the business you‚Äôre working with.\n More importantly, an automated pipeline can‚Äôt be held responsible if a project doesn‚Äôt work or if your model fails in production.\n A good data scientist will try to figure out ways to sort out production issues as they arise as well as creating a machine learning pipeline specific to the project to mitigate such issues.\n The Code-Runner Mentality I have become skeptical of what I call the New Data Scientist. Almost every day, I seem to meet a person calling themselves a data scientist when they are just glorified code runners, which refers to a person who just runs the code without understanding what goes on behind it. With so many academies and institutes providing boot-camp-based courses, code runners are in abundance right now.\nI get a lot of requests where someone asks me whether they should take a certified course from XYZ institute or a boot camp from ABC academy. My answer is neither. I find that these institutes that promise to make data scientists in droves are mainly just in the money-making business. Ultimately, going through a few notebooks and running somebody else‚Äôs code doesn‚Äôt truly make a person a data scientist.\nNow, don‚Äôt get me wrong. If someone learns best through a top-down approach where they run some code first and then read in-depth about the principles behind it, that‚Äôs perfectly fine. Data science is about more than just running code, though. Until you really understand the math and theory behind all the code, you haven‚Äôt mastered data science.\n The Dunning-Kruger Effect  The Dunning-Kruger effect is a type of cognitive bias in which a person with a little bit of knowledge about some subject overestimates their abilities because they‚Äôre unaware of how little they actually know. I see this in action constantly in data science. In fact, it might be more pronounced in this field than any other!\nI tend to think of this as a novice effect. It‚Äôs a problem that plagues people in the early stages of learning a new skill. In my view, there are three stages to a data scientist‚Äôs journey.\n  **The Dunning-Kruger Stage ‚Äî**You created your first model and think you know everything there is to know about data science.\n  **The ‚ÄúI Don‚Äôt Know Anything‚Äù Stage ‚Äî**You go to a conference or talk to your peers and suddenly realize that there is so much more to learn.\n  The ‚ÄúLifelong Learning‚Äù Stage ‚Äî You give in to the fact that there are always going to be some things you won‚Äôt know that just got introduced and so there is lifelong learning involved in pursuing data science.\n  Now, the Dunning-Kruger effect is something that most of the beginners will face. The joy of running your first program and executing it perfectly really takes you to the top of the world. And it‚Äôs totally fine to be at this stage. The problem comes when newcomers are incapable of leaving this stage and moving on to the next ones in a timely fashion. I have seen a few people who get stuck at this stage because they got into data science with the wrong expectations, thinking that it‚Äôs sexy and exciting, without understanding the field‚Äôs depth. These types of people tend to think they can just use existing models to solve problems and that they get by without understanding the math.\nFor instance, I recently interviewed a guy who had two years of experience in the field. He seemed confident. He had used data science in his job and had worked on a couple of Kaggle projects. The first few minutes of the interview went really well. He explained the higher-level concepts well enough that I decided to dig a little deeper into his mathematical understanding of the techniques he had applied in his projects. And that was where things changed. I asked him to tell me about the log loss function. When he said, ‚ÄúBut we have packages for doing all this,‚Äù I realized that this guy had never left the first stage.\n Conclusion The availability of ready-made packages and courses is democratizing the field of data science. But there is just so much more to the job that comes from hands-on experience, communicating with people, and being able to listen to different perspectives.\nSo, while some people may think of data science as a pure coding job, it‚Äôs not just about becoming a coding superstar.\nIt‚Äôs about finding the right problems that are useful for the business and coming up with best ways to solve them. To do that, you need domain knowledge, humility, a little bit of math, and, most importantly, a lifelong desire to learn.\nIf you want to learn about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\nThis story was first published here . s\n","permalink":"https://mlwhiz.com/blog/2020/05/25/democratize/","tags":["Data Science","Machine Learning","Opinion"],"title":"Don‚Äôt Democratize Data Science"},{"categories":["Data Science"],"contents":"Recently, I was reading Rolf Dobell‚Äôs The Art of Thinking Clearly, which made me think about cognitive biases in a way I never had before. I realized how deeply seated some cognitive biases are. In fact, we often don‚Äôt even consciously realize when our thinking is being affected by one. For data scientists, these biases can really change the way we work with data and make our day-to-day decisions, and generally not for the better.\n Data science is, despite the seeming objectivity of all the facts we work with, surprisingly subjective in its processes.\n As data scientists, our job is to make sense of the facts. In carrying out this analysis, we have to make subjective decisions though. So even though we work with hard facts and data, there‚Äôs a strong interpretive component to data science.\nAs a result, we data scientists need to be extremely careful, because all humans are very much susceptible to cognitive biases. We‚Äôre no exception. In fact, I have seen many instances where data scientists ended up making decisions based on pre-existing beliefs, limited data or just irrational preferences.\nIn this piece, I want to point out five of the most common types of cognitive biases. I will also offer some suggestions on how data scientists can work to avoid them and make better, more reasoned decisions.\n 1. Survivorship Bias  During World War II, researchers from the non-profit research group the Center for Naval Analyses were tasked with a problem. They needed to reinforce the military‚Äôs fighter planes at their weakest spots. To accomplish this, they turned to data. They examined every plane that came back from a combat mission and made note of where bullets had hit the aircraft. Based on that information, they recommended that the planes be reinforced at those precise spots.\nDo you see any problems with this approach?\nThe problem, of course, was that they only looked at the planes that returned and not at the planes that didn‚Äôt. Of course, data from the planes that had been shot down would almost certainly have been much more useful in determining where fatal damage to a plane was likely to have occurred, as those were the ones that suffered catastrophic damage.\nThe research team suffered from survivorship bias: they just looked at the data that was available to them without analyzing the larger situation. This is a form of selection bias in which we implicitly filter data based on some arbitrary criteria and then try to make sense out of it without realizing or acknowledging that we‚Äôre working with incomplete data.\nLet‚Äôs think about how this might apply to our work in data science. Say you begin working on a data set. You have created your features and reached a decent accuracy on your modelling task. But maybe you should ask yourself if that is the best result you can achieve. Have you tried looking for more data? Maybe adding weather forecast data to the regular sales variables that you use in your ARIMA models would help you to forecast your sales better. Or perhaps some features around holidays can tell your model why your buyers are behaving in a particular fashion around Thanksgiving or Christmas.\nRecommendation to Overcome: One way to mitigate this bias is by thinking in a rigorous, scientific way about the problem at hand and then brainstorming about any type of data that could help to solve it (rather than just starting with the data). These approaches may seem similar, but the second method restricts your vision because you don‚Äôt know what‚Äôs missing from your work. By using the first approach, you will know what data you were not able to get, and you will end up factoring that into your conclusions.\n 2. Sunk Cost Fallacy Source: We all have seen the sunk cost fallacy in action at some point, whether it be sitting through that bad movie because we have already paid for it or finishing that awful book because we were already halfway through. Everyone has been in a situation where they ended up wasting more time because they were trying to salvage the time they had already invested.\nA sunk cost, also known as a retrospective cost**,** is one that has already been incurred and cannot be recovered by any additional action. The sunk cost fallacy refers to the tendency of human beings to make decisions based on how much of an investment they have already made, which leads to even more investment but no returns whatsoever.\n Sometimes, hard as it is, the best thing to do is to let go.\n This happens often with data science projects. A project might run for more than two years without results but an investigator continues running it because so much time, money and effort have already been invested. Or a data scientist might defend her project wholeheartedly because she has invested so much in it, failing to realize that putting in more work won‚Äôt help her or the company in the long run and that it is best if the project is scrapped.\nRecommendation to Overcome: A way to save yourself from this cognitive bias is by focusing on future benefits and costs rather than the already lost past costs. You have to develop the habit, hard as it is, of ignoring the previous cost information. Of course, it is never easy for us data scientists to just disregard data. For myself, I have found that a methodical way works best in this case. I take a pen and paper to get away from all the distractions and try to come up with all the additional costs required to do a project along with the benefits I might get in the future. If the cost part of the task seems overly significant, then it is time to move on.\n 3. False Causality  As data scientists, we are always in search of patterns. The tendency means that sometimes we even find patterns where none really even exist. Our brains are so trained in this way that we will even make sense of chaos to the extent that we can.\nBecause our training wires us to seek out patterns, it‚Äôs crucial to remember the simple maxim that correlation does not imply causation. Those five words are like the hammer of the data science toolbox without which you can‚Äôt accomplish anything. Just because two variables move in tandem doesn‚Äôt necessarily mean that one causes the other.\nThis principle has been hilariously demonstrated by numerous examples. For instance,\n  by looking at fire department data, you notice that, as more firemen are dispatched to a fire, the more damage is ultimately done to a property. Thus, you might infer that more firemen are causing more damage.\n  In another famous example, an academic who was investigating the cause of crime in New York City in the 1980s found a strong correlation between the number of serious crimes committed and the amount of ice cream sold by street vendors. But should we conclude that eating ice cream drives people to crime? Since this makes little sense, we should obviously suspect that there was an unobserved variable causing both. During the summer, crime rates are the highest, and this is also when most ice cream is sold. Ice cream sales don‚Äôt cause crime, nor does crime increase ice cream sales.\n  In both of these instances, looking at the data too superficially leads to incorrect assumptions.\nRecommendation to Overcome: As data scientists, we need to be mindful of this bias when we present findings. Often, variables that might seem causal might not be on closer inspection. We should also take special care to avoid this type of mistake when creating variables of our models. At each step of the process, it‚Äôs important to ask ourselves if our independent variable is possibly just correlated to the dependent variable.\n 4. Availability Bias Have you ever said something like, ‚ÄúI know that [insert a generic statement here] because [insert one single example].‚Äù For example, someone might say, ‚ÄúYou can‚Äôt get fat from drinking beer, because Bob drinks a lot of it, and he‚Äôs thin.‚Äù If you have, then you‚Äôve suffered from availability bias. You are trying to make sense of the world with limited data.\n***People naturally tend to base decisions on information that is already available to us or things we hear about often without looking at alternatives that might be useful.***As a result, we limit ourselves to a very specific subset of information.\nThis happens often in the data science world. Data scientists tend to get and work on data that‚Äôs easier to obtain rather than looking for data that is harder to gather but might be more useful. We make do with models that we understand and that are available to us in a neat package rather than something more suitable for the problem at hand but much more difficult to come by.\nRecommendation to Overcome: A way to overcome availability bias in data science is to broaden our horizons. Commit to lifelong learning. Read. A lot. About everything. Then read some more. Meet new people. Discuss your work with other data scientists at work or in online forums. Be more open to suggestions about changes that you may have to take in your approach. By opening yourself up to new information and ideas, you can make sure that you‚Äôre less likely to work with incomplete information.\n 5. Confirmation Bias An old joke says that if you torture the data long enough, it will confess. With enough work, you can distort data to make it say what you want it to say.\nWe all hold some beliefs, and that‚Äôs fine. It‚Äôs all part of being human. What‚Äôs not OK, though, is when we let those beliefs inadvertently come into the way we form our hypotheses.\nWe can see this tendency in our everyday lives. We often interpret new information in such a way that it becomes compatible with our own beliefs. We read the news on the site that conforms most closely to our beliefs. We talk to people who are like us and hold similar views. We don‚Äôt want to get disconcerting evidence because that might lead us to change our worldview, which we might be afraid to do.\nFor example, I have seen confirmation bias in action in data science during the cost-benefit analysis stage of a project. I‚Äôve seen people clinging to the data that confirms their hypothesis while ignoring all the contradictory evidence. Obviously, doing this could have a negative impact on the benefits section of the project.\nRecommendation to Overcome: One way to fight this bias is to critically examine all your beliefs and try to find disconcerting evidence about each of your theories. By that, I mean actively seeking out evidence by going to places where you don‚Äôt normally go, talking to people you don‚Äôt normally talk to, and generally keeping an open mind.\n Conclusion  In our age of information overload, we are surrounded by so much data that our brains try desperately to make sense of the noise.\n Sometimes it is useful to be able to make some sense out of the world based on limited information. In fact, we make most of our decisions without thinking much, going with our gut feelings. The potential harm of most of our day-to-day actions is pretty small. Allowing our biases to influence our work, though, can leave us in an unfortunate situation. We may end up losing money or credibility if we make a vital decision that turns out to be wrong.\nKnowing how our brain works will help us avoid these mistakes.\nIf you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nThis story was first published here .\n","permalink":"https://mlwhiz.com/blog/2020/05/25/cogbias/","tags":["Machine Learning","Data Science","Opinion"],"title":"Five Cognitive Biases In Data Science (And how to avoid them)"},{"categories":["Deep Learning"],"contents":"I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.\nYou start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don‚Äôt interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.\nIt is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating. Going through all those Stack Overflow threads. Often wondering what has gone wrong.\nSo is there a way to do this more efficiently?\nIt turns out there is. In this blog, I will try to set up a deep learning server on EC2 with minimal effort so that I could focus on more important things.\nThis blog consists explicitly of two parts:\n  Setting up an Amazon EC2 Machine with preinstalled deep learning libraries.\n  Setting Up Jupyter Notebook using TMUX and SSH tunneling.\n  Don‚Äôt worry; it‚Äôs not as difficult as it sounds. Just follow the steps and click Next.\n Setting up Amazon EC2 Machine I am assuming that you have an AWS account, and you have access to the AWS Console . If not, you might need to sign up for an Amazon AWS account.\n First of all, we need to go to the Services tab to access the EC2 dashboard.    On the EC2 Dashboard, you can start by creating your instance.    Amazon provides Community AMIs(Amazon Machine Image) with Deep Learning software preinstalled. To access these AMIs, you need to look in the community AMIs and search for ‚ÄúUbuntu Deep Learning‚Äù in the Search Tab. You can choose any other Linux flavor, but I have found Ubuntu to be most useful for my Deep Learning needs. In the present setup, I will use The Deep Learning AMI (Ubuntu 18.04) Version 27.0    Once you select an AMI, you can select the Instance Type. It is here you specify the number of CPUs, Memory, and GPUs you will require in your system. Amazon provides a lot of options to choose from based on one‚Äôs individual needs. You can filter for GPU instances using the ‚ÄúFilter by‚Äù filter.  In this tutorial, I have gone with p2.xlarge instance, which provides NVIDIA K80 GPU with 2,496 parallel processing cores and 12GiB of GPU memory. To know about different instance types, you can look at the documentation here and the pricing here .\n  You can change the storage that is attached to the machine in the 4th step. It is okay if you don‚Äôt add storage upfront, as you can also do this later. I change the storage from 90 GB to 500 GB as most of the deep learning needs will require proper storage.    That‚Äôs all, and you can Launch the Instance after going to the Final Review instance settings Screen. Once you click on Launch, you will see this screen. Just type in any key name in the Key Pair Name and click on ‚ÄúDownload key pair‚Äù. Your key will be downloaded to your machine by the name you provided. For me, it got saved as ‚Äúaws_key.pem‚Äù. Once you do that, you can click on ‚ÄúLaunch Instances‚Äù.   Keep this key pair safe as this will be required whenever you want to login to your instance.\n You can now click on ‚ÄúView Instances‚Äù on the next page to see your instance. This is how your instance will look like:     To connect to your instance, Just open a terminal window in your Local machine and browse to the folder where you have kept your key pair file and modify some permissions.\nchmod 400 aws_key.pem\n  Once you do that, you will be able to connect to your instance by SSHing. The SSH command will be of the form:\nssh -i \u0026quot;aws_key.pem\u0026quot; ubuntu@\u0026lt;Your PublicDNS(IPv4)\u0026gt;  For me, the command was:\nssh -i \u0026quot;aws_key.pem\u0026quot; ubuntu@ec2-54-202-223-197.us-west-2.compute.amazonaws.com   Also, keep in mind that the Public DNS might change once you shut down your instance.\n You have already got your machine up and ready. This machine contains different environments that have various libraries you might need. This particular machine has MXNet, Tensorflow, and Pytorch with different versions of python. And the best thing is that we get all this preinstalled, so it just works out of the box.    Setting Up Jupyter Notebook But there are still a few things you will require to use your machine fully. One of them being Jupyter Notebooks. To set up Jupyter Notebooks with your Machine, I recommend using TMUX and tunneling. Let us go through setting up the Jupyter notebook step by step.\n1. Using TMUX to run Jupyter Notebook We will first use TMUX to run the Jupyter notebook on our instance. We mainly use this so that our notebook still runs even if the terminal connection gets lost.\nTo do this, you will need to create a new TMUX session using:\ntmux new -s StreamSession  Once you do that, you will see a new screen with a green border at the bottom. You can start your Jupyter Notebook in this machine using the usual jupyter notebook command. You will see something like:\n It will be beneficial to copy the login URL so that we will be able to get the token later when we try to login to our jupyter notebook later. In my case, it is:\n[http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872](http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872\u0026amp;token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872)  The next step is to detach our TMUX session so that it continues running in the background even when you leave the SSH shell. To do this just press Ctrl+B and then D (Don‚Äôt press Ctrl when pressing D)You will come back to the initial screen with the message that you have detached from your TMUX session.\n If you want, you can reattach to the session again using:\ntmux attach -t StreamSession  2. SSH Tunneling to access the notebook on your Local Browser The second step is to tunnel into the Amazon instance to be able to get the Jupyter notebook on your Local Browser. As we can see, the Jupyter Notebook is actually running on the localhost on the Cloud instance. How do we access it? We use SSH tunneling. Worry not, it is straightforward fill in the blanks. Just use this command on your local machine terminal window:\nssh -i \u0026quot;aws_key.pem\u0026quot; -L \u0026lt;Local Machine Port\u0026gt;:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)\u0026lt;Your PublicDNS(IPv4)\u0026gt;  For this case, I have used:\nssh -i \u0026quot;aws_key.pem\u0026quot; -L 8001:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)ec2-54-202-223-197.us-west-2.compute.amazonaws.com  This means that I will be able to use the Jupyter Notebook If I open the localhost:8001 in my local machine browser. And I surely can. We can now just input the token that we already have saved in one of our previous steps to access the notebook. For me the token is 5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872  You can just login using your token and voila we get the notebook in all its glory.\n You can now choose to work on a new project by selecting any of the different environments you want. You can come from Tensorflow or Pytorch or might be willing to get the best of both worlds. This notebook will not disappoint you.\n  Troubleshooting It might happen that once the machine is restarted, you face some problems with the NVIDIA graphics card. Specifically, in my case, the nvidia-smi command stopped working. If you encounter this problem, the solution is to download the graphics driver from the NVIDIA website .\n Above are the settings for the particular AMI I selected. Once you click on Search you will be able to see the next page:\n Just copy the download link by right-clicking and copying the link address. And run the following commands on your machine. You might need to change the link address and the file name in this.\n# When nvidia-smi doesnt work: wget [https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run\u0026amp;lang=in\u0026amp;type=Tesla](https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run\u0026amp;lang=in\u0026amp;type=Tesla) sudo sh NVIDIA-Linux-x86_64-410.129-diagnostic.run --no-drm --disable-nouveau --dkms --silent --install-libglvnd modinfo nvidia | head -7 sudo modprobe nvidia   Stop Your Instance And that‚Äôs it. You have got and up and running Deep Learning machine at your disposal, and you can work with it as much as you want. Just keep in mind to stop the instance whenever you stop working, so you won‚Äôt need to pay Amazon when you are not working on your instance. You can do it on the instances page by right-clicking on your instance. Just note that when you need to log in again to this machine, you will need to get the Public DNS (IPv4) address from the instance page back as it might have changed.\n  Conclusion I have always found it a big chore to set up a deep learning environment.\nIn this blog, we set up a new Deep Learning server on EC2 in minimal time by using Deep Learning Community AMI, TMUX, and Tunneling for the Jupyter Notebooks. This server comes preinstalled with all the deep learning libraries you might need at your work, and it just works out of the box.\nSo what are you waiting for? Just get started with Deep Learning with your own server.\nIf you want to learn more about AWS and how to use it in production settings and deploying models, I would like to call out an excellent course on AWS . Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/05/25/dls/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Computer Vision","Productivity","EC2"],"title":"Stop Worrying and Create your Deep Learning Server in 30 minutes"},{"categories":["Programming"],"contents":" Python provides us with many styles of coding.\nAnd with time, Python has regularly come up with new coding standards and tools that adhere even more to the coding standards in the Zen of Python.\n Beautiful is better than ugly.\n In this series of posts named Python Shorts , I will explain some simple but very useful constructs provided by Python, some essential tips, and some use cases I come up with regularly in my Data Science work.\nThis post is specifically about using f strings in Python that was introduced in Python 3.6.\n 3 Common Ways of Printing: Let me explain this with a simple example. Suppose you have some variables, and you want to print them within a statement.\nname = 'Andy' age = 20 print(?) ---------------------------------------------------------------- Output: I am Andy. I am 20 years old  You can do this in various ways:\na) Concatenate: A very naive way to do is to simply use + for concatenation within the print function. But that is clumsy. We would need to convert our numeric variables to string and keep care of the spaces while concatenating. And it doesn‚Äôt look good as the code readability suffers a little when we use it.\nname = 'Andy' age = 20 print(\u0026quot;I am \u0026quot; + name + \u0026quot;. I am \u0026quot; + str(age) + \u0026quot; years old\u0026quot;) ---------------------------------------------------------------- I am Andy. I am 20 years old   b) % Format: The second option is to use % formatting. But it also has its problems. For one, it is not readable. You would need to look at the first %s and try to find the corresponding variable in the list at the end. And imagine if you have a long list of variables that you may want to print.\nprint(\u0026quot;I am %s. I am %s years old\u0026quot; % (name, age))  c) str.format(): Next comes the way that has been used in most Python 3 codes and has become the standard of printing in Python. Using str.format()\nprint(\u0026quot;I am {}. I am {} years old\u0026quot;.format(name, age))  Here we use {} to denote the placeholder of the object in the list. It still has the same problem of readability, but we can also use str.format :\nprint(\u0026quot;I am {name}. I am {age} years old\u0026quot;.format(name = name, age = age))  If this seems a little too repetitive, we can use dictionaries too:\ndata = {'name':'Andy','age':20} print(\u0026quot;I am {name}. I am {age} years old\u0026quot;.format(**data))   The Fourth Way with f  Since Python 3.6, we have a new formatting option, which makes it even more trivial. We could simply use:\nprint(f\u0026quot;I am {name}. I am {age} years old\u0026quot;)  We just append f at the start of the string and use {} to include our variable name, and we get the required results.\nAn added functionality that f string provides is that we can put expressions in the {} brackets. For Example:\nnum1 = 4 num2 = 5 print(f\u0026quot;The sum of {num1} and {num2} is {num1+num2}.\u0026quot;) --------------------------------------------------------------- The sum of 4 and 5 is 9.  This is quite useful as you can use any sort of expression inside these brackets. The expression can contain dictionaries or functions. A simple example:\ndef totalFruits(apples,oranges): return apples+oranges data = {'name':'Andy','age':20} apples = 20 oranges = 30 print(f\u0026quot;{data['name']} has {totalFruits(apples,oranges)} fruits\u0026quot;) ---------------------------------------------------------------- Andy has 50 fruits  Also, you can use ‚Äô‚Äô‚Äô to use multiline strings.\nnum1 = 4 num2 = 5 print(f'''The sum of {num1} and {num2} is {num1+num2}.''') --------------------------------------------------------------- The sum of 4 and 5 is 9.  An everyday use case while formatting strings is to format floats. You can do that using f string as following\nnumFloat = 10.23456678 print(f'Printing Float with 2 decimals: {numFloat:.2f}') ----------------------------------------------------------------- Printing Float with 2 decimals: 10.23   Conclusion Until recently, I had been using Python 2 for all my work, and so was not able to check out this new feature.\nBut now, as I am shifting to Python 3, f strings has become my go-to syntax to format strings. It is easy to write and read with the ability to incorporate arbitrary expressions as well. In a way, this new function adheres to at least 3 PEP concepts ‚Äî\n Beautiful is better than ugly, Simple is better than complex and Readability counts.\n If you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/05/24/fstring/","tags":["Python","Productivity","tools"],"title":"How and Why to use f strings in Python3?"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":" Have you ever thought about how toxic comments get flagged automatically on platforms like Quora or Reddit? Or how mail gets marked as spam? Or what decides which online ads are shown to you?\nAll of the above are examples of how text classification is used in different areas. Text classification is a common task in natural language processing (NLP) which transforms a sequence of a text of indefinite length into a single category.\nOne theme that emerges from the above examples is that all have a binary target class. For example, either the comment is toxic or not toxic, or the review is fake or not fake. In short, there are only two target classes, hence the term binary.\nBut this is not always the case, and some problems might have more than two target classes. These problems are conveniently termed multiclass classifications, and it is these problems we‚Äôll focus on in this post. Some examples of multiclass classification include:\n  The sentiment of a review: positive, negative or neutral (three classes)\n  News Categorization by genre: Entertainment, education, politics, etc.\n  In this post, we will go through a multiclass text classification problem using various Deep Learning Methods.\n Dataset / Problem Description For this post, I am using the UCI ML Drug Review dataset from Kaggle. It contains over 200,000 patient drug reviews, along with related conditions. The dataset has many columns, but we will be using just two of them for our NLP Task.\nSo, our dataset mostly looks like this:\n Task: We want to classify the top disease conditions based on the drug review.\n  A Primer on word2vec embeddings: Before we go any further into text classification, we need a way to represent words numerically in a vocabulary. Why? Because most of our ML models require numbers, not text.\nOne way to achieve this goal is by using the one-hot encoding of word vectors, but this is not the right choice. Given a vast vocabulary, this representation would take a lot of space, and it cannot accurately express the similarity between different words, such as if we want to find the cosine similarity between numerical words x and y:\n Given the structure of one-hot encoded vectors, the similarity is always going to be 0 between different words.\nWord2Vec overcomes the above difficulties by providing us with a fixed-length (usually much smaller than the vocabulary size) vector representation of words. It also captures the similarity and analogous relationships between different words.\n Word2vec vectors of words are learned in such a way that they allow us to learn different analogies. This enables us to do algebraic manipulations on words that were not possible previously.\nFor example: What is king ‚Äî man + woman? The result is Queen.\nWord2Vec vectors also help us to find the similarity between words. If we look for similar words to ‚Äúgood‚Äù, we will find awesome, great, etc. It is this property of word2vec that makes it invaluable for text classification. With this, our deep learning network understands that ‚Äúgood‚Äù and ‚Äúgreat‚Äù are words with similar meanings.\nIn simple terms, word2vec creates fixed-length vectors for words, giving us a d dimensional vector for every word (and common bigrams) in a dictionary.\nThese word vectors are usually pre-trained, and provided by others after training on large corpora of texts like Wikipedia, Twitter, etc. The most commonly used pre-trained word vectors are Glove and Fast text with 300-dimensional word vectors. In this post, we will use the Glove word vectors.\n Data Preprocessing In most cases, text data is not entirely clean. Data coming from different sources have different characteristics, and this makes text preprocessing one of the most critical steps in the classification pipeline. For example, Text data from Twitter is different from the text data found on Quora or other news/blogging platforms, and each needs to be treated differently. However, the techniques we‚Äôll cover in this post are generic enough for almost any kind of data you might encounter in the jungles of NLP.\na) Cleaning Special Characters and Removing Punctuation Our preprocessing pipeline depends heavily on the word2vec embeddings we are going to use for our classification task. In principle, our preprocessing should match the preprocessing used before training the word embedding. Since most of the embeddings don‚Äôt provide vector values for punctuation and other special characters, the first thing we want to do is get rid of the special characters in our text data.\n# Some preprocesssing that will be common to all the text classification methods you will see. import re def clean_text(x): pattern = r'[^a-zA-z0-9\\s]' text = re.sub(pattern, '', x) return x  b) Cleaning Numbers Why do we want to replace numbers with #s? Because most embeddings, including Glove, have preprocessed their text in this way.\nSmall Python Trick: We use an if statement in the code below to check beforehand if a number exists in a text because an if is always faster than a re.sub command, and most of our text doesn‚Äôt contain numbers.\ndef clean_numbers(x): if bool(re.search(r'\\d', x)): x = re.sub('[0-9]{5,}', '#####', x) x = re.sub('[0-9]**{4}**', '####', x) x = re.sub('[0-9]**{3}**', '###', x) x = re.sub('[0-9]**{2}**', '##', x) return x  c) Removing Contractions Contractions are words that we write with an apostrophe. Examples of contractions are words like ‚Äúain‚Äôt‚Äù or ‚Äúaren‚Äôt‚Äù. Since we want to standardize our text, it makes sense to expand these contractions. Below we have done this using contraction mapping and regex functions.\ncontraction_dict = {\u0026quot;ain't\u0026quot;: \u0026quot;is not\u0026quot;, \u0026quot;aren't\u0026quot;: \u0026quot;are not\u0026quot;,\u0026quot;can't\u0026quot;: \u0026quot;cannot\u0026quot;, \u0026quot;'cause\u0026quot;: \u0026quot;because\u0026quot;, \u0026quot;could've\u0026quot;: \u0026quot;could have\u0026quot;} def _get_contractions(contraction_dict): contraction_re = re.compile('(**%s**)' % '|'.join(contraction_dict.keys())) return contraction_dict, contraction_re contractions, contractions_re = _get_contractions(contraction_dict) def replace_contractions(text): def replace(match): return contractions[match.group(0)] return contractions_re.sub(replace, text) # Usage replace_contractions(\u0026quot;this's a text with contraction\u0026quot;)  Apart from the above techniques, you may want to do spell correction, too. But since our post is already quite long, we‚Äôll leave that for now.\n Data Representation: Sequence Creation One thing that has made deep learning a go-to choice for NLP is the fact that we don‚Äôt have to hand-engineer features from our text data; deep learning algorithms take as input a sequence of text to learn its structure just like humans do. Since machines cannot understand words, they expect their data in numerical form. So we need to represent our text data as a series of numbers.\nTo understand how this is done, we need to understand a little about the Keras Tokenizer function. Other tokenizers are also viable, but the Keras Tokenizer is a good choice for me.\na) Tokenizer Put simply, a tokenizer is a utility function that splits a sentence into words. keras.preprocessing.text.Tokenizer tokenizes (splits) a text into tokens (words) while keeping only the words that occur the most in the text corpus.\n#Signature: Tokenizer(num_words=None, filters='!\u0026quot;#$%\u0026amp;()*+,-./:;\u0026lt;=\u0026gt;?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)  The num_words parameter keeps only a pre-specified number of words in the text. This is helpful because we don‚Äôt want our model to get a lot of noise by considering words that occur infrequently. In real-world data, most of the words we leave using the num_words parameter are normally misspelled words. The tokenizer also filters some non-wanted tokens by default and converts the text into lowercase.\nOnce fitted to the data, the tokenizer also keeps an index of words (a dictionary we can use to assign unique numbers to words), which can be accessed by tokenizer.word_index. The words in the indexed dictionary are ranked in order of frequency.\n So the whole code to use the tokenizer is as follows:\nfrom keras.preprocessing.text import Tokenizer ## Tokenize the sentences tokenizer = Tokenizer(num_words=max_features) tokenizer.fit_on_texts(list(train_X)+list(test_X)) train_X = tokenizer.texts_to_sequences(train_X) test_X = tokenizer.texts_to_sequences(test_X)  where train_X and test_X are lists of documents in the corpus.\nb) Pad Sequence Normally our model expects that each text sequence (each training example) will be of the same length (the same number of words/tokens). We can control this using the maxlen parameter.\nFor example:\n train_X = pad_sequences(train_X, maxlen=maxlen) test_X = pad_sequences(test_X, maxlen=maxlen)  Now our training data contains a list of numbers. Each list has the same length. And we also have the word_index which is a dictionary of the words that occur most in the text corpus.\nc) Label Encoding the Target Variable The Pytorch model expects the target variable as a number and not a string. We can use Label encoder from sklearn to convert our target variable.\nfrom sklearn.preprocessing import LabelEncoder le = LabelEncoder() train_y = le.fit_transform(train_y.values) test_y = le.transform(test_y.values)   Load Embedding First, we need to load the required Glove embeddings.\ndef load_glove(word_index): EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt' def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300] embeddings_index = dict(get_coefs(*o.split(\u0026quot; \u0026quot;)) for o in open(EMBEDDING_FILE)) all_embs = np.stack(embeddings_index.values()) emb_mean,emb_std = -0.005838499,0.48782197 embed_size = all_embs.shape[1] nb_words = min(max_features, len(word_index)+1) embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) for word, i in word_index.items(): if i \u0026gt;= max_features: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector else: embedding_vector = embeddings_index.get(word.capitalize()) if embedding_vector is not None: embedding_matrix[i] = embedding_vector return embedding_matrix embedding_matrix = load_glove(tokenizer.word_index)  Be sure to put the path of the folder where you download these GLoVE vectors. What does the embeddings_index contain? It‚Äôs a dictionary in which the key is the word, and the value is the word vector, a np.array of length 300. The length of this dictionary is somewhere around a billion.\nSince we only want the embeddings of words that are in our word_index, we will create a matrix that just contains required embeddings using the word index from our tokenizer.\n  Deep Learning Models 1. TextCNN The idea of using a CNN to classify text was first presented in the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim.\nRepresentation: The central concept of this idea is to see our documents as images. But how? Let‚Äôs say we have a sentence, and we have maxlen = 70 and embedding size = 300. We can create a matrix of numbers with the shape 70√ó300 to represent this sentence. Images also have a matrix where individual elements are pixel values. But instead of image pixels, the input to the task is sentences or documents represented as a matrix. Each row of the matrix corresponds to a one-word vector.\n Convolution Idea: For images, we move our conv. filter both horizontally as well as vertically, but for text we fix kernel size to filter_size x embed_size, i.e. (3,300) we are just going to move vertically down the convolution looking at three words at once, since our filter size in this case is 3. This idea seems right since our convolution filter is not splitting word embedding; it gets to look at the full embedding of each word. Also, one can think of filter sizes as unigrams, bigrams, trigrams, etc. Since we are looking at a context window of 1, 2, 3, and 5 words respectively.\nHere is the text classification CNN network coded in Pytorch .\nclass CNN_Text(nn.Module): def __init__(self): super(CNN_Text, self).__init__() filter_sizes = [1,2,3,5] num_filters = 36 n_classes = len(le.classes_) self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes]) self.dropout = nn.Dropout(0.1) self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes) def forward(self, x): x = self.embedding(x) x = x.unsqueeze(1) x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] x = torch.cat(x, 1) x = self.dropout(x) logit = self.fc1(x) return logit  2. BiDirectional RNN (LSTM/GRU) TextCNN works well for text classification because it takes care of words in close range. For example, it can see ‚Äúnew york‚Äù together. However, it still can‚Äôt take care of all the context provided in a particular text sequence. It still does not learn the sequential structure of the data, where each word is dependent on the previous word, or a word in the previous sentence.\nRNNs can help us with that. They can remember previous information using hidden states and connect it to the current task.\nLong Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for extended periods. Moreover, a bidirectional LSTM keeps the contextual information in both directions, which is pretty useful in text classification tasks (However, it won‚Äôt work for a time series prediction task as we don‚Äôt have visibility into the future in this case).\n For a simple explanation of a bidirectional RNN, think of an RNN cell as a black box taking as input a hidden state (a vector) and a word vector and giving out an output vector and the next hidden state. This box has some weights which need to be tuned using backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.\n Hidden state, Word vector -\u0026gt;(RNN Cell) -\u0026gt; Output Vector , Next Hidden state\nFor a sequence of length 4 like ‚Äúyou will never believe‚Äù, The RNN cell gives 4 output vectors, which can be concatenated and then used as part of a dense feedforward architecture.\nIn the bidirectional RNN, the only change is that we read the text in the usual fashion as well in reverse. So we stack two RNNs in parallel, and we get 8 output vectors to append.\nOnce we get the output vectors, we send them through a series of dense layers and finally, a softmax layer to build a text classifier.\nIn most cases, you need to understand how to stack some layers in a neural network to get the best results. We can try out multiple bidirectional GRU/LSTM layers in the network if it performs better.\nDue to the limitations of RNNs, such as not remembering long term dependencies, in practice, we almost always use LSTM/GRU to model long term dependencies. In this case, you can think of the RNN cell being replaced by an LSTM cell or a GRU cell in the above figure.\nHere is some code in Pytorch for this network:\nclass BiLSTM(nn.Module): def __init__(self): super(BiLSTM, self).__init__() self.hidden_size = 64 drp = 0.1 n_classes = len(le.classes_) self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True) self.linear = nn.Linear(self.hidden_size*4 , 64) self.relu = nn.ReLU() self.dropout = nn.Dropout(drp) self.out = nn.Linear(64, n_classes) def forward(self, x): *#rint(x.size())* h_embedding = self.embedding(x) *#_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))* h_lstm, _ = self.lstm(h_embedding) avg_pool = torch.mean(h_lstm, 1) max_pool, _ = torch.max(h_lstm, 1) conc = torch.cat(( avg_pool, max_pool), 1) conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) return out   Training Below is the code we use to train our BiLSTM Model. The code is well commented, so please go through the code to understand it. You might also want to look at my post on Pytorch .\nn_epochs = 6 model = BiLSTM() loss_fn = nn.CrossEntropyLoss(reduction='sum') optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001) model.cuda() # Load train and test in CUDA Memory x_train = torch.tensor(train_X, dtype=torch.long).cuda() y_train = torch.tensor(train_y, dtype=torch.long).cuda() x_cv = torch.tensor(test_X, dtype=torch.long).cuda() y_cv = torch.tensor(test_y, dtype=torch.long).cuda() # Create Torch datasets train = torch.utils.data.TensorDataset(x_train, y_train) valid = torch.utils.data.TensorDataset(x_cv, y_cv) # Create Data Loaders train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False) train_loss = [] valid_loss = [] for epoch in range(n_epochs): start_time = time.time() # Set model to train configuration model.train() avg_loss = 0. for i, (x_batch, y_batch) in enumerate(train_loader): # Predict/Forward Pass y_pred = model(x_batch) # Compute loss loss = loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() optimizer.step() avg_loss += loss.item() / len(train_loader) # Set model to validation configuration -Doesn't get trained here model.eval() avg_val_loss = 0. val_preds = np.zeros((len(x_cv),len(le.classes_))) for i, (x_batch, y_batch) in enumerate(valid_loader): y_pred = model(x_batch).detach() avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader) # keep/store predictions val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy() # Check Accuracy val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y) train_loss.append(avg_loss) valid_loss.append(avg_val_loss) elapsed_time = time.time() - start_time print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_acc={:.4f} \\t time={:.2f}s'.format( epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))  The training output looks like below:\n   Results/Prediction import scikitplot as skplt y_true = [le.classes_[x] for x **in** test_y] y_pred = [le.classes_[x] for x **in** val_preds.argmax(axis=1)] skplt.metrics.plot_confusion_matrix( y_true, y_pred, figsize=(12,12),x_tick_rotation=90)  Below is the confusion matrix for the results of the BiLSTM model. We can see that our model does reasonably well, with an 87% accuracy on the validation dataset.\n What‚Äôs interesting is that even at points where the model performs poorly, it is quite understandable. For example, the model gets confused between weight loss and obesity, or between depression and anxiety, or between depression and bipolar disorder. I am not an expert, but these diseases do feel quite similar.\n Conclusion In this post, we covered deep learning architectures like LSTM and CNN for text classification and explained the different steps used in deep learning for NLP.\nThere is still a lot that can be done to improve this model‚Äôs performance. Changing the learning rates, using learning rate schedules, using extra features, enriching embeddings, removing misspellings, etc. I hope this boilerplate code provides a go-to baseline for any text classification problem you might face.\nYou can find the full working code here on Github , or this Kaggle Kernel .\nAlso, if you want to learn more about NLP, here is an excellent course.\nIf you want to learn more about NLP, I would like to call out an excellent course on Natural Language Processing from the Advanced Machine Learning Specialization. Do check it out.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\nThis story was first published here .\n","permalink":"https://mlwhiz.com/blog/2020/05/24/multitextclass/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Text classification"],"title":"Using Deep Learning for End to End Multiclass Text Classification"},{"categories":["Awesome Guides","Data Science"],"contents":"It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.\nTo do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea. Why not create an international news dashboard for Corona? And here it is.\nThis post is about how I created the news dashboard using Streamlit and data from NewsApi and European CDC .\nTLDR; Link to the App here .\n Getting The Data The most important thing while creating this Dashboard was acquiring the data. I am using two data sources:\n1. Data from the European Centre for Disease Prevention and Control. The downloadable data file is updated daily and contains the latest available public data on COVID-19. Here is a snapshot of this data.\n def get_data(date): os.system(\u0026#34;rm cases.csv\u0026#34;) url = \u0026#34;[https://opendata.ecdc.europa.eu/covid19/casedistribution/csv](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)\u0026#34; filename = wget.download(url,\u0026#34;cases.csv\u0026#34;) casedata = pd.read_csv(filename, encoding=\u0026#39;latin-1\u0026#39;) return casedata  2. News API The second source of data comes from the News API , which lets me access articles from leading news outlets from various countries for free. The only caveat is that I could only hit the API 500 times a day, and there is a result limit of 100 results for a particular query for free accounts.\nI tried to get around those limit barriers by using streamlit caching(So I don‚Äôt hit the API a lot). I also tried to get news data from last month using multiple filters to get a lot of data.\nfrom newsapi import NewsApiClient newsapi = NewsApiClient(api_key=\u0026#39;aedb6aa9bebb4011a4eb5447019dd592\u0026#39;) The primary way the API works is by giving us access to 3 functions.\na) A function to get Recent News from a country:\njson_data = newsapi.get_top_headlines(q=q,language=\u0026#39;en\u0026#39;, country=\u0026#39;us\u0026#39;) data = pd.DataFrame(json_data[\u0026#39;articles\u0026#39;]) data.head()  b) A function to get ‚ÄúEverything‚Äù related to a query from the country. You can see the descriptions of API parameters here:\njson_data = newsapi.get_everything(q=\u0026#39;corona\u0026#39;, language=\u0026#39;en\u0026#39;, from_param=str(date.today() -timedelta(days=29)), to= str(date.today()), sources = \u0026#39;usa-today\u0026#39;, page_size=100, page = 1, sort_by=\u0026#39;relevancy\u0026#39; ) data = pd.DataFrame(json_data[\u0026#39;articles\u0026#39;]) data.head()  c) A function to get a list of sources from a Country programmatically. We can then use these sources to pull data from the ‚Äúeverything‚Äù API\ndef get_sources(country): sources = newsapi.get_sources(country=country) sources = [x[\u0026#39;id\u0026#39;] for x in sources[\u0026#39;sources\u0026#39;]] return sources sources = get_sources(country=\u0026#39;us\u0026#39;) print(sources[:5]) ------------------------------------------------------------------- [\u0026#39;abc-news\u0026#39;, \u0026#39;al-jazeera-english\u0026#39;, \u0026#39;ars-technica\u0026#39;, \u0026#39;associated-press\u0026#39;, \u0026#39;axios\u0026#39;] I used all the functions above to get data that refreshes at a particular cadence. You can see how I use these API functions in a loop to download the data by looking at my code at GitHub.\n Creating the Dashboard I wanted to have a few important information in the Dashboard that I was interested in. So I started by creating various widgets.\n1. Current World Snapshot: The first information was regarding the whole world situation. The Number of Cases and Deaths. The case and death curve in various countries? What are the fatality rates in various countries? Below is the current world situation on 28 Mar 2020.\nObservations: We can see the deaths in Italy are still on the rise, while we are seeing the deaths shooting up in Spain, France, and the United States as well. The death rates in some countries are worrying with death rates of 10.56% in Italy and 8.7% in Iraq. I suspect that the death rate statistic of 2% in the starting days of CoronaVirus was misinformed if not wrong.\n Technical Details ‚Äî To create this part of the Dashboard, I used the ECDC data. I also used a lot of HTML hacks with Streamlit, where I used bootstrap widgets as well as custom HTML to get data in the way I wanted to display it. Here are a few of the hacks:\n Using Bootstrap Cards: You can use bootstrap or, in that case, any HTML element in Streamlit if you change the parameter unsafe_allow_html to True. Do note that I am also using python f string formatting here.  st.sidebar.markdown(f\u0026#39;\u0026#39;\u0026#39;\u0026lt;div class=\u0026#34;card text-white bg-info mb-3\u0026#34; style=\u0026#34;width: 18rem\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;card-body\u0026#34;\u0026gt; \u0026lt;h5 class=\u0026#34;card-title\u0026#34;\u0026gt;Total Cases\u0026lt;/h5\u0026gt; \u0026lt;p class=\u0026#34;card-text\u0026#34;\u0026gt;{sum(casedata[\u0026#39;cases\u0026#39;]):,d}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026#39;\u0026#39;\u0026#39;, unsafe_allow_html=True) The above code is behind the Dashboard styled cards in the streamlit app sidebar.\n  Changed the width of the streamlit main page:  Again, there was no parameter given by streamlit to do this, and I was finding the page width a little too small for my use case. Adding the above code at the start of the app solved the issue.\nst.markdown( f\u0026#34;\u0026#34;\u0026#34; \u0026lt;style\u0026gt; .reportview-container .main .block-container{{ max-width: 1000px; }} \u0026lt;/style\u0026gt; \u0026#34;\u0026#34;\u0026#34;, unsafe_allow_html=True, )  2. Most Recent News from Country The primary purpose of creating this Dashboard was to get news from various outlets from top media outlets in the country.\nObservations: As here you can see, here we have the top recent news from the United Kingdom concerning cases in Ireland and Boris Johnson‚Äôs corona woes.\n Technical Details: As said before, I am using the News API to get this data. And here is how I am using a mashup of HTML and markdown to display the news results.\ndef create_most_recent_markdown(df,width=700): if len(df)\u0026gt;0: # img url img_path = df[\u0026#39;urlToImage\u0026#39;].iloc[0] if not img_path: images = [x for x in df.urlToImage.values if x is not None] if len(images)!=0: img_path = random.choice(images) else: img_path = \u0026#39;[https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png\u0026#39;](https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png\u0026#39;) img_alt = df[\u0026#39;title\u0026#39;].iloc[0] df = df[:5] **markdown_str = f\u0026#34;\u0026lt;img src=\u0026#39;{img_path}\u0026#39; width=\u0026#39;{width}\u0026#39;/\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt;\u0026#34;** for index, row in df.iterrows(): **markdown_str += f\u0026#34;[{row[\u0026#39;title\u0026#39;]}]({row[\u0026#39;url\u0026#39;]}) by {row[\u0026#39;author\u0026#39;]}\u0026lt;br\u0026gt; \u0026#34;** return markdown_str else: return \u0026#39;\u0026#39; Few things to note here:\n  The image width cannot be set using markdown so using custom HTML\n  The usage of python f strings to create the article titles and URLs.\n  If no image is found, we are defaulting to a custom image.\n   3. News Sentiment Another thing that has been bothering me in these trying times is so much negativity everywhere. I wanted to see the news covered from a positive angle if it could be in any way. So I did some simple sentiment analysis using the custom sentiment analyzer from Textblob to do this.\nI found out sentiments by news outlets as well as some of the most positive and negative news related to Coronavirus in the past 30 days. (Past 30 days because I cannot go more back with the free API).\nObservations: As you can see that one of the most positive news is Trump changing his coronavirus stance on March 17th, and I agree. The second positive report seems to be regarding some sort of solution to the problem. While the first Negative news is regarding Cardi B slamming celebrities for sowing confusion about the Coronavirus. I won‚Äôt comment on this :)\n Technical Details: To get the sentiment scores of an article I used TextBlob. Getting the sentiment scores that range from -1 to 1 is as simple as using the below function. I used a concatenation of title and description to find the sentiment as the content from the News API was truncated.\ndef textblob_sentiment(title,description): blob = TextBlob(str(title)+\u0026#34; \u0026#34;+str(description)) return blob.sentiment.polarity The main difficulty here was to have a two-column layout to give both positive and negative news. For that again, I had to use a mashup of HTML and markdown. I used the HTML table to do this. Also, note how I used markdown to convert markdown to HTML using Python f strings.\nimport markdown md = markdown.Markdown() positive_results_markdown = create_most_recent_markdown(positivedata,400) negative_results_markdown = create_most_recent_markdown(negativedata,400) html = f\u0026#39;\u0026#39;\u0026#39;\u0026lt;table style=\u0026#34;width:100%\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;\u0026lt;center\u0026gt;Most Positive News\u0026lt;/center\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;\u0026lt;center\u0026gt;Most Negative News\u0026lt;/center\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;center\u0026gt;**{md.convert(positive_results_markdown)}**\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;center\u0026gt;**{md.convert(negative_results_markdown)}**\u0026lt;/center\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt;\u0026#39;\u0026#39;\u0026#39; #print md.convert(\u0026#34;# sample heading text\u0026#34;) st.markdown(html,unsafe_allow_html=True)  4. News Source WordCloud A visualization dashboard that works with text is never really complete without a word cloud, so I thought of adding a word cloud to understand the word usage from a particular source.\nObservations: We can see Vice news using words like ‚ÄúNew‚Äù and ‚ÄúTested‚Äù a lot of times. While Business Insider used ‚ÄúChina‚Äù a lot.\n      Technical Details: Here is what I used to create this masked word cloud:\nimport cv2 def create_mask(): mask = np.array(Image.open(\u0026#34;coronavirus.png\u0026#34;)) im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) _, mask = cv2.threshold(im_gray, thresh=20, maxval=255, type=cv2.THRESH_BINARY) mask = 255 - mask return mask mask = create_mask() def create_wc_by(source): data = fulldf[fulldf[\u0026#39;source\u0026#39;]==source] text = \u0026#34; \u0026#34;.join([x for x in data.content.values if x is not None]) stopwords = set(STOPWORDS) stopwords.add(\u0026#39;chars\u0026#39;) stopwords.add(\u0026#39;coronavirus\u0026#39;) stopwords.add(\u0026#39;corona\u0026#39;) stopwords.add(\u0026#39;chars\u0026#39;) wc = WordCloud(background_color=\u0026#34;white\u0026#34;, max_words=1000, mask=mask, stopwords=stopwords, max_font_size=90, random_state=42, contour_width=3, contour_color=\u0026#39;steelblue\u0026#39;) wc.generate(text) plt.figure(figsize=[30,30]) plt.imshow(wc, interpolation=\u0026#39;bilinear\u0026#39;) plt.axis(\u0026#34;off\u0026#34;) return plt st.pyplot(create_wc_by(source),use_container_width=True)  Other Technical Considerations 1. Advanced Caching: In new streamlit release notes for 0.57.0 which just came out yesterday, streamlit has made updates to st.cache. One notable change to this release is the ‚Äúability to set expiration options for cached functions by setting the max_entries and ttl arguments‚Äù. From the documentation :\n  max_entries (int or None) ‚Äî The maximum number of entries to keep in the cache, or None for an unbounded cache. (When a new entry is added to a full cache, the oldest cached entry will be removed.) The default is None.\n  ttl (float or None) ‚Äî The maximum number of seconds to keep an entry in the cache, or None if cache entries should not expire. The default is None.\n  Two use cases where this might help would be:\n  If you‚Äôre serving your app and don‚Äôt want the cache to grow forever.\n  If you have a cached function that reads live data from a URL and should clear every few hours to fetch the latest data\n  So this is what is being used in a lot of functions to avoid hitting APIs multiple times and to prevent them from getting stale at the same time.\nFor Example, Top results from a country are fetched at a period of 360 seconds i.e., 6 minutes.\nst.cache(ttl=360,max_entries=20) def create_dataframe_top(queries,country): #Hits API Here While full results from the everything API are fetched at a period of one day.\n[@st](http://twitter.com/st).cache(ttl = 60*60*24,max_entries=20) def create_dataframe_last_30d(queries, sources): # hits API 2. Deployment: I used the amazon free ec2 instance to deploy this app at http://54.149.204.138:8501/ . If you want to know the steps,read my post on How to Deploy a Streamlit App using an Amazon Free ec2 instance? There are also a few caveats:\n  Since it is a free server, it might not take too much load.\n  I have not thoroughly tested the caching routine. I just hope that there are no memory errors with the limited memory on the server.\n  The News API is also free. There might be rate limits that might kick in even after I have tried to handle that.\n  3. Learning For folks who are lost, you might like to start with the basics first. Here is my introductory posts on Streamlit and Plotly express.\n  How to write Web apps using simple Python for Data Scientists?   Python‚Äôs One Liner graph creation library with animations Hans Rosling Style    Conclusion Here I have tried creating a dashboard for news on Coronavirus, but it is still in a nascent stage, and a lot needs to be done.\nFor one, it needs a large server. For another, a lot of time to improve the visualization and layouts. And also a lot of testing.\nAlso, we have done a few things in a roundabout way using HTML and few hacks. There are still a lot of things that I will love to have in Streamlit. I have been in talks with the Streamlit team over the new functionality that they are going to introduce, and I will try to keep you updated on the same. The good news is that Layout options are a part of the new functionality that Streamlit is working on.\nYou can find the full code for the final app here at my Github repo. And here is the full app on the web.\nIf you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/03/29/coronatimes/","tags":["Streamlit","Production","Visualization","Awesome Guides","Best Content"],"title":"A Newspaper for COVID-19‚Ää‚Äî‚ÄäThe CoronaTimes"},{"categories":["Learning Resources"],"contents":"With Coronavirus on the prowl, there has been a huge demand across the world for MOOCs as schools and universities continue to shut down.\nSo, I find it great that providers like Coursera are hosting a lot of excellent courses on their site for free, but they are a little hard to find among all the paid courses.\nWhile these courses are not providing verified certificates if you take them for free, in my view, it is the knowledge that matters than having a few certifications.\nTLDR; With thousands of individuals laid off from this crisis, I believe it is crucial to get learning resources out now to people. So here is a list of courses that are great and free to learn.\n 1. Machine Learning  Yes, you heard it right, Coursera is providing the Game Changer Machine Learning course by Andrew Ng for free right now.\nAs for my review, I think this is the one course that should be done by everyone interested in Machine Learning. For one, it contains the maths behind many of the Machine Learning algorithms and secondly Andrew Ng is a great instructor. Believe it or not, Andrew Ng not only taught but also motivated me to learn data science when I first started.\nAs for the curriculum, this course has a little of everything ‚Äî Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.\nYou might also want to go through a few of my posts while going through this course:\n   The Hitchhiker‚Äôs Guide to Feature Extraction    The 5 Classification Evaluation metrics every Data Scientist must know    The 5 Feature Selection Algorithms every Data Scientist should know    The Simple Math behind 3 Decision Tree Splitting criterions    2. Algorithms Algorithms and data structures are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are essential all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nThey will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.\nThis series of two courses offered by Robert Sedgewick covers all the essential algorithms and data structures. The \u0026lt;strong\u0026gt;first part\u0026lt;/strong\u0026gt;  of this course covers the elementary data structures, sorting, and searching algorithms, while the second part focuses on the graph and string-processing algorithms.\nYou might also like to look at a few of my posts while trying to understand some of the material in these courses.\n  3 Programming concepts for Data Scientists   A simple introduction to Linked Lists for Data Scientists   Dynamic Programming for Data Scientists    3. Bayesian Statistics: From Concept to Data Analysis  ‚ÄúFacts are stubborn things, but statistics are pliable.‚Äù ‚Äï Mark Twain\n The war between a frequentist and bayesian is never over.\nIn this course , you will learn about MLE, priors, posteriors, conjugate priors, and a whole lot of other practical scenarios where we can use Bayesian Statistics. All in all, a well-packaged course which explains both frequentist and bayesian approach to statistics.\nFrom the course website:\n This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach.\n  4. Practical Time Series Analysis Have you heard about ARIMA models, Stationarity in time series, etc. and have been boggled by these terms? This course aims to teach Time series from a fairly mathematical perspective. I was not able to find such a course for a fairly long time. And now it is free for all.\nFrom the course website:\n In practical Time Series Analysis we look at data sets that represent sequential information, such as stock prices, annual rainfall, sunspot activity, the price of agricultural products, and more. We look at several mathematical models that might be used to describe the processes which generate these types of data\n If you want to use XGBoost or Tree-based models for time series analysis, do take a look at one of my previous post here:\n  Using Gradient Boosting for Time Series prediction tasks    5. Getting Started with AWS for Machine Learning  The secret: it‚Äôs not what you know, it‚Äôs what you show.\n There are a lot of things to consider while building a great machine learning system. But often it happens that we, as data scientists, only worry about certain parts of the project.\nBut do we ever think about how we will deploy our models once we have them?\nI have seen a lot of ML projects, and a lot of them are doomed to fail as they don‚Äôt have a set plan for production from the onset.\nHaving a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.\n This course will teach you:\n  1. How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance.\n  2. How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.\n You might also look at this post of mine, where I try to talk about apps and explain how to plan for Production.\n  How to write Web apps using simple Python for Data Scientists?   How to Deploy a Streamlit App using an Amazon Free ec2 instance?   Take your Machine Learning Models to Production with these 5 simple steps    More Free Courses Also, don‚Äôt worry if you don‚Äôt want to learn the above ones. I have collected a list of some highly-rated courses that are free to audit before writing this post. You can download the excel file here. So have a stab at whatever you want to learn.\n  Continue Learning I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/03/27/covidcourses/","tags":["Productivity","Curated Resources"],"title":"5 Online Courses you can take for free during COVID-19 Epidemic"},{"categories":["Data Science","Deep Learning"],"contents":"Feeling Helpless? I know I am.\nWith the whole shutdown situation, what I thought was once a paradise for my introvert self doesn‚Äôt look so good when it is actually happening.\nI really cannot fathom being at home much longer. And this feeling of helplessness at not being able to do anything doesn‚Äôt help.\nHonestly, I would like to help with so much more in this dire situation, but here are some small ideas around which we as AI practitioners and Data Scientists can be of use.\nDonate your Computing Power NVIDIA is asking Gamers to donate their computing power to support folding@home.\n I would say that we data scientists surely have the infrastructure at hand to help in this regard.\nAnd we can do pretty much with a few clicks. It sort of feels like instant gratification, but it still is better than doing nothing.\nYou just need to download and install their software here . I downloaded the fah-installer_7.5.1_x86.exe file for my windows system. You can download it for MAC and Linux too. And you can help with CPU resources also if you don‚Äôt have GPUs.\nWhen asked for ‚ÄúCustom Install‚Äù or ‚ÄúExpress Install‚Äù, I went for the recommended option that is ‚ÄúExpress Install‚Äù. You may want to give Team ‚ÄúPC Master Race ‚Äî PCMR‚Äù number 225605. You can leave the passkey empty or you can get a passkey if you want to keep track of work done by you.\n You can also control the system resources donated to the cause. I recommend using full if you are not using any considerable compute. I am donating my two GPUs along with CPU. Till now I have been able to donate around 3 work units.\n  Come up with Novel Approaches to help One thing that is causing a lot of concern is the lack of proper testing procedures. In the UK, for instance, the current advisory is to self isolate at minor symptoms of cold due to a shortage of tests. Also, due to this shortage of tests, a lot of numbers are not entirely reliable and may be false.\n So I was just pleasantly surprised when I saw the blog from Adrian Rosebrock, where he tried to create an automatic COVID-19 detector using the COVID-19 X-ray image dataset (curated by Dr. Joseph Cohen ) along with normal X-Ray Images from the Kaggle‚Äôs Chest X-Ray Images (Pneumonia) dataset .\nAs for the results , they seem promising:\n As you can see from the results above, our automatic COVID-19 detector is obtaining ~90‚Äì92% accuracy on our sample dataset based solely on X-ray images ‚Äî no other data, including geographical location, population density, etc. was used to train this model. We are also obtaining 100% sensitivity and 80% specificity\n These results are awesome. A 100% sensitivity means to be able to capture all the Positives. And it could be used as a preliminary test for Corona.But, I am not sure at what stage these X-rays were taken as that will also play a major role. You can check out the detailed post on pyimagesearch . A caveat he mentions is the lack of data, which is quite understandable at this point in time. But if this approach works and is worked upon with other variables at hand, it might help to detect corona.\n Can we come up with other novel ways of helping those in need?\n  Spread Awareness and Mitigate Rumors through data One good thing about working with data is that we get in the habit of understanding various biases. Another essential thing that a lot of fellow data scientists have been doing is creating awareness and calling out different biases.\nI particularly liked this post,  which provides a data science perspective on Coronavirus by fast.ai founder Jeremy Howard and Rachel Thomas.\nAlso, read up on this post by Cassie Kozyrkov , which talks about the various biases around Corona and tries to take a hypothesis testing approach to the whole situation. I particularly liked this part in her post on Smarter COVID-19 Decision-Making  If no relevant information comes in, keep doing what you were planning to do. When a different action is triggered, do it.\n  It‚Äôs not enough, but ‚Ä¶ I understand that it is still not enough and honestly very less.\nA lot needs to be done on the ground to tackle this whole situation. But these are a few things that come to my mind apart from washing our hands.\nAlso, we can discuss any ideas in which the data science community can help to tackle this enormous challenge. I would like to do so much more.\n","permalink":"https://mlwhiz.com/blog/2020/03/24/coronaai/","tags":["Data Science","Artificial Intelligence","Opinion"],"title":"Can AI help in fighting against Corona?"},{"categories":["Big Data","Data Science","Awesome Guides"],"contents":"I know ‚Äî Spark is sometimes frustrating to work with.\nAlthough sometimes we can manage our big data using tools like Rapids or Parallelization , there is no way around using Spark if you are working with Terabytes of data.\nIn my l ast few posts on Spark, I explained how to work with PySpark RDDs and Dataframes . Although these posts explain a lot on how to work with RDDs and Dataframe operations, they still are not quite enough.\nWhy? Because Spark gives memory errors a lot of times, and it is only when you genuinely work on big datasets with spark, would you be able to truly work with Spark.\nThis post is going to be about ‚Äî ‚ÄúPractical Spark and memory management tips for Data Scientists.‚Äù\n 1. Map Side Joins  The syntax of joins in Spark is pretty similar to pandas:\ndf3 = df1.join(df2, df1.column == df2.column,how='left')  But I faced a problem. The df1 had around 1Billion rows while df2 had around 100 Rows. When I tried the above join, it didn‚Äôt work and failed with memory exhausted errors after running for 20 minutes.\nI was writing this code on a pretty big cluster with more than 400 executors with each executor having more than 4GB RAM. I was stumped as I tried to repartition my data frames using multiple schemes, but nothing seemed to work.\nSo what should I do? Is Spark not able to work with a mere billion rows? Not Really. I just needed to use Map-side joins or broadcasting in Spark terminology.\nfrom pyspark.sql.functions import broadcast df3 = df1.join(broadcast(df2), df1.column == df2.column,how='left')  Using the simple broadcasting code above, I was able to send the smaller df2 to all the nodes, and this didn‚Äôt take a lot of time or memory. What happens in the backend is that a copy of df2 is sent to all the partitions and each partition uses that copy to do the join. That means that there is no data movement when it comes to df1, which is a lot bigger than df2.\n 2. Spark Cluster Configurations  Set the Parallelism and worker nodes based on your task size\nWhat also made my life difficult while I was starting work with Spark was the way the Spark cluster needs to be configured. Your spark cluster might need a lot of custom configuration ad tuning based on the job you want to run.\nSome of the most important configurations and options are as follows:\na. spark.sql.shuffle.partitions and spark.default.parallelism: spark.sql.shuffle.partitions configures the number of partitions to use when shuffling data for joins or aggregations. The spark.default.parallelism is the default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by the user. The default value for these is 200.\nIn simple words, these set the degree of parallelism you want to have in your cluster.\nIf you don‚Äôt have a lot of data, the value of 200 is fine, but if you have huge data, you might want to increase these numbers. It also depends on the number of executors you have. My cluster was pretty big with 400 executors, so I kept this at 1200. A rule of thumb is to keep it as a multiple of the number of executors so that each executor ends up with multiple jobs.\nsqlContext.setConf( \u0026quot;spark.sql.shuffle.partitions\u0026quot;, 800) sqlContext.setConf( \u0026quot;spark.default.parallelism\u0026quot;, 800)  b. spark.sql.parquet.binaryAsString I was working with .parquet files in Spark, and most of my data columns were strings. But somehow whenever I loaded the data in Spark, the string columns got converted into binary format on which I was not able to use any string manipulation functions. The way I solved this was by using:\nsqlContext.setConf(\u0026quot;spark.sql.parquet.binaryAsString\u0026quot;,\u0026quot;true\u0026quot;)  The above configuration converts the binary format to string while loading parquet files. Now it is a default configuration I set whenever I work with Spark.\nc. Yarn Configurations: There are other configurations that you might need to tune that define your cluster. But these need to be set up when the cluster is starting and are not as dynamic as the above ones. The few I want to put down here are for managing memory spills on the executor nodes. Sometimes the executor core gets a lot of work.\n  spark.yarn.executor.memoryOverhead: 8192\n  yarn.nodemanager.vmem-check-enabled: False\n  There are a lot of configurations that you might want to tune while setting up your spark cluster. You can take a look at them in the official docs .\n 3. Repartitioning  Keeping the workers happy by having them handle an equal amount of data\nYou might want to repartition your data if you feel your data has been skewed while working with all the transformations and joins. The simplest way to do it is by using:\ndf = df.repartition(1000)  Sometimes you might also want to repartition by a known scheme as this scheme might be used by a certain join or aggregation operation later on. You can use multiple columns to repartition using:\ndf = df.repartition('cola', 'colb','colc','cold')  You can get the number of partitions in a data frame using:\ndf.rdd.getNumPartitions()  You can also check out the distribution of records in a partition by using the glom function. This helps in understanding the skew in the data that happens while working with various transformations.\ndf.glom().map(len).collect()   Conclusion There are a lot of things we don‚Äôt know, we don‚Äôt know. These are called unknown unknowns. It is only by multiple code failures and reading up on multiple stack overflow threads that we understand what we need.\nHere I have tried to summarize a few of the problems that I faced around memory issues and configurations while working with Spark and how to solve them. There are a lot of other configuration options in Spark, which I have not covered, but I hope this post gave you some clarity on how to set these and use them.\nNow, if you need to learn Spark basics, take a look at my previous post: \u0026lt;strong\u0026gt;The Hitchhikers guide to handle Big Data using Spark\u0026lt;/strong\u0026gt; \u0026lt;em\u0026gt;Not just an Introduction\u0026lt;/em\u0026gt;towardsdatascience.com Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on Big Data Essentials: HDFS, MapReduce and Spark RDD and Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames by Yandex on Coursera.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/03/20/practicalspark/","tags":["Big Data","Machine Learning","Data Science","Productivity"],"title":"Practical Spark Tips for Data Scientists"},{"categories":["Learning Resources","Data Science"],"contents":"Many of my followers ask me ‚Äî How difficult is it to get a job in the Data Science field? Or what should they study? Or what path they should take?\nNow the answer is not one everyone would like ‚Äî Getting into Data Science is pretty difficult, and you have to toil hard.\nI mean you have to devote time to learn data science, understand algorithms , upgrade your skills as the market progresses, keep track of old conventional skills, and, of course, search for a job in the meantime and prepare for interviews .\nYou also have to understand business problems and develop the acumen to frame business problems as data science problems. Remember, there are no fixed algorithms.\nIt gets really exerting for some and almost impossible for others.\nTo tell you about myself, I get bored quickly if I am not learning new things. I like Data Science as it gives me that opportunity.\nSo first of all, I would like to ask if you are like that?\nIf you are, and you are interested in solving new problems almost every day, then you would love data science as a field to make your career in.\nAnd here are some tips for you brave ones.\n 1. Start Small   It is better to take many small steps in the right direction than to make a great leap forward only to stumble backward ‚Äî Old Chinese Proverb\n Now, as far as beginning a career in Data Science goes, the above fits pretty nicely. More so if you are coming from a different stream(read not Computer Science, Statistics) or if you want to make a lateral switch.\nI would advise against targeting big companies like Amazon, Google, etc. This is not to discourage you; it is more on the lines of practical thinking. I have observed their interview process, and I can assure you that it‚Äôs pretty rare, if not impossible, to get these jobs without some experience.\nBut, let me also tell you that there is no shortage of opportunities. You could easily get into a startup if you know your stuff. That is how I started myself.\n 2. Keep Learning I made it my goal to move into the data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of effort to shift jobs. Here is my story if you are interested.\nIn college, I spent a lot of my time gaming. From 2013 onwards, I spent whatever time I could find to study new technologies and learning about data science.\n Nothing will work unless You do ‚Äî Maya Angelou\n Here is the way that I took to learn about data science, and any aspiring person could choose to become a self-trained data scientist.\n \u0026lt;strong\u0026gt;How did I learn Data Science?\u0026lt;/strong\u0026gt; I hope that you don‚Äôt lose hope after seeing the long list. I already told you it wouldn‚Äôt be easy.\nYou have to start with one or two courses. The rest will follow with time. Just remember that time is a luxury you can afford.\n 3. Create your Portfolio  Having a grasp of the theory is excellent, but you really don‚Äôt add value as a data scientist if you can‚Äôt write code.\nSo work on creating stuff. Try out new toy projects. Go to kaggle for inspiration. Participate in the discussion forums. But don‚Äôt stop there.\n Think creatively. Build your GitHub profile. Try to solve different problems.\n For example, in the starting phase, I created a simple graph visualization to discover interesting posts in DataScience Subreddit using d3.js and deployed it using Flask, and Heroku. I also created a Blackjack Simulator apart from solving the usual data science problems. I also implemented a code-breaking solution using MCMC.\nI also took part in various kaggle competitions , and though I don‚Äôt have much of a rank to show for it, but I ended up learning a lot.\n 4. Blogging? This is something that comes from a personal bias of mine.\n When you blog, you end up creating high quality content for others to learn, document your learnings, understand concepts better by explaining them and maybe gain some extra recognition. What else would you want?\n Honestly, I love to write, and this is not a pure requirement to become a data scientist, but it helps a lot. I noticed that I understood data science concepts much better when I explained them. And Blogging is a perfect tool for this.\nAlso, Data Science is pretty vast, and I tend to forget whatever I learned some time ago. Blogging solves this problem too. It was in 2013 that I started my blog and tried to update it with whatever I learned. And thus, I ended up documenting everything. I still consult my blogs whenever I feel stuck on some problem.\nI feel that blogging also helped me with my communication skills as it forced me to explain difficult concepts in simpler words.\nAnyway, if you don‚Äôt like to blog, you can achieve something similar by taking notes.\nAs I said, Blogging is a personal preference. And if you are interested and want to know how I started writing on medium, here is my story.\n \u0026lt;strong\u0026gt;My Data Science Blogging Journey on Medium till now\u0026lt;/strong\u0026gt;  5. Don‚Äôt be too choosy  You have an offer from an analytics company, and you are thinking if, by joining it, you are saying goodbye to data science.\nIt is a reasonably good situation to be in. While it is relatively hard to get a data science job, it might be easier to get a job as a business analyst or data analyst in an analytics company.\nI would suggest taking any job relating to analysis or reporting or something related to data. I started the same way as I began to work with analytics and switched tracks when the data science opportunity presented itself.\n Being in the vicinity of data itself will open you to such opportunities inevitably. Treat your first job just as a stepping stone.\n Once you get such a job, you will have two options:\n  Make an internal shift in the same company in the Data Science teams by creating good relationships and by showing interest, or\n  Continue your learning in your spare time, and keep giving interviews .\n  With time you would succeed. Good luck to you.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2020/02/24/job/","tags":["Jobs","Machine Learning","Data Science","Opinion"],"title":"5 tips for getting your first Data Science job in 2020"},{"categories":["Big Data","Data Science","Awesome Guides"],"contents":"Too much data is getting generated day by day.\nAlthough sometimes we can manage our big data using tools like Rapids or Parallelization , Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.\nIn my last post on Spark, I explained how to work with PySpark RDDs and Dataframes.\nAlthough this post explains a lot on how to work with RDDs and basic Dataframe operations, I missed quite a lot when it comes to working with PySpark Dataframes.\nAnd it is only when I required more functionality that I read up and came up with multiple solutions to do one single thing.\nHow to create a new column in spark?\nNow, this might sound trivial, but believe me, it isn‚Äôt. With so much you might want to do with your data, I am pretty sure you will end up using most of these column creation processes in your workflow. Sometimes to utilize Pandas functionality, or occasionally to use RDDs based partitioning or sometimes to make use of the mature python ecosystem.\nThis post is going to be about ‚Äî ‚ÄúMultiple ways to create a new column in Pyspark Dataframe.‚Äù\nIf you have PySpark installed, you can skip the Getting Started section below.\n Getting Started with Spark I know that a lot of you won‚Äôt have spark installed in your system to try and learn. But installing Spark is a headache of its own.\nSince we want to understand how it works and work with it, I would suggest that you use Spark on Databricks \u0026lt;strong\u0026gt;here\u0026lt;/strong\u0026gt; online with the community edition. Don‚Äôt worry, it is free, albeit fewer resources, but that works for us right now for learning purposes.\n Once you register and login will be presented with the following screen.\n You can start a new notebook here.\nSelect the Python notebook and give any name to your notebook.\nOnce you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.\nThe next step will be to check if the sparkcontext is present. To check if the sparkcontext is present, you have to run this command:\nsc   This means that we are set up with a notebook where we can run Spark.\n Data Here, I will work on the Movielens \u0026lt;strong\u0026gt;ml-100k.zip\u0026lt;/strong\u0026gt; dataset. 100,000 ratings from 1000 users on 1700 movies. In this zipped folder, the file we will specifically work with is the rating file. This filename is kept as ‚Äúu.data‚Äù\nIf you want to upload this data or any data, you can click on the Data tab in the left and then Add Data by using the GUI provided.\n We can then load the data using the following commands:\nratings = spark.read.load(\u0026quot;/FileStore/tables/u.data\u0026quot;,format=\u0026quot;csv\u0026quot;, sep=\u0026quot;\\t\u0026quot;, inferSchema=\u0026quot;true\u0026quot;, header=\u0026quot;false\u0026quot;) ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])  Here is how it looks:\nratings.show()   Ok, so now we are set up to begin the part we are interested in finally. How to create a new column in PySpark Dataframe?\n 1. Using Spark Native Functions Photo by Andrew James on The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.\nWe can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. We can import spark functions as:\nimport pyspark.sql.functions as F  Our first function, the F.col function gives us access to the column. So if we wanted to multiply a column by 2, we could use F.col as:\nratings_with_scale10 = ratings.withColumn(\u0026quot;ScaledRating\u0026quot;, 2*F.col(\u0026quot;rating\u0026quot;)) ratings_with_scale10.show()   We can also use math functions like F.exp function:\nratings_with_exp = ratings.withColumn(\u0026quot;expRating\u0026quot;, 2*F.exp(\u0026quot;rating\u0026quot;)) ratings_with_exp.show()   There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list here .\n 2. Spark UDFs Photo by Divide By Zero on Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I want to use the more matured Python functionality.\nTo use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()\nimport pyspark.sql.functions as F from pyspark.sql.types import * def somefunc(value): if value \u0026lt; 3: return 'low' else: return 'high' #convert to a UDF Function by passing in the function and return type of function udfsomefunc = F.udf(somefunc, StringType()) ratings_with_high_low = ratings.withColumn(\u0026quot;high_low\u0026quot;, udfsomefunc(\u0026quot;rating\u0026quot;)) ratings_with_high_low.show()    3. Using RDDs Photo by Ryan Quintal on Sometimes both the spark UDFs and SQL Functions are not enough for a particular use-case. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs. You can use this one, mainly when you need access to all the columns in the spark data frame inside a python function.\nWhatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem.\nThe process below makes use of the functionality to convert between Row and pythondict objects. We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again.\nimport math from pyspark.sql import Row def rowwise_function(row): # convert row to dict: row_dict = row.asDict() # Add a new key in the dictionary with the new column name and value. row_dict['Newcol'] = math.exp(row_dict['rating']) # convert dict to row: newrow = Row(**row_dict) # return new row return newrow # convert ratings dataframe to RDD ratings_rdd = ratings.rdd # apply our function to RDD ratings_rdd_new = ratings_rdd.map(lambda row: rowwise_function(row)) # Convert RDD Back to DataFrame ratings_new_df = sqlContext.createDataFrame(ratings_rdd_new) ratings_new_df.show()    4. Pandas UDF Photo by Pascal Bernardon on This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupby operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions.\nThe way we use it is by using the F.pandas_udf decorator. We assume here that the input to the function will be a pandas data frame. And we need to return a pandas dataframe in turn from this function.\nThe only complexity here is that we have to provide a schema for the output Dataframe. We can make that using the format below.\n# Declare the schema for the output of our function outSchema = StructType([StructField('user_id',IntegerType(),True),StructField('movie_id',IntegerType(),True),StructField('rating',IntegerType(),True),StructField('unix_timestamp',IntegerType(),True),StructField('normalized_rating',DoubleType(),True)]) # decorate our function with pandas_udf decorator [@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def subtract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.rating v = v - v.mean() pdf['normalized_rating'] =v return pdf rating_groupwise_normalization = ratings.groupby(\u0026quot;movie_id\u0026quot;).apply(subtract_mean) rating_groupwise_normalization.show()   We can also make use of this to train multiple individual models on each spark node. For that, we replicate our data and give each replication a key and some training params like max_depth, etc. Our function then takes the pandas Dataframe, runs the required model, and returns the result. The structure would look something like below.\n# 0. Declare the schema for the output of our function outSchema = StructType([StructField('replication_id',IntegerType(),True),StructField('RMSE',DoubleType(),True)]) # decorate our function with pandas_udf decorator [@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def run_model(pdf): # 1. Get hyperparam values num_trees = pdf.num_trees.values[0] depth = pdf.depth.values[0] replication_id = pdf.replication_id.values[0] # 2. Train test split Xtrain,Xcv,ytrain,ycv = train_test_split..... # 3. Create model using the pandas dataframe clf = RandomForestRegressor(max_depth = depth, num_trees=num_trees,....) clf.fit(Xtrain,ytrain) # 4. Evaluate the model rmse = RMSE(clf.predict(Xcv,ycv) # 5. return results as pandas DF res =pd.DataFrame({'replication_id':replication_id,'RMSE':rmse}) return res results = replicated_data.groupby(\u0026quot;replication_id\u0026quot;).apply(run_model)  Above is just an idea and not a working code. Though it should work with minor modifications.\n 5. Using SQL For people who like SQL , there is a way even to create columns using SQL. For this, we need to register a temporary SQL table and then use simple select queries with an additional column. One might also use it to do joins.\nratings.registerTempTable('ratings_table') newDF = sqlContext.sql('select *, 2*rating as newCol from ratings_table') newDF.show()    Conclusion Photo by Kelly Sikkema on And that is the end of this column(pun intended)\nHopefully, I‚Äôve covered the column creation process well to help you with your Spark problems. If you need to learn more of spark basics, take a look at:\n \u0026lt;strong\u0026gt;The Hitchhikers guide to handle Big Data using Spark\u0026lt;/strong\u0026gt; You can find all the code for this post at the GitHub repository or the published notebook on databricks.\nAlso, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on \u0026lt;strong\u0026gt;Big Data Essentials\u0026lt;/strong\u0026gt; , which is part of the Big Data Specialization provided by Yandex.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/24/sparkcolumns/","tags":["Big Data","Machine Learning","Data Science","Awesome Guides","Best Content"],"title":"5 Ways to add a new column in a PySpark Dataframe"},{"categories":["Data Science"],"contents":"Have you ever been frustrated by doing data exploration and manipulation with Pandas?\nWith so many ways to do the same thing, I get spoiled by choice and end up doing absolutely nothing.\nAnd then for a beginner, the problem is just the opposite as in how to do even a simple thing is not appropriately documented. Understanding Pandas syntax can be a hard thing for the uninitiated.\nSo what should one do?\nThe creators of Bamboolib had an idea that solved this problem ‚Äî Why not add a GUI to pandas?\nThe idea is to ‚ÄúLearn and use pandas without coding.‚Äù Now the idea may have started simple, but I found Bamboolib to be so much more when it comes to data exploration and data cleaning.\nThis post is about setting up and using Bamboolib for your data.\n Installing Bamboolib Installation is pretty simple with:\npip install bamboolib  To get bamboolib to work with Jupyter and Jupyterlab, I will need to install some additional extensions. Since I like working with Jupyter Notebook, I installed the Jupyter Notebook extensions via the following command:\njupyter nbextension enable --py qgrid --sys-prefix jupyter nbextension enable --py widgetsnbextension --sys-prefix jupyter nbextension install --py bamboolib --sys-prefix jupyter nbextension enable --py bamboolib --sys-prefix  If you want the process to install for Jupyterlab, here is the process .\n Verifying Bamboolib Installation To check if everything works as intended, you can open up a Jupyter notebook, and execute the following commands:\nimport bamboolib as bam import pandas as pd data = pd.read_csv(bam.titanic_csv) bam.show(data)  The first time you run this command, you will be asked to provide a Licence key. The key is needed if you want to use bamboolib over your own data. Since I wanted to use bamboolib for my own project, I got the key from one of Bamboolib founder Tobias Krabel who was gracious enough to provide it to me to review. You can, however, buy your own from https://bamboolib.8080labs.com/pricing/ . If you want to see the library in action before purchasing the key, you can try out the live demo .\n Once bamboolib is activated, the fun part starts. You can see the output of Bamboolib like this. You can choose to play with the options it provides.\n So let‚Äôs try Bamboolib with our exciting data source, we all have seen Titanic data aplenty.\nTo do this, I will be using the Mobile Price Classification data from Kaggle. In this problem, we have to create a classifier that predicts the price range of mobile phones based on the features of a mobile phone. So lets start this up with Bamboolib.\ntrain = pd.read_csv(\u0026quot;../Downloads/mobile-price-classification/train.csv\u0026quot;) bam.show(train)  We need to do a simple call to bam.show(train) to start Bamboolib.\n Easy Data Exploration Bamboolib helps a great bit for Exploratory Data analysis. Now, Data exploration is an integral part of any data science pipeline. And writing the whole code for data exploration and creating all the charts is complicated and needs a lot of patience and effort to get right. I will admit sometimes I do slack off and am not able to give enough time for it.\nBamboolib makes the whole Data Exploration exercise a breeze.\nFor example. Here is a glimpse of your data, once you click on Visualize Dataframe.\n You get to see the missing values in each column, as well as the number of unique values and a few instances.\nBut that‚Äôs not all. We can get univariate column-level statistics and information, as well. So lets get some information about our target variable ‚Äî Price Range.\n Here we deep-dive into the target column and can see univariate column statistics as well as the most important predictors for our target column. It looks like RAM and battery power are the most important predictors for the price range. Nice.\nLet‚Äôs take a look at how RAM influences the price range. We can use bivariate plots for this.\n Getting such beautiful plots with standard Python libraries like seaborn or plotly usually takes some amount of code. Although plotly_express helps a lot in this by giving simple functions for most charts, Bamboolib creates a lot of important charts for us automatically.\nAbove, we can see that as RAM increases, the price range increases. We also see a weighted F1 Score of 0.676 for the RAM Variable. You can do this for every variable in your dataset and try to get a sense of your data.\nOne can also export the code of these charts to use in some presentation/ export these charts as PNG.\nTo do this just copy the code fragment that shows above each graph. For example, you can copy and run the code to see price_range vs ram, and you will see an option to download these graphs as PNG. In the backend, they are all plotly graphs.\nbam.plot(train, 'price_range', 'ram')    GUI Based Data Munging Have you ever faced the problem of forgetting pandas code to do something and going to stack overflow and getting lost in various threads? If yes, here is a Minimal Pandas refresher. Or you can use Bamboolib as per your preference.\nBamboolib makes it so easy to do things and not get lost in the code. You can drop columns, filter, sort, join, groupby, pivot, melt (Mostly everything you would like to do with a dataset) all by using the simple GUI provided.\nFor example, here I am dropping the missing values from the target column, if any. You can add multiple conditions, as well.\n The best part is that it also gives us the code. Here the code to drop the missing values gets populated in the cell automatically.\ntrain = train.loc[train['price_range'].notna()] train.index = pd.RangeIndex(len(train))  It works just like Microsoft Excel for business users while providing all the code to slice and dice the data for the advanced ones. You can try to play with the other options to get familiar.\nHere is another example of how to use groupby. It is actually pretty intuitive.\n The code for this gets populated as:\ntrain = train.groupby(['price_range']).agg({'battery_power': ['mean'], 'clock_speed': ['std']}) train.columns = ['_'.join(multi_index) for multi_index in train.columns.ravel()] train = train.reset_index()  You can see how it takes care of multi_index as well as ravel for us, which are a bit difficult to understand and deal with.\n Conclusion The GUI of Bamboolib is pretty intuitive, and I found it an absolute joy to work with. The project is still in its beginnings, but what a beginning it has been.\nI can surely say that this library is pretty useful for beginners who want to learn to code in Pandas as it provides them access to all the necessary functions without being bothersome.\nWhile I will still focus on understanding the basics of Pandas and would advise looking at the output of Bamboolib to learn Pandas as well, I would like to see how the adoption of Bamboolib happens in the future.\nLet me know your thoughts as well in the comments.\nIf you want to learn more about Pandas, I would like to call out an excellent course on Introduction to Data Science in Python from the University of Michigan or check out my previous post on how to work with Pandas .\nI am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; ","permalink":"https://mlwhiz.com/blog/2020/02/23/bamboo/","tags":["Data Science","Visualization","Tools"],"title":"Bamboolib‚Ää‚Äî‚ÄäLearn and use Pandas without Coding"},{"categories":["Data Science","programming"],"contents":"XGBoost is one of the most used libraries fora data science.\nAt the time XGBoost came into existence, it was lightning fast compared to its nearest rival Python‚Äôs Scikit-learn GBM. But as the times have progressed, it has been rivaled by some awesome libraries like LightGBM and Catboost, both on speed as well as accuracy.\nI, for one, use LightGBM for most of the use cases where I have just got CPU for training. But when I have a GPU or multiple GPUs at my disposal, I still love to train with XGBoost.\nWhy?\nSo I could make use of the excellent GPU Capabilities provided by XGBoost in conjunction with Dask to use XGBoost in both single and multi-GPU mode.\nHow?\nThis post is about running XGBoost on Multi-GPU machines.\n Dataset:  We are going to be using the UCI Higgs dataset . This is a binary classification problem with 11M rows and 29 columns and can take a considerable time to solve.\nFrom the UCI Site:\n The data has been produced using Monte Carlo simulations. The first 21 features (columns 2‚Äì22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\n We can load this dataset into memory by using the nifty function that I borrow from this NVidia post .\nif sys.version_info[0] \u0026gt;= 3: from urllib.request import urlretrieve else: from urllib import urlretrieve data_url = \u0026#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\u0026#34; dmatrix_train_filename = \u0026#34;higgs_train.dmatrix\u0026#34; dmatrix_test_filename = \u0026#34;higgs_test.dmatrix\u0026#34; csv_filename = \u0026#34;HIGGS.csv.gz\u0026#34; train_rows = 10500000 test_rows = 500000 num_round = 1000 plot = True # return xgboost dmatrix def load_higgs(): if os.path.isfile(dmatrix_train_filename) and os.path.isfile(dmatrix_test_filename): dtrain = xgb.DMatrix(dmatrix_train_filename) dtest = xgb.DMatrix(dmatrix_test_filename) if dtrain.num_row() == train_rows and dtest.num_row() == test_rows: print(\u0026#34;Loading cached dmatrix...\u0026#34;) return dtrain, dtest if not os.path.isfile(csv_filename): print(\u0026#34;Downloading higgs file...\u0026#34;) urlretrieve(data_url, csv_filename) df_higgs_train = pandas.read_csv(csv_filename, dtype=np.float32, nrows=train_rows, header=None) dtrain = xgb.DMatrix(df_higgs_train.ix[:, 1:29], df_higgs_train[0]) dtrain.save_binary(dmatrix_train_filename) df_higgs_test = pandas.read_csv(csv_filename, dtype=np.float32, skiprows=train_rows, nrows=test_rows, header=None) dtest = xgb.DMatrix(df_higgs_test.ix[:, 1:29], df_higgs_test[0]) dtest.save_binary(dmatrix_test_filename) return dtrain, dtest dtrain, dtest = load_higgs() This function downloads the Higgs dataset and creates Dmatrix objects for later XGBoost use.\n XGBoost: The CPU Method  As we have the data loaded, we can train the XGBoost model with CPU for benchmarking purposes.\nprint(\u0026quot;Training with CPU ...\u0026quot;) param = {} param['objective'] = 'binary:logitraw' param['eval_metric'] = 'error' param['silent'] = 1 param['tree_method'] = 'hist' tmp = time.time() cpu_res = {} xgb.train(param, dtrain, num_round, evals=[(dtest, \u0026quot;test\u0026quot;)], evals_result=cpu_res) cpu_time = time.time() - tmp print(\u0026quot;CPU Training Time: %s seconds\u0026quot; % (str(cpu_time))) --------------------------------------------------------------- CPU Training Time: 717.6483490467072 seconds  This code takes 717 seconds, which is around 12 minutes to finish. That is great and commendable, but can we do better?\n XGBoost: The Single GPU Method  What is great is that we don‚Äôt have to change a lot in the above code to be able to use a single GPU for our model building.\n Why use CPU when we can use GPU?\n We change the tree_method to gpu_hist\nprint(\u0026quot;Training with Single GPU ...\u0026quot;) param = {} param['objective'] = 'binary:logitraw' param['eval_metric'] = 'error' param['silent'] = 1 param['tree_method'] = 'gpu_hist' tmp = time.time() gpu_res = {} xgb.train(param, dtrain, num_round, evals=[(dtest, \u0026quot;test\u0026quot;)], evals_result=gpu_res) gpu_time = time.time() - tmp print(\u0026quot;GPU Training Time: %s seconds\u0026quot; % (str(gpu_time))) ---------------------------------------------------------------- GPU Training Time: 78.2187008857727 seconds  And we achieve a 10x speedup with our model now finishing in 1.3 minutes. That is great, but can we do even better if we have multiple GPUs?\n XGBoost: The Multi GPU Method  I have, for example, 2 GPUs in my machine while the above code utilizes only 1 GPU. With GPU‚Äôs getting a lot cheaper now, it is not unusual for clusters to have more than 4 GPUs. So can we use multiple GPUs simultaneously?\n Two GPUs are always better than one\n To use MultiGPUs, the process is not so simple as to add a little argument as above, and there are a few steps involved.\nThe first is the difference in Data loading:\ndef load_higgs_for_dask(client): # 1. read the CSV File using Pandas df_higgs_train = pandas.read_csv(csv_filename, dtype=np.float32, nrows=train_rows, header=None).ix[:, 0:30] df_higgs_test = pandas.read_csv(csv_filename, dtype=np.float32, skiprows=train_rows, nrows=test_rows, header=None).ix[:, 0:30] # 2. Create a Dask Dataframe from Pandas Dataframe. ddf_higgs_train = dask.dataframe.from_pandas(df_higgs_train, npartitions=8) ddf_higgs_test = dask.dataframe.from_pandas(df_higgs_test, npartitions=8) ddf_y_train = ddf_higgs_train[0] del ddf_higgs_train[0] ddf_y_test = ddf_higgs_test[0] del ddf_higgs_test[0] #3. Create Dask DMatrix Object using dask dataframes ddtrain = DaskDMatrix(client, ddf_higgs_train ,ddf_y_train) ddtest = DaskDMatrix(client, ddf_higgs_test ,ddf_y_test) return ddtrain, ddtest There are multiple steps in data load as we need dask DMatrix objects to train XGBoost with multiple GPUs.\n  Read the CSV File using Pandas.\n  Create a Dask Dataframe from Pandas Dataframe, and\n  Create Dask DMatrix Object using dask data frames.\n  To use Multi-GPU for training XGBoost, we need to use Dask to create a GPU Cluster. This command creates a cluster of our GPUs that could be used by dask by using the client object later.\ncluster = LocalCUDACluster() client = Client(cluster)  We can now load our Dask Dmatrix Objects and define the training parameters. Note nthread beings set to one and tree_method set to gpu_hist\nddtrain, ddtest = load_higgs_for_dask(client) param = {} param['objective'] = 'binary:logitraw' param['eval_metric'] = 'error' param['silence'] = 1 param['tree_method'] = 'gpu_hist' param['nthread'] = 1  We can now train on Multiple GPUs using:\nprint(\u0026quot;Training with Multiple GPUs ...\u0026quot;) tmp = time.time() output = xgb.dask.train(client, param, ddtrain, num_boost_round=1000, evals=[(ddtest, 'test')]) multigpu_time = time.time() - tmp bst = output['booster'] multigpu_res = output['history'] print(\u0026quot;Multi GPU Training Time: %s seconds\u0026quot; % (str(multigpu_time))) --------------------------------------------------------------- Multi GPU Training Time: 50.08211898803711 seconds  Please note how the call to xgb.train changes to xgb.dask.train and how it also needs the dask client to work.\nThis took around 0.8 Minutes that is a 1.5x Speedup from Single GPU. I only had 2 GPUs at my disposal, so I can‚Äôt test it, but I believe that it increases linearly, i.e. more GPU and more reduction in time.\n Results Here are the results of all three setups:\n Although the difference between Multi and Single CPU looks redundant right now, it will be pretty considerable while running multiple hyperparameter tuning tasks at hand where one might need to run multiple GBM Models with different Hyperparams.\nAlso, this result can change when we scale it to many GPUs.\nSo keep scaling.\nYou can find the complete code for this post on Github .\n Continue Learning If you are interested in Deep Learning and want to use your GPU for that, I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization .\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/23/xgbparallel/","tags":["Machine Learning","Data Science","Artificial Intelligence","xgboost","Dask","Productivity"],"title":"Lightning Fast XGBoost on Multiple GPUs"},{"categories":["Data Science"],"contents":"A Machine Learning project is never really complete if we don‚Äôt have a good way to showcase it.\nWhile in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.\nAs Sten Sootla says in his satire piece which I thoroughly enjoyed:\n The secret: it‚Äôs not what you know, it‚Äôs what you show.\n This is where StreamLit comes in and provides a way to create web apps just using Python. I have been keeping close tabs on this excellent product for the past few months. In my last few posts, I talked about Working with Streamlit and how to Deploy the streamlit app using ec2 . I have also been in constant touch with the Streamlit team while they have been working continuously to make the user experience even better by releasing additional features.\nSo, have you ever had a problem with explaining how the app works to the stakeholders/business partners? Having to set up multiple calls with different stakeholders in different countries and explaining the whole process again and again?\nOr have you worked on a project that you want to share on social media? LinkedIn, Youtube, and the like?\nWith their new version, Streamlit has released a new feature called ‚ÄúRecord a Screencast‚Äù which will solve this problem for you.\nHow? Read on.\n Setting up So to check this new feature out, which is a part of Streamlit‚Äôs version 0.55.0 offering, we need to first install or upgrade streamlit. Do this by using this command:\npip install --upgrade streamlit  We also need to run Streamlit. Here I will use the demo app. You can also use any of your own apps .\nstreamlit hello  You should see something like below:\n A tab also opens up in your browser, where you can try their demo. If that doesn‚Äôt open up in the browser, you can manually go to the Local URL http://localhost:8501/ too.\n  Recording the Screencast Now the time has come to record our screencast to share with the world. You can find the option to record the screencast using the top-right menu in Streamlit.\n Once you click on that, you will get the option to record audio, and you can select the aptly named ‚ÄúStart Recording‚Äù button to start recording.\n You can then choose what you want to share ‚Äî just your streamlit app or your entire desktop. One can choose to share the whole desktop if they need to go forth between different programs like Excel sheets, powerpoints, and the streamlit app, for example. Here I choose to show just the ‚ÄúStreamlit‚Äù App and click share.\n Your screencast has now started, and you can record the explanation session for your shareholders now. Once you are done with the recording, you can click on the top-right menu again and select stop recording. Or conveniently press escape to end the recording session.\n You will be able to preview and save the session video you recorded as a .webm file, which you can aim to send to your shareholders and even share on LinkedIn/twitter/youtube for your personal projects.\n And that‚Äôs it. The process is pretty simple and doesn‚Äôt need any additional software installation from our side.\n Endnotes Streamlit has democratized the whole process of creating apps.\nI honestly like the way Streamlit is working on developing its product, keeping in mind all the pain points of its users. With this iteration, they have resolved one more pain point where users struggle to showcase their work in a meaningful way on social media sites or to explain the workings of an app multiple times to the shareholders.\nOn top of that, Streamlit is a free and open-source rather than a proprietary web app that works out of the box. I couldn‚Äôt recommend it more.\nAlso, do let me know if you want to request any additional features in Streamlit in the comments section. I will make sure to pass it on to the Streamlit team.\nIf you want to learn more about using Streamlit to create and deploy apps, take a look at my other posts:\n  \u0026lt;strong\u0026gt;How to write Web apps using simple Python for Data Scientists?\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;How to Deploy a Streamlit App using an Amazon Free ec2 instance?\u0026lt;/strong\u0026gt;   If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/23/streamlitrec/","tags":["Productivity","Tools","Visualization","Data Science","Streamlit"],"title":"Share your Projects even more easily with this New Streamlit Feature"},{"categories":["Big Data","Data Science"],"contents":"Recently I was working on tuning hyperparameters for a huge Machine Learning model.\nManual tuning was not an option since I had to tweak a lot of parameters. Hyperopt was also not an option as it works serially i.e. at a time, only a single model is being built. So it was taking up a lot of time to train each model and I was pretty short on time.\nI had to come up with a better more efficient approach if I were to meet the deadline. So I thought of the one thing that helps us data scientists in many such scenarios ‚Äî Parallelization.\nCan I parallelize my model hyperparameter search process?\nAs you would have guessed, the answer is Yes.\nThis post is about setting up a hyperparameter tuning framework for Data Science using scikit-learn/xgboost/lightgbm and pySpark.\n Grid vs Randomized? Before we get to implementing the hyperparameter search, we have two options to set up the hyperparameter search ‚Äî Grid Search or Random search.\n Starting with a 3√ó3 grid of parameters, we can see that Random search ends up doing more searches for the important parameter.\nThe figure above gives a definitive answer as to why Random search is better.\nLet‚Äôs say we have to tune two hyperparameters for our Machine Learning model. One is not important, and one is very important. In a grid search, we look at three settings for the important parameter. While in a randomized search, we search through 9 settings for the important parameter. And the amount of time we spent is the same.\nSince, Randomized search, searches more thoroughly through the whole space and provides us with better hyperparameters, we will go with it in our example.\n Setting Up Our Example At my workplace, I have access to a pretty darn big cluster with 100s of nodes. It is a data Scientist‚Äôs dream. But in this post, I am going to be using the Databricks Community Edition Free server with a toy example. If you want to set up this small server for yourself for practice, check out my post on Spark.\nYou can choose to load your data using Spark, but here I start by creating our own classification data to set up a minimal example which we can work with.\nX,y = datasets.make_classification(n_samples=10000, n_features=4, n_informative=2, n_classes=2, random_state=1,shuffle=True) train = pd.DataFrame(X) train['target'] = y # Convert this pandas Data to spark Dataframe. train_sp = spark.createDataFrame(train) # Change the column names. train_sp = train_sp.toDF(*['c0', 'c1', 'c2', 'c3', 'target'])  The train_sp spark dataset looks like:\n  The Idea ‚Äî Replicate and Apply Photo by Frank Vessia on Photo by Frank Vessia on Unsplash \nSo now we have got our training dataset in Spark. And we want to run multiple models on this DataFrame.\nSpark is inherently good with Key-Value pairs. That is all data with a particular key could be sent to a single machine. And we can apply functions to that data.\nBut we want all our data on every machine. How do we do that?\nWe replicate our data n times and add a replication_id to our data so that each key has all the data.\nOk, now we can send the whole data to multiple machines using groupby on replication_id. But how do we use pandas and scikit learn on that data?\nThe answer is: we use pandas_udf. This functionality was introduced in the Spark version 2.3.1. And this allows you to utilise pandas functionality with Spark.\nIf you don‚Äôt understand this yet, do look at the code as sometimes it is easier to understand the code.\n The Code We first replicate our train dataframe 100 times here by using cross_join with a data frame that contains a column with 1‚Äì100 replication_id.\n# replicate the spark dataframe into multiple copies replication_df = spark.createDataFrame(pd.DataFrame(list(range(1,100)),columns=['replication_id'])) replicated_train_df = train_sp.crossJoin(replication_df)   We also define a function that takes as input a pandas dataframe, gets random hyperparameters using the python random module, runs a model on data(Here I am training a scikit model, but you can replace it with any model like XGBoost or Lightgbm as well) and returns the result in the form of a Pandas Dataframe. Do take a look at the function and the comments.\n# 0. Declare the schema for the output of our function outSchema = StructType([StructField(\u0026#39;replication_id\u0026#39;,IntegerType(),True),StructField(\u0026#39;Accuracy\u0026#39;,DoubleType(),True),StructField(\u0026#39;num_trees\u0026#39;,IntegerType(),True),StructField(\u0026#39;depth\u0026#39;,IntegerType(),True),StructField(\u0026#39;criterion\u0026#39;,StringType(),True)]) # decorate our function with pandas_udf decorator @F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def run_model(pdf): # 1. Get randomized hyperparam values num_trees = random.choice(list(range(50,500))) depth = random.choice(list(range(2,10))) criterion = random.choice([\u0026#39;gini\u0026#39;,\u0026#39;entropy\u0026#39;]) replication_id = pdf.replication_id.values[0] # 2. Train test split X = pdf[[\u0026#39;c0\u0026#39;, \u0026#39;c1\u0026#39;, \u0026#39;c2\u0026#39;, \u0026#39;c3\u0026#39;]] y = pdf[\u0026#39;target\u0026#39;] Xtrain,Xcv,ytrain,ycv = train_test_split(X, y, test_size=0.33, random_state=42) # 3. Create model using the pandas dataframe clf = RandomForestClassifier(n_estimators=num_trees, max_depth = depth, criterion =criterion) clf.fit(Xtrain,ytrain) # 4. Evaluate the model accuracy = accuracy_score(clf.predict(Xcv),ycv) # 5. return results as pandas DF res =pd.DataFrame({\u0026#39;replication_id\u0026#39;:replication_id,\u0026#39;Accuracy\u0026#39;:accuracy, \u0026#39;num_trees\u0026#39;:num_trees,\u0026#39;depth\u0026#39;:depth,\u0026#39;criterion\u0026#39;:criterion}, index=[0]) return res We can now apply this pandas_udf function to our replicated dataframe using:\nresults = replicated_train_df.groupby(\u0026quot;replication_id\u0026quot;).apply(run_model)  What the above code does is that it sends all the data with the same replication id to a single machine and applies the function run_model to the data. The above call happens lazily so you won‚Äôt be able to see the results till you run the below action call.\nresults.sort(F.desc(\u0026quot;Accuracy\u0026quot;)).show()   For this toy example, the accuracy results may look pretty close to one another, but they will differ in the case of noisy real-world datasets. Since all of these 100 models run in parallel on different nodes, we can save a lot of time when doing random hyperparameter search.\nThe speedup factor certainly depends on how many nodes you have in your cluster. For me, I had 100 machines at my disposal, so I got ~ 100x speedup.\nYou can get the full code in this Databricks Notebook or get it from my GitHub repository where I keep codes for all my posts.\n Continue Learning If you want to learn more about practical data science, do take a look at the ‚ÄúHow to win a data science competition‚Äù Coursera course. I learned a lot of new things from this course taught by one of the most prolific Kaggler.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/22/hyperspark/","tags":["Big Data","Machine Learning","Data Science","Spark"],"title":"100x faster Hyperparameter Search Framework with Pyspark"},{"categories":["Data Science"],"contents":" A Machine Learning project is never really complete if we don‚Äôt have a good way to showcase it.\nWhile in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.\nAnd Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.\nNot to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.\nThis is where StreamLit comes in and delivers on its promise to create web apps just using Python.\nIn my last post on Streamlit , I talked about how to write Web apps using simple Python for Data Scientists.\nBut still, a major complaint, if you would check out the comment section of that post, was regarding the inability to deploy Streamlit apps over the web.\nAnd it was a valid complaint.\n A developer can‚Äôt show up with his laptop every time the client wanted to use the app. What is the use of such an app?\n So in this post, we will go one step further deploy our Streamlit app over the Web using an Amazon Free ec2 instance.\n Setting up the Amazon Instance Before we start with using the amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website . Works just like a simple sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.\n  Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console .\n  On the AWS Management Console, you can select ‚ÄúLaunch a Virtual Machine‚Äù. Here we are trying to set up the machine where we will deploy our Streamlit app.\n  In the first step, you need to choose the AMI template for the machine. I select the 18.04 Ubuntu Server since it is applicable for the Free Tier. And Ubuntu.\n    In the second step, I select the t2.micro instance as again it is the one which is eligible for the free tier. As you can see t2.micro is just a single CPU instance with 512 MB RAM. You can opt for a bigger machine if you are dealing with a powerful model or are willing to pay.    Keep pressing Next until you reach the ‚Äú6. Configure Security Group‚Äù tab. You will need to add a rule with Type: ‚ÄúCustom TCP Rule‚Äù, Port Range:8501, and Source: Anywhere. We use the port 8501 here since it is the custom port used by Streamlit.    You can click on ‚ÄúReview and Launch‚Äù and finally on the ‚ÄúLaunch‚Äù button to launch the instance. Once you click on Launch you might need to create a new key pair. Here I am creating a new key pair named streamlit and downloading that using the ‚ÄúDownload Key Pair‚Äù button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on ‚ÄúLaunch Instance‚Äù after downloading the key pair    You can now go to your instances to see if your instance has started. Hint: See the Instance state, it should be showing ‚ÄúRunning‚Äù     Select your instance, and copy the Public DNS(IPv4) Address from the description. It should be something starting with ec2.\n  Once you have that run the following commands in the folder you saved the streamlit.pem file. I have masked some of the information here.\n   chmod 400 streamlit.pem ssh -i \u0026quot;streamlit.pem\u0026quot; ubuntu@\u0026lt;Your Public DNS(IPv4) Address\u0026gt;   Installing Required Libraries Whoa, that was a handful. After all the above steps you should be able to see the ubuntu prompt for the virtual machine. We will need to set up this machine to run our app. I am going to be using the same streamlit_football_demo app that I used in my previous post .\nWe start by installing miniconda and adding its path to the environment variable.\nsudo apt-get update wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p ~/miniconda echo \u0026quot;PATH=$PATH:$HOME/miniconda/bin\u0026quot; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  We then install additional dependencies for our app to run. That means I install streamlit and plotly_express .\npip install streamlit pip install plotly_express  And our machine is now prepped and ready to run.\n Running Streamlit on Amazon ec2 As I am set up with the instance, I can get the code for my demo app from Github . Or you can choose to create or copy another app as you wish.\ngit clone https://github.com/MLWhiz/streamlit_football_demo.git cd streamlit_football_demo streamlit run helloworld.py   Now you can go to a browser and type the external URL to access your app. In my case the address is http://35.167.158.251:8501 . Here is the output. This app will be up right now if you want to play with it.\n  A Very Small Problem Though We are up and running with our app for the world to see. But whenever you are going to close the SSH terminal window the process will stop and so will your app.\nSo what do we do?\nTMUX to the rescue. TMUX allows us to keep running our sessions even after we leave the terminal window. It also helps with a lot of other things but I will just go through the steps we need.\nFirst, we stop our app using Ctrl+C and install tmux\nsudo apt-get install tmux  We start a new tmux session using the below command. We keep the name of our session as StreamSession. You could use any name here.\ntmux new -s StreamSession   You can see that the session name is ‚ÄúStreamSession‚Äù at the bottom of the screen. You can now start running streamlit in the tmux session.\nstreamlit run helloworld.py   You will be able to see your app at the External URL . The next step is to detach our TMUX session so that it continues running in the background when you leave the SSH shell. To do this just press Ctrl+B and then D (Don‚Äôt press Ctrl when pressing D)\n You can now close your SSH session and the app will continue running at the External URL.\nAnd Voila! We are up and running.\nPro TMUX Tip: You can reattach to the same session by using the attach command below. The best part is that you can close your SSH shell and then maybe come back after some hours and reattach to a session and keep working from wherever you were when you closed the SSH shell.\ntmux attach -t StreamSession   Simple Troubleshooting: If your app is not hosting at 8501, it means that an instance of streamlit app is already running on your system and you will need to stop that. You can do so by first finding the process ID\nps aux | grep streamlit  You will see something like:\nubuntu 20927 2.4 18.8 713780 189580 pts/3 Sl+ 19:55 0:26 /home/ubuntu/miniconda/bin/python /home/ubuntu/miniconda/bin/streamlit run helloworld.py  You will need to kill this process. You can do this simply by\nkill -9 20947   Conclusion  Streamlit has democratized the whole process to create apps, and I couldn‚Äôt recommend it more. If you want to learn more about how to create awesome web apps with Streamlit then read up my last post.\nIn this post, we deployed a simple web app on AWS using amazon ec2.\nIn the process of doing this, we created our own Amazon ec2 instance, logged into the SSH shell, installed miniconda and dependencies, ran our Streamlit application and learned about TMUX. Enough learning for a day?\nSo go and show on these Mad skills. To end on a lighter note, as Sten Sootla says in his satire piece which I thoroughly enjoyed:\n The secret: it‚Äôs not what you know, it‚Äôs what you show.\n If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization . Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/22/streamlitec2/","tags":["Streamlit","Productivity","Tools","Visualization","Data Science","ec2"],"title":"How to Deploy a Streamlit App using an Amazon Free ec2 instance?"},{"categories":["Data Science"],"contents":"Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.\nSometimes back, I wrote about the subset of Pandas functionality I end up using often. In this post, I will talk about handling most of those data manipulation cases in Python on a GPU using cuDF.\nWith a sprinkling of some recommendations throughout.\nPS: for benchmarking, all the experiments below are done on a Machine with 128 GB RAM and a Titan RTX GPU with 24 GB RAM.\n What is Rapids CuDF, and why to use it?  Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n Simply, Rapids CuDF is a library that aims to bring pandas functionality to GPU. Apart from CuDF, Rapids also provides access to cuML and cuGraph as well, which are used to work with Machine Learning algorithms and graphs on GPU , respectively.\nNow, what is the advantage of this?\nA typical GPU has over 2000 CUDA cores. Pandas, when parallelized using Dask or multiprocessing, can use eight cores or 16 CPU cores that your machine has. Now, these CPU cores are different in their power, but the CUDA cores can do easy calculations fast and thus can provide us with significant speedups.\nMy GPU Titan RTX has around 4600 cores. That means I should be able to parallelize my computations using GPU.\nBut the problem is that writing code to run for GPU is hard. And Rapids CuDF solves this problem.\nBefore we go any further, here is a simple example of how cuDF could help you. Here I try to get means of all columns in my random data frame having 100 million rows and five columns.\n That is a ~350x speedup using cuDF!!! And the code remains essentially the same. And remember, I am using a system with 128 GB RAM.\n Installation ‚Äî RAPIDS cuDF So now we are convinced that cuDF is beneficial, the simplest way to install RAPIDS is by just going to the site and check what you need using the release selector tool.\n For me, the installation command was:\nconda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.11 python=3.7 cudatoolkit=10.1  For starting up or learning, you could also get started with the Google Colab notebook, which comes pre-installed with the required RAPIDS environment.\nI will use the US Accidents dataset in this post to show the capability of CuDF Dataframes.\n Reading Data with CuDF The first thing we do is reading the data source. We can read data in cudf from the local file system\nimport cudf gdf = cudf.read_csv('US_Accidents_May19.csv')  This command took around 1 second compared to 13 seconds when I read using pd.read_csv function\n We could also have read from pandas Dataframes using:\npdf = pd.read_csv('US_Accidents_May19.csv') gdf = cudf.DataFrame.from_pandas(pdf)   On that note, we can reconvert a cuDF dataframe back to a Pandas Dataframe to take advantage of the much more mature Pandas ecosystem whenever needed.\npdf = gdf.to_pandas()   Data Snapshot Always useful to see some of the data. First, let us try the simple Head and Tail commands:\nYou can use simple head and tail commands with an option to specify the number of rows.\n# top 5 rows gdf.head() # top 50 rows gdf.head(50) # last 5 rows gdf.tail() # last 50 rows gdf.tail(50)  You can also see simple dataframe statistics with the following commands.\n# To get statistics of numerical columns gdf.describe()   You can also use normal functions like:\nprint(gdf['TMC'].mean()) # no of rows in dataframe print(len(gdf)) # Shape of Dataframe print(gdf.shape) --------------------------------------------------------------- 207.35274265463238 2243939 (2243939, 49)  Recommendation: Generally working with Jupyter notebook, I make it a point of having the first few cells in my notebook containing these snapshots of the data. This helps me see the structure of the data whenever I want to. If I don‚Äôt follow this practice, I notice that I end up repeating the .head() command a lot of times in my code.\n Handling Columns in DataFrames a. Selecting a column As with Pandas, CuDF lets you choose columns in two ways. Using the dot operator like df.Title and using square brackets like df['Title']\nI prefer the second version, mostly. Why?\nThere are a couple of reasons you would be better off with the square bracket version in the longer run.\n  If your column name contains spaces, then the dot version won‚Äôt work. For example, df.Revenue (Millions) won‚Äôt work while df['Revenue (Millions')] will.\n  It also won‚Äôt work if your column name is count or mean or any of the predefined functions.\n  Sometimes you might need to create a for loop over your column names in which your column name might be in a variable. In that case, the dot notation will not work. For Example, This works:\n  colname = 'height' df[colname] While this doesn‚Äôt:\ncolname = 'height' df.colname  Trust me. Saving a few characters is not worth it.\nRecommendation: Stop using the dot operator.\nb. Getting Column Names in a list It also works just like pandas.\ncolumnnames = cuda_df.columns  c. Specifying user-defined Column Names: Sometimes you want to change the column names as per your taste. I don‚Äôt like spaces or brackets in my column names, so I change them as such.\ngdf.columns = ['ID', 'Source', 'TMC', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance_mi', 'Description', 'Number', 'Street', 'Side', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Temperature_F', 'Wind_Chill_F', 'Humidity_%', 'Pressure_in', 'Visibility_mi', 'Wind_Direction', 'Wind_Speed_mph', 'Precipitation_in', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight'] d. Subsetting specific columns: Sometimes you only need to work with particular columns in a dataframe. e.g., to separate numerical and categorical columns, or remove unnecessary columns. Let‚Äôs say in our example, we only need a few columns\ngdf = gdf[['ID', 'Source', 'TMC', 'Severity', 'Start_Time', 'End_Time','Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng']]  e. Seeing column types: Very useful while debugging. If your code throws an error that you cannot add a str and int, you will like to run this command.\ngdf.dtypes   Apply and Lambda in CuDF apply and lambda are some of the best things I have learned to use with pandas. I use apply and lambda anytime I get stuck while building a complex logic for a new column or filter. Let\u0026rsquo;s see if we can use them in CuDF also.\na. Creating a Column You can create a new column in many ways.\nIf you want a column that is a sum or difference of columns, you can pretty much use simple basic arithmetic.\ngdf['Somecol'] = (gdf['TMC'] + gdf['Severity']/10)/2  You can also use simple apply over a series using applymap:\ndef somefunc(x): return x+2 gdf['Somecol'] = gdf['TMC'].applymap(somefunc)  But sometimes we may need to build complex logic around the creation of new columns using multiple columns.\nTo give you an example, let‚Äôs say that we want to calculate the Haversine distance based on Lats and Longs.\nHow do we do that?\nWhenever I get a hold of such problems, I use apply/lambda. Let me first show you how I will do this with pandas. A lot of the code here is taken from this post .\nfrom math import cos, sin, asin, sqrt, pi def haversine_distance(lat1, lon1, lat2, lon2): \u0026#34;\u0026#34;\u0026#34;Haversine distance formula taken from Michael Dunn\u0026#39;s StackOverflow post: https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points \u0026#34;\u0026#34;\u0026#34; x_1 = pi/180 * lat1 y_1 = pi/180 * lon1 x_2 = pi/180 * lat2 y_2 = pi/180 * lon2 dlon = y_2 - y_1 dlat = x_2 - x_1 a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) r = 6371 # Radius of earth in kilometers return c * r pdf[\u0026#39;hDistance\u0026#39;] = pdf.apply(lambda x: haversine_distance(x[\u0026#39;Start_Lat\u0026#39;],x[\u0026#39;Start_Lng\u0026#39;],x[\u0026#39;End_Lat\u0026#39;],x[\u0026#39;End_Lng\u0026#39;]),axis=1) To do the same thing in CuDF, we have to use apply_rows for applying a function to multiple rows.\ndef haversine_distance_kernel(lat1, lon1, lat2, lon2, hDistance): \u0026#34;\u0026#34;\u0026#34;Haversine distance formula taken from Michael Dunn\u0026#39;s StackOverflow post: https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points \u0026#34;\u0026#34;\u0026#34; for i, (x_1, y_1, x_2, y_2) in enumerate(zip(lat1, lon1, lat2, lon2)): x_1 = pi/180 * x_1 y_1 = pi/180 * y_1 x_2 = pi/180 * x_2 y_2 = pi/180 * y_2 dlon = y_2 - y_1 dlat = x_2 - x_1 a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) r = 6371 # Radius of earth in kilometers hDistance[i] = c * r gdf = gdf.apply_rows(haversine_distance_kernel, incols = {\u0026#39;Start_Lat\u0026#39;:\u0026#39;lat1\u0026#39;,\u0026#39;Start_Lng\u0026#39;:\u0026#39;lon1\u0026#39;,\u0026#39;End_Lat\u0026#39;:\u0026#39;lat2\u0026#39;,\u0026#39;End_Lng\u0026#39;:\u0026#39;lon2\u0026#39;}, outcols = dict(hDistance=np.float64), kwargs=dict()) See how the structure of the haversine_distance function changes and also how we call it a little bit differently. Note that this function takes hDistance as a parameter, so we even specify the output in the function call.\nIn the backend, it uses Numba for the calculations.\nNow this is all well and good, but it has a few caveats:\n  It doesn‚Äôt take as input strings, so if you wanted to use a string column, you couldn‚Äôt. This is something that CuDF has in its features list.\n  Only a few functions supported by CUDA python could be used, and not all python functions. The full list of supported functions is here .\n  So why do we use it? In this particular case, it took 48 Seconds for Pandas while only 295ms for CuDF. That is a 160x Speedup.\nb. Filtering a dataframe Pandas make filtering and subsetting dataframes pretty easy. You can filter and subset dataframes using standard operators and \u0026amp;,|,~ operators. You can do pretty much the same with cuDF.\n# Single condition df_dis_gt_2 = gdf[gdf['hDistance']\u0026gt;2] # Multiple conditions: AND And_df = gdf[(gdf['hDistance']\u0026gt;8) \u0026amp; (gdf['TMC']\u0026gt;200)] # Multiple conditions: OR Or_df = gdf[(gdf['hDistance']\u0026gt;8) | (gdf['TMC']\u0026gt;200)] # Multiple conditions: NOT Not_df = gdf[~((gdf['hDistance']\u0026gt;8) | (gdf['TMC']\u0026gt;200))]  Pretty simple stuff.\n Aggregation on Dataframes: groupby groupby will come up a lot of times whenever you want to aggregate your data. Pandas lets you do this efficiently with the groupby function like:\ndf.groupby(list of columns to groupby on).aggregate({\u0026lsquo;colname\u0026rsquo;:func1, \u0026lsquo;colname2\u0026rsquo;:func2}).reset_index()\nYou have to worry about supplying two primary pieces of information.\n  List of columns to groupby on, and\n  A dictionary of columns and functions you want to apply to those columns\n  reset_index() is a function that resets the index of a dataframe. I use this function ALWAYS whenever I do a groupby, and you might think of it as a default syntax for groupby operations.\nHelpfully the syntax remains the same for cuDF.\ngdf_gby = gdf.groupby(['Traffic_Calming','Sunrise_Sunset']).agg({'TMC':'mean','Severity':'mean'}).reset_index()   ***Caveat:***I tried the function np.mean first, which didn‚Äôt work. It provides only elementary functions sum,mean,min and max only.\n Dealing with Multiple DataFrames: Concat and Merge: a. Concat Sometimes we get data from different sources. Or someone comes to you with multiple files with each file having data for a particular year.\nHow do we create a single dataframe from these multiple dataframes?\nHere we will create our use case artificially since we have a single file. We are creating two dataframes first using the basic filter operations we already know.\nseverity_lt_3 = gdf[gdf['Severity']\u0026lt;3] severity_gte_3 = gdf[gdf['Severity']\u0026gt;=3]  Here we start with two dataframes: severity_lt_3 containing info for accidents with a severity less than 3 and severity_gte_3 providing info for accidents with severity greater than or equal to 3. We want to create a single dataframe that includes both sorts of accidents.\nfullseverity = cudf.concat([severity_lt_3,severity_gte_3])  b. Merge Most of the data that you will encounter will never come in a single file. One of the files might contain ratings for a particular movie, and another might provide the number of votes for a movie.\nIn such a case, we have two dataframes that need to be merged so that we can have all the information in a single view.\nHere we will create our use case artificially since we have a single file. We are creating two dataframes first using the basic column subset operations we already know.\naccident_times_dataframe = gdf[['ID','Start_Time','End_Time']] accident_locations_dataframe = gdf[['ID','Start_Lat','Start_Lng','End_Lat','End_Lng']]    We need to have all this information in a single dataframe. How do we do this? This syntax is also the same as Pandas.\ninformation_df = cudf.merge(accident_times_dataframe,accident_locations_dataframe,on='ID',how='left')   We provide this merge function with four attributes- 1st DF, 2nd DF, join on which column and the joining criteria:[\u0026lsquo;left\u0026rsquo;,\u0026lsquo;right\u0026rsquo;,\u0026lsquo;inner\u0026rsquo;,\u0026lsquo;outer\u0026rsquo;]\nAs far as timing is concerned, we again get a 10x speedup while doing Joins when we use cudf.\n Recommendation: I usually always end up using left join. You will rarely need to join using outer or right. Actually whenever you need to do a right join you actually just really need a left join with the order of dataframes reversed in the merge function.\n Conclusion CuDF is a step in the right direction as it provides GPU for Data Processing, which takes up a lot of time in the data science pipeline.\nHere I tried to talk about some functionality in cuDF I use often. There is quite a bit more the folks at NVIDIA are trying to implement, so do take a look at the documentation .\nAlthough some of the pandas‚Äô functionality is not yet implemented, that shouldn‚Äôt stop us from making use of the functions already implemented for time-critical applications and Kaggle.\nI, for myself, switch between cudf and pandas dataframes multiple times in my data preparation notebooks.\nIt does help a lot whenever I am a little tied up on time.\nI hope you found this post useful and worth your time. I tried to make this as simple as possible, but you may always ask me or see the documentation for doubts.\nThe whole code is posted in my Github Repo , where I keep codes for all my posts. You can find the data at Kaggle .\nAlso, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt;) .\n","permalink":"https://mlwhiz.com/blog/2020/02/22/pandas_gpu/","tags":["Machine Learning","Data Science","Artificial Intelligence","Productivity","tools"],"title":"Minimal Pandas Subset for Data Scientists on GPU"},{"categories":["Awesome Guides","Learning Resources"],"contents":"I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.\nWith those heavy steel enforced gumboots and that plastic helmet, venturing around big blast furnaces and rolling mills. Artificial safety measures, to say the least, as I knew that nothing would save me if something untoward happens. Maybe some running shoes would have helped. As for the helmet. I would just say that molten steel burns at 1370 degrees C.\nAs I realized based on my constant fear, that job was not for me, and so I made it my goal to move into the Analytics and Data Science space somewhere around in 2011. From that time, MOOCs have been my goto option for learning new things, and I ended up taking a lot of them. Good ones and bad ones.\nNow in 2020, with the Data Science field changing so rapidly, there is no shortage of resources to learn data science. But that also often poses a problem for a beginner as to where to start learning and what to learn? There are a lot of great resources on the internet, but that means there are a lot of bad ones too.\n A lot of choices may often result in stagnation as anxiety is not good when it comes to learning.\n In his book, The Paradox of Choice ‚Äî Why More Is Less, Schwartz argues that eliminating consumer choices can greatly reduce anxiety for shoppers. And the same remains true for Data Science courses as well.\nThis post is about providing recommendations to lost souls with a lot of choices on where to start their Data Science Journey.\n 1) Python 3 Programming Specialization  ‚ÄúGoodBye World‚Äù for Python 2.7!!!\n First, you need a programming language. This specialization from the University of Michigan is about learning to use Python and creating things on your own.\nYou will learn about programming fundamentals like variables, conditionals, and loops, and get to some intermediate material like keyword parameters, list comprehensions, lambda expressions, and class inheritance.\nYou might also like to go through my Python Shorts posts while going through this specialization.\n \u0026lt;strong\u0026gt;Python Shorts Posts\u0026lt;/strong\u0026gt;  2) Applied Data Science with Python  Do first, understand later\n We need to get a taste of Machine Learning before understanding it fully.\nThis specialization in Applied Data Science with Python gives an intro to many modern machine learning methods that you should know about. Not a thorough grinding, but you will get the tools to build your models.\n This skills-based specialization is intended for learners who have a basic python or programming background, and want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular python toolkits such as pandas, matplotlib, scikit-learn, nltk, and networkx to gain insight into their data.\n You might also like to go through a few of my posts while going through this specialization:\n   \u0026lt;strong\u0026gt;Minimal Pandas Subset for Data Scientists\u0026lt;/strong\u0026gt;    \u0026lt;strong\u0026gt;Python‚Äôs One Liner graph creation library with animations Hans Rosling Style\u0026lt;/strong\u0026gt;    \u0026lt;strong\u0026gt;3 Awesome Visualization Techniques for every dataset\u0026lt;/strong\u0026gt;    3) Machine Learning Theory and Fundamentals After doing these above courses, you will gain the status of what I would like to call a ‚ÄúBeginner.‚Äù\nCongrats!!!. You know stuff; you know how to implement things.\n You are Useful\n Yet, you do not fully understand all the math and grind that goes behind all these models.\nYou need to understand what goes behind the clf.fit. Its time to face the music. Nobody is going to take you seriously till you understand the Math behind your models.\n If you don‚Äôt understand it you won‚Äôt be able to improve it\n Here comes the Game Changer Machine Learning course . It contains the maths behind many of the Machine Learning algorithms.\nI will put this course as the one course you have to take as this course motivated me into getting into this field, and Andrew Ng is a great instructor. Also, this was the first course that I took myself when I started.\nThis course has a little of everything ‚Äî Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.\nYou might also want to go through a few of my posts while going through this course:\n   \u0026lt;strong\u0026gt;The Hitchhiker‚Äôs Guide to Feature Extraction\u0026lt;/strong\u0026gt;    \u0026lt;strong\u0026gt;The 5 Classification Evaluation metrics every Data Scientist must know\u0026lt;/strong\u0026gt;    \u0026lt;strong\u0026gt;The 5 Feature Selection Algorithms every Data Scientist should know\u0026lt;/strong\u0026gt;    \u0026lt;strong\u0026gt;The Simple Math behind 3 Decision Tree Splitting criterions\u0026lt;/strong\u0026gt;    4) Learn Statistical Inference  ‚ÄúFacts are stubborn things, but statistics are pliable.‚Äù‚Äï Mark Twain\n Mine √áetinkaya-Rundel teaches this course on Inferential Statistics . And it cannot get simpler than this one.\nShe is a great instructor and explains the fundamentals of Statistical inference nicely ‚Äî a must-take course.\nYou will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.\nYou might also want to go through a few of my posts while going through this specialization:\n  \u0026lt;strong\u0026gt;P-value Explained Simply for Data Scientists\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;Confidence Intervals Explained Simply for Data Scientists\u0026lt;/strong\u0026gt;    5) Learn SQL Basics for Data Science  SQL is the heart of all data ETL\n While we feel much more accomplished by creating models and coming up with the different hypotheses, the role of data munging can‚Äôt be understated.\nAnd with the ubiquitousness of SQL when it comes to ETL and data preparation tasks, everyone should know a little bit of it to at least be useful.\nSQL has also become a de facto standard of working with Big Data Tools like Apache Spark. This SQL specialization from UC Davis will teach you about SQL as well as how to use SQL for distributed computing.\nFrom the Course website:\n Through four progressively more difficult SQL projects with data science applications, you will cover topics such as SQL basics, data wrangling, SQL analysis, AB testing, distributed computing using Apache Spark, and more\n You might also want to go through a few of my posts while going through this specialization:\n  \u0026lt;strong\u0026gt;Learning SQL the Hard Way\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;The Hitchhikers guide to handle Big Data using Spark\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;5 Ways to add a new column in a PySpark Dataframe\u0026lt;/strong\u0026gt;    6) Advanced Machine Learning  In the big leagues, there is no spoonfeeding.\n You might not agree to this, but till now, whatever we have done has been spoonfed learning. The material was structured, and the Math has been minimal. But that has prepared you for the next steps. This Advanced Machine Learning specialization by Top Kaggle machine learning practitioners and CERN scientists takes another approach to learning by going through a lot of difficult concepts and guiding you through how things worked in the past and the most recent advancements in the Machine Learning World. The description on the website says:\n This specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice.\n You might like to look at a few of my posts while trying to understand some of the material in this course.\n  \u0026lt;strong\u0026gt;MCMC Intuition for Everyone\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;NLP Learning Series\u0026lt;/strong\u0026gt;    7) Deep Learning  Deep Learning is the Future\n Andrew NG is back again with his new Deep Learning Specialization . And this is Pure Gold.\nAndrew Ng has achieved mastery in explaining difficult concepts in an easy to understand way. The nomenclature he follows is different from all other tutorials and courses on the net, and I hope it catches on as it is pretty helpful in understanding all the basic concepts.\nFrom the specialization website:\n Learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing.\n You might like to look at a few of my posts while trying to understand some of the material in this course.\n  \u0026lt;strong\u0026gt;An End to End Introduction to GANs\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;Object Detection using Deep Learning Approaches: An End to End Theoretical Perspective\u0026lt;/strong\u0026gt;    8) Pytorch  Python on Fire\n I usually never advocate to learn a tool, but here I do. The reason being that it is incredible and seriously, you will be able to read code in a lot of recent research papers if you understand Pytorch. Pytorch has become a default programming language for researchers working in Deep Learning, and it will only pay for us to learn it.\nA structured way to learn Pytorch is by taking this course on Deep Neural Networks with Pytorch . From the course website:\n The course will start with Pytorch‚Äôs tensors and Automatic differentiation package. Then each section will cover different models starting off with fundamentals such as Linear Regression, and logistic/softmax regression. Followed by Feedforward deep neural networks, the role of different activation functions, normalization and dropout layers. Then Convolutional Neural Networks and Transfer learning will be covered. Finally, several other Deep learning methods will be covered.\n You might also look at this post of mine, where I try to explain how to work with PyTorch.\n  \u0026lt;strong\u0026gt;Moving from Keras to Pytorch\u0026lt;/strong\u0026gt;    9) Getting Started with AWS for Machine Learning  The secret: it‚Äôs not what you know, it‚Äôs what you show.\n There are a lot of things to consider while building a great machine learning system. But often it happens that we, as data scientists, only worry about certain parts of the project.\nBut do we ever think about how we will deploy our models once we have them?\nI have seen a lot of ML projects, and a lot of them are doomed to fail as they don‚Äôt have a set plan for production from the onset.\nHaving a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.\n This course will teach you:\n How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance. How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.   You might also look at this post of mine, where I try to talk about apps and explain how to plan for Production.\n  \u0026lt;strong\u0026gt;How to write Web apps using simple Python for Data Scientists?\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;How to Deploy a Streamlit App using an Amazon Free ec2 instance?\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;Take your Machine Learning Models to Production with these 5 simple steps\u0026lt;/strong\u0026gt;    10) Data Structures and Algorithms  Algorithms. Yes, you need them.\n Algorithms and data structures are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are essential all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nThey will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.\nOne of the best resources I found to learn algorithms is the Algorithm Specialization on Coursera by UCSanDiego . From the specialization website:\n You will learn algorithmic techniques for solving various computational problems and will implement about 100 algorithmic coding problems in a programming language of your choice. No other online course in Algorithms even comes close to offering you a wealth of programming challenges that you may face at your next job interview.\n You might also like to look at a few of my posts while trying to understand some of the material in this specialization.\n  \u0026lt;strong\u0026gt;3 Programming concepts for Data Scientists\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;A simple introduction to Linked Lists for Data Scientists\u0026lt;/strong\u0026gt;   \u0026lt;strong\u0026gt;Dynamic Programming for Data Scientists\u0026lt;/strong\u0026gt; - \u0026lt;strong\u0026gt;Handling Trees in Data Science Algorithmic Interview\u0026lt;/strong\u0026gt;    Continue Learning I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/21/ds2020/","tags":["Awesome Guides","Best Content","Curated Resources"],"title":"Become a Data Scientist in 2020 with these 10 resources"},{"categories":["Data Science","Awesome Guides"],"contents":"Recently, I got asked about how to explain confidence intervals in simple terms to a layperson. I found that it is hard to do that.\nConfidence Intervals are always a headache to explain even to someone who knows about them, let alone someone who doesn‚Äôt understand statistics.\nI went to Wikipedia to find something and here is the definition:\n In statistics , a confidence interval (CI) is a type of estimate computed from the statistics of the observed data. This proposes a range of plausible values for an unknown parameter . The interval has an associated confidence level that the true parameter is in the proposed range. This is more clearly stated as: the confidence level represents the probability that the unknown parameter lies in the stated interval. The level of confidence can be chosen by the investigator. In general terms, a confidence interval for an unknown parameter is based on sampling the distribution of a corresponding estimator . [1]  And my first thought was that might be they have written it like this so that nobody could understand it. The problem here lies with a lot of terminology and language that statisticians enjoy to employ.\nThis post is about explaining confidence intervals in an easy to understand way without all that pretentiousness.\n A Real-Life problem   Source \nLet‚Äôs start by creating a real-life scenario.\nImagine you want to find the mean height of all the people in a particular US state.\nYou could go to each person in that particular state and ask for their height, or you can do the smarter thing by taking a sample of 1000 people in the state.\nThen you can use the mean of their heights (Estimated Mean) to estimate the average of heights in the state(True Mean)\nThis is all well and good, but you being the true data scientist, are not satisfied. The estimated mean is just a single number, and you want to have a range where the true mean could lie.\nWhy do we want a range? Because in real life, we are concerned about the confidence of our estimates.\nTypically even if I ask you to guess the height of people in the particular US state, you are more inclined to say something like: ‚ÄúI believe it is between 6 foot to 6 Foot 2 Inch‚Äù rather than a point estimate like ‚ÄúIts 6 foot 2.2345 inches‚Äù.\nWe humans also like to attach a level of confidence when we give estimates. Have you ever said ‚Äî ‚ÄúI am 90% confident‚Äù.\nIn this particular example, I can be more confident about the statement- ‚ÄúI believe it is between 5 foot to 7 Foot‚Äù than ‚ÄúI believe it is between 6 foot to 6 Foot 2 Inch‚Äù as the first range is a superset of the second one.\nSo how do we get this range and quantify a confidence value?\n Strategy To understand how we will calculate the confidence intervals, we need to understand the Central Limit Theorem.\nCentral Limit Theorem: The Central Limit Theorem(CLT) simply states that if you have a population with mean Œº and standard deviation œÉ, and take random samples from the population, then the distribution of the sample means will be approximately normally distributed with mean as the population mean and estimated standard deviation s/‚àön where s is the standard deviation of the sample and n is the number of observations in the sample.\nSo knowing all this, you become curious. We already have a sample of 1000 people in the US state. Can we apply CLT?\nWe know that the mean of the sampling distribution is equal to the population mean(which we don‚Äôt know and want to estimate)and the sample deviation of the sampling distribution is given by œÉ/‚àön( i.e., the standard deviation of the sample divided by the number of observations in the sample)\n Now, you want to find intervals on the X-axis that contains the true population mean.\nSo what do we do? We cast a net from the value we know.\nTo get such ranges/intervals, we go 1.96 standard deviations away from Xbar, the sample mean in both directions. And this range is the 95% confidence interval.\nNow, when I say that I estimate the true mean to be Xbar (The sample Mean) with a confidence interval of [Xbar-1.96SD, Xbar+1.96SD], I am saying that:\n That this is an interval constructed using a certain procedure. Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 95%\n When you take 99% CI, you essentially increase the proportion and thus cast a wider net with three standard deviations.\n   Here Xbar is the sample mean(mean of the 1000 heights sample you took).\n  Z is the no of standard deviations away from the sample mean(1.96 for 95%, 2.576 for 99%) ‚Äî level of confidence you want.\n  s is the standard deviation in the sample.\n  n is the size of the sample.\n   Each line in the figure above is one such experiment where the dot signifies the sample mean, and the line signifies the range. The dotted line in this figure is the true population mean*.*\nSee how some of these intervals don‚Äôt contain the true population mean, and almost all of them(95%) do include the true population mean\n The Critical Z value As we said, Z is the no of standard deviations away from the sample mean(1.96 for 95%, 2.576 for 99%) ‚Äî level of confidence you want.\nYou can go for any arbitrary level of confidence. Say, for example, you want 90% confidence. You can get that by using the idea that the shaded area inside the normal curve needs to be 0.90.\n import scipy.stats as st p = 0.9 + (1-0.9)/2 Z = st.norm.ppf(p, loc=0, scale=1) print(Z) ---------------------------------------------------------- 1.6448536269514722   Continue Learning If you want to learn more about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data, Mine √áetinkaya-Rundel teaches \u0026lt;strong\u0026gt;Inferential Statistics\u0026lt;/strong\u0026gt; course on coursera, and it cannot get simpler than this one. She is a great instructor and explains the fundamentals of Statistical inference nicely.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/21/ci/","tags":["Statistics","Data Science","Best Content"],"title":"Confidence Intervals Explained Simply for Data Scientists"},{"categories":["Data Science","Awesome Guides"],"contents":"A Data Scientist who doesn‚Äôt know SQL is not worth his salt\nAnd that seems correct to me in every sense of the world. While we feel much more accomplished creating models and coming up with the different hypotheses, the role of data munging can‚Äôt be understated.\nAnd with the ubiquitousness of SQL when it comes to ETL and data preparation tasks, everyone should know a little bit of it to at least be useful.\nI still remember the first time I got my hands on SQL. It was the first language (if you can call it that) I learned. And it made an impact on me. I was able to automate things, and that was something I hadn‚Äôt thought of before.\nBefore SQL, I used to work with Excel ‚Äî VLOOKUPs and pivots. I was creating reporting systems, doing the same work again and again. SQL made it all go away. Now I could write a big script, and everything would be automated ‚Äî all the crosstabs and analysis generated on the fly.\nThat is the power of SQL. And though you could do anything that you do with SQL using Pandas , you still need to learn SQL to deal with systems like HIVE, Teradata and sometimes Spark too.\nThis post is about installing SQL, explaining SQL and running SQL.\n Setting up the SQL Environment Now the best way to learn SQL is to get your hands dirty with it(Same I can say for any other thing you want to learn)\nI will advise against using the web-based recipes like w3schools/tutorialspoint for SQL since you cannot use your data with those.\nAlso, I will advise you to go with learning the MySQL flavour of SQL as it is Open Source, easy to set up in your laptop and has a great client named MySQL Workbench to make your life easier.\nAs we have gotten these points out of the way, here is a step by step to get set up with MySQL:\n You can download MySQL for your particular system (MACOSX, Linux, Windows) from Download MySQL Community Server . In my case, I downloaded the DMG Archive. After that, double click and install the file. You might need to set up a password. Remember this password as it would be required to connect to the MySQL instance later.    Create a file named my.cnf and put the following in it. This is needed to give Local file read permissions to your SQL database.   [client] port= 3306 [mysqld] port= 3306 secure_file_priv='' local-infile=1  Open up System Preferences\u0026gt;MySQL. Go to Configuration and browse to the my.cnf file using the select button.    Restart the server from Instances tab by clicking stop and start.    Once you get that server running, download and install the MySQL Workbench: Download MySQL Workbench . The workbench gives you an editor to write your SQL Queries and get the results in a structured way.    Open up the MySQL workbench now and connect to SQL through it. You will see something like below.    You can see that the Local Instance connection has been set up for you beforehand. Now, you just need to click on that connection and get started using the password that we set up before for the MySQL server(You can also create a connection to an existing SQL server that might not be on your machine if you have the address, port number, username and password).    And you get an editor to write your queries on the particular database.     Check the Schemas tab on the top left to see the tables that are present. There is just a sys schema present with the table sys_config. Not an interesting data source to learn SQL. So let‚Äôs install some data to practice.\n  If you have your own data to work. Then good and fine. You can create a new schema(database) and upload it into tables using these following commands. (You can run the commands by using Cmd+Enter or by clicking the ‚ö°Ô∏èlightning button)\n   In this tutorial, however, I am going to use the Sakila Movie database which you can install using the following steps:\n  Go to MySQL Documentation and download the Sakila ZIP file.\n  Unzip the file.\n  Now go to MySQL Workbench and select File\u0026gt;Run SQL Script\u0026gt;select location sakila-db/sakila-schema.sql\n  Go to MySQL Workbench and select File \u0026gt;Run SQL Script \u0026gt;select location sakila-db/sakila-data.sql\n  Once you do that, you will see a new database added in the SCHEMA list.\n  Playing with Data Now we have some data with us. Finally.\nLet‚Äôs start with writing some queries.\nYou can try to understand the Schema of the Sakila Database in detail using the Sakila Sample Database document.\n Schema Diagram\nNow the basic syntax of any SQL query is:\nSELECT col1, SUM(col2) as col2sum, AVG(col3) as col3avg FROM table_name WHERE col4 = 'some_value' GROUP BY col1 ORDER BY col2sum DESC;  There are four elements in this query:\n  SELECT: Which Columns to select? Here we choose col1 and do SUM aggregation on col2 and AVG aggregation on col3. We also give a new name to SUM(col2) by using the as keyword. This is known as aliasing.\n  FROM: From which table should we SELECT?\n  WHERE: We can filter data using WHERE statements.\n  GROUP BY: All selected columns that are not in aggregation should be in GROUP BY.\n  ORDER BY: Sort on col2sum\n  The above query will help you with most of the simple things you want to find in a database.\nFor example, we can find out how differently censored rated movies are timed differently using:\nSELECT rating, avg(length) as length_avg FROM sakila.film group by rating order by length_avg desc;   Exercise: Frame a Question You should now come up with some questions of your own.\nFor Example, you can try to find out all the movies released in the year 2006. Or try to find all of the movies which have a rating of PG and length greater than 50 minutes.\nYou can do this by running the following on MySQL Workbench:\nselect * from sakila.film where release_year = 2006; select * from sakila.film where length\u0026gt;50 and rating=\u0026quot;PG\u0026quot;;   Joins in SQL Till now, we have learned how we can work with single tables. But in reality, we need to work with multiple tables.\nSo, the next thing we would want to learn is how to do joins.\nNow joins are an integral and an essential part of a MySQL Database and understanding them is necessary. The below visual talks about most of the joins that exist in SQL. I usually end up using just the LEFT JOIN, and INNER JOIN, so I will start with LEFT JOIN.\n The LEFT JOIN is used when you want to keep all the records in the left table(A) and merge B on the matching records. The records of A where B is not merged are kept as NULL in the resulting table. The MySQL Syntax is:\nSELECT A.col1, A.col2, B.col3, B.col4 from A LEFT JOIN B on A.col2=B.col3  Here we select col1 and col2 from table A and col3 and col4 from table B. We also specify which common columns to join on using the ON statement.\nThe INNER JOIN is used when you want to merge A and B and only to keep the common records in A and B.\nExample: To give you a use case lets go back to our Sakila database. Suppose we wanted to find out how many copies of each movie we do have in our inventory. You can get that by using:\nSELECT film_id,count(film_id) as num_copies FROM sakila.inventory group by film_id order by num_copies desc;   Does this result look interesting? Not really. IDs don‚Äôt make sense to us humans, and if we can get the names of the movies, we would be able to process the information better. So we snoop around and see that the table film has got film_id as well as the title of the film.\nSo we have all the data, but how do we get it in a single view?\nCome Joins to the rescue. We need to add the title to our inventory table information. We can do this using ‚Äî\nSELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id   This will add another column to your inventory table information. As you might notice some films are in the film table that we don‚Äôt have in the inventory. We used a left join since we wanted to keep whatever is in the inventory table and join it with its corresponding counterpart in the film table and not everything in the film table.\nSo now we have got the title as another field in the data. This is just what we wanted, but we haven‚Äôt solved the whole puzzle yet. We want title and num_copies of the title in the inventory.\nBut before we can go any further, we should understand the concept of inner queries first.\n Inner Query: Now you have a query that can give you the above result. One thing you can do is create a new table using\ncreate table sakila.temp_table as SELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id;  And then use a simple group by operation using:\nselect title, count(title) as num_copies from sakila.temp_table group by title order by num_copies desc;   But this is one step too many. And we have to create a temporary table that ends up taking space on the system.\nSQL provides us with the concept of the inner query just for these sort of issues. You can instead write all this in a single query using:\nselect temp.title, count(temp.title) as num_copies from (SELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id) temp group by title order by num_copies desc;   What we did here was sandwich our first query in parenthesis and gave that table an alias temp. We then did the group by operations considering temp just as we would consider any table. It is because of the inner query concept that we can write SQL queries that span multiple pages at some times.\n The HAVING Clause HAVING is yet another SQL construct that is useful to understand. So we have got the results, and now we want to get the films whose number of copies are less than or equal to 2.\nWe can do this by using the inner query concept and the WHERE clause. Here we nest one inner query inside another. Pretty neat.\n Or, we can use the HAVING Clause.\n The HAVING clause is used to filter on the final aggregated result. It is different from WHERE as where is used to filter the table that is used in the FROM statement. HAVING filters the final result after the GROUP BY happens.\nThere are a lot of ways to do the same thing with SQL as you have already seen in the above example. We need to try to come up with the least verbose and thus HAVING makes sense in many cases.\nIf you can follow this far, you already know more SQL than most people.\nNext thing to do: Practice.\nTry to come up with your questions on your dataset and try to find out the answers you have using SQL.\nSome questions I could provide for a start:\n  Which Actor has the most distinct films in our inventory?\n  Which Genre films are the most rented in our inventory?\n   Continue Learning This was just a simple tutorial on how to use SQL. If you want to learn more about SQL, I would like to call out an excellent course on SQL for Data Science from the University of California. Do check it out as it talks about other SQL concepts like UNION, String Manipulation, functions, Date Handling, etc.\nI am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/21/sql/","tags":["Machine Learning","Data Science","Awesome Guides","sql"],"title":"Learning SQL the Hard Way"},{"categories":["Data Science"],"contents":"We as data scientists have got laptops with quad-core, octa-core, turbo-boost. We work with servers with even more cores and computing power.\nBut do we really utilize the raw power we have at hand?\nSometimes we get limited by the limitation of tools at our disposal. And sometimes we are not willing to write all that extraneous code to save a couple of minutes. And end up realizing only later that time optimization would have helped in the long run.\nSo, can we do better?\nYes, Obviously.\nPreviously, I had written on how to make your apply function faster-using multiprocessing , but thanks to the swifter library, it is even more trivial now.\nThis post is about using the computing power we have at hand and applying it to Pandas DataFrames using Swifter.\n Problem Statement We have got a huge pandas data frame, and we want to apply a complex function to it which takes a lot of time.\nFor this post, I will generate some data with 25M rows and 4 columns.\nCan use parallelization easily to get extra performance out of our code?\nimport pandas as pd import numpy as np pdf = pd.DataFrame(np.random.randint(0,100,size=(25000000, 4)),columns=list('abcd'))  The Data looks like:\n Data Sample\n Parallelization using just a single change  Relax and Parallelize !!!\nLet‚Äôs set up a simple experiment.\nWe will try to create a new column in our dataframe. We can do this simply by using apply-lambda in Pandas.\ndef func(a,b): if a\u0026gt;50: return True elif b\u0026gt;75: return True else: return False pdf['e'] = pdf.apply(lambda x : func(x['a'],x['b']),axis=1)   The above code takes ~10 Minutes to run. And we are just doing a simple calculation on 2 columns here.\nCan we do better and what would it take?\nYes, we can do better just by adding a ‚Äúmagic word‚Äù ‚Äî Swifter.\nBut first, you need to install swifter, which is as simple as:\nconda install -c conda-forge swifter  You can then just import and append swifter keyword before the apply to use it.\nimport swifter pdf['e'] = pdf.**swifter**.apply(lambda x : func(x['a'],x['b']),axis=1)  So, Does this work?  Yes. It does. We get a 2x improvement in run time vs. just using the function as it is.\nSo what exactly is happening here?   Source : How increasing data size effects performances for Dask, Pandas and Swifter?\nSwifter chooses the best way to implement the apply possible for your function by either vectorizing your function or using Dask in the backend to parallelize your function or by maybe using simple pandas apply if the dataset is small.\nIn this particular case, Swifter is using Dask to parallelize our apply functions with the default value of npartitions = cpu_count()*2.\nFor the MacBook I am working on the CPU Count is 6 and the hyperthreading is 2. Thus CPU Count is 12 and that makes npartitions=24.\nWe could also choose to set n_partitions ourselves. Though I have observed the default value works just fine in most cases sometimes you might be able to tune this as well to gain additional speedups.\nFor example: Below I set n_partitions=12 and we get a 2x speedup again. Here reducing our number of partitions results in smaller run times as the data movement cost between the partitions is high.\n  Conclusion  Parallelization is not a silver bullet; it is buckshot.\n Parallelization won‚Äôt solve all your problems, and you would still have to work on optimizing your functions, but it is a great tool to have in your arsenal.\nTime never comes back, and sometimes we have a shortage of it. At these times we need parallelization to be at our disposal with a single word.\nAnd that word is swifter .\n Continue Learning Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/02/20/swifter/","tags":["Python","Productivity","Tools","Data Science","Machine Learning"],"title":"Add this single word to make your Pandas Apply faster"},{"categories":["Data Science","Programming"],"contents":"Algorithms and data structures are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are crucial all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nNow the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.\nWe all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people‚Äôs coding ability.\nBut let‚Äôs not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.\nThis post is about fast-tracking this study and explaining tree concepts for the data scientists so that you breeze through the next time you get asked these in an interview.\n But First, Why are Trees important for Data Science? To data scientists, Trees mean a different thing than they mean for a Software Engineer.\nFor a software engineer, a tree is just a simple Data Structure they can use to manage hierarchical relationships while for a Data Scientists trees form the basis of some of the most useful classification and regression algorithms.\nSo where do these two meet?\nThey are necessarily the same thing. Don‚Äôt be surprised. Below is how data scientists and software engineer‚Äôs look at trees.\n The only difference is that Data science tree nodes keep much more information that helps us in identifying how to traverse the tree. For example, in the case of Data science tree for prediction, we will look at the feature in the node and determine which way we want to move based on the split value.\nIf you want to write your decision tree from scratch, you might need to understand how trees work from a software engineering perspective too.\n Types of Trees: In this post, I will only be talking about two kinds of trees that get asked a lot in Data Science interview questions. Binary Trees(BT) and an extension of Binary Trees called Binary Search Trees(BST).\n1. Binary Trees: A binary tree is simply a tree in which each node has up to two children. A decision tree is an example we see in our day to day lives.\n 2. Binary Search Tree(BST): A binary search tree is a binary tree in which:\n  All left descendants of a node are less than or equal to the node, and\n  All right descendants of the node are greater than the node.\n  There are variations to this definition when it comes to equalities. Sometimes the equalities are on the right-hand side or either side. Sometimes only distinct values are allowed in the tree.\n  Source \n8 is greater than all the elements in the left subtree and smaller than all elements in the right subtree. The same could be said for any node in the tree.\n Creating a Simple Tree: So How do we construct a simple tree?\nBy definition, a tree is made up of nodes. So we start by defining the node class which we will use to create nodes. Our node class is pretty simple as it holds value for a node, the location of the left child and the location of the right child.\nclass node: def __init__(self,val): self.val = val self.left = None self.right = None We can create a simple tree now as:\nroot = node(1) root.left = node(2) root.right = node(3) Now I have noticed that we cannot really get the hang of Tree-based questions without doing some coding ourselves.\nSo let us get a little deeper into the coding part with some problems I found most interesting when it comes to trees.\nInorder Tree Traversal: There are a variety of ways to traverse a tree, but I find the inorder traversal to be most intuitive.\nWhen we do an inorder traversal on the root node on a Binary Search tree, it visits/prints the node in ascending order.\ndef inorder(node): if node: inorder(node.left) print(node.val) inorder(node.right) This above method is pretty important as it allows us to visit all the nodes.\nSo if we want to search for a node in any binary tree, we might try to use inorder tree traversal.\n Creating a Binary Search Tree from a Sorted array What kind of coders will we be if we need to create a tree piece by piece manually as we did above?\nSo can we create a BST from a sorted array of unique elements?\ndef create_bst(array,min_index,max_index): if max_index\u0026lt;min_index: return None mid = int((min_index+max_index)/2) root = node(array[mid]) leftbst = create_bst(array,min_index,mid-1) rightbst = create_bst(array,mid+1,max_index) root.left = leftbst root.right = rightbst return root a = [2,4,5,6,7] root = create_bst(a,0,len(a)-1) Trees are inherently recursive, and so we use recursion here. We take the mid element of the array and assign it as the node. We then apply the create_bst function to the left part of the array and assign it to node.left and do the same with the right part of the array.\nAnd we get our BST.\nHave we done it right? We can check it by creating the BST and then doing an inorder traversal.\ninorder(root) 2 4 5 6 7  Seems Right!\n Let‚Äôs check if our tree is a Valid BST  But again what sort of coders are we if we need to print all the elements and check manually for the BST property being satisfied?\nHere is a simple code to check if our BST is valid or not. We assume strict inequality in our Binary Search Tree.\ndef isValidBST(node, minval, maxval): if node: # Base case if node.val\u0026lt;=minval or node.val\u0026gt;=maxval: return False # Check the subtrees changing the min and max values return isValidBST(node.left,minval,node.val) \u0026amp; isValidBST(node.right,node.val,maxval) return True isValidBST(root,-float(\u0026#39;inf\u0026#39;),float(\u0026#39;inf\u0026#39;)) True  We check the subtrees recursively if they satisfy the Binary Search tree property or not. At each recursive call, we change the minval or maxval for the call to provide the function with the range of allowed values for the subtree.\n Conclusion In this post, I talked about Trees from a software engineering perspective. If you want to see trees from a data science perspective, you might take a look at this post. \u0026lt;strong\u0026gt;The Simple Math behind 3 Decision Tree Splitting criterions\u0026lt;/strong\u0026gt; Trees form the basis of some of the most asked questions in Data Science algorithmic interviews. I used to despair such tree-based questions in the past, but now I have grown to like the mental exercise involved in them. And I love the recursive structure involved in such problems.\nAnd while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.\nHere is a small notebook for you where I have put all these small concepts for you to try and run.\nTake a look at my other posts in the Algorithmic Interviews Series , if you want to learn about Recursion , Dynamic Programming or Linked Lists .\nContinue Learning If you want to read up more on Algorithms and Data structures, here is an \u0026lt;strong\u0026gt;Algorithm Specialization on Coursera by UCSanDiego\u0026lt;/strong\u0026gt; , which I highly recommend.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/01/29/altr/","tags":["Machine Learning","Algorithms","Production"],"title":"Handling Trees in Data Science Algorithmic Interview"},{"categories":["Data Science","Programming"],"contents":"Algorithms and data structures are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are important all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nNow the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.\nWe all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people‚Äôs coding ability.\nBut let‚Äôs not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.\nThis post is about fast-tracking this study and explaining linked list concepts for the data scientists in an easy to understand way.\n What are Linked Lists? The linked list is just a very simple data structure that represents a sequence of nodes.\n Each node is just an object that contains a value and a pointer to the next node. For example, In the example here we have a node that contains the data 12 and points to the next node 99. Then 99 points to node 37 and so on until we encounter a NULL Node.\n There are also doubly linked lists in which each node contains the address of the next as well as the previous node.\nBut why would we ever need Linked Lists?  We all have worked with Lists in Python.But have you ever thought of the insertion time for the list data structure?\nLets say we need to insert an element at the start of a list. Inserting or removing elements at the start in a python list requires an O(n) copy operation.\nWhat if we are faced with the problem in which there are a lot of such inserts and we need a data structure that actually does inserts in constant O(1) time?\nThere are many practical applications of a linked list that you can think about. One can use a doubly-linked list to implement a system where only the location of previous and next nodes are needed. For example, the previous page and next page functionality in the chrome browser. Or the previous and next photo in a photo editor.\nAnother benefit of using a linked list is that we don‚Äôt need to have contiguous space requirements for a linked list i.e. the nodes can reside anywhere in the memory while for a data structure like an array the nodes need to be allocated a sequence of memory.\n How do we create a Linked list in Python? We first start by defining a class that can be used to create a single node.\nclass Node: def __init__(self,val): self.val = val self.next = None We then use this class object to create multiple nodes and stitch them together to form a linked list.\nhead = Node(12) a = Node(99) b = Node(37) head.next = a a.next = b And we have our linked list, starting at head. In most cases, we only keep the variable head to define our linked list as it contains all the information we require to access the whole list.\n Common Operations or Interview Questions with the Linked Lists 1. Insert a new Node In the start, we said that we can insert an element at the start of the linked list in a constant O(1) time. Let‚Äôs see how we can do that.\ndef insert(head,val): new_head = Node(val) new_head.next = head return new_head So given the head of the node, we just create a new_head object and set its pointer to the previous head of the linked list. We just create a new node and just update a pointer.\n2. Print/Traverse the linked list Printing the elements of a linked list is pretty simple. We just go through the linked list in an iterative fashion till we encounter the None node(or the end).\ndef print(head): while head: print(head.val) head = head.next 3. Reverse a singly linked list This is more of a very common interview question on linked lists. If you are given a linked list, can you reverse that linked list in O(n) time?\nFor Example: Input: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL Output: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL  So how do we deal with this?\nWe start by iterating through the linked list and reversing the pointer direction while moving the head to the next node until there is a next node.\ndef reverseList(head): newhead = None while head: tmp = head.next head.next = newhead newhead = head head = tmp return newhead  Conclusion In this post, I talked about Linked List and its implementation.\nLinked lists form the basis of some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.\nAnd while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.\nHere is a small notebook for you where I have put all these small concepts.\nI will leave you with solving this problem by yourself ‚Äî Implement a function to check if a linked list is a palindrome.\nAlso take a look at my other posts in the series , if you want to learn about algorithms and Data structures.\nContinue Learning If you want to read up more on Algorithms and Data structures, here is an \u0026lt;strong\u0026gt;Algorithm Specialization on Coursera by UCSanDiego\u0026lt;/strong\u0026gt; , which I highly recommend.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/01/28/ll/","tags":["Machine Learning","Algorithms","Production"],"title":"A simple introduction to Linked Lists for Data Scientists"},{"categories":["Data Science","Programming"],"contents":"Algorithms and data structures are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are important all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nNow the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.\nWe all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people‚Äôs coding ability.\nBut let‚Äôs not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.\nThis post is about fast-tracking this study and explaining Dynamic Programming concepts for the data scientists in an easy to understand way.\n How Dynamic Programming Works? Let‚Äôs say that we need to find the nth Fibonacci Number.\nFibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:\ndef fib(n): if n\u0026lt;=1: return 1 return fib(n-1) + fib(n-2) This problem relates well to a recursive approach. But can you spot the problem here?\nIf you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.\n  Source \nNow, Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.\nIn Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.\ndef fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n+1): dp_sols[i] = dp_sols[i-1] + dp_sols[i-2] return dp_sols[n]  Why Dynamic Programming is Hard? Recursion is a mathematical concept and it comes naturally to us. We try to find a solution to a bigger problem by breaking it into smaller ones.\nNow Dynamic Programming entails exactly the same idea but in the case of Dynamic programming, we precompute all the subproblems that might need to be calculated in a bottom-up manner.\nWe human beings are essentially hard-wired to work in a top-down manner. Be it our learning where most people try to go into the breadth of things before going in-depth. Or be it the way we think.\nSo how does one start thinking in a bottom-up way?\nI found out that solving the below problem gives a lot of intuition in how DP works. I myself got highly comfortable with DP once I was able to solve this one and hope it helps you too.\nBasically the idea is if you can derive/solve a bigger subproblem if you know the solution to a smaller one?\n Maximum Path Sum Given a m x n grid filled with gold, find a path from top left to bottom right which maximizes the sum of gold along its path. We can only move down or right starting from (0,0)\nNow there can be decidedly many paths. We can go all the way to the right and then the bottom. Or we can take a zigzag path?\n But only one/few paths are going to make you rich.\nSo how do you even start thinking about such a problem?\nWhen we think of Dynamic Programming questions, we take a bottom-up approach. So we start by thinking about the simplest of problems. In our case, the simplest of problems to solve is the base case. What is the maximum value of Gold we can acquire if we just had to reach cell (0,0)?\nAnd the answer to that is pretty simple ‚Äî It is the cell value itself.\n So we move on to a little harder problem.\n What about cell (0,1) and cell (1,0)?\nThese are also pretty simple. We can reach (0,1)and (1,0) through only (0,0) and hence the maximum gold we can obtain is the value in cell (0,1)/(1,0) plus the maximum gold we can have when we reach cell(0,0)\nWhat about cell(0,2)? Again only one path. So if we know the solution to (0,1) we can just add the value of cell (0,2) to get the solution for (0,2)\nLet‚Äôs now try to do the same for an arbitrary cell. We want to derive a relation here.\n ***So in the case of an arbitrary cell, we can reach it from the top or from the left.***If we know the solutions to the top and left of the cell, we can definitely compute the solution to the arbitrary current target cell.\n Coding Once we have the intuition the coding exercise is pretty straightforward. We start by calculating the solutions for the first row and first column. And then we continue to calculate the other values in the grid using the relation we got previously.\ndef maxPathSum(grid): m = len(grid) n = len(grid[0]) # sol keeps the solutions for each point in the grid. sol = list(grid) # we start by calculating solutions for the first row for i in range(1,n): sol[0][i] += sol[0][i-1] # we then calculate solutions for the first column for i in range(1,m): sol[i][0] += sol[i-1][0] # we then calculate all the solutions in the grid for i in range(1,m): for j in range(1,n): sol[i][j] += max(sol[i-1][j],sol[i][j-1]) # return the last element return sol[-1][-1]  Conclusion In this post, I talked about how I think about Dynamic Programming questions.\nI start by asking myself the simplest problem I could solve and if I can solve the bigger problem by using the solutions to the simpler problem.\nDynamic Programming forms the basis of some of the most asked questions in Data Science/Machine Learning job interviews, and a good understanding of these might help you land your dream job.\nSo go out there and do some problems with Leetcode/HackerRank. The problems are surely interesting.\nAlso take a look at my other posts in the series , if you want to learn about algorithms and Data structures.\nContinue Learning If you want to read up more on Algorithms and Data structures, here is an \u0026lt;strong\u0026gt;Algorithm Specialization on Coursera by UCSanDiego\u0026lt;/strong\u0026gt; , which I highly recommend.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea\n","permalink":"https://mlwhiz.com/blog/2020/01/28/dp/","tags":["Machine Learning","Algorithms","Production"],"title":"Dynamic Programming for Data Scientists"},{"categories":["Data Science","Awesome Guides"],"contents":"Have you ever faced an issue where you have such a small sample for the positive class in your dataset that the model is unable to learn?\nIn such cases, you get a pretty high accuracy just by predicting the majority class, but you fail to capture the minority class, which is most often the point of creating the model in the first place.\nSuch datasets are a pretty common occurrence and are called as an imbalanced dataset.\n Imbalanced datasets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class\n Imbalanced datasets can be found for different use cases in various domains:\n  Finance: Fraud detection datasets commonly have a fraud rate of ~1‚Äì2%\n  Ad Serving: Click prediction datasets also don‚Äôt have a high clickthrough rate.\n  Transportation/Airline: Will Airplane failure occur?\n  Medical: Does a patient has cancer?\n  Content moderation: Does a post contain NSFW content?\n  So how do we solve such problems?\nThis post is about explaining the various techniques you can use to handle imbalanced datasets.\n 1. Random Undersampling and Oversampling   Source \nA widely adopted and perhaps the most straightforward method for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).\nLet us first create some example imbalanced data.\nfrom sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[\u0026#39;target\u0026#39;] = y We can now do random oversampling and undersampling using:\nnum_0 = len(X[X[\u0026#39;target\u0026#39;]==0]) num_1 = len(X[X[\u0026#39;target\u0026#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[\u0026#39;target\u0026#39;]==0].sample(num_1) , X[X[\u0026#39;target\u0026#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[\u0026#39;target\u0026#39;]==0] , X[X[\u0026#39;target\u0026#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180   2. Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.\nIt provides a variety of methods to undersample and oversample.\na. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.\nIn this algorithm, we end up removing the majority element from the Tomek link, which provides a better decision boundary for a classifier.\n  Source \nfrom imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=\u0026#39;majority\u0026#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.\n  Source \nfrom imblearn.over_sampling import SMOTE smote = SMOTE(ratio=\u0026#39;minority\u0026#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.\n 3. Class weights in the models  Most of the machine learning models provide a parameter called class_weights. For example, in a random forest classifier using, class_weights we can specify a higher weight for the minority class using a dictionary.\nfrom sklearn.linear_model import LogisticRegression clf = LogisticRegression(class_weight={0:1,1:10}) But what happens exactly in the background?\nIn logistic Regression, we calculate loss per example using binary cross-entropy:\nLoss = -ylog(p) - (1-y)log(1-p) In this particular form, we give equal weight to both the positive and the negative classes. When we set class_weight as class_weight = {0:1,1:20}, the classifier in the background tries to minimize:\nNewLoss = -20ylog(p) - 1(1-y)log(1-p) So what happens exactly here?\n  If our model gives a probability of 0.3 and we misclassify a positive example, the NewLoss acquires a value of -20log(0.3) = 10.45\n  If our model gives a probability of 0.7 and we misclassify a negative example, the NewLoss acquires a value of -log(0.3) = 0.52\n  That means we penalize our model around twenty times more when it misclassifies a positive minority example in this case.\nHow can we compute class_weights?\nThere is no one method to do this, and this should be constructed as a hyperparameter search problem for your particular problem.\nBut if you want to get class_weights using the distribution of the y variable, you can use the following nifty utility from sklearn.\nfrom sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight(\u0026#39;balanced\u0026#39;, np.unique(y), y)  4. Change your Evaluation Metric  Choosing the right evaluation metric is pretty essential whenever we work with imbalanced datasets. Generally, in such cases, the F1 Score is what I want as my \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;evaluation metric\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; .\nThe F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.\n So how does it help?\nLet us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.\nSo we create a model that predicts ‚ÄúNo‚Äù for the whole training set.\nWhat is the accuracy(Normally the most used evaluation metric)?\nIt is more than 99%, and so according to accuracy, this model is pretty good, but it is worthless.\nNow, what is the F1 score?\nOur precision here is 0. What is the recall of our positive class? It is zero. And hence the F1 score is also 0.\nAnd thus we get to know that the classifier that has an accuracy of 99% is worthless for our case. And hence it solves our problem.\n Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low, and if the recall is low again, your F1 score is low.\n If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.\n How to Use? You can calculate the F1 score for binary prediction problems using:\nfrom sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred) This is one of the functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.\n# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)\u0026gt;thresh) if score \u0026gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh  5. Miscellaneous  Various other methods might work depending on your use case and the problem you are trying to solve:\na) Collect more Data This is a definite thing you should try if you can. Getting more data with more positive examples is going to help your models get a more varied perspective of both the majority and minority classes.\nb) Treat the problem as anomaly detection You might want to treat your classification problem as an anomaly detection problem.\nAnomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data\nYou can use Isolation forests or autoencoders for anomaly detection.\nc) Model-based Some models are particularly suited for imbalanced datasets.\nFor example, in boosting models, we give more weights to the cases that get misclassified in each tree iteration.\n Conclusion There is no one size fits all when working with imbalanced datasets. You will have to try multiple things based on your problem.\nIn this post, I talked about the usual suspects that come to my mind whenever I face such a problem.\nA suggestion would be to try using all of the above approaches and try to see whatever works best for your use case.\nIf you want to learn more about imbalanced datasets and the problems they pose, I would like to call out this \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;excellent course\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; by Andrew Ng. This was the one that got me started. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2020/01/28/imbal/","tags":["Best Content","Machine Learning","Data Science","Statistics"],"title":"The 5 most useful Techniques to Handle Imbalanced datasets"},{"categories":null,"contents":"Collecting and analysing data, including but not limited to text, images, and video formats, is a huge part of various industries. It can be an incredibly complex process to sift through massive amounts of data and leverage it to benefit your business by discovering key patterns. Many people who begin learning data science or are considering taking it up are often employed in other industries, to begin with. They may be afraid that pursuing this new area will leave them high and dry with few prospects, and considering how taking up data science requires a good background of Probability and Statistics, they may not think it\u0026rsquo;s worth the effort. However, many industries benefit from the expertise of data scientists, such as those on the following list below:\n 1. The Transportation Industry In India, Analytics India Mag reports that the Delhi government is planning to invest in a new traffic management system using radar-based monitoring with the help of AI. For example, the Allahabad police are using this new technology to direct traffic at the Kumbh Mela, one of the world‚Äôs largest religious gatherings. India‚Äôs Railway minister Piyush Goyal adds, ‚ÄúAI can transform Indian Railways in terms of safety, passenger amenities, better revenues, growth and efficiency.‚Äù Railway Age describes how data science leads to vital insights into the degradation of track and equipment components, identifying areas of potential failure for improvement and repair. Due to recent developments in technology, ‚Äúrailroads are starting to look at the predictive analytics tools within this field of Data Science to help optimise and plan maintenance and improve safety,‚Äù while providing new and actionable insights.\n 2. The Legal Industry Although Forbes claims that the legal sector is slow to adopt big data, currently lagging behind other industries, they also claim that ‚Äúin the digital age, legal providers that fail to make this investment will be unable to compete with those that do.‚Äù Special Counsel‚Äôs data analysis page explains that the right methods can make all the difference for law firms, as these enable them to better manage information, make effective decisions, and mitigate risks. For one, database administration can help optimise database performance through automated monitoring systems that provide streamlined projects from start to finish. Data engineering and warehousing can also centralise data from law enterprises hosted across multiple sites while simultaneously analysing a large quantity of information. Also, if a law firm is struggling with a database containing duplicate amounts of information along with out-dated data, analytics can help organise these documents by theme and eliminate any unnecessary information.\n 3. The Healthcare Industry Research published by the Journal of the American Medical Informatics Association suggests that the demand for data scientists in the healthcare sector is growing incredibly swiftly. Through the methods of pattern recognition, statistical analysis, and deep learning algorithms, data science is revolutionising the healthcare industry. The invaluable ability to process massive amounts of data related to laboratory results, as well as clinical reports, leads to a quicker and more efficient diagnostic service. AI in healthcare is being urged by India\u0026rsquo;s Niti Aayog, who states that \u0026ldquo;by taking advantage of AI, India could grow its gross domestic product by 1.3 percentage points annually to US$957 billion by 2035.\u0026rdquo; In addition, Union Health Minister Dr. Harsh Vardhan adds that ‚Äúthe potential of AI in public health is being explored in our country,\u0026quot; with \u0026ldquo;the Ministry of Health and Family Welfare working towards using AI in a safe and effective way in public health in India.‚Äù\nTo sum it all up, data science provides invaluable benefits across a wide range of industries. In this day and age, collecting data is ultimately useless if you can‚Äôt utilise it in an efficient manner. Data science organises this data and provides real-world applications that impact customers in meaningful ways.\n For anyone who‚Äôs interested in beginning a career in data science and is in need of some helpful guides, check out the MLWHiz Archive\n","permalink":"https://mlwhiz.com/blog/2020/01/15/ind/","tags":null,"title":"3 Industries That Benefit from Data Science"},{"categories":["Data Science"],"contents":"Time series prediction problems are pretty frequent in the retail domain.\nCompanies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.\nWhile there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.\nIn this post, we will try to solve the time series problem using XGBoost.\nThe main things I am going to focus on are the sort of features such a setup takes and how to create such features.\n Dataset  Kaggle master Kazanova along with some of his friends released a ‚ÄúHow to win a data science competition‚Äù Coursera course. The Course involved a final project which itself was a time series prediction problem.\nIn this competition, we are given a challenging time-series dataset consisting of daily sales data, provided by one of the largest Russian software firms ‚Äî 1C Company.\nWe have to predict total sales for every product and store in the next month.\nHere is how the data looks like:\n We are given the data at a daily level, and we want to build a model which predicts total sales for every product and store in the next month.\nThe variable date_block_num is a consecutive month number, used for convenience. January 2013 is 0, and October 2015 is 33. You can think of it as a proxy to month variable. I think all the other variables are self-explanatory.\nSo how do we approach this sort of a problem?\n Data Preparation The main thing that I noticed is that the data preparation and feature generation aspect is by far the most important thing when we attempt to solve the time series problem using regression.\n1. Do Basic EDA and remove outliers sales = sales[sales[\u0026#39;item_price\u0026#39;]\u0026lt;100000] sales = sales[sales[\u0026#39;item_cnt_day\u0026#39;]\u0026lt;=1000] 2. Group data at a level you want your predictions to be: We start with creating a dataframe of distinct date_block_num, store and item combinations.\nThis is important because in the months we don‚Äôt have a data for an item store combination, the machine learning algorithm needs to be told explicitly that the sales are zero.\nfrom itertools import product # Create \u0026#34;grid\u0026#34; with columns index_cols = [\u0026#39;shop_id\u0026#39;, \u0026#39;item_id\u0026#39;, \u0026#39;date_block_num\u0026#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[\u0026#39;date_block_num\u0026#39;].unique(): cur_shops = sales.loc[sales[\u0026#39;date_block_num\u0026#39;] == block_num, \u0026#39;shop_id\u0026#39;].unique() cur_items = sales.loc[sales[\u0026#39;date_block_num\u0026#39;] == block_num, \u0026#39;item_id\u0026#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=\u0026#39;int32\u0026#39;)) grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) grid.head()  The grid dataFrame contains all the shop, items and month combinations.\nWe then merge the Grid with Sales to get the monthly sales DataFrame. We also replace all the NA‚Äôs with zero for months that didn‚Äôt have any sales.\nsales_m = sales.groupby([\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]).agg({\u0026#39;item_cnt_day\u0026#39;: \u0026#39;sum\u0026#39;,\u0026#39;item_price\u0026#39;: np.mean}).reset_index() # Merging sales numbers with the grid dataframe sales_m = pd.merge(grid,sales_m,on=[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;],how=\u0026#39;left\u0026#39;).fillna(0) # adding the category id too from the items table. sales_m = pd.merge(sales_m,items,on=[\u0026#39;item_id\u0026#39;],how=\u0026#39;left\u0026#39;)   3. Create Target Encodings To create target encodings, we group by a particular column and take the mean/min/sum etc. of the target column on it. These features are the first features we create in our model.\nPlease note that these features may induce a lot of leakage/overfitting in our system and thus we don‚Äôt use them directly in our models. We will use the lag based version of these features in our models which we will create next.\ngroupcollist = [\u0026#39;item_id\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_category_id\u0026#39;] aggregationlist = [(\u0026#39;item_price\u0026#39;,np.mean,\u0026#39;avg\u0026#39;),(\u0026#39;item_cnt_day\u0026#39;,np.sum,\u0026#39;sum\u0026#39;),(\u0026#39;item_cnt_day\u0026#39;,np.mean,\u0026#39;avg\u0026#39;)] for type_id in groupcollist: for column_id,aggregator,aggtype in aggregationlist: # get numbers from sales data and set column names mean_df = sales_m.groupby([type_id,\u0026#39;date_block_num\u0026#39;]).aggregate(aggregator).reset_index()[[column_id,type_id,\u0026#39;date_block_num\u0026#39;]] mean_df.columns = [type_id+\u0026#39;_\u0026#39;+aggtype+\u0026#39;_\u0026#39;+column_id,type_id,\u0026#39;date_block_num\u0026#39;] # merge new columns on sales_m data sales_m = pd.merge(sales_m,mean_df,on=[\u0026#39;date_block_num\u0026#39;,type_id],how=\u0026#39;left\u0026#39;) We group by item_id, shop_id, and item_category_id and aggregate on the item_price and item_cnt_day column to create the following new features:\n We could also have used featuretools for this. Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.\n 4. Create Lag Features The next set of features our model needs are the lag based Features.\nWhen we create regular classification models, we treat training examples as fairly independent of each other. But in case of time series problems, at any point in time, the model needs information on what happened in the past.\nWe can‚Äôt do this for all the past days, but we can provide the models with the most recent information nonetheless using our target encoded features.\nlag_variables = [\u0026#39;item_id_avg_item_price\u0026#39;,\u0026#39;item_id_sum_item_cnt_day\u0026#39;,\u0026#39;item_id_avg_item_cnt_day\u0026#39;,\u0026#39;shop_id_avg_item_price\u0026#39;,\u0026#39;shop_id_sum_item_cnt_day\u0026#39;,\u0026#39;shop_id_avg_item_cnt_day\u0026#39;,\u0026#39;item_category_id_avg_item_price\u0026#39;,\u0026#39;item_category_id_sum_item_cnt_day\u0026#39;,\u0026#39;item_category_id_avg_item_cnt_day\u0026#39;,\u0026#39;item_cnt_day\u0026#39;] lags = [1 ,2 ,3 ,4, 5, 12] # we will keep the results in thsi dataframe sales_means = sales_m.copy() for lag in lags: sales_new_df = sales_m.copy() sales_new_df.date_block_num+=lag # subset only the lag variables we want sales_new_df = sales_new_df[[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]+lag_variables] sales_new_df.columns = [\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]+ [lag_feat+\u0026#39;_lag_\u0026#39;+str(lag) for lag_feat in lag_variables] # join with date_block_num,shop_id and item_id sales_means = pd.merge(sales_means, sales_new_df,on=[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;] ,how=\u0026#39;left\u0026#39;) So we aim to add past information for a few features in our data. We do it for all the new features we created and the item_cnt_day feature.\nWe fill the NA‚Äôs with zeros once we have the lag features.\nfor feat in sales_means.columns: if \u0026#39;item_cnt\u0026#39; in feat: sales_means[feat]=sales_means[feat].fillna(0) elif \u0026#39;item_price\u0026#39; in feat: sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median()) We end up creating a lot of lag features with different lags:\n'item_id_avg_item_price_lag_1','item_id_sum_item_cnt_day_lag_1', 'item_id_avg_item_cnt_day_lag_1','shop_id_avg_item_price_lag_1', 'shop_id_sum_item_cnt_day_lag_1','shop_id_avg_item_cnt_day_lag_1','item_category_id_avg_item_price_lag_1','item_category_id_sum_item_cnt_day_lag_1','item_category_id_avg_item_cnt_day_lag_1', 'item_cnt_day_lag_1', 'item_id_avg_item_price_lag_2', 'item_id_sum_item_cnt_day_lag_2','item_id_avg_item_cnt_day_lag_2', 'shop_id_avg_item_price_lag_2','shop_id_sum_item_cnt_day_lag_2', 'shop_id_avg_item_cnt_day_lag_2','item_category_id_avg_item_price_lag_2','item_category_id_sum_item_cnt_day_lag_2','item_category_id_avg_item_cnt_day_lag_2', 'item_cnt_day_lag_2', ...   Modelling 1. Drop the unrequired columns As previously said, we are going to drop the target encoded features as they might induce a lot of overfitting in the model. We also lose the item_name and item_price feature.\ncols_to_drop = lag_variables[:-1] + [\u0026#39;item_name\u0026#39;,\u0026#39;item_price\u0026#39;] for col in cols_to_drop: del sales_means[col] 2. Take a recent bit of data only When we created the lag variables, we induced a lot of zeroes in the system. We used the maximum lag as 12. To counter that we remove the first 12 months indexes.\nsales_means = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]\u0026gt;11] 3. Train and CV Split When we do a time series split, we usually don‚Äôt take a cross-sectional split as the data is time-dependent. We want to create a model that sees till now and can predict the next month well.\nX_train = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]\u0026lt;33] X_cv = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]==33] Y_train = X_train[\u0026#39;item_cnt_day\u0026#39;] Y_cv = X_cv[\u0026#39;item_cnt_day\u0026#39;] del X_train[\u0026#39;item_cnt_day\u0026#39;] del X_cv[\u0026#39;item_cnt_day\u0026#39;] 4. Create Baseline  Before we proceed with modelling steps, lets check the RMSE of a naive model, as we want to have an RMSE to compare to. We assume that we are going to predict the last month sales as current month sale for our baseline model. We can quantify the performance of our model using this baseline RMSE.\nfrom sklearn.metrics import mean_squared_error sales_m_test = sales_m[sales_m[\u0026#39;date_block_num\u0026#39;]==33] preds = sales_m.copy() preds[\u0026#39;date_block_num\u0026#39;]=preds[\u0026#39;date_block_num\u0026#39;]+1 preds = preds[preds[\u0026#39;date_block_num\u0026#39;]==33] preds = preds.rename(columns={\u0026#39;item_cnt_day\u0026#39;:\u0026#39;preds_item_cnt_day\u0026#39;}) preds = pd.merge(sales_m_test,preds,on = [\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;],how=\u0026#39;left\u0026#39;)[[\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;,\u0026#39;preds_item_cnt_day\u0026#39;,\u0026#39;item_cnt_day\u0026#39;]].fillna(0) # We want our predictions clipped at (0,20). Competition Specific preds[\u0026#39;item_cnt_day\u0026#39;] = preds[\u0026#39;item_cnt_day\u0026#39;].clip(0,20) preds[\u0026#39;preds_item_cnt_day\u0026#39;] = preds[\u0026#39;preds_item_cnt_day\u0026#39;].clip(0,20) baseline_rmse = np.sqrt(mean_squared_error(preds[\u0026#39;item_cnt_day\u0026#39;],preds[\u0026#39;preds_item_cnt_day\u0026#39;])) print(baseline_rmse) 1.1358170090812756   5. Train XGB We use the XGBRegressor object from the xgboost scikit API to build our model. Parameters are taken from this kaggle kernel . If you have time, you can use hyperopt to automatically find out the hyperparameters yourself.\nfrom xgboost import XGBRegressor model = XGBRegressor( max_depth=8, n_estimators=1000, min_child_weight=300, colsample_bytree=0.8, subsample=0.8, eta=0.3, seed=42) model.fit( X_train, Y_train, eval_metric=\u0026#34;rmse\u0026#34;, eval_set=[(X_train, Y_train), (X_cv, Y_cv)], verbose=True, early_stopping_rounds = 10)  After running this, we can see RMSE in ranges of 0.93 on the CV set. And that is pretty impressive based on our baseline validation RMSE of 1.13. And so we work on deploying this model as part of our continuous integration effort.\n5. Plot Feature Importance We can also see the important features that come from XGB.\nfeature_importances = pd.DataFrame({\u0026#39;col\u0026#39;: columns,\u0026#39;imp\u0026#39;:model.feature_importances_}) feature_importances = feature_importances.sort_values(by=\u0026#39;imp\u0026#39;,ascending=False) px.bar(feature_importances,x=\u0026#39;col\u0026#39;,y=\u0026#39;imp\u0026#39;)   Conclusion In this post, we talked about how we can use trees for even time series modelling. The purpose was not to get perfect scores on the kaggle leaderboard but to gain an understanding of how such models work.\n When I took part in this competition as part of the course , a couple of years back, using trees I reached near the top of the leaderboard.\nOver time people have worked a lot on tweaking the model, hyperparameter tuning and creating even more informative features. But the basic approach has remained the same.\nYou can find the whole running code on GitHub .\nTake a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of ways to improve your models using feature engineering and hyperparameter tuning.\nI am going to be writing more beginner-friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/12/28/timeseries/","tags":["Machine Learning","Data Science","Algorithms","Awesome Guides"],"title":"Using Gradient Boosting for Time Series prediction tasks"},{"categories":["Data Science","Awesome Guides"],"contents":" Creating a great machine learning system is an art.\n There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.\nBut do we ever think about how we will deploy our models once we have them?\nI have seen a lot of ML projects, and a lot of them are doomed to fail as they don‚Äôt have a set plan for production from the onset.\nThis post is about the process requirements for a successful ML project ‚Äî One that goes to production.\n 1. Establish a Baseline at the onset You don‚Äôt really have to have a model to get the baseline results.\nLet us say we will be using RMSE as an evaluation metric for our time series models. We evaluated the model on the test set, and the RMSE came out to be 3.64.\nIs 3.64 a good RMSE? How do we know? We need a baseline RMSE.\nThis could come from a currently employed model for the same task. Or by using some very simple heuristic. For a Time series model, a baseline to defeat is last day prediction. i.e., predict the number on the previous day.\nOr how about Image classification task. Take 1000 labelled samples and get them classified by humans. And Human accuracy can be your Baseline. If a human is not able to get a 70% prediction accuracy on the task, you can always think of automating a process if your models reach a similar level.\nLearning: Try to be aware of the results you are going to get before you create your models. Setting some out of the world expectations is only going to disappoint you and your client.\n 2. Continuous Integration is the Way Forward  You have created your model now. It performs better than the baseline/your current model on your local test dataset. Should we go forward?\nWe have two choices-\n  Go into an endless loop in improving our model further.\n  Test our model in production settings, get more insights about what could go wrong and then continue improving our model with continuous integration.\n  I am a fan of the second approach. In his awesome¬†third course named Structuring Machine learning projects in the Coursera¬†Deep Learning Specialization , Andrew Ng says ‚Äî\n ‚ÄúDon‚Äôt start off trying to design and build the perfect system. Instead, build and train a basic system quickly ‚Äî perhaps in just a few days. Even if the basic system is far from the ‚Äúbest‚Äù system you can build, it is valuable to examine how the basic system functions: you will quickly find clues that show you the most promising directions in which to invest your time.‚Äù\n Done is better than perfect.\nLearning: If your new model is better than the current model in production or your new model is better than the baseline, it doesn‚Äôt make sense to wait to go to production.\n 3. Your model might break into Production Is your model better than the Baseline? It performed better on the local test dataset, but will it really work well on the whole?\nTo test the validity of your assumption that your model is better than the existing model, you can set up an A/B test. Some users(Test group)see predictions from your model while some users(Control) see the predictions from the previous model.\nIn fact, this is the right way to deploy your model. And you might find that indeed your model is not as good as it seems.\n Being wrong is not wrong really, what‚Äôs wrong is to not anticipate that we could be wrong.\n It is hard to point out the real reason behind why your model performs poorly in production settings, but some causes could be:\n  You might see the data coming in real-time to be significantly different from the training data.\n  Or you have not done the preprocessing pipeline correctly.\n  Or you do not measure the performance correctly.\n  Or maybe there is a bug in your implementation.\n  Learning: Don‚Äôt go into production with a full scale. A/B test is always an excellent way to go forward. Have something ready to fall back upon(An older model perhaps). There might always be things that might break, which you couldn‚Äôt have anticipated.\n 4. Your model might not even go to Production I have created this impressive ML model, it gives 90% accuracy, but it takes around 10 seconds to fetch a prediction.\nIs that acceptable? For some use-cases maybe, but really no.\nIn the past, there have been many Kaggle competitions whose winners ended up creating monster ensembles to take the top spots on the leaderboard. Below is a particular mindblowing example model which was used to win Otto classification challenge on Kaggle:\n Another example is the Netflix Million dollar Recommendation Engine Challenge. The Netflix team ended up¬†never using the wining solution due to the engineering costs involved.\nSo how do you make your models accurate yet easy on the machine?\n Teacher ‚Äî Student Model: Source \nHere comes the concept of Teacher-Student models or Knowledge distillation. In Knowledge distillation, we train a smaller student model on a bigger already trained teacher model.\nHere we use the soft labels/probabilities from the teacher model and use it as the training data for the Student model.\n The point is that the teacher is outputting class probabilities ‚Äî ‚Äúsoft labels‚Äù rather than ‚Äúhard labels‚Äù. For example, a fruit classifier might say ‚ÄúApple 0.9, Pear 0.1‚Äù instead of ‚ÄúApple 1.0, Pear 0.0‚Äù . Why bother? Because these ‚Äúsoft labels‚Äù are more informative than the original ones ‚Äî telling the student that yes, a particular apple does very slightly resemble a pear. Student models can often come very close to teacher-level performance, even while using 1‚Äì2 orders of magnitude fewer parameters! ‚Äî Source  Learning: Sometimes, we don‚Äôt have a lot of compute available at prediction time, and so we want to have a lighter model. We can try to build simpler models or try using knowledge distillation for such use cases.\n 5. Maintainance and Feedback Loop  The world is not constant and so are your model weights\n The world around us is rapidly changing, and what might be applicable two months back might not be relevant now. In a way, the models we build are reflections of the world, and if the world is changing our models should be able to reflect this change.\n Model performance deteriorates typically with time.\nFor this reason, we must think of ways to upgrade our models as part of the maintenance cycle at the onset itself.\nThe frequency of this cycle depends entirely on the business problem that you are trying to solve. In an Ad prediction system where the users tend to be fickle and buying patterns emerge continuously, the frequency needs to be pretty high. While in a review sentiment analysis system, the frequency need not be that high as language doesn‚Äôt change its structure quite so much.\n Feedback Loop: Source \nI would also like to acknowledge the importance of the feedback loop in a machine learning system. Let‚Äôs say that you predicted that a particular image is a dog with low probability in a dog vs cat classifier. Can we learn something from these low confidence examples? You can send it to manual review to check if it could be used to retrain the model or not. In this way, we train our classifier on instances it is unsure about.\nLearning: When thinking of production, come up with a plan to maintain and improve the model using feedback as well.\n Conclusion These are some of the things I find important before thinking of putting a model into production.\nWhile this is not an exhaustive list of things that you need to think about and things that could go wrong, it might undoubtedly act as food for thought for the next time you create your machine learning system.\nIf you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent¬†third course named Structuring Machine learning projects in the Coursera¬†Deep Learning Specialization . Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at¬†Medium or Subscribe to my¬†blog ","permalink":"https://mlwhiz.com/blog/2019/12/25/prod/","tags":["Production","Machine Learning","Data Science"],"title":"Take your Machine Learning Models to Production with these 5 simple steps"},{"categories":["Learning Resources","Data Science"],"contents":"People ask me a lot about how to land a data science job? Or how to switch careers or how to study for a job interview?\nMostly my answer is to do some MOOCs , create some projects, participate in Kaggle, try to get in a startup and don‚Äôt give up.\nBut yet there are some things everyone should understand about data science jobs.\nData science jobs involve a lot of to and fro communication and involve a lot of people handling skills. And as such, even if you don‚Äôt realise you will be inadvertently tested on these skills.\nSometimes you may feel like ‚Äî I am a coder and let me code in peace. Or how does my behaviour matter? The point is it does.\nThis post is about explaining some of the worst mistakes people do in a data science interview so that you don‚Äôt end up repeating them.\n 1. Lose your Entitlement This is what I often call the Survivorship Bias in Data Science.\nSo, you want to be a Rockstar Data Scientist. Maybe get a job in the booming sector. HBR did say that there is going to be a shortage of data scientists and you feel like you are just the right person for the job.\nI do take interviews a lot right now, and I see a lot of people suffering from Survivorship Bias.\nJust a while back, I interviewed a guy already having some experience in the field. Let‚Äôs call him Andy. I asked Andy a simple Math based question. This is a data science interview, so I guess he should have expected it. Right?\nNo. His answer was ‚Äî\n We have packages for doing all this.\n I ignored that. Letting it go as a one-off.\nAsked a different question, he said why am I asking Math questions?\nThe thing this told me about Andy is that he feels entitled. Just because Andy sees a lot of people getting into Data Science, he thinks he should also get in.\nWhat Andy doesn‚Äôt understand is that for every successful Data Scientist, there are a lot of people who don‚Äôt make it.\nAll Andy sees are the survivors. And that is a mistake.\n 2. The Overconfidence Effect  So, I recently also interviewed Chris for a Data Science role.\nI started by asking about his projects and past work.\nHe confidently explained his projects. We talked about his various projects for the first 30 minutes, and I was pretty much convinced that he has a place in the team.\nTill now I hadn‚Äôt asked much of technical questions and here is where I started getting sceptical. The thing was Chris would explain anything I asked pretty confidently, albeit wrongly. He would try to explain to me concepts he wouldn‚Äôt know.\n I felt as if he thought that maybe I didn‚Äôt know the answer to my own questions. And so he can luck out by saying anything.\n And this happened two or three times in the interview.\nNow don‚Äôt get me wrong ‚Äî Confidence is good. And a healthy bit of it is necessary. But be Confident and wrong, and you spell disaster.\nCan I trust Chris to handle business? What would happen if he committed something wrongly to the business or made lofty claims and didn‚Äôt realise them afterwards?\n Overconfidence has been called the most ‚Äúpervasive and potentially catastrophic‚Äù of all the cognitive biases to which human beings fall victim.It has been blamed for lawsuits, strikes, wars, and stock market bubbles and crashes. ‚Äî Wikipedia\n In fact, I would prefer a non-confident and wrong person. At least then I would know to check my facts.\n 3. Keyword Stuffing Keyword stuffing is the practice of overloading one‚Äôs resume with skills they may not know.\nThe typical excuses to do this are ‚Äî A lot of people do this. HR might not select me. The system works like that only.\n You might call it a necessary evil. I will call it you setting up yourself up for a failure.\n And you might get that occasional interview call when you do this; the odds are pretty much stacked against you as I will grill you on your resume. And I will expect you to know it well.\nIf Mark puts Decision Trees in his resume, Mark should expect a question around it.\nOr if Mark says that he has implemented a Machine Learning algorithm from Scratch, I would not be wrong to expect Mark to explain the algorithm‚Äôs nitty-gritty details.\nThe point is that it is alright if you don‚Äôt know everything. Nobody ever does. But lie on your resume, and you have made my job easier as these kind of lies are pretty easy to capture.\n Conclusion I thought a lot before writing this article as I might sound a little harsh in this one. But I guess it is necessary to let people know about their mistakes.\nSome people will disagree with me here on how I can judge people from such small mistakes.\n I would say that an interview is about judging someone in a limited time frame.  Furthermore, a lot of companies now take behavioural rounds apart from the regular data science rounds and being ingenious, or entitled or overconfident will obviously not help you.\nSo, I would say that being respectful and nice goes a long way and you should aim for it in life and not just in the interview room.\nThat way, you also get a lot of practice, and you become a better person as well.\nPS: All the names used are just placeholders and these are my own personal views.\n","permalink":"https://mlwhiz.com/blog/2019/12/24/mistakes/","tags":["Machine Learning","Data Science","Opinion"],"title":"3 Mistakes you should not make in a Data Science Interview"},{"categories":["Programming","Data Science"],"contents":"Algorithms are an integral part of data science. While most of us data scientists don‚Äôt take a proper algorithms course while studying, they are important all the same.\nMany companies ask data structures and algorithms as part of their interview process for hiring data scientists.\nNow the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.\nWe all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people‚Äôs coding ability.\nBut let‚Äôs not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms.\nThis post is about fast-tracking that study and panning some essential algorithms concepts for the data scientists in an easy to understand way.\n 1. Recursion/Memoization Recursion is where a function being defined is applied within its own definition. Put simply; recursion is when a function calls itself. Google does something pretty interesting when you search for recursion there.\n Hope you get the joke. While recursion may seem a little bit daunting to someone just starting, it is pretty simple to understand. And it is a beautiful concept once you know it.\nThe best example I find for explaining recursion is to calculate the factorial of a number.\ndef factorial(n): if n==0: return 1 return n*factorial(n-1) We can see how factorial is a recursive function quite easily.\nFactorial(n) = n*Factorial(n-1)\nSo how does it translates to programming?\nA function for a recursive call generally consists of two things:\n  A base case ‚Äî The case where the recursion ends.\n  A recursive formulation- a formulaic way to move towards the base case.\n  A lot of problems you end up solving are recursive. It applies to data science, as well.\nFor example, A decision tree is just a binary tree, and tree algorithms are generally recursive. Or, we do use sort in a lot of times. The algorithm responsible for that is called mergesort, which in itself is a recursive algorithm. Another one is binary search, which includes finding an element in an array.\nNow we have got a basic hang of recursion, let us try to find the nth Fibonacci Number. Fibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:\ndef fib(n): if n\u0026lt;=1: return 1 return fib(n-1) + fib(n-2) But do you spot the problem here?\nIf you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.\n Can we do better? Yes, we can. We can change our implementation a little bit an add a dictionary to add some storage to our method. Now, this memo dictionary gets updated any time a number has been calculated. If that number appears again, we don‚Äôt calculate it again but give results from the memo dictionary. This addition of storage is called Memoization.\nmemo = {} def fib_memo(n): if n in memo: return memo[n] if n\u0026lt;=1: memo[n]=1 return 1 memo[n] = fib_memo(n-1) + fib_memo(n-2) return memo[n] Usually, I like to write the recursive function first, and if it is making a lot of calls to the same parameters again and again, I add a dictionary to memorize solutions.\nHow much does it help?\n This is the run time comparison for different values of n. We can see the runtime for Fibonacci without Memoization increases exponentially, while for the memoized function, the time is linear.\n 2. Dynamic programming  Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.\nIn Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.\ndef fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n+1): dp_sols[i] = dp_sols[i-1] + dp_sols[i-2] return dp_sols[n]  Above is the comparison of runtimes for DP vs. Memoization. We can see that they are both linear, but DP still is a little bit faster.\nWhy? Because Dynamic Programming made only one call exactly to each subproblem in this case.\nThere is an excellent story on how Bellman who developed Dynamic Programming framed the term:\n Where did the name, dynamic programming, come from? The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson . He was Secretary of Defense, and he actually had a pathological fear and hatred of the word research. What title, what name, could I choose? In the first place, I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word ‚Äúprogramming‚Äù. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let‚Äôs kill two birds with one stone. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n  3. Binary Search Let us say we have a sorted array of numbers, and we want to find out a number from this array. We can go the linear route that checks every number one by one and stops if it finds the number. The problem is that it takes too long if the array contains millions of elements. Here we can use a Binary search.\n  Source:mathwarehouse.com|Finding 37 ‚Äî There are 3.7 trillion fish in the ocean, they‚Äôre looking for one    # Returns index of target in nums array if present, else -1 def binary_search(nums, left, right, target): # Base case if right \u0026gt;= left: mid = int((left + right)/2) # If target is present at the mid, return if nums[mid] == target: return mid # Target is smaller than mid search the elements in left elif nums[mid] \u0026gt; target: return binary_search(nums, left, mid-1, target) # Target is larger than mid, search the elements in right else: return binary_search(nums, mid+1, right, target) else: # Target is not in nums return -1 nums = [1,2,3,4,5,6,7,8,9] print(binary_search(nums, 0, len(nums)-1,7)) This is an advanced case of a recursion based algorithm where we make use of the fact that the array is sorted. Here we recursively look at the middle element and see if we want to search in the left or right of the middle element. This makes our searching space go down by a factor of 2 every step.\nAnd thus the run time of this algorithm is O(logn) as opposed to O(n) for linear search.\nHow much does that matter? Below is a comparison in run times. We can see that the Binary search is pretty fast compared to Linear search.\n  Conclusion In this post, I talked about some of the most exciting algorithms that form the basis for programming.\nThese algorithms are behind some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.\nAnd while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.\nAlso take a look at my other posts in the series , if you want to learn about algorithms and Data structures.\nContinue Learning If you want to read up more on Algorithms, here is an \u0026lt;strong\u0026gt;Algorithm Specialization on Coursera by UCSanDiego\u0026lt;/strong\u0026gt; , which I highly recommend to learn the basics of algorithms.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/12/09/pc/","tags":["Machine Learning","Algorithms","Production"],"title":"3 Programming concepts for Data Scientists"},{"categories":["Awesome Guides","Data Science"],"contents":"A Machine Learning project is never really complete if we don‚Äôt have a good way to showcase it.\nWhile in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.\nAnd Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.\nNot to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.\nSo, are we doomed to learn web frameworks? Or to call our developer friend for silly doubts in the middle of the night?\nThis is where StreamLit comes in and delivers on its promise to create web apps just using Python.\n Zen of Python: Simple is better than complex and Streamlit makes it dead simple to create apps.\n This post is about understanding how to create apps that support data science projects using Streamlit.\nTo understand more about the architecture and the thought process that led to streamlit, have a look at this excellent post by one of the original developers/founder Adrien Treuille .\n Installation Installation is as simple as running the command:\npip install streamlit\nTo see if our installation is successful, we can just run:\nstreamlit hello\nThis should show you a screen that says:\n You can go to the local URL: localhost:8501 in your browser to see a Streamlit app in action. The developers have provided some cool demos that you can play with. Do take your time and feel the power of the tool before coming back.\n  Streamlit Hello World Streamlit aims to make app development easy using simple Python.\nSo let us write a simple app to see if it delivers on that promise.\nHere I start with a simple app which we will call the Hello World of streamlit. Just paste the code given below in a file named helloworld.py\nimport streamlit as st x = st.slider(\u0026#39;x\u0026#39;) st.write(x, \u0026#39;squared is\u0026#39;, x * x) And, on the terminal run:\nstreamlit run helloworld.py And voila, you should be able to see a simple app in action in your browser at localhost:8501 that allows you to move a slider and gives the result.\n It was pretty easy. In the above app, we used two features from Streamlit:\n  the st.slider widget that we can slide to change the output of the web app.\n  and the versatile st.write command. I am amazed at how it can write anything from charts, dataframes, and simple text. More on this later.\n  Important: Remember that every time we change the widget value, the whole app runs from top to bottom.\n Streamlit Widgets Widgets provide us a way to control our app. The best place to read about the widgets is the API reference documentation itself but I will describe some most prominent ones that you might end up using.\n1. Slider streamlit.slider(label, min_value=None, max_value=None, value=None, step=None, format=None) We already saw st.slider in action above. It can be used with min_value,max_value, and step for getting inputs in a range.\n2. Text Input The simplest way to get user input be it some URL input or some text input for sentiment analysis. It just needs a single label for naming the textbox.\nimport streamlit as st url = st.text_input(\u0026#39;Enter URL\u0026#39;) st.write(\u0026#39;The Entered URL is\u0026#39;, url) This is how the app looks:\n Tip: You can just change the file helloworld.py and refresh the browser. The way I work is to open and change helloworld.py in sublime text and see the changes in the browser side by side.\n3. Checkbox One use case for checkboxes is to hide or show/hide a specific section in an app. Another could be setting up a boolean value in the parameters for a function. st.checkbox() takes a single argument, which is the widget label. In this app, the checkbox is used to toggle a conditional statement.\nimport streamlit as st import pandas as pd import numpy as np df = pd.read_csv(\u0026#34;football_data.csv\u0026#34;) if st.checkbox(\u0026#39;Show dataframe\u0026#39;): st.write(df)  4. SelectBox We can use st.selectbox to choose from a series or a list. Normally a use case is to use it as a simple dropdown to select values from a list.\nimport streamlit as st import pandas as pd import numpy as np df = pd.read_csv(\u0026#34;football_data.csv\u0026#34;) option = st.selectbox( \u0026#39;Which Club do you like best?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) st.write(\u0026#39;You selected:\u0026#39;, option)  5. MultiSelect We can also use multiple values from a dropdown. Here we use st.multiselect to get multiple values as a list in the variable options\nimport streamlit as st import pandas as pd import numpy as np df = pd.read_csv(\u0026#34;football_data.csv\u0026#34;) options = st.multiselect( \u0026#39;What are your favorite clubs?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) st.write(\u0026#39;You selected:\u0026#39;, options)   Creating Our Simple App Step by Step So much for understanding the important widgets. Now, we are going to create a simple app using multiple widgets at once.\nTo start simple, we will try to visualize our football data using streamlit. It is pretty much simple to do this with the help of the above widgets.\nimport streamlit as st import pandas as pd import numpy as np df = pd.read_csv(\u0026#34;football_data.csv\u0026#34;) clubs = st.multiselect(\u0026#39;Show Player for clubs?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) nationalities = st.multiselect(\u0026#39;Show Player from Nationalities?\u0026#39;, df[\u0026#39;Nationality\u0026#39;].unique()) # Filter dataframe new_df = df[(df[\u0026#39;Club\u0026#39;].isin(clubs)) \u0026amp; (df[\u0026#39;Nationality\u0026#39;].isin(nationalities))] # write dataframe to screen st.write(new_df) Our simple app looks like:\n That was easy. But it seems pretty basic right now. Can we add some charts?\nStreamlit currently supports many libraries for plotting.Plotly, Bokeh, Matplotlib, Altair, and Vega charts being some of them. Plotly Express also works, although they didn‚Äôt specify it in the docs. It also has some inbuilt chart types that are ‚Äúnative‚Äù to Streamlit, like st.line_chart and st.area_chart.\nWe will work with plotly_express here. Here is the code for our simple app. We just used four calls to streamlit. Rest is all simple python.\nimport streamlit as st import pandas as pd import numpy as np import plotly_express as px df = pd.read_csv(\u0026#34;football_data.csv\u0026#34;) clubs = st.multiselect(\u0026#39;Show Player for clubs?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) nationalities = st.multiselect(\u0026#39;Show Player from Nationalities?\u0026#39;, df[\u0026#39;Nationality\u0026#39;].unique()) new_df = df[(df[\u0026#39;Club\u0026#39;].isin(clubs)) \u0026amp; (df[\u0026#39;Nationality\u0026#39;].isin(nationalities))] st.write(new_df) # create figure using plotly express fig = px.scatter(new_df, x =\u0026#39;Overall\u0026#39;,y=\u0026#39;Age\u0026#39;,color=\u0026#39;Name\u0026#39;) # Plot! st.plotly_chart(fig)   Improvements In the start we said that each time we change any widget, the whole app runs from start to end. This is not feasible when we create apps that will serve deep learning models or complicated machine learning models. Streamlit covers us in this aspect by introducing Caching.\n1. Caching In our simple app. We read the pandas dataframe again and again whenever a value changes. While it works for the small data we have, it will not work for big data or when we have to do a lot of processing on the data. Let us use caching using the st.cache decorator function in streamlit like below.\nimport streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(\u0026#34;football_data.csv\u0026#34;) Or for more complex and time taking functions that need to run only once(think loading big Deep Learning models), using:\n@st.cache def complex_func(a,b): DO SOMETHING COMPLEX # Won\u0026#39;t run again and again. complex_func(a,b) When we mark a function with Streamlit‚Äôs cache decorator, whenever the function is called streamlit checks the input parameters that you called the function with.\nIf this is the first time Streamlit has seen these params, it runs the function and stores the result in a local cache.\nWhen the function is called the next time, if those params have not changed, Streamlit knows it can skip executing the function altogether. It just uses the results from the cache.\n2. Sidebar For a cleaner look based on your preference, you might want to move your widgets into a sidebar, something like Rshiny dashboards. This is pretty simple. Just add st.sidebar in your widget‚Äôs code.\nimport streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(\u0026#34;football_data.csv\u0026#34;) clubs = st.sidebar.multiselect(\u0026#39;Show Player for clubs?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) nationalities = st.sidebar.multiselect(\u0026#39;Show Player from Nationalities?\u0026#39;, df[\u0026#39;Nationality\u0026#39;].unique()) new_df = df[(df[\u0026#39;Club\u0026#39;].isin(clubs)) \u0026amp; (df[\u0026#39;Nationality\u0026#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =\u0026#39;Overall\u0026#39;,y=\u0026#39;Age\u0026#39;,color=\u0026#39;Name\u0026#39;) # Plot! st.plotly_chart(fig)  3. Markdown? I love writing in Markdown. I find it less verbose than HTML and much more suited for data science work. So, can we use Markdown with the streamlit app?\nYes, we can. There are a couple of ways to do this. In my view, the best one is to use Magic commands . Magic commands allow you to write markdown as easily as comments. You could also have used the command st.markdown\nimport streamlit as st import pandas as pd import numpy as np import plotly_express as px \u0026#39;\u0026#39;\u0026#39; # Club and Nationality App This very simple webapp allows you to select and visualize players from certain clubs and certain nationalities. \u0026#39;\u0026#39;\u0026#39; df = st.cache(pd.read_csv)(\u0026#34;football_data.csv\u0026#34;) clubs = st.sidebar.multiselect(\u0026#39;Show Player for clubs?\u0026#39;, df[\u0026#39;Club\u0026#39;].unique()) nationalities = st.sidebar.multiselect(\u0026#39;Show Player from Nationalities?\u0026#39;, df[\u0026#39;Nationality\u0026#39;].unique()) new_df = df[(df[\u0026#39;Club\u0026#39;].isin(clubs)) \u0026amp; (df[\u0026#39;Nationality\u0026#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =\u0026#39;Overall\u0026#39;,y=\u0026#39;Age\u0026#39;,color=\u0026#39;Name\u0026#39;) \u0026#39;\u0026#39;\u0026#39; ### Here is a simple chart between player age and overall \u0026#39;\u0026#39;\u0026#39; st.plotly_chart(fig)   Conclusion Streamlit has democratized the whole process to create apps, and I couldn‚Äôt recommend it more.\nIn this post, we created a simple web app. But the possibilities are endless. To give an example here is face GAN from the streamlit site. And it works by just using the same guiding ideas of widgets and caching.\n I love the default colors and styles that the developers have used, and I found it much more comfortable than using Dash, which I was using until now for my demos. You can also include audio and video in your streamlit apps.\nOn top of that, Streamlit is a free and open-source rather than a proprietary web app that just works out of the box.\nIn the past, I had to reach out to my developer friends for any single change in a demo or presentation; now it is relatively trivial to do that.\n I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.\n I don‚Äôt have an idea if it will perform well in a production environment yet, but its a boon for the small proof of concept projects and demos. I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.\nYou can find the full code for the final app here .\nIf you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about \u0026lt;strong\u0026gt;Data Visualization and applied plotting\u0026lt;/strong\u0026gt; from the University of Michigan, which is a part of a pretty good \u0026lt;strong\u0026gt;Data Science Specialization with Python\u0026lt;/strong\u0026gt; in itself. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/12/07/streamlit/","tags":["Python","Awesome Guides","Best Content","Machine Learning","Data Science","Visualization","Productivity","Tools","Streamlit"],"title":"How to write Web apps using simple Python for Data Scientists?"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"Object Detection is a helpful tool to have in your coding repository.\nIt forms the backbone of many fantastic industrial applications. Some of them being self-driving cars, medical imaging and face detection.\nIn my last post on Object detection, I talked about how Object detection models evolved.\nBut what good is theory, if we can‚Äôt implement it?\nThis post is about implementing and getting an object detector on our custom dataset of weapons.\nThe problem we will specifically solve today is that of Instance Segmentation using Mask-RCNN.\n Instance Segmentation Can we create masks for each object in the image? Specifically something like:\n The most common way to solve this problem is by using Mask-RCNN. The architecture of Mask-RCNN looks like below:\n  Source \nEssentially, it comprises of:\n  A backbone network like resnet50/resnet101\n  A Region Proposal network\n  ROI-Align layers\n  Two output layers ‚Äî one to predict masks and one to predict class and bounding box.\n  There is a lot more to it. If you want to learn more about the theory, read my last post\u0026ndash; Demystifying Object Detection and Instance Segmentation for Data Scientists This post is mostly going to be about the code .\n 1. Creating your Custom Dataset for Instance Segmentation  The use case we will be working on is a weapon detector. A weapon detector is something that can be used in conjunction with street cameras as well as CCTV‚Äôs to fight crime. So it is pretty nifty.\nSo, I started with downloading 40 images each of guns and swords from the open image dataset and annotated them using the VIA tool. Now setting up the annotation project in VIA is petty important, so I will try to explain it step by step.\n1. Set up VIA VIA is an annotation tool, using which you can annotate images both bounding boxes as well as masks. I found it as one of the best tools to do annotation as it is online and runs in the browser itself.\nTo use it, open http://www.robots.ox.ac.uk/~vgg/software/via/via.html You will see a page like:\n The next thing we want to do is to add the different class names in the region_attributes. Here I have added ‚Äògun‚Äô and ‚Äòsword‚Äô as per our use case as these are the two distinct targets I want to annotate.\n 2. Annotate the Images I have kept all the files in the folder data. Next step is to add the files we want to annotate. We can add files in the data folder using the ‚ÄúAdd Files‚Äù button in the VIA tool. And start annotating along with labels as shown below after selecting the polyline tool.\n 3. Download the annotation file Click on save project on the top menu of the VIA tool.\n Save file as via_region_data.json by changing the project name field. This will save the annotations in COCO format.\n4. Set up the data directory structure We will need to set up the data directories first so that we can do object detection. In the code below, I am creating a directory structure that is required for the model that we are going to use.\nfrom random import random import os from glob import glob import json # Path to your images image_paths = glob(\u0026#34;data/*\u0026#34;) #Path to your annotations from VIA tool annotation_file = \u0026#39;via_region_data.json\u0026#39; #clean up the annotations a little annotations = json.load(open(annotation_file)) cleaned_annotations = {} for k,v in annotations[\u0026#39;_via_img_metadata\u0026#39;].items(): cleaned_annotations[v[\u0026#39;filename\u0026#39;]] = v # create train and validation directories ! mkdir procdata ! mkdir procdata/val ! mkdir procdata/train train_annotations = {} valid_annotations = {} # 20% of images in validation folder for img in image_paths: # Image goes to Validation folder if random()\u0026lt;0.2: os.system(\u0026#34;cp \u0026#34;+ img + \u0026#34; procdata/val/\u0026#34;) img = img.split(\u0026#34;/\u0026#34;)[-1] valid_annotations[img] = cleaned_annotations[img] else: os.system(\u0026#34;cp \u0026#34;+ img + \u0026#34; procdata/train/\u0026#34;) img = img.split(\u0026#34;/\u0026#34;)[-1] train_annotations[img] = cleaned_annotations[img] # put different annotations in different folders with open(\u0026#39;procdata/val/via_region_data.json\u0026#39;, \u0026#39;w\u0026#39;) as fp: json.dump(valid_annotations, fp) with open(\u0026#39;procdata/train/via_region_data.json\u0026#39;, \u0026#39;w\u0026#39;) as fp: json.dump(train_annotations, fp) After running the above code, we will get the data in the below folder structure:\n- procdata - train - img1.jpg - img2.jpg - via_region_data.json - val - img3.jpg - img4.jpg - via_region_data.json   2. Setup the Coding Environment We will use the code from the matterport/Mask_RCNN GitHub repository. You can start by cloning the repository and installing the required libraries.\ngit clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip install -r requirements.txt Once we are done with installing the dependencies and cloning the repo, we can start with implementing our project.\nWe make a copy of the samples/balloon directory in Mask_RCNN folder and create a samples/guns_and_swords directory where we will continue our work:\ncp -r samples/balloon samples/guns_and_swords Setting up the Code We start by renaming and changing balloon.py in the samples/guns_and_swords directory to gns.py. The balloon.py file right now trains for one target. I have extended it to use multiple targets. In this file, we change:\n  balloonconfig to gnsConfig\n  BalloonDataset to gnsDataset : We changed some code here to get the target names from our annotation data and also give multiple targets.\n  And some changes in the train function\n  Showing only the changed gnsConfig here to get you an idea. You can take a look at the whole gns.py code here.\nclass gnsConfig(Config): \u0026#34;\u0026#34;\u0026#34;Configuration for training on the toy dataset. Derives from the base Config class and overrides some values. \u0026#34;\u0026#34;\u0026#34; # Give the configuration a recognizable name NAME = \u0026#34;gns\u0026#34; # We use a GPU with 16GB memory, which can fit three image. # Adjust down if you use a smaller GPU. IMAGES_PER_GPU = 3 # Number of classes (including background) NUM_CLASSES = 1 + 2 # Background + sword + gun # Number of training steps per epoch  3. Visualizing Images and Masks Once we are done with changing the gns.py file,we can visualize our masks and images. You can do simply by following this Visualize Dataset.ipynb notebook.\n  4. Train the MaskRCNN Model with Transfer Learning To train the maskRCNN model, on the Guns and Swords dataset, we need to run one of the following commands on the command line based on if we want to initialise our model with COCO weights or imagenet weights:\n# Train a new model starting from pre-trained COCO weights python3 gns.py train ‚Äî dataset=/path/to/dataset ‚Äî weights=coco # Resume training a model that you had trained earlier python3 gns.py train ‚Äî dataset=/path/to/dataset ‚Äî weights=last # Train a new model starting from ImageNet weights python3 gns.py train ‚Äî dataset=/path/to/dataset ‚Äî weights=imagenet The command with weights=last will resume training from the last epoch. The weights are going to be saved in the logs directory in the Mask_RCNN folder.\nThis is how the loss looks after our final epoch.\n Visualize the losses using Tensorboard You can take advantage of tensorboard to visualise how your network is performing. Just run:\ntensorboard --logdir ~/objectDetection/Mask_RCNN/logs/gns20191010T1234 You can get the tensorboard at\nhttps://localhost:6006  Here is how our mask loss looks like:\n We can see that the validation loss is performing pretty abruptly. This is expected as we only have kept 20 images in the validation set.\n 5. Prediction on New Images Predicting a new image is also pretty easy. Just follow the prediction.ipynb notebook for a minimal example using our trained model. Below is the main part of the code.\n# Function taken from utils.dataset def load_image(image_path): \u0026#34;\u0026#34;\u0026#34;Load the specified image and return a [H,W,3] Numpy array. \u0026#34;\u0026#34;\u0026#34; # Load image image = skimage.io.imread(image_path) # If grayscale. Convert to RGB for consistency. if image.ndim != 3: image = skimage.color.gray2rgb(image) # If has an alpha channel, remove it for consistency if image.shape[-1] == 4: image = image[..., :3] return image # path to image to be predicted image = load_image(\u0026#34;../../../data/2c8ce42709516c79.jpg\u0026#34;) # Run object detection results = model.detect([image], verbose=1) # Display results ax = get_ax(1) r = results[0] a = visualize.display_instances(image, r[\u0026#39;rois\u0026#39;], r[\u0026#39;masks\u0026#39;], r[\u0026#39;class_ids\u0026#39;], dataset.class_names, r[\u0026#39;scores\u0026#39;], ax=ax, title=\u0026#34;Predictions\u0026#34;) This is how the result looks for some images in the validation set:\n  Improvements The results don‚Äôt look very promising and leave a lot to be desired, but that is to be expected because of very less training data(60 images). One can try to do the below things to improve the model performance for this weapon detector.\n  We just trained on 60 images due to time constraints. While we used transfer learning the data is still too less ‚Äî Annotate more data.\n  Train for more epochs and longer time. See how validation loss and training loss looks like.\n  Change hyperparameters in the mrcnn/config file in the Mask_RCNN directory. For information on what these hyperparameters mean, take a look at my previous post. The main ones you can look at:\n  # if you want to provide different weights to different losses LOSS_WEIGHTS ={\u0026#39;rpn_class_loss\u0026#39;: 1.0, \u0026#39;rpn_bbox_loss\u0026#39;: 1.0, \u0026#39;mrcnn_class_loss\u0026#39;: 1.0, \u0026#39;mrcnn_bbox_loss\u0026#39;: 1.0, \u0026#39;mrcnn_mask_loss\u0026#39;: 1.0} # Length of square anchor side in pixels RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512) # Ratios of anchors at each cell (width/height) # A value of 1 represents a square anchor, and 0.5 is a wide anchor RPN_ANCHOR_RATIOS = [0.5, 1, 2]  Conclusion In this post, I talked about how to implement Instance segmentation using Mask-RCNN for a custom dataset.\nI tried to make the coding part as simple as possible and hope you find the code useful. In the next part of this post, I will deploy this model using a web app. So stay tuned.\nYou can download the annotated weapons data as well as the code at Github .\nIf you want to know more about various Object Detection techniques, motion estimation, object tracking in video etc., I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization .\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/12/06/weapons/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Object detection","Instance segmentation"],"title":"Implementing Object Detection and Instance Segmentation for Data Scientists"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"I like deep learning a lot but Object Detection is something that doesn‚Äôt come easily to me.\nAnd Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.\nIt is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.\nThis post is about distilling that history into an easy explanation and explaining the gory details for Object Detection and Instance Segmentation.\n Introduction We all know about the image classification problem. Given an image can you find out the class the image belongs to?\nWe can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets.\n ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer‚Äôs outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.\n But there are lots of other interesting problems in the Image domain:\n  Source \nThese problems can be divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:\n  Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class?\n  Classification+Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number of objects(commonly 1) in the image.\n  Object Detection: A More general case of the Classification+Localization problem. In a real-world setting, we don‚Äôt know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them?\n  Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won‚Äôt be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.\n  As you can see all the problems have something of a similar flavour but a little different than each other. In this post, I will focus mainly on Object Detection and Instance segmentation as they are the most interesting.I will go through the 4 most famous techniques for object detection and how they improved with time and new ideas.\n Classification+Localization So lets first try to understand how we can solve the problem when we have a single object in the image. How to solve the Classification+Localization case.\n üí° Treat localization as a regression problem!\n  Input Data Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting, we used to have data in the form (X,y) where X is the image and y used to be the class label.\nIn the Classification+Localization setting, we will have data normally in the form (X,y), where X is still the image and y is an array containing (class_label, x,y,w,h) where,\nx = bounding box top left corner x-coordinate\ny = bounding box top left corner y-coordinate\nw = width of the bounding box in pixels\nh = height of the bounding box in pixels\nModel So in this setting, we create a multi-output model which takes an image as the input and has (n_labels + 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).\nLoss Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).\n Loss = alpha*Softmax_Loss + (1-alpha)*L2_Loss\n Since these two losses would be on a different scale, the alpha hyper-parameter is something that needs to be tuned.\nThere is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function.\nAnd herein lies the essence of the whole Deep Learning framework ‚Äî Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem*. And that is what we are going to see a lot going forward.*\n Object Detection So how does this idea of localization using regression get mapped to Object Detection? It doesn‚Äôt.\nWe don‚Äôt have a fixed number of objects. So we can‚Äôt have 4 outputs denoting, the bounding box coordinates.\nOne naive idea could be to apply CNN to many different crops of the image. CNN classifies each crop as an object class or background class. This is intractable. There could be a lot of such crops that you can create.\nRegion Proposals: So, if just there was just a method(Normally called Region Proposal Network)which could find some smaller number of cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is the basic idea behind RCNN-The first major success in object detection.\nAnd that is what selective search (Uijlings et al, ‚Äú Selective Search for Object Recognition ‚Äù, IJCV 2013) provided.\nSo what are Region Proposals?\n  Find ‚Äúblobby‚Äù image regions that are likely to contain objects\n  Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU\n  So, how exactly the region proposals are made?\nSelective Search for Object Recognition : This paper finds regions in two steps.\nFirst, we start with a set of some initial regions using Efficient GraphBased Image Segmentation .\n Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ‚àà V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels.\n In this paper they take an approach:\n Each edge (vi , vj )‚àà E has a corresponding weight w((vi , vj )), which is a non-negative measure of the similarity between neighboring elements vi and vj . In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ‚àà S corresponds to a connected component in a graph.\n  Put simply, they use graph-based methods to find connected components in an image and the edges are made on some measure of similarity between pixels.\nAs you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. And that is the second step.\nFor that, the authors of Selective Search for Object Recognition apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm, they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill to provide us with much better region proposals.\n   1. R-CNN So now we have our region proposals. How do we exactly use them in R-CNN?\n  Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.\n Along with this, the authors have also used a class-specific bounding box regressor, that takes:\nInput : (Px, Py, Ph, Pw) ‚Äî the location of the proposed region.\nTarget: (Gx, Gy, Gh, Gw) ‚Äî Ground truth labels for the region.\nThe goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G)\nTraining R-CNN What is the input to an RCNN?\nSo we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)\nNext, we treat all region proposals with ‚â• 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box‚Äôs class and the rest as negative. We train class-specific SVM‚Äôs\nSo every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class-specific data.\nTest Time R-CNN At test time we predict detection boxes using class-specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Thus, non-maximum suppression is an integral part of the object detection pipeline.\nFirst, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed.\nThis process is recursively applied on all the remaining boxes until we are left with good bounding boxes only.\n Problems with RCNN:   Training is slow.\n  Inference (detection) is slow. 47s / image with VGG16 ‚Äî Since the Convnet needs to be run many times.\n  Need for speed. So Fast R-CNN.\n 2. Fast R-CNN  üí° So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM/classifier to classify those crops.\n Something like:\n  From Paper : Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all ‚Äúbackground‚Äù class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.\n üí°Idea So the basic idea is to have to run the convolution only once in the image rather than so many convolution networks in R-CNN. Then we can map the ROI proposals using some method and filter the last convolution layer and just run a final classifier on that.\nThis idea depends a little upon the architecture of the model that gets used too.\nSo the architecture that the authors have proposed is:\n We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net‚Äôs first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network‚Äôs last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.\n Don‚Äôt worry if you don‚Äôt understand the above. This obviously is a little confusing, so let us break this down. But for that, we need to see VGG16 architecture first.\n The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).\n Now the location of ROI coordinates is in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512.\nAs we are using VGG, we have transformed the image (224 x 224 x 3) into (14 x 14 x 512) ‚Äî i.e. the height and width are divided by 16. We can map ROIs coordinates onto the feature map just by dividing them by 16.\n In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the ‚Äúthings‚Äù it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.\n What is ROI pooling?\nRemember that the final classifier runs for each crop. And so each crop needs to be of the same size. And that is what ROI Pooling does.\n In the above image, our region proposal is (0,3,5,7) in x,y,w,h format.\nWe divide that area into 4 regions since we want to have an ROI pooling layer of 2x2. We divide the whole area into buckets by rounding 5/2 and 7/2 and then just do a max-pool.\n How do you do ROI-Pooling on Areas smaller than the target size? if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7. After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K + 1 categories(+1 for Background) and category-specific bounding-box regressors.\nTraining Fast-RCNN What is the input to a Fast- RCNN?\nPretty much similar to R-CNN: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)\nNext, we treat all region proposals with ‚â• 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box‚Äôs class and the rest as negative. This time we have a dense layer on top, and we use multi-task loss.\nSo every ROI becomes a training example. The main difference is that there is a concept of multi-task loss:\nA Fast R-CNN network has two sibling output layers.\nThe first outputs a discrete probability distribution (per RoI), p = (p0, . . . , pK), over K + 1 categories. As usual, p is computed by a softmax over the K+1 outputs of a fully connected layer.\nThe second sibling layer outputs bounding-box regression offsets, t= (tx, ty, tw, th), for each of the K object classes. Each training RoI is labelled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labelled RoI to jointly train for classification and bounding-box regression\n Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class.\nProblem: Region proposals are still taking up most of the time. Can we reduce the time taken for Region proposals?\n  3. Faster-RCNN The next question that got asked was: Can the network itself do region proposals?\n The intuition is that: With FastRCNN we‚Äôre already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes + bbox coordinates?\n  How does the Region Proposal Network work? One of the main ideas in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.\nSo, first of all, we define anchor centres on the image.\n The anchor centres are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224/14).\nThis is how anchors look like:\n   So we start with some predefined regions we think our objects could be with Anchors.\n  Our Region Proposal Network(RPN) classifies which regions have the object and the offset of the object bounding box. Training is done using the same logic. 1 if IOU for anchor with bounding box\u0026gt;0.5 0 otherwise.\n  Non-Maximum suppression to reduce region proposals\n  Fast RCNN detection network on top of proposals\n  Faster-RCNN Loss The whole network is then jointly trained with 4 losses:\n  RPN classify object / not object\n  RPN regress box coordinates offset\n  Final classification score (object classes)\n  Final box coordinates offset\n  Performance   Instance Segmentation Now comes the most interesting part ‚Äî Instance segmentation. Can we create masks for each individual object in the image? Specifically something like:\n  Mask-RCNN The same authors come to rescue again. The basic idea is to add another output layer that predicts the mask. And to use ROIAlign instead of ROIPooling.\n  Source: Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign\nMask R-CNN adopts the same two-stage procedure, with an identical first stage (RPN).\nIn the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.\nROIAlign vs ROIPooling In ROI pooling we lose the exact location-based information. See how we arbitrarily divided our region into 4 different sized boxes. For a classification task, it works well.\nBut for providing masks on a pixel level, we don‚Äôt want to lose this information. And hence we don‚Äôt quantize the pooling layer and use bilinear interpolation to find out values that properly aligns the extracted features with the input. See how 0.8 differs from 0.88\n  Source \nTraining During training, we define a multi-task loss on each sampled RoI as\nL = Lcls + Lbox + Lmask\nThe classification loss Lcls and bounding-box loss Lbox are identical as in Faster R-CNN. The mask branch has a K √ó m √ó m ‚Äî dimensional output for each RoI, which encodes K binary masks of resolution m √ó m, one for each of the K classes.\nTo this, we apply a per-pixel sigmoid and define Lmask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, Lmask is only defined on the kth mask (other mask outputs do not contribute to the loss).\nMask Prediction The mask layer is K √ó m √ó m dimensional where K is the number of classes. The m√óm floating-number mask output is resized to the RoI size and binarized at a threshold of 0.5 to get final masks.\n Conclusion  Congrats for reaching the end. This post was a long one.\nIn this post, I talked about some of the most important advancements in the field of Object detection and Instance segmentation and tried to explain them as easily as I can.\nThis is my own understanding of these papers with inputs from many blogs and slides on the internet and I sincerely thank the creators. Let me know if you find something wrong with my understanding.\nObject detection is a vast field and there are a lot of other methods that dominate this field. Some of them being U-net, SSD and YOLO.\nThere is no dearth of resources to learn them so I would encourage you to go and take a look at them. You have got a solid backing/understanding now.\nIn this post, I didn‚Äôt write about coding and implementation. So stay tuned for my next post in which we will train a Mask RCNN model for a custom dataset.\nIf you want to know more about various Object Detection techniques, motion estimation, object tracking in video etc., I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization .\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/12/05/od/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Object detection","Instance Segmentation"],"title":"Demystifying Object Detection and Instance Segmentation for Data Scientists"},{"categories":["Data Science"],"contents":"Data Science is the study of algorithms.\nI grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series .\nHow many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features?\nLast time I wrote a post titled ‚Äú The 5 Feature Selection Algorithms every Data Scientist should know ‚Äù in which I talked about using correlation or tree-based methods and adding some structure to the process of feature selection.\nRecently I got introduced to another novel way of feature selection called Permutation Importance and really liked it.\nSo, this post is explaining how permutation importance works and how we can code it using ELI5.\n What is Permutation Importance? Simply speaking, we can attribute importance to a feature based on how our evaluation metric(F1, Accuracy AUC, etc.) changes if we remove a particular feature from our dataset.\nIt could be pretty straightforward ‚Äî We remove a feature from our dataset and train the classifier and see how the evaluation metric changes. And we do it for all features.\nSo we fit our model at least n times, where n is the number of features in the model. It is so much work and computation. Can we do better?\n  Source : We permute a feature and predict using the updated dataset. Intuitively, if our accuracy or any evaluation metric doesn‚Äôt take a hit, we can say that the feature is not important. If our accuracy does take a hit, we consider this feature important.\nYes, we can. To calculate permutation importance, we shuffle/permute the values for a single feature and make predictions using the resulting dataset.\nThe predictions are then used to calculate our evaluation metric. Intuitively, if our accuracy or any evaluation metric doesn‚Äôt take a hit, we can say that the feature is not important. If our accuracy does take a hit, we consider this feature important.\n Data We will try to do this using a dataset to understand it better.\nI am going to be using a football player dataset and try to find out the most important features using it.\n Don‚Äôt worry if you don‚Äôt understand football terminologies. I will try to keep it at a minimum.\nYou can see the full code here in this Kaggle Kernel .\nSome simple Data Preprocessing We have done some basic preprocessing such as removing nulls and one hot encoding. We also convert the problem to a classification problem using:\ny = traindf[\u0026#39;Overall\u0026#39;]\u0026gt;=80 Here we use High Overall as a proxy for a great player. Our dataset(X) looks like below and has 223 columns.\n  Implementation 1. For sklearn Models ELI5 library makes it quite easy for us to use permutation importance for sklearn models. First, we train our model.\nfrom sklearn.ensemble import RandomForestClassifier my_model = RandomForestClassifier(n_estimators=100, random_state=0).fit(X, y) Then we use the function PermutationImportance from the eli5.sklearn module.\nfrom eli5.sklearn import PermutationImportance import eli5 perm = PermutationImportance(my_model,n_iter=2).fit(X, y) eli5.show_weights(perm, feature_names = X.columns.tolist()) The results look like:\n Here we note that Reactions, Interceptions and BallControl are the most important features to access a player‚Äôs quality.\n2. For BlackBox Models or Non-sklearn models  We can also use eli5 to calculate feature importance for non scikit-learn models also. Here we train a LightGBM model.\nimport numpy as np from lightgbm import LGBMClassifier lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40) lgbc.fit(X,y) We will need to create a wrapper for our score function to calculate our evaluation metric.\nfrom sklearn.metrics import accuracy_score #define a score function. In this case I use accuracy def score(X, y): y_pred = lgbc.predict(X) return accuracy_score(y, y_pred) We can now use get_score_importances function from eli5.permutation_importance to get the final feature importances.\nfrom eli5.permutation_importance import get_score_importances # This function takes only numpy arrays as inputs base_score, score_decreases = get_score_importances(score, np.array(X), y) feature_importances = np.mean(score_decreases, axis=0) We can see the top 5 features using:\nfeature_importance_dict = {} for i, feature_name in enumerate(X.columns): feature_importance_dict[feature_name]=feature_importances[i] print(dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:5])) {'Reactions': 0.019626631422435127, 'Interceptions': 0.004075114268406832, 'BallControl': 0.0025001376727793235, 'ShortPassing': 0.0012996310369513431, 'Strength': 0.0009251610771518149}   Conclusion  Feature engineering and feature selection are critical parts of any machine learning pipeline.\nWe strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.\nIn this post, I tried to explain Permutation importance as a feature selection method. It helps us find feature importance for any BlackBox model, unlike the techniques in my previous post on feature selection.\nIf you want to learn more about feature engineering/selection, I would like to call out the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of intuitive ways to improve your model using useful feature engineering/selection techniques. Definitely recommended.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/12/04/blackbox/","tags":["Machine Learning","Data Science","Statistics","Interpretability"],"title":"How to find Feature importances for BlackBox Models?"},{"categories":null,"contents":"Technological developments have paved the way for new niche industries, where professions like data science have appeared.\nData scientists have the knowledge and expertise to perform the work that data analysts do, and then some. They analyze and interpret complex data sets of varying structures, and are able to solve obscure problems with codes, models, and machine-learning algorithms.\nAs you can see in our post ‚ÄòHow did I Learn Data Science?‚Äô, there are many steps to be taken if you want to be a specialist in the field.\nFrom learning the basics of probability and statistics, to learning Data Science in Python to create your own work, Machine Learning, Spark, Linux Shell, Inferential Statistics, NLP, and algorithms‚Äì‚Äìaspiring data scientists must train themselves, whether through online programs and tutorials or through local classes with established mentors.\nThe language of data science is universal, and the field is gaining traction across the globe. Information Week notes there has been an 8% increase in data science job searches from 2017 to 2019, and actual job postings grew by 55%. A lot of professionals spend years learning their trade, and many of them even have master‚Äôs degrees, at the very least.\nHowever, Citizen Data Scientists have emerged as well. These people incorporate their own expertise and unique abilities to analytics tasks, as their primary job function is external to the field of analytics. They are said to serve complementary roles to traditional data scientists.\nWhen collaboration between the two occurs, skills diversify and consolidated knowledge is used for analyses that have been successful. Currently, there are over 2 million students studying the data science courses on learning platform Udemy, with the beginner courses being taken nearly 100,000 times. Nowadays, learning is made possible anywhere. Data science is clearly a booming industry, with many countries having taken a huge interest in the field as businesses continue to expand.\nNow is the time for data scientists to shine. And here are some of the top cities for them to thrive in:\n 1. Bengaluru, India India is projected to have the biggest world economy as an area often targeted for outsourcing‚Äì‚Äìincluding tech.\nBengaluru has been dubbed the Silicon Valley of India, as it has the best analytics market in the country.\nForeign tech firms have likewise been hosted in this city. It‚Äôs become a hub for technological advancements, AI labs, and both tech startups and giants who have contributed significantly to the growing sector. People have flocked to Bengaluru because of its cost of living as well, making it even more ideal for graduates just starting out.\n 2. Dublin, Ireland As Europe‚Äôs newest cloud gateway, it houses server farms for big names like Microsoft, Google, Amazon, and Facebook. This is what draws the cream of the crop in terms of data scientists to Dublin. The average income of the top data scientists in the country is roughly $100,000, which is three times the average salary in Ireland. Despite these facts, the National Analytics Maturity Study found that companies are struggling to find competent people to fill in data science and analysis jobs. This dilemma may continue to increase as more companies incorporate data-related jobs in the coming years. Now may be the best time to consider Dublin, if you need a bit of a change.\n 3. Lexington Park, Maryland, The United States Lexington Park hosts a high number of tech jobs, with almost a fourth of the city being employed in the STEM field. Therefore, it has the highest median STEM wage in the entire country, with most employees earning six figures.\nOver 2,500 data science jobs are found in the city, and the area has more data science jobs per thousand, in contrast to any other place in the US.\n 4. London, United Kingdom London is known as the world‚Äôs hub for AI and FinTech. The UK government is highly invested in this field and has participated in a ¬£1 billion deal together with 50 tech-involved businesses worldwide. While in part this comes from its dwindling economy and the uncertainty surrounding the Brexit deal, it continues to offer some of the highest paying and most reputable jobs in the field. They often host events related to the industry, such as the Deep Learning Summit, Strata Data Conference, and ODSC‚Äôs European Conference. It also houses the Alan Turing Institute-the national institute of data science and AI.\n 5. Singapore, Singapore Standing at number 6 in the list of the world‚Äôs Smart Cities, Singapore has seen all kinds of developments in self-driving vehicles, AI, and IT. Like the UK, it receives strong government backing in the form of investments for city infrastructure, which makes use of advanced technology. Many tech startups, giants, and VCs have viewed the city as the place to be in Southeast Asia.\nFor anyone who‚Äôs interested in beginning a career in data science and is in need of some helpful guides, check out the MLWHiz Archive\nPost written by: Oliver Williams\n","permalink":"https://mlwhiz.com/blog/2019/11/27/cities/","tags":null,"title":"Top 5 Cities for Data Scientists to Thrive In"},{"categories":["Data Science","Awesome Guides"],"contents":"Decision Trees are great and are useful for a variety of tasks. They form the backbone of most of the best performing models in the industry like XGboost and Lightgbm.\nBut how do they work exactly? In fact, this is one of the most asked questions in ML/DS interviews.\nWe generally know they work in a stepwise manner and have a tree structure where we split a node using some feature on some criterion.\nBut how do these features get selected and how a particular threshold or value gets chosen for a feature?\nIn this post, I will talk about three of the main splitting criteria used in Decision trees and why they work. This is something that has been written about repeatedly but never really well enough.\n1. Gini Impurity According to Wikipedia,\n Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n In simple terms, Gini impurity is the measure of impurity in a node. Its formula is:\n where J is the number of classes present in the node and p is the distribution of the class in the node.\nSo to understand the formula a little better, let us talk specifically about the binary case where we have nodes with only two classes.\nSo in the below five examples of candidate nodes labelled A-E and with the distribution of positive and negative class shown, which is the ideal condition to be in?\nI reckon you would say A or E and you are right. What is the worst situation to be in? C, I suppose as the data is precisely 50:50 in that node.\n Now, this all looks good, intuitively. Gini Impurity gives us a way to quantify it.\nLet us calculate the Gini impurity for all five nodes separately and check the values.\n ‚úÖ Gini Impurity works as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Gini Impurity.\nWe could also see the plot of Gini Impurity for the binary case to verify the above.\n ‚ùìSo how do we exactly use it in a Decision Tree?\nSuppose, we have the UCI Heart Disease data. The ‚Äútarget‚Äù field refers to the presence of heart disease in the patient. It is 0 (no presence) or 1.\n We now already have a measure in place(Gini Impurity) using which we can evaluate a split on a particular variable with a certain threshold(continuous) or value(categorical).\nCategorical Variable Splits For simplicity, let us start with a categorical variable ‚Äî sex.\nIf we split by Sex, our tree will look like below:\n Notice that we use Sex=0 and Sex!=0 so that this generalises well to categories with multiple levels. Our root node has 165 +ve examples and 138 -ve examples. And we get two child nodes when we split by sex.\nWe already know how to calculate the impurity for a node. So we calculate the impurity of the left child as well as the right child.\nI_Left = 1 - (72/96)**2 - (24/96)**2 I_Right = 1 - (93/207)**2 - (114/207)**2 print(\u0026#34;Left Node Impurity:\u0026#34;,I_Left) print(\u0026#34;Right Node Impurity:\u0026#34;,I_Right) Left Node Impurity: 0.375 Right Node Impurity: 0.4948540222642302  We get two numbers here. We need to get a single number which provides the impurity of a single split. So what do we do? Should, we take an average? We can take an average, but what will happen if one node gets only one example and another node has all other examples?\nTo mitigate the above, we take a weighted average of the two impurities weighted by the number of examples in the individual node. In code:\ngender_split_impurity = 96/(96+207)*I_Left + 207/(96+207)*I_Right print(gender_split_impurity) 0.45688047065576126  Continuous Variable Splits We can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and created a tree.\n I_Left = 1 - (58/126)**2 - (68/126)**2 I_Right = 1 - (107/177)**2 - (70/177)**2 print(\u0026#34;Left Node Impurity:\u0026#34;,I_Left) print(\u0026#34;Right Node Impurity:\u0026#34;,I_Right) Left Node Impurity: 0.49685059208868737 Right Node Impurity: 0.47815123368125373  Just by looking at both the impurities close to 0.5, we can infer that it is not a good split. Still, we calculate our weighted Gini impurity as before:\nchol_split_impurity = 126/(126+177)*I_Left + 177/(126+177)*I_Right print(chol_split_impurity) 0.48592720450414695  Since the chol_split_impurity\u0026gt;gender_split_impurity, we split based on Gender.\nIn reality, we evaluate a lot of different splits. With different threshold values for a continuous variable. And all the levels for categorical variables. And then choose the split which provides us with the lowest weighted impurity in the child nodes.\n 2. Entropy  Another very popular way to split nodes in the decision tree is Entropy. Entropy is the measure of Randomness in the system. The formula for Entropy is:\n where C is the number of classes present in the node and p is the distribution of the class in the node.\nSo again talking about the binary case we talked about before. What is the value of Entropy for all the 5 cases from A-E?\n Entropy values work as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Entropy.\nWe could also see the plot of Entropy for the binary case to verify the above.\n So how do we exactly use Entropy in a Decision Tree?\nWe are using the Heartrate example as before. We now already have a measure in place(Entropy) using which we can evaluate a split on an individual variable with a certain threshold(continuous) or value(categorical).\nCategorical Variable Splits For simplicity, let us start with a categorical variable ‚Äî sex.\nIf we split by Sex, our tree will look like below:\n If we split on Gender\nWe already know how to calculate the randomness for a node. So we calculate the randomness of the left child as well as the right child.\nE_Left = -(72/96)*np.log2(72/96) - (24/96)*np.log2(24/96) E_Right = -(93/207)*np.log2(93/207) - (114/207)*np.log2(114/207) print(\u0026#34;Left Node Randomness:\u0026#34;,E_Left) print(\u0026#34;Right Node Randomness:\u0026#34;,E_Right) Left Node Randomness: 0.8112781244591328 Right Node Randomness: 0.992563136012236  We get two numbers here. We need to get a single number which provides the Randomness of a single split. So what do we do? We again take a weighted average where we weight by the number of examples in the individual node. In code:\ngender_split_randomness = 96/(96+207)*E_Left + 207/(96+207)*E_Right print(gender_split_randomness) 0.9351263006686785  Continuous Variable Splits Again as before, we can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and create a tree.\n E_Left = -(58/126)*np.log2(58/126) - (68/126)*np.log2(68/126) E_Right = -(107/177)*np.log2(107/177) - (70/177)*np.log2(70/177) print(\u0026#34;Left Node Randomness:\u0026#34;,E_Left) print(\u0026#34;Right Node Randomness:\u0026#34;,E_Right) Left Node Randomness: 0.9954515828457715 Right Node Randomness: 0.9682452182690404  Just by looking at both the randomness close to 1, we can infer that it is not a good split. Still, we calculate our weighted Entropy as before:\nchol_split_randomness = 126/(126+177)*E_Left + 177/(126+177)*E_Right print(chol_split_randomness) 0.9795587560138196  Since the chol_split_randomness\u0026gt;gender_split_randomness, we split based on Gender. Precisely the same results we got from Gini.\n 3. Variance Gini Impurity and Entropy work pretty well for the classification scenario.\nBut what about regression?\nIn the case of regression, the most common split measure used is just the weighted variance of the nodes. It makes sense too: We want minimum variation in the nodes after the split.\n We want a regression task for this. So, we have the data for 50 startups, and we want to predict Profit.\n Categorical Variable Splits Let us try a split by a categorical variable ‚áíState=Florida.\nIf we split by State=FL, our tree will look like below:\n Overall Variance then is just the weighted sums of individual variances:\noverall_variance = 16/(16+34)*Var_Left + 34/(16+34)*Var_Right print(overall_variance) 1570582843  Continuous Variable Splits Again as before, we can split by a continuous variable too. Let us try to split using R\u0026amp;D spend feature in the dataset. We chose a threshold of 100000 and create a tree.\n Splitting on R\u0026amp;D\nJust by looking at this, we can see it is better than our previous split. So, we find the overall variance in this case:\noverall_variance = 14/(14+36)*419828105 + 36/(14+36)*774641406 print(overall_variance) 675293681.7199999  Since the overall_variance(R\u0026amp;D\u0026gt;=100000)\u0026lt; overall_variance(State==FL), we prefer a split based on R\u0026amp;D.\nContinue Learning  If you want to learn more about Data Science, I would like to call out this \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;excellent course\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; by Andrew Ng. This was the one that got me started. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/11/12/dtsplits/","tags":["Data Science","Statistics","Math"],"title":"The Simple Math behind 3 Decision Tree Splitting criterions"},{"categories":["Data Science","Awesome Guides"],"contents":"Recently, I got asked about how to explain p-values in simple terms to a layperson. I found that it is hard to do that.\nP-Values are always a headache to explain even to someone who knows about them let alone someone who doesn‚Äôt understand statistics.\nI went to Wikipedia to find something and here is the definition:\n In statistical hypothesis testing, the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two groups) would be equal to, or more extreme than, the actual observed results.\n And my first thought was that might be they have written it like this so that nobody could understand it. The problem here lies with a lot of terminology and language that statisticians enjoy to employ.\nThis post is about explaining p-values in an easy to understand way without all that pretentiousness of statisticians.\n A Real-Life problem In our lives, we certainly believe one thing over another.\nFrom the obvious ones like ‚Äî The earth is round. Or that the earth revolves around the Sun. The Sun rises in the east.\nTo the more non-obvious ones with varying level of uncertainties - Exercising reduces weight? Or that Trump is going to win/lose in his next election? Or that a particular drug works? Or that sleeping for 8 hours is good for your health?\nWhile the former category is facts, the latter category differs from person to person.\nSo, what if I come to you and say that exercising does not affect weight?\nAll the gym-goers may call me not so kind words. But is there a mathematical and logical structure in which someone can disprove me?\nThis brings us to the notion of Hypothesis testing.\n Hypothesis Testing  So the statement I made in the above example ‚Äî exercising does not affect weight. This statement is my Hypothesis. Let‚Äôs call it Null hypothesisfor now. For now, it is the status quo as in we consider it to be true.\nThe Alternative Hypothesis from people who swear by exercising is ‚Äî exercising does reduce weight.\nBut how do we test these hypotheses? We collect Data. We collect weight loss data for a sample of 10 people who regularly exercise for over 3 months.\nWeightLoss Sample Mean = 2 kg Sample Standard Deviation = 1 kg  Does this prove that exercise does reduce weight? From a cursory look, it sort of looks like that exercising does have its benefits as people who exercise have lost on an average 2 kgs.\nBut you will find that such clear cut findings are not always the case when you do hypothesis testing. What if the weight loss mean for people who do exercise was just 0.2 kg. Would you still be so sure that exercise does reduce weight?\nSo how can we quantify this and put some maths behind it all?\nLet us set up our experiment to do this.\n Experiment Let‚Äôs go back to our Hypotheses again:\nH¬∫: Exercising does not affect weight. Or equivalently ùúá = 0\nH·¥¨: Exercise does reduce weight. Or equivalently ùúá\u0026gt;0\nWe see our data sample of 10 people, and we try to find out the value of\nObserved Mean(Weightloss of People who exercise) = 2 kg\nObserved Sample Standard Deviation = 1 kg\nNow a good question to ask ourselves is ‚Äî Assuming that the null hypothesis is true, what is the probability of observing a sample mean of 2 kg or more extreme than 2 kg?\nAssuming we can calculate this ‚Äî If this probability value is meagre (lesser than a threshold value), we reject our null hypothesis. And otherwise, we fail to reject our null hypothesis. Why fail to reject and not accept? I will answer this later.\nThis probability value is actually the p-value. Simply, it is just the probability of observing what we observed or extreme results if we assume our null hypothesis to be true.\nThe statisticians call the threshold as the significance level(ùú∂), and in most of the cases, ùú∂ is taken to be 0.05.\nSo how do we answer: Assuming that the null hypothesis is true, what is the probability of getting a value of 2 kg or more than 2 kg?\nAnd here comes our favourite distribution, Normal distribution in the picture.\n The Normal Distribution We create a Sampling Distribution of the mean of the WeightLoss samples assuming our Null hypothesis is True.\nCentral Limit Theorem: The central limit theorem simply states that if you have a population with mean Œº and standard deviation œÉ, and take random samples from the population, then the distribution of the sample means will be approximately normally distributed with mean as the population mean and standard deviation œÉ/‚àön. Where œÉ is the standard deviation of the sample and n is the number of observations in the sample.\nNow we already know the mean of our population as given by our null hypothesis. So, we use that and have a normal distribution whose mean is 0. And whose standard deviation is given by 1/‚àö10\n This is, in fact, the distribution of the mean of the samples from the population. We observed a particular value of the mean that is Xobserved = 2 kg.\nNow we can use some statistical software to find the area under this particular curve:\nfrom scipy.stats import norm import numpy as np p = 1-norm.cdf(2, loc=0, scale = 1/np.sqrt(10)) print(p) 1.269814253745949e-10  As such, this is a very small probability p-value ( less than the significance level of 0.05) for the mean of a sample to take a value of 2 or more.\nAnd so we can reject our Null hypothesis. And we can call our results statistically significant as in they don‚Äôt just occur due to mere chance.\n The Z statistic You might have heard about the Z statistic too when you have read about Hypothesis testing. Again as I said, terminology.\nThat is the extension of basically the same above idea where we use a standard normal with mean 0 and variance 1 as our sampling distribution after transforming our observed value x using:\n This makes it easier to use statistical tables. In our running example, our z statistic is:\nz = (2-0)/(1/np.sqrt(10)) print(z) 6.324555320336758  Just looking at the Z statistic of \u0026gt;6 should give you an idea that the observed value is at least six standard deviations away and so the p-value should be very less. We can still find the p-value using:\nfrom scipy.stats import norm import numpy as np p = 1-norm.cdf(z, loc=0, scale=1) print(p) 1.269814253745949e-10  As you can see, we get the same answer using the Z statistic.\n An Important Distinction  So we said before that we reject our null hypothesis as in we got sufficient evidence to prove that our null hypothesis is false.\nBut what if the p-value was higher than the significance level. Then we say that we fail to reject the null hypothesis. Why don‚Äôt we say accept the null hypothesis?\nThe best intuitive example of this is using trial courts. In a trial court, the null hypothesis is that the accused is not guilty. Then we see some evidence to disprove the null hypothesis.\nIf we are not able to disprove the null hypotheses the judge doesn‚Äôt say that the accused hasn‚Äôt committed the crime. The judge only says that based on the given evidence, we are not able to convict the accused.\nAnother example to drive this point forward: Assuming that we are exploring life on an alien planet. And our null hypothesis(H¬∫) is that there is no life on the planet. We roam around a few miles for some time and look for people/aliens on that planet. If we see any alien, we can reject the null hypothesis in favour of the alternative.\nBut if we don‚Äôt see any alien, can we definitively say that there is no alien life on the planet or accept our null hypotheses? Maybe we needed to explore more, or perhaps we needed more time and we may have found an alien. So, in this case, we cannot accept the null hypothesis; we can only fail to reject it. Or In Cassie Kozyrkov‚Äôs words from whom the example comes, we can say that ‚Äúwe learned nothing interesting‚Äù.\n In STAT101 class, they teach you to write a convoluted paragraph when that happens. (‚ÄúWe fail to reject the null hypothesis and conclude that there is insufficient statistical evidence to support the existence of alien life on this planet.‚Äù) I‚Äôm convinced that the only purpose of this expression is to strain students‚Äô wrists. I‚Äôve always allowed my undergraduate students to write it like it is: we learned nothing interesting.\n  In essence, hypothesis testing is just about checking if our observed values make the null hypothesis look ridiculous. If yes, we reject the null hypothesis and call our results statistically significant. And otherwise we have learned nothing interesting, and we continue with our status quo.\n Continue Learning If you want to learn more about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data, Mine √áetinkaya-Rundel teaches Inferential Statistics course on coursera and it cannot get simpler than this one. She is a great instructor and explains the fundamentals of Statistical inference nicely.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/11/11/pval/","tags":["Data Science","Statistics"],"title":"P-value Explained Simply for Data Scientists"},{"categories":["Data Science","Natural Language Processing"],"contents":"Explain Like I am 5.\nIt is the basic tenets of learning for me where I try to distill any concept in a more palatable form. As Feynman said:\n I couldn‚Äôt do it. I couldn‚Äôt reduce it to the freshman level. That means we don‚Äôt really understand it.\n So, when I saw the ELI5 library that aims to interpret machine learning models, I just had to try it out.\nOne of the basic problems we face while explaining our complex machine learning classifiers to the business is interpretability.\nSometimes the stakeholders want to understand ‚Äî what is causing a particular result? It may be because the task at hand is very critical and we cannot afford to take a wrong decision. Think of a classifier that takes automated monetary actions based on user reviews.\nOr it may be to understand a little bit more about the business/the problem space.\nOr it may be to increase the social acceptance of your model.\nThis post is about interpreting complex text classification models.\n The Dataset: To explain how ELI5 works, I will be working with the stack overflow dataset on Kaggle. This dataset contains around 40000 posts and the corresponding tag for the post.\nThis is how the dataset looks:\n And given below is the distribution for different categories.\n This is a balanced dataset and thus suited well for our purpose of understanding.\nSo let us start. You can follow along with the code in this Kaggle Kernel  Staring Simple:  Let us first try to use a simple scikit-learn pipeline to build our text classifier which we will try to interpret later. In this pipeline, I will be using a very simple count vectorizer along with Logistic regression.\nfrom sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegressionCV from sklearn.pipeline import make_pipeline # Creating train-test Split X = sodata[[\u0026#39;post\u0026#39;]] y = sodata[[\u0026#39;tags\u0026#39;]] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # fitting the classifier vec = CountVectorizer() clf = LogisticRegressionCV() pipe = make_pipeline(vec, clf) pipe.fit(X_train.post, y_train.tags) Let‚Äôs see the results we get:\nfrom sklearn import metrics def print_report(pipe): y_actuals = y_test[\u0026#39;tags\u0026#39;] y_preds = pipe.predict(X_test[\u0026#39;post\u0026#39;]) report = metrics.classification_report(y_actuals, y_preds) print(report) print(\u0026#34;accuracy: {:0.3f}\u0026#34;.format(metrics.accuracy_score(y_actuals, y_preds))) print_report(pipe)  The above is a pretty simple Logistic regression model and it performs pretty well. We can check out its weights using the below function:\nfor i, tag in enumerate(clf.classes_): coefficients = clf.coef_[i] weights = list(zip(vec.get_feature_names(),coefficients)) print(\u0026#39;Tag:\u0026#39;,tag) print(\u0026#39;Most Positive Coefficients:\u0026#39;) print(sorted(weights,key=lambda x: -x[1])[:10]) print(\u0026#39;Most Negative Coefficients:\u0026#39;) print(sorted(weights,key=lambda x: x[1])[:10]) print(\u0026#34;--------------------------------------\u0026#34;) ------------------------------------------------------------ OUTPUT: ------------------------------------------------------------ Tag: python Most Positive Coefficients: [('python', 6.314761719932758), ('def', 2.288467823831321), ('import', 1.4032539284357077), ('dict', 1.1915110448370732), ('ordered', 1.1558015932799253), ('print', 1.1219958415166653), ('tuples', 1.053837204818975), ('elif', 0.9642251085198578), ('typeerror', 0.9595246314353266), ('tuple', 0.881802590839166)] Most Negative Coefficients: [('java', -1.8496383139251245), ('php', -1.4335540858871623), ('javascript', -1.3374796382615586), ('net', -1.2542682749949605), ('printf', -1.2014123042575882), ('objective', -1.1635960146614717), ('void', -1.1433460304246827), ('var', -1.059642972412936), ('end', -1.0498078813349798), ('public', -1.0134828865993966)] -------------------------------------- Tag: ruby-on-rails Most Positive Coefficients: [('rails', 6.364037640161158), ('ror', 1.804826792986176), ('activerecord', 1.6892552000017307), ('ruby', 1.41428459023012), ('erb', 1.3927336940889532), ('end', 1.3650227017877463), ('rb', 1.2280121863441906), ('gem', 1.1988196865523322), ('render', 1.1035255831838242), ('model', 1.0813278895692746)] Most Negative Coefficients: [('net', -1.5818801311532575), ('php', -1.3483618692617583), ('python', -1.201167422237274), ('mysql', -1.187479885113293), ('objective', -1.1727511956332588), ('sql', -1.1418573958542007), ('messageform', -1.0551060751109618), ('asp', -1.0342831159678236), ('ios', -1.0319120624686084), ('iphone', -0.9400116321217807)] -------------------------------------- .......  And that is all pretty good. We can see the coefficients make sense and we can try to improve our model using this information.\nBut above was a lot of code. ELI5 makes this exercise pretty simple for us. We just have to use the below command:\nimport eli5 eli5.show_weights(clf, vec=vec, top=20)  Now as you can see the weights value for Python is the same as from the values we got from the function we wrote manually. And it is much prettier and wholesome to explore.\nBut that is just the tip of the iceberg. ELI5 can also help us to debug our models as we can see below.\nUnderstanding our Simple Text Classification Model Let us now try to find out why a particular example is misclassified. I am using an example which was originally from the class Python but got misclassified as Java:\ny_preds = pipe.predict(sodata[\u0026#39;post\u0026#39;]) sodata[\u0026#39;predicted_label\u0026#39;] = y_preds misclassified_examples = sodata[(sodata[\u0026#39;tags\u0026#39;]!=sodata[\u0026#39;predicted_label\u0026#39;])\u0026amp;(sodata[\u0026#39;tags\u0026#39;]==\u0026#39;python\u0026#39;)\u0026amp;(sodata[\u0026#39;predicted_label\u0026#39;]==\u0026#39;java\u0026#39;)] eli5.show_prediction(clf, misclassified_examples[\u0026#39;post\u0026#39;].values[1], vec=vec)   In the above example, the classifier predicts Java with a low probability. And we can examine a lot of things going on in the above example to improve our model. For example:\n  We get to see that the classifier is taking a lot of digits into consideration(not good)which brings us to the conclusion of cleaning up the digits. Or replacing DateTime objects with a DateTime token.\n  Also see that while dictionary has a negative weight for Java, the word dictionaries has a positive weight. So maybe stemming could also help.\n  We also see that there are words like \u0026lt;pre\u0026gt;\u0026lt;code\u0026gt; that are influencing our classifier. These words should be removed while cleaning.\n  Why is the word date influencing the results? Something to think about.\n  We can take a look at more examples to get more such ideas. You get the gist.\nGoing Deep And Complex This is all good and fine but*** what if models that we use don‚Äôt provide weights for the individual features like LSTM?*** It is with these models that explainability can play a very important role.\n To understand how to do this, we first create a TextCNN model on our data. Not showing the model creation process in the interest of preserving space but think of it as a series of preprocessing steps and then creating the deep learning model. If interested, you can check out the modelling steps in this Kaggle kernel .\nThings get interesting from our point of view when we have a trained black-box model object.\nELI5 provides us with the eli5.lime.TextExplainer to debug our prediction - to check what was important in the document to make a prediction decision.\nTo use \u0026lt;strong\u0026gt;TextExplainer\u0026lt;/strong\u0026gt; instance, we pass a document to explain and a black-box classifier (a predict function which returns probabilities) to the \u0026lt;strong\u0026gt;fit()\u0026lt;/strong\u0026gt; method. From the documentation this is how our predict function should look like:\n predict (callable) ‚Äî Black-box classification pipeline. predict should be a function which takes a list of strings (documents) and return a matrix of shape (n_samples, n_classes) with probability values - a row per document and a column per output label.\n So to use ELI5 we will need to define our own function which takes as input a list of strings (documents) and return a matrix of shape (n_samples, n_classes). You can see how we first preprocess and then predict.\ndef predict_complex(docs): # preprocess the docs as required by our model val_X = tokenizer.texts_to_sequences(docs) val_X = pad_sequences(val_X, maxlen=maxlen) y_preds = model.predict([val_X], batch_size=1024, verbose=0) return y_preds Given below is how we can use TextExplainer. Using the same misclassified example as before in our simple classifier.\nimport eli5 from eli5.lime import TextExplainer te = TextExplainer(random_state=2019) te.fit(sodata[\u0026#39;post\u0026#39;].values[0], predict_complex) te.show_prediction(target_names=list(encoder.classes_))  This time it doesn‚Äôt get misclassified. You can see that the presence of keywords dict and list is what is influencing the decision of our classifier. One can try to see more examples to find more insights.\nSo how does this work exactly?\n \u0026lt;strong\u0026gt;TextExplainer\u0026lt;/strong\u0026gt; generates a lot of texts similar to the document by removing some of the words, and then trains a white-box classifier which predicts the output of the black-box classifier and not the true labels. The explanation we see is for this white-box classifier.\nThis is, in essence, a little bit similar to the Teacher-Student model distillation, where we use a simpler model to predict outputs from a much more complex teacher model.\nPut simply, it tries to create a simpler model that emulates a complex model and then shows us the simpler model weights.\nConclusion  Understanding is crucial. Being able to interpret our models can help us to understand our models better and in turn, explain them better.\n ELI5 provides us with a good way to do this. It works for a variety of models and the documentation for this library is one of the best I have ever seen.\nAlso, I love the decorated output the ELI5 library provides with the simple and fast way it provides to interpret my models. And debug them too.\nTo use ELI5 with your models you can follow along with the code in this Kaggle Kernel Continue Learning If you want to learn more about NLP and how to create Text Classification models, I would like to call out the \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Natural Language Processing\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; course in the \u0026lt;strong\u0026gt;Advanced machine learning specialization\u0026lt;/strong\u0026gt; . Do check it out. It talks about a lot of beginners to advanced level topics in NLP. You might also like to take a look at some of my posts on NLP in the NLP Learning series.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/11/08/interpret_models/","tags":["Machine Learning","Data Science","Python","Statistics","Natural Language Processing","Interpretability"],"title":"Adding Interpretability to Multiclass Text Classification models"},{"categories":["Awesome Guides","Data Science"],"contents":"What do we want to optimize for? Most of the businesses fail to answer this simple question.\nEvery business problem is a little different, and it should be optimized differently.\nWe all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?\nWhat if we are predicting the number of asteroids that will hit the earth.\nJust say zero all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable. What should we do in such cases?\n Designing a Data Science project is much more important than the modeling itself.\n This post is about various evaluation metrics and how and when to use them.\n 1. Accuracy, Precision, and Recall:  A. Accuracy Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.\nAccuracy = (TP+TN)/(TP+FP+FN+TN)\nAccuracy is the proportion of true results among the total number of cases examined.\nWhen to use? Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.\nCaveats Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.\nB. Precision Let‚Äôs start with precision, which answers the following question: what proportion of predicted Positives is truly Positive?\nPrecision = (TP)/(TP+FP)\nIn the asteroid prediction problem, we never predicted a true positive.\nAnd thus precision=0\nWhen to use? Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.\nCaveats Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money.\nC. Recall Another very useful measure is recall, which answers a different question: what proportion of actual Positives is correctly classified?\nRecall = (TP)/(TP+FN)\nIn the asteroid prediction problem, we never predicted a true positive.\nAnd thus recall is also equal to 0.\nWhen to use? Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.\nCaveats Recall is 1 if we predict 1 for all examples.\nAnd thus comes the idea of utilizing tradeoff of precision vs. recall ‚Äî F1 Score.\n 2. F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.\nThe F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.\n Let us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.\nSo if we say ‚ÄúNo‚Äù for the whole training set. Our precision here is 0. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.\nAnd hence the F1 score is also 0. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.\nWhen to use? We want to have a model with both good precision and recall.\n Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.\n If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.  How to Use? You can calculate the F1 score for binary prediction problems using:\nfrom sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred) This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.\n# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)\u0026gt;thresh) if score \u0026gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh Caveats The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.\nTo solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.\n Here we give Œ≤ times as much importance to recall as precision.\nfrom sklearn.metrics import fbeta_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] fbeta_score(y_true, y_pred,beta=0.5) F1 Score can also be used for Multiclass problems. See this awesome blog post by Boaz Shmueli for details.\n 3. Log Loss/Binary Crossentropy Log loss is a pretty good evaluation metric for binary classifiers and it is sometimes the optimization objective as well in case of Logistic regression and Neural Networks.\nBinary Log loss for an example is given by the below formula where p is the probability of predicting 1.\n  As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1.\nWhen to Use? When the output of a classifier is prediction probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view of the performance of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.\nHow to Use? from sklearn.metrics import log_loss # where y_pred are probabilities and y_true are binary class labels log_loss(y_true, y_pred, eps=1e-15) Caveats It is susceptible in case of imbalanced datasets. You might have to introduce class weights to penalize minority errors more or you may use this after balancing your dataset.\n 4. Categorical Crossentropy The log loss also generalizes to the multiclass problem. The classifier in a multiclass setting must assign a probability to each class for all examples. If there are N samples belonging to M classes, then the Categorical Crossentropy is the summation of -ylogp values:\n $y_{ij}$ is 1 if the sample i belongs to class j else 0\n$p_{ij}$ is the probability our classifier predicts of sample i belonging to class j.\nWhen to Use? When the output of a classifier is multiclass prediction probabilities. We generally use Categorical Crossentropy in case of Neural Nets. In general, minimizing Categorical cross-entropy gives greater accuracy for the classifier.\nHow to Use? from sklearn.metrics import log_loss # Where y_pred is a matrix of probabilities with shape ***= (n_samples, n_classes)*** and y_true is an array of class labels log_loss(y_true, y_pred, eps=1e-15) Caveats: It is susceptible in case of imbalanced datasets.\n 5. AUC AUC is the area under the ROC curve.\nAUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes\nWhat is the ROC curve?\n We have got the probabilities from our classifier. We can use various threshold values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we will have a ROC curve.\nWhere True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.\nSensitivty = TPR(True Positive Rate)= Recall = TP/(TP+FP)\nand False positive rate or FPR is just the proportion of false we are capturing using our algorithm.\n1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)\n Here we can use the ROC curves to decide on a Threshold value. The choice of threshold value will also depend on how the classifier is intended to be used.\nIf it is a cancer classification application you don‚Äôt want your threshold to be as big as 0.5. Even if a patient has a 0.3 probability of having cancer you would classify him to be 1.\nOtherwise, in an application for reducing the limits on the credit card, you don‚Äôt want your threshold to be as less as 0.5. You are here a little worried about the negative effect of decreasing limits on customer satisfaction.\nWhen to Use? AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So, for example, if you as a marketer want to find a list of users who will respond to a marketing campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which you will create a list of users to send the marketing campaign.\nAnother benefit of using AUC is that it is classification-threshold-invariant like log loss. It measures the quality of the model‚Äôs predictions irrespective of what classification threshold is chosen, unlike F1 score or accuracy which depend on the choice of threshold.\nHow to Use? import numpy as np from sklearn.metrics import roc_auc_score y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8]) print(roc_auc_score(y_true, y_scores)) Caveats Sometimes we will need well-calibrated probability outputs from our models and AUC doesn‚Äôt help with that.\n Conclusion An important step while creating our machine learning pipeline is evaluating our different models against each other. A bad choice of an evaluation metric could wreak havoc to your whole system.\nSo, always be watchful of what you are predicting and how the choice of evaluation metric might affect/alter your final predictions.\nAlso, the choice of an evaluation metric should be well aligned with the business objective and hence it is a bit subjective. And you can come up with your own evaluation metric as well.\nContinue Learning If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization . Do check it out. It talks about the pitfalls and a lot of basic ideas to improve your models.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/11/07/eval_metrics/","tags":["Awesome Guides","Best Content","Machine Learning","Data Science","Statistics","math"],"title":"The 5 Classification Evaluation metrics every Data Scientist must know"},{"categories":["Data Science"],"contents":"We, as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.\nWe are used to seeing our users in rows with their attributes as columns. But does the real world behave like that?\nIn a connected world, users cannot be considered as independent entities. They have got certain relationships with each other, and we would sometimes like to include such relationships while building our machine learning models.\nNow while in a relational database, we cannot use such relations between different rows(users), in a graph database, it is relatively trivial to do that.\nNow, as we know, Python has a great package called Networkx to do this. But the problem with that is that it is not scalable.\nA GPU can help solve our scalability problems with its many cores and parallelization. And that is where RAPIDS.ai CuGraph comes in.\n The RAPIDS cuGraph library is a collection of graph analytics that process data found in GPU Dataframes ‚Äî see cuDF . cuGraph aims to provide a NetworkX-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.\n In this post, I am going to be talking about some of the most essential graph algorithms you should know and how to implement them using Python with cuGraph.\n Installation To install cuGraph you can just use the simple command that you can choose from rapids.ai based on your system and configuration.\n The command I used is below and I used a nightly build(recommended):\nconda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge -c anaconda cudf=0.10 cuml=0.10 cugraph=0.10   1. Connected Components  We all know how clustering works?\nYou can think of Connected Components in very layman‚Äôs terms as a sort of a hard clustering algorithm which finds clusters/islands in related/connected data.\nAs a concrete example: Say you have data about roads joining any two cities in the world. And you need to find out all the continents in the world and which city they contain.\nHow will you achieve that? Come on, give some thought.\nThe connected components algorithm that we use to do this is based on a special case of BFS/DFS. I won‚Äôt talk much about how it works here, but we will see how to get the code up and running using Networkx as well as cuGraph.\nApplications From a Retail Perspective: Let us say, we have a lot of customers using a lot of accounts. One way in which we can use the Connected components algorithm is to find out distinct families in our dataset.\nWe can assume edges(roads) between CustomerIDs based on same credit card usage, or same address or same mobile number, etc. Once we have those connections, we can then run the connected component algorithm on the same to create individual clusters to which we can then assign a family ID.\nWe can then use these family IDs to provide personalized recommendations based on family needs. We can also use this family ID to fuel our classification algorithms by creating grouped features based on family.\nFrom a Finance Perspective: Another use case would be to capture fraud using these family IDs. If an account has done fraud in the past, it is highly probable that the connected accounts are also susceptible to fraud.\nThe possibilities are only limited by your imagination.\nCode We will be using the Networkx module in Python for creating and analyzing our graphs.\nLet us start with an example graph which we are using for our purpose. Contains cities and distance information between them.\n We first start by creating a list of edges along with the distances which we will add as the weight of the edge:\nedgelist = [[\u0026#39;Mannheim\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 85], [\u0026#39;Mannheim\u0026#39;, \u0026#39;Karlsruhe\u0026#39;, 80], [\u0026#39;Erfurt\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 186], [\u0026#39;Munchen\u0026#39;, \u0026#39;Numberg\u0026#39;, 167], [\u0026#39;Munchen\u0026#39;, \u0026#39;Augsburg\u0026#39;, 84], [\u0026#39;Munchen\u0026#39;, \u0026#39;Kassel\u0026#39;, 502], [\u0026#39;Numberg\u0026#39;, \u0026#39;Stuttgart\u0026#39;, 183], [\u0026#39;Numberg\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 103], [\u0026#39;Numberg\u0026#39;, \u0026#39;Munchen\u0026#39;, 167], [\u0026#39;Stuttgart\u0026#39;, \u0026#39;Numberg\u0026#39;, 183], [\u0026#39;Augsburg\u0026#39;, \u0026#39;Munchen\u0026#39;, 84], [\u0026#39;Augsburg\u0026#39;, \u0026#39;Karlsruhe\u0026#39;, 250], [\u0026#39;Kassel\u0026#39;, \u0026#39;Munchen\u0026#39;, 502], [\u0026#39;Kassel\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 173], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Mannheim\u0026#39;, 85], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 217], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Kassel\u0026#39;, 173], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Numberg\u0026#39;, 103], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Erfurt\u0026#39;, 186], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 217], [\u0026#39;Karlsruhe\u0026#39;, \u0026#39;Mannheim\u0026#39;, 80], [\u0026#39;Karlsruhe\u0026#39;, \u0026#39;Augsburg\u0026#39;, 250],[\u0026#34;Mumbai\u0026#34;, \u0026#34;Delhi\u0026#34;,400],[\u0026#34;Delhi\u0026#34;, \u0026#34;Kolkata\u0026#34;,500],[\u0026#34;Kolkata\u0026#34;, \u0026#34;Bangalore\u0026#34;,600],[\u0026#34;TX\u0026#34;, \u0026#34;NY\u0026#34;,1200],[\u0026#34;ALB\u0026#34;, \u0026#34;NY\u0026#34;,800]] Now we want to find out distinct continents and their cities from this graph.\nFirst, we will need to create a cudf dataframe with edges in it. Right now, I am creating a pandas dataframe and converting it to cudf dataframe, but in a real-life scenario, we will read from a csv file of edges.\nimport cugraph import cudf import pandas as pd # create a pandas dataframe of edges pandas_df = pd.DataFrame(edgelist) pandas_df.columns = [\u0026#39;src\u0026#39;,\u0026#39;dst\u0026#39;,\u0026#39;distance\u0026#39;] # create a pandas dataframe of reversed edges as we have a undirected graph rev_pandas_df = pandas_df.copy() rev_pandas_df.columns = [\u0026#39;dst\u0026#39;,\u0026#39;src\u0026#39;,\u0026#39;distance\u0026#39;] rev_pandas_df = rev_pandas_df[[\u0026#39;src\u0026#39;,\u0026#39;dst\u0026#39;,\u0026#39;distance\u0026#39;]] # concat all edges pandas_df = pd.concat([pandas_df,rev_pandas_df]) Now our pandas df contains edges in both directions. And our node names in src and dst columns are in str format. Apparently, cuGraph doesn\u0026rsquo;t like that and only works with integer node IDs.\n# CuGraph works with only integer node IDs unique_destinations = set() for [src,dst,dis] in edgelist: unique_destinations.add(src) unique_destinations.add(dst) # create a map of city and a unique id city_id_dict = {} for i, city in enumerate(unique_destinations): city_id_dict[city]=i # create 2 columns that contain the integer IDs for src and dst pandas_df[\u0026#39;src_int\u0026#39;] = pandas_df[\u0026#39;src\u0026#39;].apply(lambda x : city_id_dict[x]) pandas_df[\u0026#39;dst_int\u0026#39;] = pandas_df[\u0026#39;dst\u0026#39;].apply(lambda x : city_id_dict[x]) Now comes the main part that we should focus on:\ncuda_g = cudf.DataFrame.from_pandas(pandas_df) # cugraph needs node IDs to be int32 and weights to be float cuda_g[\u0026#39;src_int\u0026#39;] = cuda_g[\u0026#39;src_int\u0026#39;].astype(np.int32) cuda_g[\u0026#39;dst_int\u0026#39;] = cuda_g[\u0026#39;dst_int\u0026#39;].astype(np.int32) cuda_g[\u0026#39;distance\u0026#39;] = cuda_g[\u0026#39;distance\u0026#39;].astype(np.float) G = cugraph.Graph() G.add_edge_list(cuda_g[\u0026#34;src_int\u0026#34;],cuda_g[\u0026#34;dst_int\u0026#34;] , cuda_g[\u0026#39;distance\u0026#39;]) cugraph.weakly_connected_components(G) The output of the last call is a cudf dataframe.\n As we can see, the labels correspond to Connected Components ID.\n 2. Shortest Path  Continuing with the above example only, we are given a graph with the cities of Germany and the respective distance between them.\nYou want to find out how to go from Frankfurt (The starting node) to Munchen by covering the shortest distance.\nThe algorithm that we use for this problem is called Dijkstra. In Dijkstra‚Äôs own words:\n What is the shortest way to travel from Rotterdam to Groningen , in general: from given city to given city. It is the algorithm for the shortest path , which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fianc√©e, and tired, we sat down on the caf√© terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in ‚Äô59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually that algorithm became, to my great amazement, one of the cornerstones of my fame. ‚Äî Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001 [3]  Applications   Variations of the Dijkstra algorithm is used extensively in Google Maps to find the shortest routes.\n  You are in a Walmart Store. You have different Aisles and distance between all the aisles. You want to provide the shortest pathway to the customer from Aisle A to Aisle D.\n    You have seen how LinkedIn shows up 1st-degree connections, 2nd-degree connections. What goes on behind the scenes?  Code We already have our Graph as before. We can find the shortest distance from a source node to all nodes in the graph.\n# get distances from source node 0 distances = cugraph.sssp(G, 0) # filter infinite distances distances = cugraph.traversal.filter_unreachable(distances) distances  Now if we have to find the path between node 0 and 14 we can use the distances cudf.\n# Getting the path is as simple as: path = [] dest = 14 while dest != 0: dest = distances[distances[\u0026#39;vertex\u0026#39;] == dest][\u0026#39;predecessor\u0026#39;].values[0] path.append(dest) # reverse the list and print print(path[::-1]) [0, 11, 9]   3. Pagerank  This is the page sorting algorithm that powered google for a long time. It assigns scores to pages based on the number and quality of incoming and outgoing links.\nApplications Pagerank can be used anywhere where we want to estimate node importance in any network.\n  It has been used for finding the most influential papers using citations.\n  Has been used by Google to rank pages\n  It can be used to rank tweets- User and Tweets as nodes. Create Link between user if user A follows user B and Link between user and Tweets if user tweets/retweets a tweet.\n  Recommendation engines\n  Code For this exercise, we are going to be using Facebook social network data.\n# Loading the file as cudf fb_cudf = cudf.read_csv(\u0026#34;facebook_combined.txt\u0026#34;, sep=\u0026#39; \u0026#39;, names=[\u0026#39;src\u0026#39;, \u0026#39;dst\u0026#39;],dtype =[\u0026#39;int32\u0026#39;,\u0026#39;int32\u0026#39;]) # adding reverse edges also rev_fb_cudf = fb_cudf[[\u0026#39;dst\u0026#39;,\u0026#39;src\u0026#39;]] rev_fb_cudf.columns = [\u0026#39;src\u0026#39;,\u0026#39;dst\u0026#39;] fb_cudf = cudf.concat([fb_cudf,rev_fb_cudf]) Creating the graph\n# creating the graph fb_G = cugraph.Graph() fb_G.add_edge_list(fb_cudf[\u0026#34;src\u0026#34;],fb_cudf[\u0026#34;dst\u0026#34;])  Now we want to find the users having high influence capability.\nIntuitively, the Pagerank algorithm will give a higher score to a user who has a lot of friends who in turn have a lot of FB Friends.\n# Call cugraph.pagerank to get the pagerank scores fb_pagerank = cugraph.pagerank(fb_G) fb_pagerank.sort_values(by=\u0026#39;pagerank\u0026#39;,ascending=False).head()   4. Link Prediction  Continuing along with our Facebook example. You might have seen recommended friends in your Facebook account. How can we create our small recommender?\nCan we predict which edges will be connected in the future based on current edges?\nA straightforward and fast approach to do this is by using the Jaccard Coefficient.\nApplications There could be many applications of link predictions. We could predict\n  Authors who are going to connect for co-authorships in a citation network\n  Who will become friends in a social network?\n  Idea We calculate the Jaccard coefficient between two nodes i and j as :\n Where the numerator is the number of common neighbors of i and j, and the denominator is the total number of distinct neighbors of i and j.\n So in the figure, the half red and green nodes are the common neighbors of both A and B. And they have a total of 5 distinct neighbors. So the JaccardCoeff(A, B) is 2/5\nCode We first create a cudf_nodes cudf with all possible node combinations.\nmax_vertex_id = fb_pagerank[\u0026#39;vertex\u0026#39;].max() data = [] for x in range(0,max_vertex_id+1): for y in range(0,max_vertex_id+1): data.append([x,y]) cudf_nodes =cudf.from_pandas(pd.DataFrame(data)) cudf_nodes.columns = [\u0026#39;src\u0026#39;,\u0026#39;dst\u0026#39;] cudf_nodes[\u0026#39;src\u0026#39;] = cudf_nodes[\u0026#39;src\u0026#39;].astype(np.int32) cudf_nodes[\u0026#39;dst\u0026#39;] = cudf_nodes[\u0026#39;dst\u0026#39;].astype(np.int32) We can then calculate the Jaccard coefficient between nodes as:\njaccard_coeff_between_nodes = cugraph.link_prediction.jaccard(fb_G,cudf_nodes[\u0026#34;src\u0026#34;],cudf_nodes[\u0026#34;dst\u0026#34;]) jaccard_coeff_between_nodes.head()  But we are still not done. We need to remove the edges where the source==destination and the edges which are already present in the graph. We will do this using simple join and filter operations which work particularly similar to pandas.\njaccard_coeff_between_nodes=jaccard_coeff_between_nodes[jaccard_coeff_between_nodes[\u0026#39;source\u0026#39;]!=jaccard_coeff_between_nodes[\u0026#39;destination\u0026#39;]] fb_cudf.columns = [\u0026#39;source\u0026#39;, \u0026#39;destination\u0026#39;] fb_cudf[\u0026#39;edgeflag\u0026#39;]=1 jaccard_coeff_joined_with_edges = jaccard_coeff_between_nodes.merge(fb_cudf,on= [\u0026#39;source\u0026#39;, \u0026#39;destination\u0026#39;],how=\u0026#39;left\u0026#39;) # We just want to see the jaccard coeff of new edges new_edges_jaccard_coeff = jaccard_coeff_joined_with_edges[jaccard_coeff_joined_with_edges[\u0026#39;edgeflag\u0026#39;]!=1] This is our final sorted dataframe with the Jaccard coefficient between unconnected nodes. We know what friends to recommend to our platform users.\nnew_edges_jaccard_coeff.sort_values(by=\u0026#39;jaccard_coeff\u0026#39;,ascending=False)   Basic Network Statistics  There are a lot of basic measures which you want to know about your network.\nHere is how you get them in your network\nprint(\u0026#34;Number of Nodes\u0026#34;,fb_G.number_of_nodes()) print(\u0026#34;Number of Edges\u0026#34;,fb_G.number_of_edges()) Number of Nodes 4039 Number of Edges 176468  You can also compute the indegree and outdegree for each node.\nIn a directed graph this corresponds to no of followers and no of follows.\nfb_G.degrees().head()   Performance Benchmarks I won‚Äôt do any justice to this post if I don‚Äôt add certain benchmarks for the different algorithms.\nIn my benchmark study, I use three datasets in increasing order of scale from the Stanford Large Network Dataset Collection.\n   ego-Facebook : Undirected graph with 4 K nodes and 88 K edges from Facebook\n   ego-Twitter : Directed graph with 81 K nodes and 1.7 M edges from Twitter\n   ego-Gplus : Directed graph with 107 K nodes and 13.6 M edges from Google+\n  Here are the results of the experiments I performed on NVIDIA \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Tesla V100 32 GB GPU\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; . Thanks to Josh Patterson from NVIDIA and Richard Ulrich at Walmart Labs for arranging that for me. All the times are given in milliseconds:\n I didn‚Äôt add Jaccard coefficients in the results as it didn‚Äôt run even for facebook using networkX. For cuGraph it had millisecond-level latencies.\nLet us visualize these results:\n    Caveats Rapids cuGraph is an excellent library for graph analysis, but I feel some things are still missing. Maybe we will get them in the next version.\n  A little bit of inconvenience that we have to use numbered nodes with data type int32 only. Renumbering helps with that. See my notebook for the benchmark for the exact code. Check the function cugraph.symmetrize_df too for creating undirected graphs.\n  Some algorithms are still not implemented. For instance, I could not find MST, Centrality measures, etc.\n  More example notebooks are needed to document best practices. I might be going to be work on some of those.\n  No visualization component in the library. I have to go to networkx to plot graphs.\n  But despite that, I would also like to add that the idea to provide graph analysis with GPU is so great that I can live with these small problems. And the way they have made the API so similar to pandas and networkx adds to its value.\nI remember how using GPU needed a lot of code in the past. RAPIDS has aimed to make GPU ubiquitous, and that is a fabulous initiative.\n Conclusion  In this post, I talked about some of the most powerful graph algorithms that have changed the way we live and how to scale them with GPUs.\nI love the way Rapids AI has been working to make GPUs accessible to the typical developer/data scientist and to think that we hadn‚Äôt heard about it till a year back. They have come a long way.\nAlso, here are the newest version 0.9 documentation for cuDF and cuGraph .\nYou can get the running code in this Google Colab Notebook , and the code with benchmarks on my Github repository as Google Colab fell short on resources while benchmarking.\n Continue Learning If you want to read up more on Graph Algorithms here is a Graph Analytics for Big Data course on Coursera by UCSanDiego , which I highly recommend to learn the basics of graph theory.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/10/20/cugraph/","tags":["Graphs","Artificial Intelligence","Machine Learning","Data Science"],"title":"4 Graph Algorithms on Steroids for data Scientists with cuGraph"},{"categories":["Data Science"],"contents":"When we create our machine learning models, a common task that falls on us is how to tune them.\nPeople end up taking different manual approaches. Some of them work, and some don‚Äôt, and a lot of time is spent in anticipation and running the code again and again.\nSo that brings us to the quintessential question: Can we automate this process?\nA while back, I was working on an in-class competition from the \u0026lt;strong\u0026gt;‚ÄúHow to win a data science competition‚Äù\u0026lt;/strong\u0026gt; Coursera course. Learned a lot of new things, one among them being Hyperopt ‚Äî A bayesian Parameter Tuning Framework.\nAnd I was amazed. I left my Mac with hyperopt in the night. And in the morning I had my results. It was awesome, and I did avoid a lot of hit and trial.\nThis post is about automating hyperparameter tuning because our time is more important than the machine.\n So, What is Hyperopt?  From the Hyperopt site:\n Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions\n In simple terms, this means that we get an optimizer that could minimize/maximize any function for us. For example, we can use this to minimize the log loss or maximize accuracy.\nAll of us know how grid search or random-grid search works.\nA grid search goes through the parameters one by one, while a random search goes through the parameters randomly.\nHyperopt takes as an input space of hyperparameters in which it will search and moves according to the result of past trials.\n Thus, Hyperopt aims to search the parameter space in an informed way.\n I won‚Äôt go in the details. But if you want to know more about how it works, take a look at this \u0026lt;strong\u0026gt;paper\u0026lt;/strong\u0026gt; by J Bergstra. Here is the \u0026lt;strong\u0026gt;documentation\u0026lt;/strong\u0026gt; from Github.\n Our Dataset To explain how hyperopt works, I will be working on the heart dataset from UCI precisely because it is a simple dataset. And why not do some good using Data Science apart from just generating profits?\nThis dataset predicts the presence of a heart disease given some variables.\nThis is a snapshot of the dataset :\n This is how the target distribution looks like:\n  Hyperopt Step by Step  So, while trying to run hyperopt, we will need to create two Python objects:\n  An Objective function: The objective function takes the hyperparameter space as the input and returns the loss. Here we call our objective function objective\n  A dictionary of hyperparams: We will define a hyperparam space by using the variable space which is actually just a dictionary. We could choose different distributions for different hyperparameter values.\n  In the end, we will use the fmin function from the hyperopt package to minimize our objective through the space.\nYou can follow along with the code in this Kaggle Kernel .\n 1. Create the objective function Here we create an objective function which takes as input a hyperparameter space:\n  We first define a classifier, in this case, XGBoost. Just try to see how we access the parameters from the space. For example space[‚Äòmax_depth‚Äô]\n  We fit the classifier to the train data and then predict on the cross-validation set.\n  We calculate the required metric we want to maximize or minimize.\n  Since we only minimize using fmin in hyperopt, if we want to minimize logloss we just send our metric as is. If we want to maximize accuracy we will try to minimize -accuracy\n  from sklearn.metrics import accuracy_score from hyperopt import hp, fmin, tpe, STATUS_OK, Trials import numpy as np import xgboost as xgb def objective(space): # Instantiate the classifier clf = xgb.XGBClassifier(n_estimators =1000,colsample_bytree=space[\u0026#39;colsample_bytree\u0026#39;], learning_rate = .3, max_depth = int(space[\u0026#39;max_depth\u0026#39;]), min_child_weight = space[\u0026#39;min_child_weight\u0026#39;], subsample = space[\u0026#39;subsample\u0026#39;], gamma = space[\u0026#39;gamma\u0026#39;], reg_lambda = space[\u0026#39;reg_lambda\u0026#39;]) eval_set = [( X, y), ( Xcv, ycv)] # Fit the classsifier clf.fit(X, y, eval_set=eval_set, eval_metric=\u0026#34;rmse\u0026#34;, early_stopping_rounds=10,verbose=False) # Predict on Cross Validation data pred = clf.predict(Xcv) # Calculate our Metric - accuracy accuracy = accuracy_score(ycv, pred\u0026gt;0.5) # return needs to be in this below format. We use negative of accuracy since we want to maximize it. return {\u0026#39;loss\u0026#39;: -accuracy, \u0026#39;status\u0026#39;: STATUS_OK } 2. Create the Space for your classifier  Now, we create the search space for hyperparameters for our classifier.\nTo do this, we end up using many of hyperopt built-in functions which define various distributions.\nAs you can see in the code below, we use uniform distribution between 0.7 and 1 for our subsample hyperparameter. We also give a label for the subsample parameter x_subsample. You need to provide different labels for each hyperparam you define. I generally add a x_ before my parameter name to create this label.\nspace ={\u0026#39;max_depth\u0026#39;: hp.quniform(\u0026#34;x_max_depth\u0026#34;, 4, 16, 1), \u0026#39;min_child_weight\u0026#39;: hp.quniform (\u0026#39;x_min_child\u0026#39;, 1, 10, 1), \u0026#39;subsample\u0026#39;: hp.uniform (\u0026#39;x_subsample\u0026#39;, 0.7, 1), \u0026#39;gamma\u0026#39; : hp.uniform (\u0026#39;x_gamma\u0026#39;, 0.1,0.5), \u0026#39;colsample_bytree\u0026#39; : hp.uniform (\u0026#39;x_colsample_bytree\u0026#39;, 0.7,1), \u0026#39;reg_lambda\u0026#39; : hp.uniform (\u0026#39;x_reg_lambda\u0026#39;, 0,1) } You can also define a lot of other distributions too. Some of the most useful stochastic expressions currently recognized by hyperopt‚Äôs optimization algorithms are:\n  hp.choice(label, options) ‚Äî Returns one of the options, which should be a list or tuple.\n  hp.randint(label, upper) ‚Äî Returns a random integer in the range [0, upper).\n  hp.uniform(label, low, high) ‚Äî Returns a value uniformly between low and high.\n  hp.quniform(label, low, high, q) ‚Äî Returns a value like round(uniform(low, high) / q) * q\n  hp.normal(label, mu, sigma) ‚Äî Returns a real value that‚Äôs normally-distributed with mean mu and standard deviation sigma.\n  There are a lot of other distributions. You can check them out here .\n 3. And finally, Run Hyperopt  Once we run this, we get the best parameters for our model. Turns out we achieved an accuracy of 90% by just doing this on the problem.\ntrials = Trials() best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials) print(best)  Now we can retrain our XGboost algorithm with these best params, and we are done.\n Conclusion Running the above gives us pretty good hyperparams for our learning algorithm. And that saves me a lot of time to think about various other hypotheses and testing them.\nI tend to use this a lot while tuning my models. From my experience, the most crucial part in this whole procedure is setting up the hyperparameter space, and that comes by experience as well as knowledge about the models.\nSo, Hyperopt is an awesome tool to have in your repository but never neglect to understand what your models does. It will be very helpful in the long run.\nYou can get the full code in this Kaggle Kernel .\n Continue Learning If you want to learn more about practical data science, do take a look at the \u0026lt;strong\u0026gt;‚ÄúHow to win a data science competition‚Äù\u0026lt;/strong\u0026gt; Coursera course. Learned a lot of new things from this course taught by one of the most prolific Kaggler.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nAlso, a small disclaimer - There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.\n","permalink":"https://mlwhiz.com/blog/2019/10/10/hyperopt2/","tags":["Machine Learning","Data Science"],"title":"Automate Hyperparameter Tuning for your models"},{"categories":["Data Science"],"contents":"Creating a great machine learning system is an art.\nThere are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.\nMost of the time that happens to be modeling, but in reality, the success or failure of a Machine Learning project depends on a lot of other factors.\n A machine learning pipeline is more than just creating Models\n It is essential to understand what happens before training a model and after training the model and deploying it in production.\nThis post is about explaining what is involved in an end to end data project pipeline. Something I did learn very late in my career.\n 1. Problem Definition  This one is obvious ‚Äî Define a problem.\nAnd, this may be the most crucial part of the whole exercise.\nSo, how to define a problem for Machine learning?\nWell, that depends on a lot of factors. Amongst all the elements that we consider, the first one should be to understand how it will benefit the business.\nThat is the holy grail of any data science project. If your project does not help business, it won‚Äôt get deployed. Period.\nOnce you get an idea and you determine business compatibility, you need to define a success metric.\nNow, what does success look like?\nIs it 90% accuracy or 95% accuracy or 99% accuracy.\nWell, I may be happy with a 70% prediction accuracy since an average human won‚Äôt surpass that accuracy ever and in the meantime, you get to automate the process.\nBeware,this is not the time to set lofty targets; it is the time to be logical and sensible about how every 1 percent accuracy change could affect success.\nFor example: For a click prediction problem/Fraud application, a 1% accuracy increase will boost the business bottom line compared to a 1% accuracy increase in review sentiment prediction.\n Not all accuracy increases are created equal\n  2. Data  There are several questions you will need to answer at the time of data acquisition and data creation for your machine learning model.\nThe most important question to answer here is: Does your model need to work in realtime?\nIf that is the case, you can‚Äôt use a system like Hive/Hadoop for data storage as such systems could introduce a lot of latency and are suitable for offline batch processing.\nDoes your model need to be trained in Realtime?\n If the performance of your ML model decreases with time as in the above figure, you might want to consider Real-time training. RT training might be beneficial for most of the click prediction systems as internet trends change rather quickly.\nIs there an inconsistency between test and train data?\nOr in simple words ‚Äî do you suspect that the production data comes from a different distribution from training data?\nFor example: In a realtime training for a click prediction problem, you show the user the ad, and he doesn‚Äôt click. Is it a failure example? Maybe the user clicks typically after 10 minutes. But you have already created the data and trained your model on that.\nThere are a lot of factors you should consider while preparing data for your models. You need to ask questions and think about the process end to end to be successful at this stage.\n 3. Evaluation  How will we evaluate the performance of our Model?\nThe gold standard here is the train-test-validation split.\nFrequently making a train-validation-test set, by sampling, we forgot about an implicit assumption ‚Äî Data is rarely ever IID(independently and identically distributed).\nIn simple terms, our assumption that each data point is independent of each other and comes from the same distribution is faulty at best if not downright incorrect.\nFor an internet company, a data point from 2007 is very different from a data point that comes in 2019. They don‚Äôt come from the same distribution because of a lot of factors- internet speed being the foremost.\nIf you have a cat vs. dog prediction problem, you are pretty much good with Random sampling. But, in most of the machine learning models, the task is to predict the future.\nYou can think about splitting your data using the time variable rather than sampling randomly from the data. For example: for the click prediction problem you can have all your past data till last month as training data and data for last month as validation.\nThe next thing you will need to think about is the baseline model.\nLet us say we use RMSE as an evaluation metric for our time series models. We evaluated the model on the test set, and the RMSE came out to be 4.8.\nIs that a good RMSE? How do we know? We need a baseline RMSE. This could come from a currently employed model for the same task. Or by using some simple model. For Time series model, a baseline to defeat is last day prediction. i.e., predict the number on the previous day.\nFor NLP classification models, I usually set the baseline to be the evaluation metric(Accuracy, F1, log loss) of Logistic regression models on Countvectorizer(Bag of words).\nYou should also think about how you will be breaking evaluation in multiple groups so that your model doesn‚Äôt induce unnecessary biases.\n Last year, Amazon was in the news for a secret AI recruiting tool that showed bias against women. To save our Machine Learning model from such inconsistencies, we need to evaluate our model on different groups. Maybe our model is not so accurate for women as it is for men because there is far less number of women in training data.\nOr maybe a model predicting if a product is going to be bought or not given a view works pretty well for a specific product category and not for other product categories.\nKeeping such things in mind beforehand and thinking precisely about what could go wrong with a particular evaluation approach is something that could definitely help us in designing a good ML system.\n 4. Features  Good Features are the backbone of any machine learning model. And often the part where you would spend the most time. I have seen that this is the part which you can tune for maximum model performance.\n Good feature creation often needs domain knowledge, creativity, and lots of time.\n On top of that, the feature creation exercise might change for different models. For example, feature creation is very different for Neural networks vs. XGboost.\nUnderstanding various methods for Feature creation is a pretty big topic in itself. I have written a post here on feature creation. Do take a look:\n \u0026lt;strong\u0026gt;The Hitchhiker‚Äôs Guide to Feature Extraction\u0026lt;/strong\u0026gt; Once you create a lot of features, the next thing you might want to do is to remove redundant features. Here are some methods to do that\n \u0026lt;strong\u0026gt;The 5 Feature Selection Algorithms every Data Scientist should know\u0026lt;/strong\u0026gt;  5. Modeling  Now comes the part we mostly tend to care about. And why not? It is the piece that we end up delivering at the end of the project. And this is the part for which we have spent all those hours on data acquisition and cleaning, feature creation and whatnot.\nSo what do we need to think while creating a model?\nThe first question that you may need to ask ourselves is that if your model needs to be interpretable?\nThere are quite a lot of use cases where the business may want an interpretable model. One such use case is when we want to do attribution modeling. Here we define the effect of various advertising streams(TV, radio, newspaper, etc.) on the revenue. In such cases, understanding the response from each advertisement stream becomes essential.\nIf we need to maximize the accuracy or any other metric, we will still want to go for black-box models like NeuralNets or XGBoost.\nApart from model selection, there should be other things on your mind too:\n  Model Architecture: How many layers for NNs, or how many trees for GBT or how you need to create feature interactions for Linear models.\n  How to tune hyperparameters?: You should try to automate this part. There are a lot of tools in the market for this. I tend to use hyperopt.\n   6. Experimentation  Now you have created your model.\nIt performs better than the baseline/your current model. How should we go forward?\nWe have two choices-\n  Go into an endless loop in improving our model further.\n  Test our model in production settings, get more insights about what could go wrong and then continue improving our model with continuous integration.\n  I am a fan of the latter approach. In his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization , Andrew Ng says ‚Äî\n ‚ÄúDon‚Äôt start off trying to design and build the perfect system. Instead, build and train a basic system quickly ‚Äî perhaps in just a few days. Even if the basic system is far from the ‚Äúbest‚Äù system you can build, it is valuable to examine how the basic system functions: you will quickly find clues that show you the most promising directions in which to invest your time.‚Äù\n One thing I would also like to stress is continuous integration. If your current model performs better than the existing model, why not deploy it in production rather than running after incremental gains?\nTo test the validity of your assumption that your model being better than the existing model, you can set up an A/B test. Some users(Test group)see your model while some users(Control) see the predictions from the previous model.\nYou should always aim to minimize the time to first online experiment for your model. This not only generated value but also lets you understand the shortcomings of your model with realtime feedback which you can then work on.\n Conclusion  Nothing is simple in Machine learning. And nothing should be assumed.\n You should always remain critical of any decisions you have taken while building an ML pipeline.\nA simple looking decision could be the difference between the success or failure of your machine learning project.\nSo think wisely and think a lot.\nThis post was part of increasing my understanding of the Machine Learning ecosystem and is inspired by a great set of videos by the Facebook engineering team.\nIf you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization . Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/09/26/building_ml_system/","tags":["Machine Learning","Data Science","Production","Awesome Guides","Best Content"],"title":"6 Important Steps to build a Machine Learning System"},{"categories":["Data Science"],"contents":"I always get confused whenever someone talks about generative vs. discriminative classification models.\nI end up reading it again and again, yet somehow it eludes me.\nSo I thought of writing a post on it to improve my understanding.\nThis post is about understanding Generative Models and how they differ from Discriminative models.\nIn the end, we will create a simple generative model ourselves.\n Discriminative vs. Generative Classifiers Problem Statement: Having some input data, X we want to classify the data into labels y.\nA generative model learns the joint probability distribution p(x,y) while a discriminative model learns the conditional probability distribution p(y|x)\nSo really, what is the difference? They both look pretty much the same.\nSuppose we have a small sample of data:\n(x,y) : [(0,1), (1,0), (1,0), (1, 1)]\nThen p(x,y) is\n While p(y|x) is\n As you can see, they model different probabilities.\nThe discriminative distribution p(y|x) could be used straightforward to classify an example x into a class y. An example of a discriminative classification model is Logistic regression, where we try to model P(y|X).\n Generative algorithms model p(x,y). An example is the Naive Bayes model in which we try to model P(X,y) and then use the Bayes equation to predict.\n The Central Idea Behind Generative Classification   Fit each class separately with a probability distribution.\n  To classify a new point, find out which distribution is it most probable to come from.\n  Don‚Äôt worry if you don‚Äôt understand yet. You will surely get it by the end of this post.\n A Small Example  Let us work with the iris dataset.\nFor our simple example, we will work with a single x variable SepalLength and our target variable Species.\nLet us see the distribution of sepal length with Species. I am using plotly_express for this.\nimport plotly_express as px px.histogram(iris, x = \u0026#39;SepalLengthCm\u0026#39;,color = \u0026#39;Species\u0026#39;,nbins=20)  To create generative models, we need to find out two sets of values:\n1. Probability of individual classes: To get individual class probability is fairly trivial- For example, the number of instances in our dataset, which is setosa divided by the total number of cases in the dataset.\np_setosa = len(iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-setosa\u0026#39;])/len(iris) p_versicolor = len(iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-versicolor\u0026#39;])/len(iris) p_virginica = len(iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-virginica\u0026#39;])/len(iris) print(p_setosa,p_versicolor,p_virginica) 0.3333333333333333 0.3333333333333333 0.3333333333333333  The iris dataset is pretty much balanced.\n2. The probability distribution of x for each class: Here we fit a probability distribution over our X. We assume here that the X data is distributed normally. And hence we can find the sample means and variance for these three distributions(As we have three classes)\nimport numpy as np import seaborn as sns from scipy import stats import matplotlib.pyplot as plt sns.set(style=\u0026#34;ticks\u0026#34;) # calculate the pdf over a range of values xx = np.arange(min(iris[\u0026#39;SepalLengthCm\u0026#39;]), max(iris[\u0026#39;SepalLengthCm\u0026#39;]),0.001) x = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-setosa\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] sns.distplot(x, kde = False, norm_hist=True,color=\u0026#39;skyblue\u0026#39;,label = \u0026#39;Setosa\u0026#39;) yy = stats.norm.pdf(xx,loc=np.mean(x),scale=np.std(x)) plt.plot(xx, yy, \u0026#39;skyblue\u0026#39;, lw=2) x = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-versicolor\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] sns.distplot(x, kde = False, norm_hist=True,color=\u0026#39;green\u0026#39;,label = \u0026#39;Versicolor\u0026#39;) yy = stats.norm.pdf(xx,loc=np.mean(x),scale=np.std(x)) plt.plot(xx, yy, \u0026#39;green\u0026#39;, lw=2) x = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-virginica\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] g = sns.distplot(x, kde = False, norm_hist=True,color=\u0026#39;red\u0026#39;,label = \u0026#39;Virginica\u0026#39;) yy = stats.norm.pdf(xx,loc=np.mean(x),scale=np.std(x)) plt.plot(xx, yy, \u0026#39;red\u0026#39;, lw=2) sns.despine() g.figure.set_size_inches(20,10) g.legend()  In the above graph, I have fitted three normal distributions for each of the species just using sample means and variances for each of the three species.\nSo, how do we predict using this?\nLet us say we get a new example with SepalLength = 7 cm.\n Since we see that the maximum probability comes for Virginica, we predict virginica for x=7, and based on the graph too; it looks pretty much the right choice.\nYou can get the values using the code too.\nx = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-setosa\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] print(\u0026#34;Setosa\u0026#34;,stats.norm.pdf(7,loc=np.mean(x),scale=np.std(x))*.33) x = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-versicolor\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] print(\u0026#34;Versicolor\u0026#34;,stats.norm.pdf(7,loc=np.mean(x),scale=np.std(x))*.33) x = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-virginica\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] print(\u0026#34;Virginica\u0026#34;,stats.norm.pdf(7,loc=np.mean(x),scale=np.std(x))*.33) Setosa 3.062104211904799e-08 Versicolor 0.029478757465669376 Virginica 0.16881724812694823  This is all well and good. But when do we ever work with a single variable?\nLet us extend our example for two variables. This time let us use PetalLength too.\npx.scatter(iris, \u0026#39;SepalLengthCm\u0026#39;, \u0026#39;PetalLengthCm\u0026#39;,color = \u0026#39;Species\u0026#39;)  So how do we proceed in this case?\nThe first time we had fit a Normal Distribution over our single x, this time we will fit Bivariate Normal.\nimport numpy as np import seaborn as sns from scipy import stats import matplotlib.pyplot as plt from matplotlib.mlab import bivariate_normal sns.set(style=\u0026#34;ticks\u0026#34;) # SETOSA x1 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-setosa\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] x2 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-setosa\u0026#39;][\u0026#39;PetalLengthCm\u0026#39;] sns.scatterplot(x1,x2, color=\u0026#39;skyblue\u0026#39;,label = \u0026#39;Setosa\u0026#39;) mu_x1=np.mean(x1) mu_x2=np.mean(x2) sigma_x1=np.std(x1)**2 sigma_x2=np.std(x2)**2 xx = np.arange(min(x1), max(x1),0.001) yy = np.arange(min(x2), max(x2),0.001) X, Y = np.meshgrid(xx, yy) Z = bivariate_normal(X,Y, sigma_x1, sigma_x2, mu_x1, mu_x2) plt.contour(X,Y,Z,colors=\u0026#39;skyblue\u0026#39;) # VERSICOLOR x1 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-versicolor\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] x2 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-versicolor\u0026#39;][\u0026#39;PetalLengthCm\u0026#39;] sns.scatterplot(x1,x2,color=\u0026#39;green\u0026#39;,label = \u0026#39;Versicolor\u0026#39;) mu_x1=np.mean(x1) mu_x2=np.mean(x2) sigma_x1=np.std(x1)**2 sigma_x2=np.std(x2)**2 xx = np.arange(min(x1), max(x1),0.001) yy = np.arange(min(x2), max(x2),0.001) X, Y = np.meshgrid(xx, yy) Z = bivariate_normal(X,Y, sigma_x1, sigma_x2, mu_x1, mu_x2) plt.contour(X,Y,Z,colors=\u0026#39;green\u0026#39;) # VIRGINICA x1 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-virginica\u0026#39;][\u0026#39;SepalLengthCm\u0026#39;] x2 = iris[iris[\u0026#39;Species\u0026#39;]==\u0026#39;Iris-virginica\u0026#39;][\u0026#39;PetalLengthCm\u0026#39;] g = sns.scatterplot(x1, x2, color=\u0026#39;red\u0026#39;,label = \u0026#39;Virginica\u0026#39;) mu_x1=np.mean(x1) mu_x2=np.mean(x2) sigma_x1=np.std(x1)**2 sigma_x2=np.std(x2)**2 xx = np.arange(min(x1), max(x1),0.001) yy = np.arange(min(x2), max(x2),0.001) X, Y = np.meshgrid(xx, yy) Z = bivariate_normal(X,Y, sigma_x1, sigma_x2, mu_x1, mu_x2) plt.contour(X,Y,Z,colors=\u0026#39;red\u0026#39;) sns.despine() g.figure.set_size_inches(20,10) g.legend() Here is how it looks:\n Now the rest of the calculations remains the same.\nJust the normal gets replaced by Bivariate normal in the above equations. And as you can see, we get a pretty better separation amongst the classes by using the bivariate normal.\nAs an extension to this case for multiple variables(more than 2), we can use the multivariate normal distribution.\n Conclusion Generative models are good at generating data. But at the same time, creating such models that capture the underlying distribution of data is extremely hard.\nGenerative modeling involves a lot of assumptions, and thus, these models don‚Äôt perform as well as discriminative models in the classification setting. In the above example also we assumed that the distribution is normal, which might not be correct and hence may induce a bias.\nBut understanding how they work is helpful all the same. One class of such models is called generative adversarial networks which are pretty useful for generating new images and are pretty interesting too.\n Here is the kernel with all the code along with the visualizations.\nIf you want to learn more about generative models and Machine Learning, I would recommend this Machine Learning Fundamentals course from the University of San Diego. The above post is by and large inspired from content from this course in the MicroMasters from SanDiego , which I am currently working on to structure my Data Science learning.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/09/23/generative_approach_to_classification/","tags":["Machine Learning","Data Science","Statistics","Math"],"title":"A Generative Approach to Classification"},{"categories":["Data Science","Awesome Guides"],"contents":"We as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.\nWe are used to seeing our users in rows with their attributes as columns. But does the real world really behave like that?\nIn a connected world, users cannot be considered as independent entities. They have got certain relationships between each other and we would sometimes like to include such relationships while building our machine learning models.\nNow while in a relational database, we cannot use such relations between different rows(users), in a graph database it is fairly trivial to do that.\nIn this post, I am going to be talking about some of the most important graph algorithms you should know and how to implement them using Python.\nAlso, here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.\n 1. Connected Components  We all know how clustering works?\nYou can think of Connected Components in very layman‚Äôs terms as a sort of a hard clustering algorithm which finds clusters/islands in related/connected data.\nAs a concrete example: Say you have data about roads joining any two cities in the world. And you need to find out all the continents in the world and which city they contain.*\nHow will you achieve that? Come on give some thought.\nThe connected components algorithm that we use to do this is based on a special case of BFS/DFS. I won‚Äôt talk much about how it works here, but we will see how to get the code up and running using Networkx.\nApplications From a Retail Perspective: Let us say, we have a lot of customers using a lot of accounts. One way in which we can use the Connected components algorithm is to find out distinct families in our dataset.\nWe can assume edges(roads) between CustomerIDs based on same credit card usage, or same address or same mobile number, etc. Once we have those connections, we can then run the connected component algorithm on the same to create individual clusters to which we can then assign a family ID.\nWe can then use these family IDs to provide personalized recommendations based on family needs. We can also use this family ID to fuel our classification algorithms by creating grouped features based on family.\nFrom a Finance Perspective: Another use case would be to capture fraud using these family IDs. If an account has done fraud in the past, it is highly probable that the connected accounts are also susceptible to fraud.\nThe possibilities are only limited by your own imagination.\nCode We will be using the Networkx module in Python for creating and analyzing our graphs.\nLet us start with an example graph which we are using for our purpose. Contains cities and distance information between them.\n We first start by creating a list of edges along with the distances which we will add as the weight of the edge:\nedgelist = [[\u0026#39;Mannheim\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 85], [\u0026#39;Mannheim\u0026#39;, \u0026#39;Karlsruhe\u0026#39;, 80], [\u0026#39;Erfurt\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 186], [\u0026#39;Munchen\u0026#39;, \u0026#39;Numberg\u0026#39;, 167], [\u0026#39;Munchen\u0026#39;, \u0026#39;Augsburg\u0026#39;, 84], [\u0026#39;Munchen\u0026#39;, \u0026#39;Kassel\u0026#39;, 502], [\u0026#39;Numberg\u0026#39;, \u0026#39;Stuttgart\u0026#39;, 183], [\u0026#39;Numberg\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 103], [\u0026#39;Numberg\u0026#39;, \u0026#39;Munchen\u0026#39;, 167], [\u0026#39;Stuttgart\u0026#39;, \u0026#39;Numberg\u0026#39;, 183], [\u0026#39;Augsburg\u0026#39;, \u0026#39;Munchen\u0026#39;, 84], [\u0026#39;Augsburg\u0026#39;, \u0026#39;Karlsruhe\u0026#39;, 250], [\u0026#39;Kassel\u0026#39;, \u0026#39;Munchen\u0026#39;, 502], [\u0026#39;Kassel\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 173], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Mannheim\u0026#39;, 85], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Wurzburg\u0026#39;, 217], [\u0026#39;Frankfurt\u0026#39;, \u0026#39;Kassel\u0026#39;, 173], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Numberg\u0026#39;, 103], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Erfurt\u0026#39;, 186], [\u0026#39;Wurzburg\u0026#39;, \u0026#39;Frankfurt\u0026#39;, 217], [\u0026#39;Karlsruhe\u0026#39;, \u0026#39;Mannheim\u0026#39;, 80], [\u0026#39;Karlsruhe\u0026#39;, \u0026#39;Augsburg\u0026#39;, 250],[\u0026#34;Mumbai\u0026#34;, \u0026#34;Delhi\u0026#34;,400],[\u0026#34;Delhi\u0026#34;, \u0026#34;Kolkata\u0026#34;,500],[\u0026#34;Kolkata\u0026#34;, \u0026#34;Bangalore\u0026#34;,600],[\u0026#34;TX\u0026#34;, \u0026#34;NY\u0026#34;,1200],[\u0026#34;ALB\u0026#34;, \u0026#34;NY\u0026#34;,800]] Let us create a graph using Networkx:\ng = nx.Graph() for edge in edgelist: g.add_edge(edge[0],edge[1], weight = edge[2]) Now we want to find out distinct continents and their cities from this graph.\nWe can now do this using the connected components algorithm as:\nfor i, x in enumerate(nx.connected_components(g)): print(\u0026#34;cc\u0026#34;+str(i)+\u0026#34;:\u0026#34;,x) cc0: {'Frankfurt', 'Kassel', 'Munchen', 'Numberg', 'Erfurt', 'Stuttgart', 'Karlsruhe', 'Wurzburg', 'Mannheim', 'Augsburg'} cc1: {'Kolkata', 'Bangalore', 'Mumbai', 'Delhi'} cc2: {'ALB', 'NY', 'TX'}  As you can see we are able to find distinct components in our data. Just by using Edges and Vertices. This algorithm could be run on different data to satisfy any use case that I presented above.\n 2. Shortest Path  Continuing with the above example only, we are given a graph with the cities of Germany and the respective distance between them.\nYou want to find out how to go from Frankfurt (The starting node) to Munchen by covering the shortest distance.\nThe algorithm that we use for this problem is called Dijkstra. In Dijkstra‚Äôs own words:\n What is the shortest way to travel from Rotterdam to Groningen , in general: from given city to given city. It is the algorithm for the shortest path , which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fianc√©e, and tired, we sat down on the caf√© terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in ‚Äô59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually that algorithm became, to my great amazement, one of the cornerstones of my fame. ‚Äî Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001 [3]  Applications   Variations of the Dijkstra algorithm is used extensively in Google Maps to find the shortest routes.\n  You are in a Walmart Store. You have different Aisles and distance between all the aisles. You want to provide the shortest pathway to the customer from Aisle A to Aisle D.\n    You have seen how LinkedIn shows up 1st-degree connections, 2nd-degree connections. What goes on behind the scenes?   Code print(nx.shortest_path(g, \u0026#39;Stuttgart\u0026#39;,\u0026#39;Frankfurt\u0026#39;,weight=\u0026#39;weight\u0026#39;)) print(nx.shortest_path_length(g, \u0026#39;Stuttgart\u0026#39;,\u0026#39;Frankfurt\u0026#39;,weight=\u0026#39;weight\u0026#39;)) ['Stuttgart', 'Numberg', 'Wurzburg', 'Frankfurt'] 503  You can also find Shortest paths between all pairs using:\nfor x in nx.all_pairs_dijkstra_path(g,weight=\u0026#39;weight\u0026#39;): print(x) ('Mannheim', {'Mannheim': ['Mannheim'], 'Frankfurt': ['Mannheim', 'Frankfurt'], 'Karlsruhe': ['Mannheim', 'Karlsruhe'], 'Augsburg': ['Mannheim', 'Karlsruhe', 'Augsburg'], 'Kassel': ['Mannheim', 'Frankfurt', 'Kassel'], 'Wurzburg': ['Mannheim', 'Frankfurt', 'Wurzburg'], 'Munchen': ['Mannheim', 'Karlsruhe', 'Augsburg', 'Munchen'], 'Erfurt': ['Mannheim', 'Frankfurt', 'Wurzburg', 'Erfurt'], 'Numberg': ['Mannheim', 'Frankfurt', 'Wurzburg', 'Numberg'], 'Stuttgart': ['Mannheim', 'Frankfurt', 'Wurzburg', 'Numberg', 'Stuttgart']}) ('Frankfurt', {'Frankfurt': ['Frankfurt'], 'Mannheim': ['Frankfurt', 'Mannheim'], 'Kassel': ['Frankfurt', 'Kassel'], 'Wurzburg': ['Frankfurt', 'Wurzburg'], 'Karlsruhe': ['Frankfurt', 'Mannheim', 'Karlsruhe'], 'Augsburg': ['Frankfurt', 'Mannheim', 'Karlsruhe', 'Augsburg'], 'Munchen': ['Frankfurt', 'Wurzburg', 'Numberg', 'Munchen'], 'Erfurt': ['Frankfurt', 'Wurzburg', 'Erfurt'], 'Numberg': ['Frankfurt', 'Wurzburg', 'Numberg'], 'Stuttgart': ['Frankfurt', 'Wurzburg', 'Numberg', 'Stuttgart']}) ....   3. Minimum Spanning Tree  Now we have another problem. We work for a water pipe laying company or an internet fiber company. We need to connect all the cities in the graph we have using the minimum amount of wire/pipe. How do we do this?\n Applications   Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for)\n  MST is used for approximating the traveling salesman problem\n  Clustering ‚Äî First construct MST and then determine a threshold value for breaking some edges in the MST using Intercluster distances and Intracluster distances.\n  Image Segmentation ‚Äî It was used for Image segmentation where we first construct an MST on a graph where pixels are nodes and distances between pixels are based on some similarity measure(color, intensity, etc.)\n  Code # nx.minimum_spanning_tree(g) returns a instance of type graph nx.draw_networkx(nx.minimum_spanning_tree(g))  As you can see the above is the wire we gotta lay.\n 4. Pagerank  This is the page sorting algorithm that powered google for a long time. It assigns scores to pages based on the number and quality of incoming and outgoing links.\nApplications Pagerank can be used anywhere where we want to estimate node importance in any network.\n  It has been used for finding the most influential papers using citations.\n  Has been used by Google to rank pages\n  It can be used to rank tweets- User and Tweets as nodes. Create Link between user if user A follows user B and Link between user and Tweets if user tweets/retweets a tweet.\n  Recommendation engines\n  Code For this exercise, we are going to be using Facebook data. We have a file of edges/links between facebook users. We first create the FB graph using:\n# reading the dataset fb = nx.read_edgelist(\u0026#39;../input/facebook-combined.txt\u0026#39;, create_using = nx.Graph(), nodetype = int) This is how it looks:\npos = nx.spring_layout(fb) import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (20, 15) plt.axis(\u0026#39;off\u0026#39;) nx.draw_networkx(fb, pos, with_labels = False, node_size = 35) plt.show()  Now we want to find the users having high influence capability.\nIntuitively, the Pagerank algorithm will give a higher score to a user who has a lot of friends who in turn have a lot of FB Friends.\npageranks = nx.pagerank(fb) print(pageranks) {0: 0.006289602618466542, 1: 0.00023590202311540972, 2: 0.00020310565091694562, 3: 0.00022552359869430617, 4: 0.00023849264701222462, ........}  We can get the sorted PageRank or most influential users using:\nimport operator sorted_pagerank = sorted(pagerank.items(), key=operator.itemgetter(1),reverse = True) print(sorted_pagerank) [(3437, 0.007614586844749603), (107, 0.006936420955866114), (1684, 0.0063671621383068295), (0, 0.006289602618466542), (1912, 0.0038769716008844974), (348, 0.0023480969727805783), (686, 0.0022193592598000193), (3980, 0.002170323579009993), (414, 0.0018002990470702262), (698, 0.0013171153138368807), (483, 0.0012974283300616082), (3830, 0.0011844348977671688), (376, 0.0009014073664792464), (2047, 0.000841029154597401), (56, 0.0008039024292749443), (25, 0.000800412660519768), (828, 0.0007886905420662135), (322, 0.0007867992190291396),......]  The above IDs are for the most influential users.\nWe can see the subgraph for the most influential user:\nfirst_degree_connected_nodes = list(fb.neighbors(3437)) second_degree_connected_nodes = [] for x in first_degree_connected_nodes: second_degree_connected_nodes+=list(fb.neighbors(x)) second_degree_connected_nodes.remove(3437) second_degree_connected_nodes = list(set(second_degree_connected_nodes)) subgraph_3437 = nx.subgraph(fb,first_degree_connected_nodes+second_degree_connected_nodes) pos = nx.spring_layout(subgraph_3437) node_color = [\u0026#39;yellow\u0026#39; if v == 3437 else \u0026#39;red\u0026#39; for v in subgraph_3437] node_size = [1000 if v == 3437 else 35 for v in subgraph_3437] plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (20, 15) plt.axis(\u0026#39;off\u0026#39;) nx.draw_networkx(subgraph_3437, pos, with_labels = False, node_color=node_color,node_size=node_size ) plt.show()   5. Centrality Measures There are a lot of centrality measures which you can use as features to your machine learning models. I will talk about two of them. You can look at other measures here .\nBetweenness Centrality: It is not only the users who have the most friends that are important, the users who connect one geography to another are also important as that lets users see content from diverse geographies. Betweenness centrality quantifies how many times a particular node comes in the shortest chosen path between two other nodes.\nDegree Centrality: It is simply the number of connections for a node.\nApplications Centrality measures can be used as a feature in any machine learning model.\nCode Here is the code for finding the Betweenness centrality for the subgraph.\npos = nx.spring_layout(subgraph_3437) betweennessCentrality = **nx.betweenness_centrality(**subgraph_3437**,normalized=True, endpoints=True)** node_size = [v * 10000 for v in betweennessCentrality.values()] plt.figure(figsize=(20,20)) nx.draw_networkx(subgraph_3437, pos=pos, with_labels=False, node_size=node_size ) plt.axis(\u0026#39;off\u0026#39;)  You can see the nodes sized by their betweenness centrality values here. They can be thought of as information passers. Breaking any of the nodes with a high betweenness Centrality will break the graph into many parts.\n Conclusion In this post, I talked about some of the most influential graph algorithms that have changed the way we live.\nWith the advent of so much social data, network analysis could help a lot in improving our models and generating value.\nAnd even understanding a little more about the world.\nThere are a lot of graph algorithms out there, but these are the ones I like the most. Do look into the algorithms in more detail if you like. In this post, I just wanted to get the required breadth into the area.\nLet me know if you feel I have left your favorite algorithm in the comments.\nHere is the Kaggle Kernel with the whole code.\nIf you want to read up more on Graph Algorithms here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/09/02/graph_algs/","tags":["Big Data","Machine Learning","Data Science","Graphs"],"title":"Data Scientists, The 5 Graph Algorithms that you should know"},{"categories":["Data Science","Awesome Guides"],"contents":"One of the main tasks while working with text data is to create a lot of text-based features.\nOne could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text.\nWhile it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python‚Äôs regex module.\nFor example, let\u0026rsquo;s say you are tasked with finding the number of punctuations in a particular piece of text. Using text from Dickens here.\nHow do you normally go about it?\nA simple enough way is to do something like:\ntarget = [\u0026#39;;\u0026#39;,\u0026#39;.\u0026#39;,\u0026#39;,\u0026#39;,\u0026#39;‚Äì\u0026#39;] string = \u0026#34;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way ‚Äì in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.**\u0026#34; num_puncts = 0 for punct in target: if punct in string: num_puncts+=string.count(punct) print(num_puncts) 19 And that is all but fine if we didn‚Äôt have the re module at our disposal. With re it is simply 2 lines of code:\nimport re pattern = r\u0026#34;[;.,‚Äì]\u0026#34; print(len(re.findall(pattern,string)))  19 This post is about one of the most commonly used regex patterns and some regex functions I end up using regularly.\nWhat is regex? In simpler terms, a regular expression(regex) is used to find patterns in a given string.\nThe pattern we want to find could be anything.\nWe can create patterns that resemble an email or a mobile number. We can create patterns that find out words that start with a and ends with z from a string.\nIn the above example:\nimport re pattern = r\u0026#39;[,;.,‚Äì]\u0026#39; print(len(re.findall(pattern,string))) The pattern we wanted to find out was r‚Äô[,;.,‚Äì]‚Äô. This pattern captures any of the 4 characters we wanted to capture. I find regex101 a great tool for testing patterns. This is how the pattern looks when applied to the target string.\n As we can see we are able to find all the occurrences of ,;.,‚Äì in the target string as required.\nI use the above tool whenever I need to test a regex. Much faster than running a python program again and again and much easier to debug.\nSo now we know that we can find patterns in a target string but how do we really create these patterns?\n Creating Patterns  The first thing we need to learn while using regex is how to create patterns.\nI will go through some most commonly used patterns one by one.\nAs you would think, the simplest pattern is a simple string.\npattern = r\u0026#39;times\u0026#39; string = \u0026#34;It was the best of times, it was the worst of times.\u0026#34; print(len(re.findall(pattern,string))) But that is not very useful. To help with creating complex patterns regex provides us with special characters/operators. Let us go through some of these operators one by one. Please wait for the gifs to load.\n1. the [] operator This is the one we used in our first example. We want to find one instance of any character within these square brackets.\n[abc]- will find all occurrences of a or b or c.\n[a-z]- will find all occurrences of a to z.\n[a-z0‚Äì9A-Z]- will find all occurrences of a to z, A to Z and 0 to 9.\n We can easily use this pattern as below in Python:\npattern = r\u0026#39;[a-zA-Z]\u0026#39; string = \u0026#34;It was the best of times, it was the worst of times.\u0026#34; print(len(re.findall(pattern,string))) There are other functionalities in regex apart from .findall but we will get to them a little bit later.\n2. The dot Operator The dot operator(.) is used to match a single instance of any character except the newline character.\nThe best part about the operators is that we can use them in conjunction with one another.\nFor example, We want to find out the substrings in the string that start with small d or Capital D and end with e with a length of 6.\n 3. Some Meta Sequences There are some patterns that we end up using again and again while using regex. And so regex has created a few shortcuts for them. The most useful shortcuts are:\n\\w, Matches any letter, digit or underscore. Equivalent to [a-zA-Z0‚Äì9_]\n\\W, Matches anything other than a letter, digit or underscore.\n\\d, Matches any decimal digit. Equivalent to [0‚Äì9].\n\\D, Matches anything other than a decimal digit.\n4. The Plus and Star operator  The dot character is used to get a single instance of any character. What if we want to find more.\nThe Plus character +, is used to signify 1 or more instance of the leftmost character.\nThe Star character *, is used to signify 0 or more instance of the leftmost character.\nFor example, if we want to find out all substrings that start with d and end with e, we can have zero characters or more characters between d and e. We can use: d\\w*e\nIf we want to find out all substrings that start with d and end with e with at least one character between d and e, we can use: d\\w+e\n We could also have used a more generic approach using \\w{n} - Repeat \\w exactly n number of times.\n\\w{n,} - Repeat \\w at least n times or more.\n\\w{n1, n2} - Repeat \\w at least n1 times but no more than n2 times.\n5. ^ Caret Operator and $ Dollar operator. ^ Matches the start of a string, and $ Matches the end of the string.\n 6. Word Boundary This is an important concept.\nDid you notice how I always matched substring and never a word in the above examples?\nSo, what if we want to find all words that start with d?\nCan we use d\\w* as the pattern? Let\u0026rsquo;s see using the web tool.\n  Regex Functions Till now we have only used the findall function from the re package, but it also supports a lot more functions. Let us look into the functions one by one.\n1. findall We already have used findall. It is one of the regex functions I end up using most often. Let us understand it a little more formally.\nInput: Pattern and test string\nOutput: List of strings.\n#USAGE: pattern = r\u0026#39;[iI]t\u0026#39; string = \u0026#34;It was the best of times, it was the worst of times.\u0026#34; matches = re.findall(pattern,string) for match in matches: print(match) It it  2. Search  Input: Pattern and test string\nOutput: Location object for the first match.\n#USAGE: pattern = r\u0026#39;[iI]t\u0026#39; string = \u0026#34;It was the best of times, it was the worst of times.\u0026#34; location = re.search(pattern,string) print(location) \u0026lt;_sre.SRE_Match object; span=(0, 2), match='It'\u0026gt;  We can get this location object‚Äôs data using\nprint(location.group()) 'It'  3. Substitute This is another great functionality. When you work with NLP you sometimes need to substitute integers with X‚Äôs. Or you might need to redact some document. Just the basic find and replace in any of the text editors.\nInput: search pattern, replacement pattern, and the target string\nOutput: Substituted string\nstring = \u0026#34;It was the best of times, it was the worst of times.\u0026#34; string = re.sub(r\u0026#39;times\u0026#39;, r\u0026#39;life\u0026#39;, string) print(string) It was the best of life, it was the worst of life.   Some Case Studies: Regex is used in many cases when validation is required. You might have seen prompts on websites like ‚ÄúThis is not a valid email address‚Äù. While such a prompt could be written using multiple if and else conditions, regex is probably the best for such use cases.\n1. PAN Numbers  In India, we have got PAN Numbers for Tax identification rather than SSN numbers in the US. The basic validation criteria for PAN is that it must have all its letters in uppercase and characters in the following order:\n\u0026lt;char\u0026gt;\u0026lt;char\u0026gt;\u0026lt;char\u0026gt;\u0026lt;char\u0026gt;\u0026lt;char\u0026gt;\u0026lt;digit\u0026gt;\u0026lt;digit\u0026gt;\u0026lt;digit\u0026gt;\u0026lt;digit\u0026gt;\u0026lt;char\u0026gt;  So the question is:\nIs ‚ÄòABcDE1234L‚Äô a valid PAN?\nHow would you normally attempt to solve this without regex? You will most probably write a for loop and keep an index going through the string. With regex it is as simple as below:\nmatch=re.search(r\u0026#39;[A-Z]{5}[0‚Äì9]{4}[A-Z]\u0026#39;,\u0026#39;ABcDE1234L\u0026#39;) if match: print(True) else: print(False) False  2. Find Domain Names  Sometimes we have got a large text document and we have got to find out instances of telephone numbers or email IDs or domain names from the big text document.\nFor example, Suppose you have this text:\n\u0026lt;div class=\u0026#34;reflist\u0026#34; style=\u0026#34;list-style-type: decimal;\u0026#34;\u0026gt; \u0026lt;ol class=\u0026#34;references\u0026#34;\u0026gt; \u0026lt;li id=\u0026#34;cite_note-1\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^ [\u0026#34;Train (noun)\u0026#34;](http://www.askoxford.com/concise_oed/train?view=uk). \u0026lt;i\u0026gt;(definition ‚Äì Compact OED)\u0026lt;/i\u0026gt;. Oxford University Press\u0026lt;span class=\u0026#34;reference-accessdate\u0026#34;\u0026gt;. Retrieved 2008-03-18\u0026lt;/span\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span title=\u0026#34;ctx_ver=Z39.88-2004\u0026amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain\u0026amp;rft.atitle=Train+%28noun%29\u0026amp;rft.genre=article\u0026amp;rft_id=http%3A%2F%2Fwww.askoxford.com%2Fconcise_oed%2Ftrain%3Fview%3Duk\u0026amp;rft.jtitle=%28definition+%E2%80%93+Compact+OED%29\u0026amp;rft.pub=Oxford+University+Press\u0026amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\u0026#34; class=\u0026#34;Z3988\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;display:none;\u0026#34;\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;cite_note-2\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^\u0026lt;/b\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;reference-text\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;citation book\u0026#34;\u0026gt;Atchison, Topeka and Santa Fe Railway (1948). \u0026lt;i\u0026gt;Rules: Operating Department\u0026lt;/i\u0026gt;. p. 7.\u0026lt;/span\u0026gt;\u0026lt;span title=\u0026#34;ctx_ver=Z39.88-2004\u0026amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain\u0026amp;rft.au=Atchison%2C+Topeka+and+Santa+Fe+Railway\u0026amp;rft.aulast=Atchison%2C+Topeka+and+Santa+Fe+Railway\u0026amp;rft.btitle=Rules%3A+Operating+Department\u0026amp;rft.date=1948\u0026amp;rft.genre=book\u0026amp;rft.pages=7\u0026amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\u0026#34; class=\u0026#34;Z3988\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;display:none;\u0026#34;\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;cite_note-3\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^ [Hydrogen trains](http://www.hydrogencarsnow.com/blog2/index.php/hydrogen-vehicles/i-hear-the-hydrogen-train-a-comin-its-rolling-round-the-bend/)\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;cite_note-4\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^ [Vehicle Projects Inc. Fuel cell locomotive](http://www.bnsf.com/media/news/articles/2008/01/2008-01-09a.html)\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;cite_note-5\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^\u0026lt;/b\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;reference-text\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;citation book\u0026#34;\u0026gt;Central Japan Railway (2006). \u0026lt;i\u0026gt;Central Japan Railway Data Book 2006\u0026lt;/i\u0026gt;. p. 16.\u0026lt;/span\u0026gt;\u0026lt;span title=\u0026#34;ctx_ver=Z39.88-2004\u0026amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain\u0026amp;rft.au=Central+Japan+Railway\u0026amp;rft.aulast=Central+Japan+Railway\u0026amp;rft.btitle=Central+Japan+Railway+Data+Book+2006\u0026amp;rft.date=2006\u0026amp;rft.genre=book\u0026amp;rft.pages=16\u0026amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\u0026#34; class=\u0026#34;Z3988\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;display:none;\u0026#34;\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li id=\u0026#34;cite_note-6\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;mw-cite-backlink\u0026#34;\u0026gt;\u0026lt;b\u0026gt;^ [\u0026#34;Overview Of the existing Mumbai Suburban Railway\u0026#34;](http://web.archive.org/web/20080620033027/http://www.mrvc.indianrail.gov.in/overview.htm). _Official webpage of Mumbai Railway Vikas Corporation_. Archived from [the original](http://www.mrvc.indianrail.gov.in/overview.htm) on 2008-06-20\u0026lt;span class=\u0026#34;reference-accessdate\u0026#34;\u0026gt;. Retrieved 2008-12-11\u0026lt;/span\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span title=\u0026#34;ctx_ver=Z39.88-2004\u0026amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain\u0026amp;rft.atitle=Overview+Of+the+existing+Mumbai+Suburban+Railway\u0026amp;rft.genre=article\u0026amp;rft_id=http%3A%2F%2Fwww.mrvc.indianrail.gov.in%2Foverview.htm\u0026amp;rft.jtitle=Official+webpage+of+Mumbai+Railway+Vikas+Corporation\u0026amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\u0026#34; class=\u0026#34;Z3988\u0026#34;\u0026gt;\u0026lt;span style=\u0026#34;display:none;\u0026#34;\u0026gt; \u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/div\u0026gt; And you need to find out all the primary domains from this text- askoxford.com;bnsf.com;hydrogencarsnow.com;mrvc.indianrail.gov.in;web.archive.org\nHow would you do this?\nmatch=re.findall(r\u0026#39;http(s:|:)\\/\\/([www.|ww2.|)([0-9a-z.A-Z-]*\\.\\w{2,3})\u0026#39;,string)](http://www.|ww2.|)([0-9a-z.A-Z-]*\\.\\w{2,3})\u0026#39;,string)) for elem in match: print(elem) (':', 'www.', 'askoxford.com') (':', 'www.', 'hydrogencarsnow.com') (':', 'www.', 'bnsf.com') (':', '', 'web.archive.org') (':', 'www.', 'mrvc.indianrail.gov.in') (':', 'www.', 'mrvc.indianrail.gov.in')  | is the or operator here and match returns tuples where the pattern part inside () is kept.\n3. Find Email Addresses:  Below is a regex to find email addresses in a long text.\nmatch=re.findall(r\u0026#39;([\\w0-9-._]+@[\\w0-9-.]+[\\w0-9]{2,3})\u0026#39;,string) These are advanced examples but if you try to understand these examples for yourself you should be fine with the info provided.\n Conclusion  While it might look a little daunting at first, regex provides a great degree of flexibility when it comes to data manipulation, creating features and finding patterns.\nI use it quite regularly when I work with text data and it can also be included while working on data validation tasks.\nI am also a fan of the regex101 tool and use it frequently to check my regexes. I wonder if I would be using regexes as much if not for this awesome tool.\nAlso if you want to \u0026lt;strong\u0026gt;learn more about NLP\u0026lt;/strong\u0026gt; here is an excellent course. You can start for free with the 7-day Free Trial.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/09/01/regex/","tags":["Python","Productivity","Machine Learning","Data Science","Awesome Guides","Best Content","Tools"],"title":"The Ultimate Guide to using the Python regex module"},{"categories":["Data Science","Awesome Guides"],"contents":"I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.\nBut I didn‚Äôt like it and so I left that.\nI made it my goal to move into the analytics and data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of efforts to shift.\nNow, people on social networks ask me how I got started in the data science field. So I thought of giving a definitive answer.\n It is not really impossible to do this but it will take a lot of time and effort. Fortunately, I had an ample supply of both.\n Given below is the way that I took, and any aspiring person could choose to become a self-trained data scientist.\nSome of the courses are not the same I did since some of them don‚Äôt exist and some have been merged into bigger specializations. But I have tried to keep it as similar to my experience as possible.\nAlso, I hope that you don‚Äôt lose hope after seeing the long list. You have to start with one or two courses. The rest will follow with time. Remember we have ample time.\nFollow in order. I have tried to include everything that comes to my mind, including some post links which I think could be beneficial.\n Introduction to Probability and Statistics   \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;Stat 110\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; : The quintessential Probability and Statistics course you gotta take. All the lectures and notes are available on Youtube and his site for free.\nIf not for the content then for Prof. Joseph Blitzstein sense of humor. The above picture is a testament to that.\nI took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that.\nApart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.\nThis is a challenging class for a beginner but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving the proofs quickly. Sometimes the same proof was done in different ways to facilitate the learning of a concept.\nOne of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts.\nThe inclusion of Gambler‚Äôs Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox, etc. made this course much much more exciting and enjoyable than any ordinary Statistics Course.\nIt will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions.\nHe has also got a textbook based on this course, which is an excellent text and a must for any bookshelf.\n   Introduction to Python and Data Science:  Do first, understand later\n We need to get a taste of machine learning before understanding it fully. This segment is made up of three parts. These are not the exact courses I took to learn Python and getting an intro to data science. But they are quite similar and they serve the purpose.\na) Introduction to Data Science in Python  This course is about learning to use Python and creating things on your own. You will learn about Python Libraries like Numpy, Pandas for data science.\nYou might also like my posts on Minimal Pandas for Data Scientists and small shorts on advanced python while going through this course.\nCourse description from Website:\n This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses.\n b) Applied Machine Learning in Python   This course gives an intro to many modern machine learning methods that you should know about. Not a thorough grinding but you will get the tools to build your own models. You will learn scikit-learn, which is the python library to create all sorts of models.\nThe focus here is to start creating things as soon as possible. No one likes to wait too long to get something useful, and you will become useful after this course.\n This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial.\n c) Visualizations  A well made visualization is worth more than any PPT\n One thing you also need to learn about is Visualizations. This is an area which is constantly evolving with a lot of new libraries coming frequently. The libraries I use most are Seaborn and Plotly.\nYou could take a look at the below posts to get started with both basic and advanced visualizations.\n \u0026lt;strong\u0026gt;Python‚Äôs One-Liner graph creation library with animations Hans Rosling Style\u0026lt;/strong\u0026gt;  \u0026lt;strong\u0026gt;3 Awesome Visualization Techniques for every dataset\u0026lt;/strong\u0026gt;  Machine Learning Fundamentals  After doing these above courses, you will gain the status of what I would like to call a ‚ÄúBeginner.‚Äù\nCongrats!!!. You know stuff; you know how to implement things.\nYet you do not fully understand all the math and grind that goes behind all these models.\nYou need to understand what goes behind the clf.fit\n If you don‚Äôt understand it you won‚Äôt be able to improve it\n Here comes the Game Changer Machine Learning course . Contains the maths behind many of the Machine Learning algorithms.\nI will put this course as the one course you gotta take as this course motivated me into getting in this field, and Andrew Ng is a great instructor. Also, this was the first course that I took myself when I started.\nThis course has a little of everything ‚Äî Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.\nAfter this one, you are done with the three musketeers of the trade.\nYou know Python, you understand Statistics, and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D‚Äôartagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that add a new freshness to our data science journey.\nHere comes Big Data for you.\n Big Data Analytics Using Spark   Big Data is omnipresent. Deal with it.\n The whole big data ecosystem has changed a lot since the time I learned Hadoop. And Spark was the new kid on the block at that time. Those days‚Ä¶\nThe courses I took are pretty redundant as of now so I would try to recommend something suitable for this era. The best course I could find that embodies most of what I learned through scattered sources is Big Data Analytics Using Spark .\nFrom the course website, after doing this course, you will learn:\n Programming Spark using Pyspark Identifying the computational tradeoffs in a Spark application Performing data loading and cleaning using Spark and Parquet Modeling data through statistical and machine learning methods  You could also take a look at my recent post on Spark.\n \u0026lt;strong\u0026gt;The Hitchhikers guide to handle Big Data using Spark\u0026lt;/strong\u0026gt;  Understand Linux Shell Not a hard requirement but a good to have skill. Shell is a big friend of data scientists. It allows you to do simple data-related tasks in the terminal itself. I couldn‚Äôt emphasize how much time shell saves for me every day.\nYou can read the below post by me to know about this: \u0026lt;strong\u0026gt;Impress Onlookers with your newly acquired Shell Skills\u0026lt;/strong\u0026gt; If you would like to take a course, you can look at The UNIX workbench course on Coursera.\nCongrats you are a ‚ÄúHacker‚Äù now.\n You have got all the main tools in your belt to be a data scientist.\n On to more advanced topics. From here, it depends on you what you want to learn.\nYou may want to take a totally different approach than what I took going from here. There is no particular order. ‚ÄúAll Roads Lead to Rome‚Äù as long as you are moving.\n Learn Statistical Inference  Mine √áetinkaya-Rundel teaches this course on Inferential Statistics . And it cannot get simpler than this one.\nShe is a great instructor and explains the fundamentals of Statistical inference nicely ‚Äî a must-take course.\nYou will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.\n Deep Learning   It is all about layers\n  \u0026lt;strong\u0026gt;Intro\u0026lt;/strong\u0026gt; ‚Äî Making neural nets uncool again. This is a code-first class for neural nets. An excellent Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.\nAdvanced ‚Äî You can try out this Deep Learning Specialization by Andrew Ng again. Pure Gold.\n \u0026lt;strong\u0026gt;Advanced Math Book\u0026lt;/strong\u0026gt; ‚Äî A math-intensive book by Yoshua Bengio \u0026amp; Ian Goodfellow\nTake a look at below post if you want to learn Pytorch.\n \u0026lt;strong\u0026gt;Moving from Keras to Pytorch\u0026lt;/strong\u0026gt;  Learn NLP, Use Deep Learning with Text and create Chatbots  Reading is overrated. Let the machine do it.\n Natural Language Processing is something which captured my attention a while back.\nI wrote a series of 6 posts on it. If you want, you can take a look.\n \u0026lt;strong\u0026gt;NLP Learning Series ‚Äî Towards Data Science\u0026lt;/strong\u0026gt;  Algorithms, Graph Algorithms, and More   Algorithms. Yes, you need them.\n Apart from that if you want to learn about Python and the underlying intricacies of the language you can take the \u0026lt;strong\u0026gt;Computer Science Mini Specialization from RICE university\u0026lt;/strong\u0026gt; too.\nThis is a series of 6 short but good courses.\nI worked on these courses as Data science will require you to do a lot of programming. And the best way to learn to program is by doing it.\nThe lectures are good, but the problems and assignments are awesome. If you work on this, you will learn Object-Oriented Programming, Graph algorithms, and creating games in Python. Pretty cool stuff.\nYou could also take a look at:\n \u0026lt;strong\u0026gt;The 5 Feature Selection Algorithms every Data Scientist should know\u0026lt;/strong\u0026gt;  \u0026lt;strong\u0026gt;The 5 Sampling Algorithms every Data Scientist need to know\u0026lt;/strong\u0026gt;  Some Advanced Math Topics  Math ‚Äî The power behind it all\n I am writing it last here but don‚Äôt underestimate the importance of Math in Data Science. You might want to look a little into these courses if you want to refresh your concepts.\n \u0026lt;strong\u0026gt;Linear Algebra By Gilbert Strang\u0026lt;/strong\u0026gt; ‚Äî A Great Class by a great Teacher. I would definitely recommend this class to anyone who wants to learn Linear Algebra.\n \u0026lt;strong\u0026gt;Multivariate Calculus\u0026lt;/strong\u0026gt; ‚Äî MIT Open Courseware\n \u0026lt;strong\u0026gt;Convex Optimization\u0026lt;/strong\u0026gt; ‚Äî a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.\n Conclusion  The Machine learning field is evolving, and new advancements are made every day. That‚Äôs why I didn‚Äôt put the third tier.\n The maximum I can call myself is a ‚ÄúHacker,‚Äù and my learning continues.\n Everyone has their own path, and here I provided mine to become a data scientist. And this is in no way perfect as obviously, a lot of things can be added to it.\nThough I did not complete any professional training, I consider myself more of a Computer science engineer than a mechanical engineer now due to the above courses.\nI hope they help you too.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/08/12/resources/","tags":["Big Data","Machine Learning","Data Science","Python","Statistics","Visualization"],"title":"How did I learn Data Science?"},{"categories":["Data Science","Awesome Guides"],"contents":"Data Science is the study of algorithms.\nI grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series .\nHow many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.\nWe sometimes end up using correlation or tree-based methods to find out the important features.\nCan we add some structure to it?\nThis post is about some of the most common feature selection techniques one can use while working with data.\n Why Feature Selection? Before we proceed, we need to answer this question. Why don‚Äôt we give all the features to the ML algorithm and let it decide which feature is important?\nSo there are three reasons why we don‚Äôt:\n1. Curse of dimensionality ‚Äî Overfitting  If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won‚Äôt generalize to the new samples. And thus we learn absolutely nothing.\n2. Occam‚Äôs Razor: We want our models to be simple and explainable. We lose explainability when we have a lot of features.\n3. Garbage In Garbage out: Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.\nAlso, a large number of features make a model bulky, time-taking, and harder to implement in production.\n So What do we do?  We select only useful features.\nFortunately, Scikit-learn has made it pretty much easy for us to make the feature selection. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major buckets\n  Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.\n  Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination\n  Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.\n  So enough of theory let us start with our five feature selection methods.\nWe will try to do this using a dataset to understand it better.\nI am going to be using a football player dataset to find out what makes a good player great?\nDon‚Äôt worry if you don‚Äôt understand football terminologies. I will try to keep it at a minimum.\nHere is the Kaggle Kernel with the code to try out yourself.\n Some Simple Data Preprocessing We have done some basic preprocessing such as removing Nulls and one hot encoding. And converting the problem to a classification problem using:\ny = traindf['Overall']\u0026gt;=87  Here we use High Overall as a proxy for a great player.\nOur dataset(X) looks like below and has 223 columns.\n  1. Pearson Correlation  This is a filter-based method.\nWe check the absolute value of the Pearson‚Äôs correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.\ndef cor_selector(X, y,num_feats): cor_list = [] feature_name = X.columns.tolist() # calculate the correlation with y for each feature for i in X.columns.tolist(): cor = np.corrcoef(X[i], y)[0, 1] cor_list.append(cor) # replace NaN with 0 cor_list = [0 if np.isnan(i) else i for i in cor_list] # feature name cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist() # feature selection? 0 for not select, 1 for select cor_support = [True if i in cor_feature else False for i in feature_name] return cor_support, cor_feature cor_support, cor_feature = cor_selector(X, y,num_feats) print(str(len(cor_feature)), \u0026#39;selected features\u0026#39;)  2. Chi-Squared This is another filter-based method.\nIn this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.\n Let us create a small example of how we calculate the chi-squared statistic for a sample.\nSo let‚Äôs say we have 75 Right-Forwards in our dataset and 25 Non-Right-Forwards. We observe that 40 of the Right-Forwards are good, and 35 are not good. Does this signify that the player being right forward affects the overall performance?\n We calculate the chi-squared value:\nTo do this, we first find out the values we would expect to be falling in each bucket if there was indeed independence between the two categorical variables.\nThis is simple. We multiply the row sum and the column sum for each cell and divide it by total observations.\nso Good and NotRightforward Bucket Expected value= 25(Row Sum)*60(Column Sum)/100(Total Observations)\nWhy is this expected? Since there are 25% notRightforwards in the data, we would expect 25% of the 60 good players we observed in that cell. Thus 15 players.\nThen we could just use the below formula to sum over all the 4 cells:\n I won‚Äôt show it here, but the chi-squared statistic also works in a hand-wavy way with non-negative numerical and categorical features.\nWe can get chi-squared features from our dataset as:\nfrom sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from sklearn.preprocessing import MinMaxScaler X_norm = MinMaxScaler().fit_transform(X) chi_selector = SelectKBest(chi2, k=num_feats) chi_selector.fit(X_norm, y) chi_support = chi_selector.get_support() chi_feature = X.loc[:,chi_support].columns.tolist() print(str(len(chi_feature)), \u0026#39;selected features\u0026#39;)  3. Recursive Feature Elimination  This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.\nFrom sklearn Documentation:\n The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n As you would have guessed, we could use any estimator with the method. In this case, we use LogisticRegression, and the RFE observes the coef_ attribute of the LogisticRegression object\nfrom sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5) rfe_selector.fit(X_norm, y) rfe_support = rfe_selector.get_support() rfe_feature = X.loc[:,rfe_support].columns.tolist() print(str(len(rfe_feature)), \u0026#39;selected features\u0026#39;)  4. Lasso: SelectFromModel  This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.\nFor example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.\nHere we use Lasso to select variables.\nfrom sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\u0026#34;l1\u0026#34;), max_features=num_feats) embeded_lr_selector.fit(X_norm, y) embeded_lr_support = embeded_lr_selector.get_support() embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist() print(str(len(embeded_lr_feature)), \u0026#39;selected features\u0026#39;)  5. Tree-based: SelectFromModel  This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.\nWe can also use RandomForest to select features based on feature importance.\nWe calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.\nfrom sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats) embeded_rf_selector.fit(X, y) embeded_rf_support = embeded_rf_selector.get_support() embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist() print(str(len(embeded_rf_feature)), \u0026#39;selected features\u0026#39;) We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.\nfrom sklearn.feature_selection import SelectFromModel from lightgbm import LGBMClassifier lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40) embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats) embeded_lgb_selector.fit(X, y) embeded_lgb_support = embeded_lgb_selector.get_support() embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist() print(str(len(embeded_lgb_feature)), \u0026#39;selected features\u0026#39;)  Bonus  Why use one, when we can have all?\nThe answer is sometimes it won‚Äôt be possible with a lot of data and time crunch.\nBut whenever possible, why not do this?\n# put all selection together feature_selection_df = pd.DataFrame({\u0026#39;Feature\u0026#39;:feature_name, \u0026#39;Pearson\u0026#39;:cor_support, \u0026#39;Chi-2\u0026#39;:chi_support, \u0026#39;RFE\u0026#39;:rfe_support, \u0026#39;Logistics\u0026#39;:embeded_lr_support, \u0026#39;Random Forest\u0026#39;:embeded_rf_support, \u0026#39;LightGBM\u0026#39;:embeded_lgb_support}) # count the selected times for each feature feature_selection_df[\u0026#39;Total\u0026#39;] = np.sum(feature_selection_df, axis=1) # display the top 100 feature_selection_df = feature_selection_df.sort_values([\u0026#39;Total\u0026#39;,\u0026#39;Feature\u0026#39;] , ascending=False) feature_selection_df.index = range(1, len(feature_selection_df)+1)  We check if we get a feature based on all the methods. In this case, as we can see Reactions and LongPassing are excellent attributes to have in a high rated player. And as expected Ballcontrol and Finishing occupy the top spot too.\n Conclusion  Feature engineering and feature selection are critical parts of any machine learning pipeline.\nWe strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.\nIn this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.\nI also tried to provide some intuition into these methods, but you should probably try to see more into it and try to incorporate these methods into your work.\nDo read my post on feature engineering too if you are interested.\n If you want to learn more about Data Science, I would like to call out this \u0026lt;strong\u0026gt;excellent course\u0026lt;/strong\u0026gt; by Andrew Ng. This was the one that got me started. Do check it out.\n Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/08/07/feature_selection/","tags":["Machine Learning","Data Science","Statistics","Awesome Guides","Math"],"title":"The 5 Feature Selection Algorithms every Data Scientist should know"},{"categories":["Data Science","Awesome Guides"],"contents":"Data Science is the study of algorithms.\nI grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.\nThis post is about some of the most common sampling techniques one can use while working with data.\nSimple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.\nBelow we select 100 sample points from a dataset.\nsample_df = df.sample(100)  Stratified Sampling  Assume that we need to estimate the average number of votes for each candidate in an election. Assume that the country has 3 towns:\nTown A has 1 million factory workers,\nTown B has 2 million workers, and\nTown C has 3 million retirees.\nWe can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation.\nInstead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of the sample.\nYou can do something like this pretty easily with Python:\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)  Reservoir Sampling  I love this problem statement:\nSay you have a stream of items of large and unknown length that we can only iterate over once.\nCreate an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.\nHow can we do that?\nLet us assume we have to sample 5 objects out of an infinite stream such that each element has an equal probability of getting selected.\nimport random def generator(max): number = 1 while number \u0026lt; max: number += 1 yield number # Create as stream generator stream = generator(10000) # Doing Reservoir Sampling from the stream k=5 reservoir = [] for i, element in enumerate(stream): if i+1\u0026lt;= k: reservoir.append(element) else: probability = k/(i+1) if random.random() \u0026lt; probability: # Select item in stream and remove one of the k items already selected reservoir[random.choice(range(0,k))] = element print(reservoir) [1369, 4108, 9986, 828, 5589]  It can be mathematically proved that in the sample each element has the same probability of getting selected from the stream.\nHow?\nIt always helps to think of a smaller problem when it comes to mathematics.\nSo, let us think of a stream of only 3 items and we have to keep 2 of them.\nWe see the first item, we hold it in the list as our reservoir has space. We see the second item, we hold it in the list as our reservoir has space.\nWe see the third item. Here is where things get interesting. We choose the third item to be in the list with probability 2/3.\nLet us now see the probability of first item getting selected:\nThe probability of removing the first item is the probability of element 3 getting selected multiplied by the probability of Element 1 getting randomly chosen as the replacement candidate from the 2 elements in the reservoir. That probability is:\n2/3*1/2 = 1/3\nThus the probability of 1 getting selected is:\n1‚Äì1/3 = 2/3\nWe can have the exact same argument for the Second Element and we can extend it for many elements.\nThus each item has the same probability of getting selected: 2/3 or in general k/n\n Random Undersampling and Oversampling  It is too often that we encounter an imbalanced dataset.\nA widely adopted technique for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).\nLet us first create some example imbalanced data.\nfrom sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[\u0026#39;target\u0026#39;] = y We can now do random oversampling and undersampling using:\nnum_0 = len(X[X[\u0026#39;target\u0026#39;]==0]) num_1 = len(X[X[\u0026#39;target\u0026#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[\u0026#39;target\u0026#39;]==0].sample(num_1) , X[X[\u0026#39;target\u0026#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[\u0026#39;target\u0026#39;]==0] , X[X[\u0026#39;target\u0026#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180   Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.\nIt provides a variety of methods to undersample and oversample.\na. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.\nIn this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier.\n from imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=\u0026#39;majority\u0026#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.\n from imblearn.over_sampling import SMOTE smote = SMOTE(ratio=\u0026#39;minority\u0026#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.\n Conclusion  Algorithms are the lifeblood of data science.\nSampling is an important topic in data science and we really don‚Äôt talk about it as much as we should.\nA good sampling strategy sometimes could pull the whole project forward. A bad sampling strategy could give us incorrect results. So one should be careful while selecting a sampling strategy.\nSo use sampling, be it at work or at bars.\nIf you want to learn more about Data Science, I would like to call out this \u0026lt;em\u0026gt;\u0026lt;strong\u0026gt;excellent course\u0026lt;/strong\u0026gt;\u0026lt;/em\u0026gt; by Andrew Ng. This was the one that got me started. Do check it out.\nThanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/07/30/sampling/","tags":["Machine Learning","Data Science","Statistics","Algorithms","Math"],"title":"The 5 Sampling Algorithms every Data Scientist need to know"},{"categories":["Data Science"],"contents":"Exploration and Exploitation play a key role in any business.\nAnd any good business will try to ‚Äúexplore‚Äù various opportunities where it can make a profit.\nAny good business at the same time also tries to focus on a particular opportunity it has found already and tries to ‚Äúexploits‚Äù it.\nLet me explain this further with a thought experiment.\nThought Experiment: Assume that we have infinite slot machines. Every slot machine has some win probability. But we don‚Äôt know these probability values.\nYou have to operate these slot machines one by one. How do you come up with a strategy to maximize your outcome from these slot machines in minimum time.\n You will most probably start by trying out some machines.\nWould you stick to a particular machine that has an okayish probability(exploitation) or would you keep searching for better machines(exploration)?\nIt is the exploration-exploitation tradeoff.\nAnd the question is how do we balance this tradeoff such that we get maximum profits?\nThe answer is Bayesian Bandits.\n Why? Business Use-cases: There are a lot of places where such a thought experiment could fit.\n  AB Testing: You have a variety of assets that you can show at the website. Each asset has a particular probability of success(getting clicked by the user).\n  Ad Clicks: You have a variety of ads that you can show to the user. Each advert has a particular probability of clickthrough\n  Finance: which stock pick gives the highest return.\n  We as human beings are faced with the exact same problem ‚Äî Explore or exploit and we handle it quite brilliantly mostly. Should we go find a new job or should we earn money doing the thing we know would give us money?\n  In this post, we will focus on AB Testing but this experiment will work for any of the above problems.\n Problem Statement:  The problem we have is that we have different assets which we want to show on our awesome website but we really don‚Äôt know which one to show.\nOne asset is blue(B), another red(R) and the third one green(G).\nOur UX team say they like the blue one. But you like the green one.\nWhich one to show on our website?\n Bayes Everywhere: Before we delve down into the algorithm we will use to solve this, let us revisit the Bayes theorem.\n Just remember that Bayes theorem says that Posterior ~ likelihood*Prior\n Beta Distribution The beta distribution is a continuous probability distribution defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by Œ± and Œ≤.The PDF of the beta distribution is:\n And, the pdf looks like below for different values of Œ± and Œ≤:\n The beta distribution is frequently applied to model the behavior of probabilities as it lies in the range [0,1]. The beta distribution is a suitable model for the random behavior of percentages and proportions too.\n Bayesian Bandits So after knowing the above concepts let us come back to our present problem.\nWe have three assets.\n For the sake of this problem, let\u0026rsquo;s assume that we know the click probabilities of these assets also.\nThe win probability of blue is 0.3, red is 0.8 and green is 0.4. Please note that in real life, we won‚Äôt know this.\nThese probabilities are going to be hidden from our algorithm and we will see how our algorithm will still converge to these real probabilities.\nSo, what are our priors(beliefs) about the probability of each of these assets?\nSince we have not observed any data we cannot have prior beliefs about any of our three assets.\nWe need to model our prior probabilities and we will use beta distribution to do that. See the curve for beta distribution above for Œ± = 1 and Œ≤=1.\nIt is actually just a uniform distribution over the range [0,1]. And that is what we want for our prior probabilities for our assets. We don‚Äôt have any information yet so we start with a uniform probability distribution over our probability values.\nSo we can denote the prior probabilities of each of our asset using a beta distribution.\nStrategy:   We will sample a random variable from each of the 3 distributions for assets.\n  We will find out which random variable is maximum and will show the one asset which gave the maximum random variable.\n  We will get to know if that asset is clicked or not.\n  We will update the prior for the asset using the information in step 3.\n  Repeat.\n  Updating the Prior: The reason we took beta distribution to model our probabilities is because of its great mathematical properties.\n If the prior is f(Œ±,Œ≤), then the posterior distribution is again beta, given by f(Œ±+#success, Œ≤+#failures)\nwhere #success is the number of clicks and #failures are the number of views minus the number of clicks.\n Let us Code  We have every bit of knowledge we require for writing some code now. I will be using pretty much simple and standard Python functionality to do this but there exist tools like pyMC and such for this sort of problem formulations.\nLet us work through this problem step by step.\nWe have three assets with different probabilities.\nreal_probs_dict = {\u0026#39;R\u0026#39;:0.8,\u0026#39;G\u0026#39;:0.4,\u0026#39;B\u0026#39;:0.3} assets = [\u0026#39;R\u0026#39;,\u0026#39;G\u0026#39;,\u0026#39;B\u0026#39;] We will be trying to see if our strategy given above works or not.\n\u0026#39;\u0026#39;\u0026#39; This function takes as input three tuples for alpha,beta that specify priorR,priorG,priorB And returns R,G,B along with the maximum value sampled from these three distributions. We can sample from a beta distribution using scipy. \u0026#39;\u0026#39;\u0026#39; def find_asset(priorR,priorG,priorB): red_rv = scipy.stats.beta.rvs(priorR[0],priorR[1]) green_rv = scipy.stats.beta.rvs(priorG[0],priorG[1]) blue_rv = scipy.stats.beta.rvs(priorB[0],priorB[1]) return assets[np.argmax([red_rv,green_rv,blue_rv])] \u0026#39;\u0026#39;\u0026#39; This is a helper function that simulates the real world using the actual probability value of the assets. In real life we won\u0026#39;t have this function and our user click input will be the proxy for this function. \u0026#39;\u0026#39;\u0026#39; def simulate_real_website(asset, real_probs_dict): #simulate a coin toss with probability. Asset clicked or not. if real_probs_dict[asset]\u0026gt; scipy.stats.uniform.rvs(0,1): return 1 else: return 0 \u0026#39;\u0026#39;\u0026#39; This function takes as input the selected asset and returns the posteriors for the selected asset. \u0026#39;\u0026#39;\u0026#39; def update_posterior(asset,priorR,priorG,priorB,outcome): if asset==\u0026#39;R\u0026#39;: priorR=(priorR[0]+outcome,priorR[1]+1-outcome) elif asset==\u0026#39;G\u0026#39;: priorG=(priorG[0]+outcome,priorG[1]+1-outcome) elif asset==\u0026#39;B\u0026#39;: priorB=(priorB[0]+outcome,priorB[1]+1-outcome) return priorR,priorG,priorB \u0026#39;\u0026#39;\u0026#39; This function runs the strategy once. \u0026#39;\u0026#39;\u0026#39; def run_strategy_once(priorR,priorG,priorB): # 1. get the asset asset = find_asset(priorR,priorG,priorB) # 2. get the outcome from the website/users outcome = simulate_real_website(asset, real_probs_dict) # 3. update prior based on outcome priorR,priorG,priorB = update_posterior(asset,priorR,priorG,priorB,outcome) return asset,priorR,priorG,priorB Let us run this strategy multiple times and collect the data.\npriorR,priorG,priorB = (1,1),(1,1),(1,1) data = [(\u0026#34;_\u0026#34;,priorR,priorG,priorB)] for i in range(50): asset,priorR,priorG,priorB = run_strategy_once(priorR,priorG,priorB) data.append((asset,priorR,priorG,priorB)) This is the result of our runs. You can see the functions I used to visualize the posterior distributions here at kaggle . As you can see below, we have pretty much converged to the best asset by the end of 20 runs. And the probabilities are also estimated roughly what they should be.\nAt the start, we have a uniform prior. As we go through with the runs we see that the ‚Äúred‚Äù asset‚Äôs posterior distribution converges towards a higher mean and as such a higher probability of pick. But remember that doesn‚Äôt mean that the green asset and blue asset are not going to be picked ever.\n Let us also see how many times each asset is picked in the 50 runs we did.\nPick 1 : G ,Pick 2 : R ,Pick 3 : R ,Pick 4 : B ,Pick 5 : R ,Pick 6 : R ,Pick 7 : R ,Pick 8 : R ,Pick 9 : R ,Pick 10 : R ,Pick 11 : R ,Pick 12 : R ,Pick 13 : R ,Pick 14 : R ,Pick 15 : R ,Pick 16 : R ,Pick 17 : R ,Pick 18 : R ,Pick 19 : R ,Pick 20 : R ,Pick 21 : R ,Pick 22 : G ,Pick 23 : R ,Pick 24 : R ,Pick 25 : G ,Pick 26 : G ,Pick 27 : R ,Pick 28 : R ,Pick 29 : R ,Pick 30 : R ,Pick 31 : R ,Pick 32 : R ,Pick 33 : R ,Pick 34 : R ,Pick 35 : R ,Pick 36 : R ,Pick 37 : R ,Pick 38 : R ,Pick 39 : G ,Pick 40 : B ,Pick 41 : R ,Pick 42 : R ,Pick 43 : R ,Pick 44 : R ,Pick 45 : B ,Pick 46 : R ,Pick 47 : R ,Pick 48 : R ,Pick 49 : R ,Pick 50 : R  We can see that although we are mostly picking R we still end up sometimes picking B(Pick 45) and G(Pick 44) in the later runs too. Overall we can see that in the first few runs, we are focussing on exploration and as we go towards later runs we focus on exploitation.\n End Notes: We saw how solving this problem using the Bayesian approach could help us converge to a good solution while maximizing our profit and not discarding any asset.\nAn added advantage of such an approach is that it is self-learning and could self-correct by itself if the probability of click on the red decreases and blue asset increases. This case might happen when the user preferences change for example.\nThe whole code is posted in the Kaggle Kernel .\nAlso, if you want to learn more about Bayesian Statistics, one of the newest and best resources that you can keep an eye on is the \u0026lt;strong\u0026gt;Bayesian Methods for Machine Learning\u0026lt;/strong\u0026gt; course in the \u0026lt;strong\u0026gt;Advanced machine learning specialization\u0026lt;/strong\u0026gt; I am going to be writing more of such posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/07/21/bandits/","tags":["Data Science","Statistics","Algorithms"],"title":"Bayesian Bandits explained simply"},{"categories":["Awesome Guides","Data Science"],"contents":"Pandas is a vast library.\nData manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.\nStill, I generally have some issues with it.\nThere are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.\nThis has inspired me to come up with a minimal subset of pandas functions I use while coding.\nI have tried it all, and currently, I stick to a particular way. It is like a mind map.\nSometimes because it is fast and sometimes because it‚Äôs more readable and sometimes because I can do it with my current knowledge. And sometimes because I know that a particular way will be a headache in the long run(think multi-index)\nThis post is about handling most of the data manipulation cases in Python using a straightforward, simple, and matter of fact way.\nWith a sprinkling of some recommendations throughout.\nI will be using a data set of 1,000 popular movies on IMDB in the last ten years. You can also follow along in the Kaggle Kernel .\n Some Default Pandas Requirements  As good as the Jupyter notebooks are, some things still need to be specified when working with Pandas.\n***Sometimes your notebook won‚Äôt show you all the columns. Sometimes it will display all the rows if you print the dataframe. ***You can control this behavior by setting some defaults of your own while importing Pandas. You can automate it using this addition to your notebook.\nFor instance, this is the setting I use.\nimport pandas as pd # pandas defaults pd.options.display.max_columns = 500 pd.options.display.max_rows = 500  Reading Data with Pandas  The first thing we do is reading the data source and so here is the code for that.\ndf = pd.read_csv(\u0026#34;IMDB-Movie-Data.csv\u0026#34;) Recommendation: I could also have used pd.read_table to read the file. The thing is that pd.read_csv has default separator as , and thus it saves me some code. I also genuinely don‚Äôt understand the use of pd.read_table\nIf your data is in some SQL Datasource, you could have used the following code. You get the results in the dataframe format.\n# Reading from SQL Datasource import MySQLdb from pandas import DataFrame from pandas.io.sql import read_sql db = MySQLdb.connect(host=\u0026#34;localhost\u0026#34;, # your host, usually localhost user=\u0026#34;root\u0026#34;, # your username passwd=\u0026#34;password\u0026#34;, # your password db=\u0026#34;dbname\u0026#34;) # name of the data base query = \u0026#34;SELECT * FROM tablename\u0026#34; df = read_sql(query, db)  Data Snapshot  Always useful to see some of the data.\nYou can use simple head and tail commands with an option to specify the number of rows.\n# top 5 rows df.head() # top 50 rows df.head(50) # last 5 rows df.tail() # last 50 rows df.tail(50) You can also see simple dataframe statistics with the following commands.\n# To get statistics of numerical columns df.describe()  # To get maximum value of a column. When you take a single column you can think of it as a list and apply functions you would apply to a list. You can also use min for instance. print(max(df[\u0026#39;rating\u0026#39;])) # no of rows in dataframe print(len(df)) # Shape of Dataframe print(df.shape) 9.0 1000 (1000,12)  Recommendation: Generally working with Jupyter notebook,I make it a point of having the first few cells in my notebook containing these snapshots of the data. This helps me see the structure of the data whenever I want to. If I don‚Äôt follow this practice, I notice that I end up repeating the .head() command a lot of times in my code.\n Handling Columns in Dataframes  a. Selecting a column For some reason Pandas lets you choose columns in two ways. Using the dot operator like df.Title and using square brackets like df['Title']\nI prefer the second version, mostly. Why?\nThere are a couple of reasons you would be better off with the square bracket version in the longer run.\n  If your column name contains spaces, then the dot version won‚Äôt work. For example, df.Revenue (Millions) won‚Äôt work while df['Revenue (Millions)'] will.\n  It also won‚Äôt work if your column name is count or mean or any of pandas predefined functions.\n  Sometimes you might need to create a for loop over your column names in which your column name might be in a variable. In that case, the dot notation will not work. For Example, This works:\n  colname = \u0026#39;height\u0026#39; df[colname] While this doesn‚Äôt:\ncolname = \u0026#39;height\u0026#39; df.colname Trust me. Saving a few characters is not worth it.\nRecommendation: Stop using the dot operator. It is a construct that originated from a different language(R) and respectfully should be left there.\nb. Getting Column Names in a list You might need a list of columns for some later processing.\ncolumnnames = df.columns c. Specifying user-defined Column Names: Sometimes you want to change the column names as per your taste. I don‚Äôt like spaces in my column names, so I change them as such.\ndf.columns = [\u0026#39;Rank\u0026#39;, \u0026#39;Title\u0026#39;, \u0026#39;Genre\u0026#39;, \u0026#39;Description\u0026#39;, \u0026#39;Director\u0026#39;, \u0026#39;Actors\u0026#39;, \u0026#39;Year\u0026#39;, \u0026#39;Runtime_Minutes\u0026#39;, \u0026#39;Rating\u0026#39;, \u0026#39;Votes\u0026#39;, \u0026#39;Revenue_Millions\u0026#39;, \u0026#39;Metascore\u0026#39;] I could have used another way.\nThis is the one case where both of the versions are important. When I have to change a lot of column names, I go with the way above. When I have to change the name of just one or two columns I use:\ndf.rename(columns = {\u0026#39;Revenue (Millions)\u0026#39;:\u0026#39;Rev_M\u0026#39;,\u0026#39;Runtime (Minutes)\u0026#39;:\u0026#39;Runtime_min\u0026#39;},inplace=True) d. Subsetting specific columns: Sometimes you only need to work with particular columns in a dataframe. e.g., to separate numerical and categorical columns, or remove unnecessary columns. Let‚Äôs say in our example; we don‚Äôt need the description, director, and actor column.\ndf = df[[\u0026#39;Rank\u0026#39;, \u0026#39;Title\u0026#39;, \u0026#39;Genre\u0026#39;, \u0026#39;Year\u0026#39;,\u0026#39;Runtime_min\u0026#39;, \u0026#39;Rating\u0026#39;, \u0026#39;Votes\u0026#39;, \u0026#39;Rev_M\u0026#39;, \u0026#39;Metascore\u0026#39;]] e. Seeing column types: Very useful while debugging. If your code throws an error that you cannot add a str and int, you will like to run this command.\ndf.dtypes  Applying Functions on DataFrame: Apply and Lambda  apply and lambda are some of the best things I have learned to use with pandas.\nI use apply and lambda anytime I get stuck while building a complex logic for a new column or filter.\na. Creating a Column You can create a new column in many ways.\nIf you want a column that is a sum or difference of columns, you can pretty much use simple basic arithmetic. Here I get the average rating based on IMDB and Normalized Metascore.\ndf[\u0026#39;AvgRating\u0026#39;] = (df[\u0026#39;Rating\u0026#39;] + df[\u0026#39;Metascore\u0026#39;]/10)/2 But sometimes we may need to build complex logic around the creation of new columns.\nTo give you a convoluted example, let‚Äôs say that we want to build a custom movie score based on a variety of factors.\nSay, If the movie is of the thriller genre, I want to add 1 to the IMDB rating subject to the condition that IMDB rating remains less than or equal to 10. And If a movie is a comedy I want to subtract one from the rating.\nHow do we do that?\nWhenever I get a hold of such complex problems, I use apply/lambda. Let me first show you how I will do this.\ndef custom_rating(genre,rating): if \u0026#39;Thriller\u0026#39; in genre: return min(10,rating+1) elif \u0026#39;Comedy\u0026#39; in genre: return max(0,rating-1) else: return rating df[\u0026#39;CustomRating\u0026#39;] = df.apply(lambda x: custom_rating(x[\u0026#39;Genre\u0026#39;],x[\u0026#39;Rating\u0026#39;]),axis=1) The general structure is:\n  You define a function that will take the column values you want to play with to come up with your logic. Here the only two columns we end up using are genre and rating.\n  You use an apply function with lambda along the row with axis=1. The general syntax is:\n  df.apply(lambda x: func(x[\u0026#39;col1\u0026#39;],x[\u0026#39;col2\u0026#39;]),axis=1) You should be able to create pretty much any logic using apply/lambda since you just have to worry about the custom function.\nb. Filtering a dataframe  Pandas make filtering and subsetting dataframes pretty easy. You can filter and subset dataframes using normal operators and \u0026amp;,|,~ operators.\n# Single condition: dataframe with all movies rated greater than 8 df_gt_8 = df[df[\u0026#39;Rating\u0026#39;]\u0026gt;8] # Multiple conditions: AND - dataframe with all movies rated greater than 8 and having more than 100000 votes And_df = df[(df[\u0026#39;Rating\u0026#39;]\u0026gt;8) \u0026amp; (df[\u0026#39;Votes\u0026#39;]\u0026gt;100000)] # Multiple conditions: OR - dataframe with all movies rated greater than 8 or having a metascore more than 90 Or_df = df[(df[\u0026#39;Rating\u0026#39;]\u0026gt;8) | (df[\u0026#39;Metascore\u0026#39;]\u0026gt;80)] # Multiple conditions: NOT - dataframe with all emovies rated greater than 8 or having a metascore more than 90 have to be excluded Not_df = df[~((df[\u0026#39;Rating\u0026#39;]\u0026gt;8) | (df[\u0026#39;Metascore\u0026#39;]\u0026gt;80))] Pretty simple stuff.\nBut sometimes we may need to do complex filtering operations.\nAnd sometimes we need to do some operations which we won‚Äôt be able to do using just the above format.\nFor instance: Let us say we want to filter those rows where the number of words in the movie title is greater than or equal to than 4.\nHow would you do it?\nTrying the below will give you an error. Apparently, you cannot do anything as simple as split with a series.\nnew_df = df[len(df[\u0026#39;Title\u0026#39;].split(\u0026#34; \u0026#34;))\u0026gt;=4] AttributeError: 'Series' object has no attribute 'split'  One way is first to create a column which contains no of words in the title using apply and then filter on that column.\n#create a new column df[\u0026#39;num_words_title\u0026#39;] = df.apply(lambda x : len(x[\u0026#39;Title\u0026#39;].split(\u0026#34; \u0026#34;)),axis=1) #simple filter on new column new_df = df[df[\u0026#39;num_words_title\u0026#39;]\u0026gt;=4] And that is a perfectly fine way as long as you don‚Äôt have to create a lot of columns. But I prefer this:\nnew_df = df[df.apply(lambda x : len(x[\u0026#39;Title\u0026#39;].split(\u0026#34; \u0026#34;))\u0026gt;=4,axis=1)] What I did here is that my apply function returns a boolean which can be used to filter.\nNow once you understand that you just have to create a column of booleans to filter, you can use any function/logic in your apply statement to get however complex a logic you want to build.\nLet us see another example. I will try to do something a little complex to show the structure.\nWe want to find movies for which the revenue is less than the average revenue for that particular year?\nyear_revenue_dict = df.groupby([\u0026#39;Year\u0026#39;]).agg({\u0026#39;Rev_M\u0026#39;:np.mean}).to_dict()[\u0026#39;Rev_M\u0026#39;] def bool_provider(revenue, year): return revenue\u0026lt;year_revenue_dict[year] new_df = df[df.apply(lambda x : bool_provider(x[\u0026#39;Rev_M\u0026#39;],x[\u0026#39;Year\u0026#39;]),axis=1)] We have a function here which we can use to write any logic. That provides a lot of power for advanced filtering as long as we can play with simple variables.\nc. Change Column Types I even use apply to change the column types since I don‚Äôt want to remember the syntax for changing column type and also since it lets me do much more complicated things.\nThe usual syntax to change column type is astype in Pandas. So if I had a column named price in my data in an str format. I could do this:\ndf[\u0026#39;Price\u0026#39;] = newDf[\u0026#39;Price\u0026#39;].astype(\u0026#39;int\u0026#39;) But sometimes it won‚Äôt work as expected.\nYou might get the error: ValueError: invalid literal for long() with base 10: ‚Äò13,000‚Äô. That is you cannot cast a string with ‚Äú,‚Äù to an int. To do that we first have to get rid of the comma.\nAfter facing this problem time and again, I have stopped using astype altogether now and just use apply to change column types.\ndf[\u0026#39;Price\u0026#39;] = df.apply(lambda x: int(x[\u0026#39;Price\u0026#39;].replace(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;)),axis=1) And lastly, there is progress_apply  progress_apply is a function that comes with tqdm package.\nAnd this has saved me a lot of time.\nSometimes when you have got a lot of rows in your data, or you end up writing a pretty complex apply function, you will see that apply might take a lot of time.\nI have seen apply taking hours when working with Spacy. In such cases, you might like to see the progress bar with apply.\nYou can use tqdm for that.\nAfter the initial imports at the top of your notebook, just replace apply with progress_apply and everything remains the same.\nfrom tqdm import tqdm, tqdm_notebook tqdm_notebook().pandas() df.progress_apply(lambda x: custom_rating_function(x[\u0026#39;Genre\u0026#39;],x[\u0026#39;Rating\u0026#39;]),axis=1) And you get progress bars.\n ***Recommendation:***vWhenever you see that you have to create a column with custom complex logic, think of apply and lambda. Try using progress_apply too.\n Aggregation on Dataframes: groupby  groupby will come up a lot of times whenever you want to aggregate your data. Pandas lets you do this efficiently with the groupby function.\nThere are a lot of ways that you can use groupby. I have seen a lot of versions, but I prefer a particular style since I feel the version I use is easy, intuitive, and scalable for different use cases.\ndf.groupby(list of columns to groupby on).aggregate({\u0026#39;colname\u0026#39;:func1, \u0026#39;colname2\u0026#39;:func2}).reset_index() Now you see it is pretty simple. You just have to worry about supplying two primary pieces of information.\n  List of columns to groupby on, and\n  A dictionary of columns and functions you want to apply to those columns\n  reset_index() is a function that resets the index of a dataframe. I apply this function ALWAYS whenever I do a groupby, and you might think of it as a default syntax for groupby operations.\nLet us check out an example.\n# Find out the sum of votes and revenue by year import numpy as np df.groupby([\u0026#39;Year\u0026#39;]).aggregate({\u0026#39;Votes\u0026#39;:np.sum, \u0026#39;Rev_M\u0026#39;:np.sum}).reset_index()  You might also want to group by more than one column. It is fairly straightforward.\ndf.groupby([\u0026#39;Year\u0026#39;,\u0026#39;Genre\u0026#39;]).aggregate({\u0026#39;Votes\u0026#39;:np.sum, \u0026#39;Rev_M\u0026#39;:np.sum}).reset_index()  Recommendation: Stick to one syntax for groupby. Pick your own if you don‚Äôt like mine but stick to one.\n Dealing with Multiple Dataframes: Concat and Merge:  a. concat Sometimes we get data from different sources. Or someone comes to you with multiple files with each file having data for a particular year.\nHow do we create a single dataframe from a single dataframe?\nHere we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic filter operations we already know.\nmovies_2006 = df[df[\u0026#39;Year\u0026#39;]==2006] movies_2007 = df[df[\u0026#39;Year\u0026#39;]==2007] Here we start with two dataframes: movies_2006 containing info for movies released in 2006 and movies_2007 containing info for movies released in 2007. We want to create a single dataframe that includes movies from both 2006 and 2007\nmovies_06_07 = pd.concat([movies_2006,movies_2007]) b. merge Most of the data that you will encounter will never come in a single file. One of the files might contain ratings for a particular movie, and another might provide the number of votes for a movie.\nIn such a case we have two dataframes which need to be merged so that we can have all the information in a single view.\nHere we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic column subset operations we already know.\nrating_dataframe = df[[\u0026#39;Title\u0026#39;,\u0026#39;Rating\u0026#39;]] votes_dataframe = df[[\u0026#39;Title\u0026#39;,\u0026#39;Votes\u0026#39;]]  We need to have all this information in a single dataframe. How do we do this?\nrating_vote_df = pd.merge(rating_dataframe,votes_dataframe,on=\u0026#39;Title\u0026#39;,how=\u0026#39;left\u0026#39;) rating_vote_df.head()  We provide this merge function with four attributes- 1st DF, 2nd DF, join on which column and the joining criteria:['left','right','inner','outer']\nRecommendation: I usually always end up using left join. You will rarely need to join using outer or right. Actually whenever you need to do a right join you actually just really need a left join with the order of dataframes reversed in the merge function.\n Reshaping Dataframes: Melt and pivot_table(reverseMelt)  Most of the time, we don‚Äôt get data in the exact form we want.\nFor example, sometimes we might have data in columns which we might need in rows.\nLet us create an artificial example again. You can look at the code below that I use to create the example, but really it doesn‚Äôt matter.\ngenre_set = set() for genre in df[\u0026#39;Genre\u0026#39;].unique(): for g in genre.split(\u0026#34;,\u0026#34;): genre_set.add(g) for genre in genre_set: df[genre] = df[\u0026#39;Genre\u0026#39;].apply(lambda x: 1 if genre in x else 0) working_df = df[[\u0026#39;Title\u0026#39;,\u0026#39;Rating\u0026#39;, \u0026#39;Votes\u0026#39;, \u0026#39;Rev_M\u0026#39;]+list(genre_set)] working_df.head() So we start from a working_df like this:\n Now, this is not particularly a great structure to have data in. We might like it better if we had a dataframe with only one column Genre and we can have multiple rows repeated for the same movie. So the movie ‚ÄòPrometheus‚Äô might be having three rows since it has three genres. How do we make that work?\nWe use melt:\nreshaped_df = pd.melt(working_df,id_vars = [\u0026#39;Title\u0026#39;,\u0026#39;Rating\u0026#39;,\u0026#39;Votes\u0026#39;,\u0026#39;Rev_M\u0026#39;],value_vars = list(genre_set),var_name = \u0026#39;Genre\u0026#39;, value_name =\u0026#39;Flag\u0026#39;) reshaped_df.head()  So in this melt function, we provided five attributes:\n  dataframe_name = working_df\n  id_vars: List of vars we want in the current form only.\n  value_vars: List of vars we want to melt/put in the same column\n  var_name: name of the column for value_vars\n  value_name: name of the column for value of value_vars\n  There is still one thing remaining. For Prometheus, we see that it is a thriller and the flag is 0. The flag 0 is unnecessary data we can filter out, and we will have our results. We keep only the genres with flag 1\nreshaped_df = reshaped_df[reshaped_df[\u0026#39;Flag\u0026#39;]==1]  What if we want to go back?\nWe need the values in a column to become multiple columns. How? We use pivot_table\nre_reshaped_df = reshaped_df.pivot_table(index=[\u0026#39;Title\u0026#39;,\u0026#39;Rating\u0026#39;,\u0026#39;Votes\u0026#39;,\u0026#39;Rev_M\u0026#39;], columns=\u0026#39;Genre\u0026#39;, values=\u0026#39;Flag\u0026#39;, aggfunc=\u0026#39;sum\u0026#39;).reset_index() re_reshaped_df.head()  We provided four attributes to the pivot_table function.\n  index: We don‚Äôt want to change these column structures\n  columns: explode this column into multiple columns\n  values: use this column to aggregate\n  aggfunc: the aggregation function.\n  We can then fill the missing values by 0 using fillna\nre_reshaped_df=re_reshaped_df.fillna(0)  Recommendation: Multiple columns to one column: melt and One column to multiple columns: pivot_table . There are other ways to do melt ‚Äî stack and different ways to do pivot_table: pivot,unstack. Stay away from them and just use melt and pivot_table. There are some valid reasons for this like unstack and stack will create multi-index and we don‚Äôt want to deal with that, and pivot cannot take multiple columns as the index.\nConclusion   With Pandas, less choice is more\n Here I have tried to profile some of the most useful functions in pandas I end up using most often.\nPandas is a vast library with a lot of functionality and custom options. That makes it essential that you should have a mindmap where you stick to a particular syntax for a specific thing.\nHere I have shared mine, and you can proceed with it and make it better as your understanding of the library grows.\nI hope you found this post useful and worth your time. I tried to make this as simple as possible, but you may always ask me or see the documentation for doubts.\nWhole code and data are posted in the Kaggle Kernel .\nAlso, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/07/20/pandas_subset/","tags":["Python","Awesome Guides","Best Content","Machine Learning","Data Science","Productivity"],"title":"Minimal Pandas Subset for Data Scientists"},{"categories":["Big Data","Data Science","Awesome Guides"],"contents":"Big Data has become synonymous with Data engineering.\nBut the line between Data Engineering and Data scientists is blurring day by day.\nAt this point in time, I think that Big Data must be in the repertoire of all data scientists.\nReason: Too much data is getting generated day by day\nAnd that brings us to Spark.\nNow most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.\nSo I thought of giving it a shot.\nThis post is going to be about ‚Äî ‚ÄúHow to make Spark work?‚Äù\nThis post is going to be quite long. Actually my longest post on medium, so go pick up a Coffee.\nHow it all started?-MapReduce   Suppose you are tasked with cutting all the trees in the forest. Perhaps not a good business with all the global warming, but here it serves our purpose and we are talking hypothetically, so I will continue. You have two options:\n  Get Batista with an electric powered chainsaw to do your work and make him cut each tree one by one.\n  Get 500 normal guys with normal axes and make them work on different trees.\n  Which would you prefer?\nAlthough Option 1 is still the way some people would go, the need for option 2 led to the emergence of MapReduce.\nIn Bigdata speak, we call the Batista solution as scaling vertically/scaling-upas in we add/stuff a lot of RAM and hard disk in a single worker.\nAnd the second solution is called scaling horizontally/scaling-sideways. As in you connect a lot of ordinary machines(with less RAM) together and use them in parallel.\nNow, vertical scaling has certain benefits over Horizontal scaling:\n  It is fast if the size of the problem is small: Think 2 trees. Batista would be through with both of them with his awesome chainsaw while our two guys would be still hacking with their axes.\n  It is easy to understand. This is how we have always done things. We normally think about things in a sequential pattern and that is how our whole computer architecture and design has evolved.\n  But, Horizontal Scaling is\n  Less Expensive: Getting 50 normal guys itself is much cheaper than getting a single guy like Batista. Apart from that Batista needs a lot of care and maintenance to keep him cool and he is very sensitive to even small things just like machines with a high amount of RAM.\n  Faster when the size of the problem is big: Now imagine 1000 trees and 1000 workers vs a single Batista. With Horizontal Scaling, if we face a very large problem we will just hire 100 or maybe 1000 more cheap workers. It doesn‚Äôt work like that with Batista. You have to increase RAM and that means more cooling infrastructure and more maintenance costs.\n    MapReduce is what makes the second option possible by letting us use a cluster of computers for parallelization.\nNow, MapReduce looks like a fairly technical term. But let us break it a little. MapReduce is made up of two terms:\nMap: It is basically the apply/map function. We split our data into n chunks and send each chunk to a different worker(Mapper). If there is any function we would like to apply over the rows of Data our worker does that.\nReduce: Aggregate the data using some function based on a groupby key. It is basically a groupby.\nOf course, there is a lot going in the background to make the system work as intended.\nDon‚Äôt worry, if you don‚Äôt understand it yet. Just keep reading. Maybe you will understand it when we use MapReduce ourselves in the examples I am going to provide.\n Why Spark?   Hadoop was the first open source system that introduced us to the MapReduce paradigm of programming and Spark is the system that made it faster, much much faster(100x).\nThere used to be a lot of data movement in Hadoop as it used to write intermediate results to the file system.\nThis affected the speed at which you could do analysis.\nSpark provided us with an in-memory model, so Spark doesn‚Äôt write too much to the disk while working.\nSimply, Spark is faster than Hadoop and a lot of people use Spark now.\nSo without further ado let us get started.\n Getting Started with Spark Installing Spark is actually a headache of its own.\nSince we want to understand how it works and really work with it, I would suggest that you use Sparks on Databricks here online with the community edition. Don‚Äôt worry it is free.\n  Once you register and login will be presented with the following screen.\n  You can start a new notebook here.\nSelect the Python notebook and give any name to your notebook.\nOnce you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.\nThe next step will be to check if the sparkcontext is present. To check if the sparkcontext is present you just have to run this command:\nsc   This means that we are set up with a notebook where we can run Spark.\n Load Some Data The next step is to upload some data we will use to learn Spark. Just click on ‚ÄòImport and Explore Data‚Äô on the home tab.\nI will end up using multiple datasets by the end of this post but let us start with something very simple.\nLet us add the file shakespeare.txt which you can download from here .\n  You can see that the file is loaded to /FileStore/tables/shakespeare.txt location.\n Our First Spark Program I like to learn by examples so let‚Äôs get done with the ‚ÄúHello World‚Äù of Distributed computing: The WordCount Program.\n# Distribute the data - Create a RDD lines = sc.textFile(\u0026#34;/FileStore/tables/shakespeare.txt\u0026#34;) # Create a list with all words, Create tuple (word,1), reduce by key i.e. the word counts = (lines.flatMap(lambda x: x.split(\u0026#39; \u0026#39;)) .map(lambda x: (x, 1)) .reduceByKey(lambda x,y : x + y)) # get the output on local output = counts.take(10) # print output for (word, count) in output: print(\u0026#34;%s: %i\u0026#34; % (word, count))   So that is a small example which counts the number of words in the document and prints 10 of them.\nAnd most of the work gets done in the second command.\nDon‚Äôt worry if you are not able to follow this yet as I still need to tell you about the things that make Spark work.\nBut before we get into Spark basics, Let us refresh some of our Python Basics. Understanding Spark becomes a lot easier if you have used functional programming with Python.\nFor those of you who haven‚Äôt used it, below is a brief intro.\n A functional approach to programming in Python   1. Map map is used to map a function to an array or a list. Say you want to apply some function to every element in a list.\nYou can do this by simply using a for loop but python lambda functions let you do this in a single line in Python.\nmy_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(lambda x:x**2,my_list) print(list(squared_list)) ------------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] In the above example, you could think of map as a function which takes two arguments ‚Äî A function and a list.\nIt then applies the function to every element of the list.\nWhat lambda allows you to do is write an inline function. In here the part lambda x:x**2 defines a function that takes x as input and returns x¬≤.\nYou could have also provided a proper function in place of lambda. For example:\ndef squared(x): return x**2 my_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(squared,my_list) print(list(squared_list)) ------------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] The same result, but the lambda expressions make the code compact and a lot more readable.\n2. Filter The other function that is used extensively is the filter function. This function takes two arguments ‚Äî A condition and the list to filter.\nIf you want to filter your list using some condition you use filter.\nmy_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want only the even numbers in my list. filtered_list = filter(lambda x:x%2==0,my_list) print(list(filtered_list)) --------------------------------------------------------------- [2, 4, 6, 8, 10] 3. Reduce The next function I want to talk about is the reduce function. This function will be the workhorse in Spark.\nThis function takes two arguments ‚Äî a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.\nimport functools my_list = [1,2,3,4,5] # Lets say I want to sum all elements in my list. sum_list = functools.reduce(lambda x,y:x+y,my_list) print(sum_list) In python2 reduce used to be a part of Python, now we have to use reduce as a part of functools.\nHere the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:\n Reduce function first sends 1,2 ; the lambda function returns 3 Reduce function then sends 3,3 ; the lambda function returns 6 Reduce function then sends 6,4 ; the lambda function returns 10 Reduce function finally sends 10,5 ; the lambda function returns 15 A condition on the lambda function we use in reduce is that it must be:\n  commutative that is a + b = b + a and\n  associative that is (a + b) + c == a + (b + c).\n  In the above case, we used sum which is commutative as well as associative. Other functions that we could have used: max, min, * etc.\n Moving Again to Spark As we have now got the fundamentals of Python Functional Programming out of the way, lets again head to Spark.\nBut first, let us delve a little bit into how spark works. Spark actually consists of two things a driver and workers.\nWorkers normally do all the work and the driver makes them do that work.\nRDD An RDD(Resilient Distributed Dataset) is a parallelized data structure that gets distributed across the worker nodes. They are the basic units of Spark programming.\nIn our wordcount example, in the first line\nlines = sc.textFile(\u0026quot;/FileStore/tables/shakespeare.txt\u0026quot;)  We took a text file and distributed it across worker nodes so that they can work on it in parallel. We could also parallelize lists using the function sc.parallelize\nFor example:\ndata = [1,2,3,4,5,6,7,8,9,10] new_rdd = sc.parallelize(data,4) new_rdd --------------------------------------------------------------- ParallelCollectionRDD[22] at parallelize at PythonRDD.scala:267 In Spark, we can do two different types of operations on RDD: Transformations and Actions.\n  Transformations: Create new datasets from existing RDDs\n  Actions: Mechanism to get results out of Spark\n   Transformation Basics   So let us say you have got your data in the form of an RDD.\nTo requote your data is now accessible to the worker machines. You want to do some transformations on the data now.\nYou may want to filter, apply some function, etc.\nIn Spark, this is done using Transformation functions.\nSpark provides many transformation functions. You can see a comprehensive list \u0026lt;strong\u0026gt;here\u0026lt;/strong\u0026gt; . Some of the main ones that I use frequently are:\n1. Map: Applies a given function to an RDD.\nNote that the syntax is a little bit different from Python, but it necessarily does the same thing. Don‚Äôt worry about collect yet. For now, just think of it as a function that collects the data in squared_rdd back to a list.\ndata = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) squared_rdd = rdd.map(lambda x:x**2) squared_rdd.collect() ------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 2. Filter: Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.\ndata = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) filtered_rdd = rdd.filter(lambda x:x%2==0) filtered_rdd.collect() ------------------------------------------------------ [2, 4, 6, 8, 10] 3. distinct: Returns only distinct elements in an RDD.\ndata = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10] rdd = sc.parallelize(data,4) distinct_rdd = rdd.distinct() distinct_rdd.collect() ------------------------------------------------------ [8, 4, 1, 5, 9, 2, 10, 6, 3, 7]  4. flatmap: Similar to map, but each input item can be mapped to 0 or more output items.\ndata = [1,2,3,4] rdd = sc.parallelize(data,4) flat_rdd = rdd.flatMap(lambda x:[x,x**3]) flat_rdd.collect() ------------------------------------------------------ [1, 1, 2, 8, 3, 27, 4, 64] 5. Reduce By Key: The parallel to the reduce in Hadoop MapReduce.\nNow Spark cannot provide the value if it just worked with Lists.\nIn Spark, there is a concept of pair RDDs that makes it a lot more flexible. Let\u0026rsquo;s assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data.\ndata = [(\u0026#39;Apple\u0026#39;,\u0026#39;Fruit\u0026#39;,200),(\u0026#39;Banana\u0026#39;,\u0026#39;Fruit\u0026#39;,24),(\u0026#39;Tomato\u0026#39;,\u0026#39;Fruit\u0026#39;,56),(\u0026#39;Potato\u0026#39;,\u0026#39;Vegetable\u0026#39;,103),(\u0026#39;Carrot\u0026#39;,\u0026#39;Vegetable\u0026#39;,34)] rdd = sc.parallelize(data,4) Right now our RDD rdd holds tuples.\nNow we want to find out the total sum of revenue that we got from each category.\nTo do that we have to transform our rdd to a pair rdd so that it only contains key-value pairs/tuples.\ncategory_price_rdd = rdd.map(lambda x: (x[1],x[2])) category_price_rdd.collect() ----------------------------------------------------------------- [(‚ÄòFruit‚Äô, 200), (‚ÄòFruit‚Äô, 24), (‚ÄòFruit‚Äô, 56), (‚ÄòVegetable‚Äô, 103), (‚ÄòVegetable‚Äô, 34)] Here we used the map function to get it in the format we wanted. When working with textfile, the RDD that gets formed has got a lot of strings. We use map to convert it into a format that we want.\nSo now our category_price_rdd contains the product category and the price at which the product sold.\nNow we want to reduce on the key category and sum the prices. We can do this by:\ncategory_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y) category_total_price_rdd.collect() --------------------------------------------------------- [(‚ÄòVegetable‚Äô, 137), (‚ÄòFruit‚Äô, 280)] 6. Group By Key: Similar to reduceByKey but does not reduces just puts all the elements in an iterator. For example, if we wanted to keep as key the category and as the value all the products we would use this function.\nLet us again use map to get data in the required form.\ndata = [(\u0026#39;Apple\u0026#39;,\u0026#39;Fruit\u0026#39;,200),(\u0026#39;Banana\u0026#39;,\u0026#39;Fruit\u0026#39;,24),(\u0026#39;Tomato\u0026#39;,\u0026#39;Fruit\u0026#39;,56),(\u0026#39;Potato\u0026#39;,\u0026#39;Vegetable\u0026#39;,103),(\u0026#39;Carrot\u0026#39;,\u0026#39;Vegetable\u0026#39;,34)] rdd = sc.parallelize(data,4) category_product_rdd = rdd.map(lambda x: (x[1],x[0])) category_product_rdd.collect() ------------------------------------------------------------ [(\u0026#39;Fruit\u0026#39;, \u0026#39;Apple\u0026#39;), (\u0026#39;Fruit\u0026#39;, \u0026#39;Banana\u0026#39;), (\u0026#39;Fruit\u0026#39;, \u0026#39;Tomato\u0026#39;), (\u0026#39;Vegetable\u0026#39;, \u0026#39;Potato\u0026#39;), (\u0026#39;Vegetable\u0026#39;, \u0026#39;Carrot\u0026#39;)] We then use groupByKey as:\ngrouped_products_by_category_rdd = category_product_rdd.groupByKey() findata = grouped_products_by_category_rdd.collect() for data in findata: print(data[0],list(data[1])) ------------------------------------------------------------ Vegetable [\u0026#39;Potato\u0026#39;, \u0026#39;Carrot\u0026#39;] Fruit [\u0026#39;Apple\u0026#39;, \u0026#39;Banana\u0026#39;, \u0026#39;Tomato\u0026#39;] Here the groupByKey function worked and it returned the category and the list of products in that category.\n Action Basics   You have filtered your data, mapped some functions on it. Done your computation.\nNow you want to get the data on your local machine or save it to a file or show the results in the form of some graphs in excel or any visualization tool.\nYou will need actions for that. A comprehensive list of actions is provided \u0026lt;strong\u0026gt;here\u0026lt;/strong\u0026gt; .\nSome of the most common actions that I tend to use are:\n1. collect: We have already used this action many times. It takes the whole RDD and brings it back to the driver program.\n2. reduce: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\nrdd = sc.parallelize([1,2,3,4,5]) rdd.reduce(lambda x,y : x+y) --------------------------------- 15 3. take: Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. take returns a list with the first n elements of the RDD.\nrdd = sc.parallelize([1,2,3,4,5]) rdd.take(3) --------------------------------- [1, 2, 3] 4. takeOrdered: takeOrdered returns the first n elements of the RDD using either their natural order or a custom comparator.\nrdd = sc.parallelize([5,3,12,23]) # descending order rdd.takeOrdered(3,lambda s:-1*s) ---- [23, 12, 5] rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)]) # descending order rdd.takeOrdered(3,lambda s:-1*s[1]) --- [(12, 344), (3, 34), (23, 29)] We have our basics covered finally. Let us get back to our wordcount example\n Understanding The WordCount Example   Now we sort of understand the transformations and the actions provided to us by Spark.\nIt should not be difficult to understand the wordcount program now. Let us go through the program line by line.\nThe first line creates an RDD and distributes it to the workers.\nlines = sc.textFile(\u0026quot;/FileStore/tables/shakespeare.txt\u0026quot;)  This RDD lines contains a list of sentences in the file. You can see the rdd content using take\nlines.take(5) -------------------------------------------- ['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ', 'William Shakespeare', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever. You may copy it, give it away or']  This RDD is of the form:\n['word1 word2 word3','word4 word3 word2']  This next line is actually the workhorse function in the whole script.\ncounts = (lines.flatMap(lambda x: x.split(' ')) .map(lambda x: (x, 1)) .reduceByKey(lambda x,y : x + y))  It contains a series of transformations that we do to the lines RDD. First of all, we do a flatmap transformation.\nThe flatmap transformation takes as input the lines and gives words as output. So after the flatmap transformation, the RDD is of the form:\n['word1','word2','word3','word4','word3','word2']  Next, we do a map transformation on the flatmap output which converts the RDD to :\n[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]  Finally, we do a reduceByKey transformation which counts the number of time each word appeared.\nAfter which the RDD approaches the final desirable form.\n[('word1',1),('word2',2),('word3',2),('word4',1)]  This next line is an action that takes the first 10 elements of the resulting RDD locally.\noutput = counts.take(10)  This line just prints the output\nfor (word, count) in output: print(\u0026quot;%s: %i\u0026quot; % (word, count))  And that is it for the wordcount program. Hope you understand it now.\n So till now, we talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don‚Äôt do wordcount in real life.\nWe have to work on bigger problems which are much more complex. Worry not! Whatever we have learned till now will let us do that and more.\n Spark in Action with Example   Let us work with a concrete example which takes care of some usual transformations.\nWe will work on Movielens ml-100k.zip dataset which is a stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies. Released 4/1998.\nThe Movielens dataset contains a lot of files but we are going to be working with 3 files only:\n  Users: This file name is kept as ‚Äúu.user‚Äù, The columns in this file are:\n[\u0026lsquo;user_id\u0026rsquo;, \u0026lsquo;age\u0026rsquo;, \u0026lsquo;sex\u0026rsquo;, \u0026lsquo;occupation\u0026rsquo;, \u0026lsquo;zip_code\u0026rsquo;]\n  Ratings: This file name is kept as ‚Äúu.data‚Äù, The columns in this file are:\n[\u0026lsquo;user_id\u0026rsquo;, \u0026lsquo;movie_id\u0026rsquo;, \u0026lsquo;rating\u0026rsquo;, \u0026lsquo;unix_timestamp\u0026rsquo;]\n  Movies: This file name is kept as ‚Äúu.item‚Äù, The columns in this file are:\n[\u0026lsquo;movie_id\u0026rsquo;, \u0026lsquo;title\u0026rsquo;, \u0026lsquo;release_date\u0026rsquo;, \u0026lsquo;video_release_date\u0026rsquo;, \u0026lsquo;imdb_url\u0026rsquo;, and 18 more columns\u0026hellip;..]\n  Let us start by importing these 3 files into our spark instance using ‚ÄòImport and Explore Data‚Äô on the home tab.\n  Our business partner now comes to us and asks us to find out the 25 most rated movie titles from this data. How many times a movie has been rated?\nLet us load the data in different RDDs and see what the data contains.\nuserRDD = sc.textFile(\u0026#34;/FileStore/tables/u.user\u0026#34;) ratingRDD = sc.textFile(\u0026#34;/FileStore/tables/u.data\u0026#34;) movieRDD = sc.textFile(\u0026#34;/FileStore/tables/u.item\u0026#34;) print(\u0026#34;userRDD:\u0026#34;,userRDD.take(1)) print(\u0026#34;ratingRDD:\u0026#34;,ratingRDD.take(1)) print(\u0026#34;movieRDD:\u0026#34;,movieRDD.take(1)) ----------------------------------------------------------- userRDD: [\u0026#39;1|24|M|technician|85711\u0026#39;] ratingRDD: [\u0026#39;196\\t242\\t3\\t881250949\u0026#39;] movieRDD: [\u0026#39;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\u0026#39;] We note that to answer this question we will need to use the ratingRDD. But the ratingRDD does not have the movie name.\nSo we would have to merge movieRDD and ratingRDD using movie_id.\nHow we would do that in Spark?\nBelow is the code. We also use a new transformation leftOuterJoin. Do read the docs and comments in the below code.\n# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating. RDD_movid_rating = ratingRDD.map(lambda x : (x.split(\u0026#34;\\t\u0026#34;)[1],x.split(\u0026#34;\\t\u0026#34;)[2])) print(\u0026#34;RDD_movid_rating:\u0026#34;,RDD_movid_rating.take(4)) # Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title. RDD_movid_title = movieRDD.map(lambda x : (x.split(\u0026#34;|\u0026#34;)[0],x.split(\u0026#34;|\u0026#34;)[1])) print(\u0026#34;RDD_movid_title:\u0026#34;,RDD_movid_title.take(2)) # merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin(). See the transformation document. rdd_movid_title_rating = RDD_movid_rating.leftOuterJoin(RDD_movid_title) print(\u0026#34;rdd_movid_title_rating:\u0026#34;,rdd_movid_title_rating.take(1)) # use the RDD in previous step to create (movie,1) tuple pair RDD rdd_title_rating = rdd_movid_title_rating.map(lambda x: (x[1][1],1 )) print(\u0026#34;rdd_title_rating:\u0026#34;,rdd_title_rating.take(2)) # Use the reduceByKey transformation to reduce on the basis of movie_title rdd_title_ratingcnt = rdd_title_rating.reduceByKey(lambda x,y: x+y) print(\u0026#34;rdd_title_ratingcnt:\u0026#34;,rdd_title_ratingcnt.take(2)) # Get the final answer by using takeOrdered Transformation print \u0026#34;#####################################\u0026#34; print \u0026#34;25 most rated movies:\u0026#34;,rdd_title_ratingcnt.takeOrdered(25,lambda x:-x[1]) print \u0026#34;#####################################\u0026#34; OUTPUT: --------------------------------------------------------------------RDD_movid_rating: [('242', '3'), ('302', '3'), ('377', '1'), ('51', '2')] RDD_movid_title: [('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)')] rdd_movid_title_rating: [('1440', ('3', 'Above the Rim (1994)'))] rdd_title_rating: [('Above the Rim (1994)', 1), ('Above the Rim (1994)', 1)] rdd_title_ratingcnt: [('Mallrats (1995)', 54), ('Michael Collins (1996)', 92)] ##################################### 25 most rated movies: [('Star Wars (1977)', 583), ('Contact (1997)', 509), ('Fargo (1996)', 508), ('Return of the Jedi (1983)', 507), ('Liar Liar (1997)', 485), ('English Patient, The (1996)', 481), ('Scream (1996)', 478), ('Toy Story (1995)', 452), ('Air Force One (1997)', 431), ('Independence Day (ID4) (1996)', 429), ('Raiders of the Lost Ark (1981)', 420), ('Godfather, The (1972)', 413), ('Pulp Fiction (1994)', 394), ('Twelve Monkeys (1995)', 392), ('Silence of the Lambs, The (1991)', 390), ('Jerry Maguire (1996)', 384), ('Chasing Amy (1997)', 379), ('Rock, The (1996)', 378), ('Empire Strikes Back, The (1980)', 367), ('Star Trek: First Contact (1996)', 365), ('Back to the Future (1985)', 350), ('Titanic (1997)', 350), ('Mission: Impossible (1996)', 344), ('Fugitive, The (1993)', 336), ('Indiana Jones and the Last Crusade (1989)', 331)] #####################################  Star Wars is the most rated movie in the Movielens Dataset.\nNow we could have done all this in a single command using the below command but the code is a little messy now.\nI did this to show that you can use chaining functions with Spark and you could bypass the process of variable creation.\nprint(((ratingRDD.map(lambda x : (x.split(\u0026#34;\\t\u0026#34;)[1],x.split(\u0026#34;\\t\u0026#34;)[2]))). leftOuterJoin(movieRDD.map(lambda x : (x.split(\u0026#34;|\u0026#34;)[0],x.split(\u0026#34;|\u0026#34;)[1])))). map(lambda x: (x[1][1],1)). reduceByKey(lambda x,y: x+y). takeOrdered(25,lambda x:-x[1])) Let us do one more. For practice:\nNow we want to find the most highly rated 25 movies using the same dataset. We actually want only those movies which have been rated at least 100 times.\n# We already have the RDD rdd_movid_title_rating: [(u\u0026#39;429\u0026#39;, (u\u0026#39;5\u0026#39;, u\u0026#39;Day the Earth Stood Still, The (1951)\u0026#39;))] # We create an RDD that contains sum of all the ratings for a particular movie rdd_title_ratingsum = (rdd_movid_title_rating. map(lambda x: (x[1][1],int(x[1][0]))). reduceByKey(lambda x,y:x+y)) print(\u0026#34;rdd_title_ratingsum:\u0026#34;,rdd_title_ratingsum.take(2)) # Merge this data with the RDD rdd_title_ratingcnt we created in the last step # And use Map function to divide ratingsum by rating count. rdd_title_ratingmean_rating_count = (rdd_title_ratingsum. leftOuterJoin(rdd_title_ratingcnt). map(lambda x:(x[0],(float(x[1][0])/x[1][1],x[1][1])))) print(\u0026#34;rdd_title_ratingmean_rating_count:\u0026#34;,rdd_title_ratingmean_rating_count.take(1)) # We could use take ordered here only but we want to only get the movies which have count # of ratings more than or equal to 100 so lets filter the data RDD. rdd_title_rating_rating_count_gt_100 = (rdd_title_ratingmean_rating_count. filter(lambda x: x[1][1]\u0026gt;=100)) print(\u0026#34;rdd_title_rating_rating_count_gt_100:\u0026#34;,rdd_title_rating_rating_count_gt_100.take(1)) # Get the final answer by using takeOrdered Transformation print(\u0026#34;#####################################\u0026#34;) print (\u0026#34;25 highly rated movies:\u0026#34;) print(rdd_title_rating_rating_count_gt_100.takeOrdered(25,lambda x:-x[1][0])) print(\u0026#34;#####################################\u0026#34;) OUTPUT: ------------------------------------------------------------ rdd_title_ratingsum: [('Mallrats (1995)', 186), ('Michael Collins (1996)', 318)] rdd_title_ratingmean_rating_count: [('Mallrats (1995)', (3.4444444444444446, 54))] rdd_title_rating_rating_count_gt_100: [('Butch Cassidy and the Sundance Kid (1969)', (3.949074074074074, 216))] ##################################### 25 highly rated movies: [('Close Shave, A (1995)', (4.491071428571429, 112)), (\u0026quot;Schindler's List (1993)\u0026quot;, (4.466442953020135, 298)), ('Wrong Trousers, The (1993)', (4.466101694915254, 118)), ('Casablanca (1942)', (4.45679012345679, 243)), ('Shawshank Redemption, The (1994)', (4.445229681978798, 283)), ('Rear Window (1954)', (4.3875598086124405, 209)), ('Usual Suspects, The (1995)', (4.385767790262173, 267)), ('Star Wars (1977)', (4.3584905660377355, 583)), ('12 Angry Men (1957)', (4.344, 125)), ('Citizen Kane (1941)', (4.292929292929293, 198)), ('To Kill a Mockingbird (1962)', (4.292237442922374, 219)), (\u0026quot;One Flew Over the Cuckoo's Nest (1975)\u0026quot;, (4.291666666666667, 264)), ('Silence of the Lambs, The (1991)', (4.28974358974359, 390)), ('North by Northwest (1959)', (4.284916201117318, 179)), ('Godfather, The (1972)', (4.283292978208232, 413)), ('Secrets \u0026amp; Lies (1996)', (4.265432098765432, 162)), ('Good Will Hunting (1997)', (4.262626262626263, 198)), ('Manchurian Candidate, The (1962)', (4.259541984732825, 131)), ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', (4.252577319587629, 194)), ('Raiders of the Lost Ark (1981)', (4.252380952380952, 420)), ('Vertigo (1958)', (4.251396648044692, 179)), ('Titanic (1997)', (4.2457142857142856, 350)), ('Lawrence of Arabia (1962)', (4.23121387283237, 173)), ('Maltese Falcon, The (1941)', (4.2101449275362315, 138)), ('Empire Strikes Back, The (1980)', (4.204359673024523, 367))] #####################################  We have talked about RDDs till now as they are very powerful.\nYou can use RDDs to work with non-relational databases too.\nThey let you do a lot of things that you couldn‚Äôt do with SparkSQL?\nYes, you can use SQL with Spark too which I am going to talk about now.\n Spark DataFrames   Spark has provided DataFrame API for us Data Scientists to work with relational data. Here is the documentation for the adventurous folks.\nRemember that in the background it still is all RDDs and that is why the starting part of this post focussed on RDDs.\nI will start with some common functionalities you will need to work with Spark DataFrames. Would look a lot like Pandas with some syntax changes.\n1. Reading the File ratings = spark.read.load(\u0026#34;/FileStore/tables/u.data\u0026#34;,format=\u0026#34;csv\u0026#34;, sep=\u0026#34;\\t\u0026#34;, inferSchema=\u0026#34;true\u0026#34;, header=\u0026#34;false\u0026#34;) 2. Show File We have two ways to show files using Spark Dataframes.\nratings.show()   display(ratings)   I prefer display as it looks a lot nicer and clean.\n3. Change Column names Good functionality. Always required. Don‚Äôt forget the * in front of the list.\nratings = ratings.toDF(*[\u0026#39;user_id\u0026#39;, \u0026#39;movie_id\u0026#39;, \u0026#39;rating\u0026#39;, \u0026#39;unix_timestamp\u0026#39;]) display(ratings)   4. Some Basic Stats print(ratings.count()) #Row Count print(len(ratings.columns)) #Column Count --------------------------------------------------------- 100000 4 We can also see the dataframe statistics using:\ndisplay(ratings.describe())   5. Select a few columns display(ratings.select(\u0026#39;user_id\u0026#39;,\u0026#39;movie_id\u0026#39;))   6. Filter Filter a dataframe using multiple conditions:\ndisplay(ratings.filter((ratings.rating==5) \u0026amp; (ratings.user_id==253)))   7. Groupby We can use groupby function with a spark dataframe too. Pretty much same as a pandas groupby with the exception that you will need to import pyspark.sql.functions\nfrom pyspark.sql import functions as F display(ratings.groupBy(\u0026#34;user_id\u0026#34;).agg(F.count(\u0026#34;user_id\u0026#34;),F.mean(\u0026#34;rating\u0026#34;))) Here we have found the count of ratings and average rating from each user_id\n  8. Sort display(ratings.sort(\u0026#34;user_id\u0026#34;))   We can also do a descending sort using F.desc function as below.\n# descending Sort from pyspark.sql import functions as F display(ratings.sort(F.desc(\u0026#34;user_id\u0026#34;)))    Joins/Merging with Spark Dataframes I was not able to find a pandas equivalent of merge with Spark DataFrames but we can use SQL with dataframes and thus we can merge dataframes using SQL.\nLet us try to run some SQL on Ratings.\nWe first register the ratings df to a temporary table ratings_table on which we can run sql operations.\nAs you can see the result of the SQL select statement is again a Spark Dataframe.\nratings.registerTempTable(\u0026#39;ratings_table\u0026#39;) newDF = sqlContext.sql(\u0026#39;select * from ratings_table where rating\u0026gt;4\u0026#39;) display(newDF)   Let us now add one more Spark Dataframe to the mix to see if we can use join using the SQL queries:\n#get one more dataframe to join movies = spark.read.load(\u0026#34;/FileStore/tables/u.item\u0026#34;,format=\u0026#34;csv\u0026#34;, sep=\u0026#34;|\u0026#34;, inferSchema=\u0026#34;true\u0026#34;, header=\u0026#34;false\u0026#34;) # change column names movies = movies.toDF(*[\u0026#34;movie_id\u0026#34;,\u0026#34;movie_title\u0026#34;,\u0026#34;release_date\u0026#34;,\u0026#34;video_release_date\u0026#34;,\u0026#34;IMDb_URL\u0026#34;,\u0026#34;unknown\u0026#34;,\u0026#34;Action\u0026#34;,\u0026#34;Adventure\u0026#34;,\u0026#34;Animation \u0026#34;,\u0026#34;Children\u0026#34;,\u0026#34;Comedy\u0026#34;,\u0026#34;Crime\u0026#34;,\u0026#34;Documentary\u0026#34;,\u0026#34;Drama\u0026#34;,\u0026#34;Fantasy\u0026#34;,\u0026#34;Film_Noir\u0026#34;,\u0026#34;Horror\u0026#34;,\u0026#34;Musical\u0026#34;,\u0026#34;Mystery\u0026#34;,\u0026#34;Romance\u0026#34;,\u0026#34;Sci_Fi\u0026#34;,\u0026#34;Thriller\u0026#34;,\u0026#34;War\u0026#34;,\u0026#34;Western\u0026#34;]) display(movies)   Now let us try joining the tables on movie_id to get the name of the movie in the ratings table.\nmovies.registerTempTable(\u0026#39;movies_table\u0026#39;) display(sqlContext.sql(\u0026#39;select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id\u0026#39;))   Let us try to do what we were doing earlier with the RDDs. Finding the top 25 most rated movies:\nmostrateddf = sqlContext.sql(\u0026#39;select movie_id,movie_title, count(user_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title order by num_ratings desc \u0026#39;) display(mostrateddf)   And finding the top 25 highest rated movies having more than 100 votes:\nhighrateddf = sqlContext.sql(\u0026#39;select movie_id,movie_title, avg(rating) as avg_rating,count(movie_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title having num_ratings\u0026gt;100 order by avg_rating desc \u0026#39;) display(highrateddf)   I have used GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query. That shows that you can do pretty much complex stuff using sqlContext.sql\n A Small Note About Display You can also use display command to display charts in your notebooks.\n  You can see more options when you select Plot Options.\n   Converting from Spark Dataframe to RDD and vice versa: Sometimes you may want to convert to RDD from a spark Dataframe or vice versa so that you can have the best of both worlds.\nTo convert from DF to RDD, you can simply do :\nhighratedrdd =highrateddf.rdd highratedrdd.take(2)   To go from an RDD to a dataframe:\nfrom pyspark.sql import Row # creating a RDD first data = [(\u0026#39;A\u0026#39;,1),(\u0026#39;B\u0026#39;,2),(\u0026#39;C\u0026#39;,3),(\u0026#39;D\u0026#39;,4)] rdd = sc.parallelize(data) # map the schema using Row. rdd_new = rdd.map(lambda x: Row(key=x[0], value=int(x[1]))) # Convert the rdd to Dataframe rdd_as_df = sqlContext.createDataFrame(rdd_new) display(rdd_as_df)   RDD provides you with more control at the cost of time and coding effort. While Dataframes provide you with familiar coding platform. And now you can move back and forth between these two.\nConclusion   This was a big post and congratulations if you reached the end.\nSpark has provided us with an interface where we could use transformations and actions on our data. Spark also has the Dataframe API to ease the transition of Data scientists to Big Data.\nHopefully, I‚Äôve covered the basics well enough to pique your interest and help you get started with Spark.\nYou can find all the code at the GitHub repository.\nAlso, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on Big Data Essentials which is part of the Big Data Specialization provided by Yandex.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/","tags":["Big Data","Machine Learning","Data Science","Awesome Guides","Best Content"],"title":"The Hitchhikers guide to handle Big Data using Spark"},{"categories":["Data Science"],"contents":"  I love Jupyter notebooks and the power they provide.\nThey can be used to present findings as well as share code in the most effective manner which was not easy with the previous IDEs.\nYet there is something still amiss.\nThere are a few functionalities I aspire in my text editor which don‚Äôt come by default in Jupyter.\nBut fret not. Just like everything in Python, Jupyter too has third-party extensions.\nThis post is about some of the most useful extensions I found.\n 1. Collapsible Headings The one extension, I like most is collapsible headings.\nIt makes the flow of the notebook easier to comprehend and also helps in creating presentable notebooks.\nTo get this one, install the jupyter_contrib_nbextensions package with this command on the terminal window:\nconda install -c conda-forge jupyter_contrib_nbextensions  Once the package is installed, we can start jupyter notebook using:\njupyter notebook  Once you go to the home page of your jupyter notebook, you can see that a new tab for NBExtensions is created.\n  And we can get a lot of extensions using this package.\n  This is how it looks:\n   2. Automatic Imports   Automation is the future.\nOne thing that bugs me is that whenever I open a new Jupyter notebook in any of my data science projects, I need to copy paste a lot of libraries and default options for some of them.\nTo tell you about some of the usual imports I use:\n  Pandas and numpy ‚Äî In my view, Python must make these two as a default import.\n  Seaborn, matplotlib, plotly_express\n  change some pandas and seaborn default options.\n  Here is the script that I end up pasting over and over again.\nimport pandas as pd import numpy as np import plotly_express as px import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline *# We dont Probably need the Gridlines. Do we? If yes comment this line* sns.set(style=\u0026#34;ticks\u0026#34;) # pandas defaults pd.options.display.max_columns = 500 pd.options.display.max_rows = 500 Is there a way I can automate this?\nJust go to the nbextensions tab and select the snippets extension.\nYou will need to make the following changes to the snippets.json file. You can find this file at /miniconda3/envs/py36/share/jupyter/nbextensions/snippets location. The py36 in this location here is my conda virtualenv. It took me some time to find this location for me. Yours might be different. Please note that you don‚Äôt have to change at the site-packages location.\n{ \u0026#34;snippets\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;example\u0026#34;, \u0026#34;code\u0026#34; : [ \u0026#34;# This is an example snippet!\u0026#34;, \u0026#34;# To create your own, add a new snippet block to the\u0026#34;, \u0026#34;# snippets.json file in your jupyter nbextensions directory:\u0026#34;, \u0026#34;# /nbextensions/snippets/snippets.json\u0026#34;, \u0026#34;import this\u0026#34; ] }, { \u0026#34;name\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;code\u0026#34; : [ \u0026#34;# This is A snippet for all data related tasks\u0026#34;, \u0026#34;import pandas as pd\u0026#34; \u0026#34;import numpy as np\u0026#34; \u0026#34;import plotly_express as px\u0026#34; \u0026#34;import seaborn as sns\u0026#34; \u0026#34;import matplotlib.pyplot as plt\u0026#34; \u0026#34;%matplotlib inline\u0026#34; \u0026#34;# We dont Probably need the Gridlines. Do we? If yes comment this line\u0026#34; \u0026#34;sns.set(style=\u0026#39;ticks\u0026#39;)\u0026#34; \u0026#34;# pandas defaults\u0026#34; \u0026#34;pd.options.display.max_columns = 500\u0026#34; \u0026#34;pd.options.display.max_rows = 500\u0026#34; ] } ] } You can see this extension in action below.\n  Pretty cool. Right? I also use this to create basic snippets for my deep learning notebooks and NLP based notebooks.\n 3. Execution Time We have used %time as well as decorator based timer functions to measure time for our functions. You can also use this excellent extension to do that.\nPlus it looks great.\nJust select the ExecutionTime extension from the NBextensions list and you will have an execution result at the bottom of the cell after every cell execution as well as the time when the cell was executed.\n   Other Extensions   NBExtensions has a lot of extensions. Some other extensions from NBExtensions I like and you might want to look at:\n  Limit Output: Ever had your notebook hang since you printed a lot of text in your notebook. This extension limits the number of characters that can be printed below a code cell\n  2to3Convertor: Having problems with your old python2 notebooks. Tired of changing the print statements. This one is a good one.\n     Live Markdown Preview: Some of us like writing our blogs using Markdown in a jupyter notebook. Sometimes it can be hectic as you make errors in writing. Now you can see Live-preview of the rendered output of markdown cells while editing their source.     Conclusion I love how there is a package for everything with Python. And that holds good with the Jupyter notebook too.\nThe jupyter_contrib_nbextensions package works great out of the box.\nIt has made my life a lot easier when it comes to checking execution times, scrolling through the notebook, and repetitive tasks.\nThere are many other extensions this package does provide. Do take a look at them and try to see which ones you find useful.\nAlso, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/06/28/jupyter_extensions/","tags":["Python","Machine Learning","Data Science","Tools","Productivity"],"title":"3 Great Additions for your Jupyter Notebooks"},{"categories":["Deep Learning","Computer Vision","Awesome Guides"],"contents":"  I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.\nIn this post, I will help the reader to understand how they can create and build such applications on their own.\nI will try to keep this post as intuitive as possible for starters while not dumbing it down too much.\nThis post is about understanding how GANs work.\n Task Overview I will work on creating our own anime characters using anime characters dataset.\nThe DC-GAN flavor of GANs which I will use here is widely applicable not only to generate Faces or new anime characters; it can also be used to create modern fashion styles, for general content creation and sometimes for data augmentation purposes as well.\nAs per my view, GANs will change the way video games and special effects are generated. The approach could create realistic textures or characters on demand.\nYou can find the full code for this chapter in the Github Repository . I have also uploaded the code to Google Colab so that you can try it yourself.\n Using DCGAN architecture to generate anime images As always before we get into the coding, it helps to delve a little bit into the theory.\nThe main idea of DC-GAN‚Äôs stemmed from the paper UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS written in 2016 by Alec Radford, Luke Metz, and Soumith Chintala.\nAlthough I am going to explain the paper in the next few sections, do take a look at it. It is an excellent paper.\n INTUITION: Brief Intro to GANs for Generating Fake Images   Generator vs. Discriminator    Typically, GANs employ two dueling neural networks to train a computer to learn the nature of a data set well enough to generate convincing fakes.\nWe can think of this as two systems where one Neural Network works to generate fakes (Generator), and another neural network (Discriminator) tries to classify which image is a fake.\nAs both generator and discriminator networks do this repetitively, the networks eventually get better at their respective tasks.\nThink of this as simple as swordplay. Two noobs start sparring with each other. After a while, both become better at swordplay.\nOr you could think of this as a robber(generator) and a policeman(Discriminator). After a lot of thefts, the robber becomes better at thieving while the policeman gets better at catching the robber. In an ideal world.\nThe Losses in these neural networks are primarily a function of how the other network performs:\n  Discriminator network loss is a function of generator network quality- Loss is high for the discriminator if it gets fooled by the generator‚Äôs fake images\n  Generator network loss is a function of discriminator network quality ‚Äî Loss is high if the generator is not able to fool the discriminator.\n  In the training phase, we train our Discriminator and Generator networks sequentially intending to improve both the Discriminator and Generator performance.\nThe objective is to end up with weights that help Generators to generate realistic looking images. In the end, we can use the Generator Neural network to generate fake images from Random Noise.\n Generator architecture One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.\n     The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.\nEssentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:\n     How do we get such an architecture?\nIn the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:\n  tc   We don‚Äôt have to worry about any weights right now as the network itself will learn those while training.\nOnce we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don‚Äôt half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.\nNow, What are Transpose convolutions? In most simple terms, transpose convolutions provide us with a way to upsample images. While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:\n     Q: We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don‚Äôt we use Un-pooling?\nIt is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.\nDiscriminator architecture Now, as we have understood the generator architecture, here is the discriminator as a black box.\nIn practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:\n     Takes an image as input and predicts if it is real/fake. Every image conv net ever.\nData preprocessing and visualization The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:\nfilenames = glob.glob(\u0026#39;animeface-character-dataset/*/*.pn*\u0026#39;) plt.figure(figsize=(10, 8)) for i in range(5): img = plt.imread(filenames[i], 0) plt.subplot(4, 5, i+1) plt.imshow(img) plt.title(img.shape) plt.xticks([]) plt.yticks([]) plt.tight_layout() plt.show() The resultant output is as follows:\n     We get to see the sizes of the images and the images themselves.\nWe also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.\nWe will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.\n# A function to normalize image pixels. def norm_img(img): \u0026#39;\u0026#39;\u0026#39;A function to Normalize Images. Input: img : Original image as numpy array. Output: Normailized Image as numpy array \u0026#39;\u0026#39;\u0026#39; img = (img / 127.5) - 1 return img def denorm_img(img): \u0026#39;\u0026#39;\u0026#39;A function to Denormailze, i.e. recreate image from normalized image Input: img : Normalized image as numpy array. Output: Original Image as numpy array \u0026#39;\u0026#39;\u0026#39; img = (img + 1) * 127.5 return img.astype(np.uint8) def sample_from_dataset(batch_size, image_shape, data_dir=None): \u0026#39;\u0026#39;\u0026#39;Create a batch of image samples by sampling random images from a data directory. Resizes the image using image_shape and normalize the images. Input: batch_size : Sample size required image_size : Size that Image should be resized to data_dir : Path of directory where training images are placed. Output: sample : batch of processed images \u0026#39;\u0026#39;\u0026#39; sample_dim = (batch_size,) + image_shape sample = np.empty(sample_dim, dtype=np.float32) all_data_dirlist = list(glob.glob(data_dir)) sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size) for index,img_filename in enumerate(sample_imgs_paths): image = Image.open(img_filename) image = image.resize(image_shape[:-1]) image = image.convert(\u0026#39;RGB\u0026#39;) image = np.asarray(image) image = norm_img(image) sample[index,...] = image return sample As you will see, we will be using the preceding defined functions in the training part of our code.\nImplementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.\nGenerating noise vector for Generator   Kids: Normal Noise generators    The following code block is a helper function to create a noise vector of predefined length for a Generator. It will generate the noise which we want to convert to an image using our generator architecture.\nWe use a normal distribution\n to generate the noise vector:\ndef gen_noise(batch_size, noise_shape): \u0026#39;\u0026#39;\u0026#39; Generates a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) Input: batch_size : size of batch noise_shape: shape of noise vector, normally kept as 100 Output:a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) \u0026#39;\u0026#39;\u0026#39; return np.random.normal(0, 1, size=(batch_size,)+noise_shape) Generator architecture The Generator is the most crucial part of the GAN.\nHere, I create a generator by adding some transposed convolution layers to upsample the noise vector to an image.\nAs you will notice, this generator architecture is not the same as given in the Original DC-GAN paper.\nI needed to make some architectural changes to fit our data better, so I added a convolution layer in the middle and removed all dense layers from the generator architecture, making it fully convolutional.\nI also use a lot of Batchnorm layers with a momentum of 0.5 and leaky ReLU activation. I use Adam optimizer with Œ≤=0.5. The following code block is the function I will use to create the generator:\ndef get_gen_normal(noise_shape): \u0026#39;\u0026#39;\u0026#39; This function takes as input shape of the noise vector and creates the Keras generator architecture. \u0026#39;\u0026#39;\u0026#39; kernel_init = \u0026#39;glorot_uniform\u0026#39; gen_input = Input(shape = noise_shape) # Transpose 2D conv layer 1. generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \u0026#34;valid\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(gen_input) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 2. generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 3. generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 4. generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # conv 2D layer 1. generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(generator) # Tanh activation to get final normalized image generator = Activation(\u0026#39;tanh\u0026#39;)(generator) # defining the optimizer and compiling the generator model. gen_opt = Adam(lr=0.00015, beta_1=0.5) generator_model = Model(input = gen_input, output = generator) generator_model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=gen_opt, metrics=[\u0026#39;accuracy\u0026#39;]) generator_model.summary() return generator_model You can plot the final generator model:\nplot_model(generator, to_file=\u0026#39;gen_plot.png\u0026#39;, show_shapes=True, show_layer_names=True)   Generator Architecture    Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.\nHere is the architecture of the discriminator:\ndef get_disc_normal(image_shape=(64,64,3)): dropout_prob = 0.4 kernel_init = \u0026#39;glorot_uniform\u0026#39; dis_input = Input(shape = image_shape) # Conv layer 1: discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(dis_input) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 2: discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 3:  discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 4: discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = \u0026#34;same\u0026#34;, data_format = \u0026#34;channels_last\u0026#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator)#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator) # Flatten discriminator = Flatten()(discriminator) # Dense Layer discriminator = Dense(1)(discriminator) # Sigmoid Activation discriminator = Activation(\u0026#39;sigmoid\u0026#39;)(discriminator) # Optimizer and Compiling model dis_opt = Adam(lr=0.0002, beta_1=0.5) discriminator_model = Model(input = dis_input, output = discriminator) discriminator_model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=dis_opt, metrics=[\u0026#39;accuracy\u0026#39;]) discriminator_model.summary() return discriminator_model plot_model(discriminator, to_file=\u0026#39;dis_plot.png\u0026#39;, show_shapes=True, show_layer_names=True)   Discriminator Architecture    Training  Understanding how the training works in GAN is essential. And maybe a little interesting too.\nI start by creating our discriminator and generator using the functions defined in the previous section:\ndiscriminator = get_disc_normal(image_shape) generator = get_gen_normal(noise_shape) The generator and discriminator are then combined to create the final GAN.\ndiscriminator.trainable = False # Optimizer for the GAN opt = Adam(lr=0.00015, beta_1=0.5) #same as generator # Input to the generator gen_inp = Input(shape=noise_shape) GAN_inp = generator(gen_inp) GAN_opt = discriminator(GAN_inp) # Final GAN gan = Model(input = gen_inp, output = GAN_opt) gan.compile(loss = \u0026#39;binary_crossentropy\u0026#39;, optimizer = opt, metrics=[\u0026#39;accuracy\u0026#39;]) plot_model(gan, to_file=\u0026#39;gan_plot.png\u0026#39;, show_shapes=True, show_layer_names=True) This is the architecture of our whole GAN:\n The Training Loop This is the main region where we need to understand how the blocks we have created until now assemble and work together to work as one.\n# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(\u0026#34;Begin step: \u0026#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) # Genearate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) # Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 10th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir+step_num+\u0026#34;_image.png\u0026#34;) # Create the labels for real and fake data. We don\u0026#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(\u0026#34;Disc: real loss: %ffake loss: %f\u0026#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) # Train the generator using a random vector of noise and its labels (1\u0026#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(\u0026#34;GAN loss: %f\u0026#34; % (gan_metrics[0])) # Log results by opening a file in append mode text_file = open(log_dir+\u0026#34;\\\\training_log.txt\u0026#34;, \u0026#34;a\u0026#34;) text_file.write(\u0026#34;Step: %dDisc: real loss: %ffake loss: %fGAN loss: %f\\n\u0026#34; % (tot_step, dis_metrics_real[0], dis_metrics_fake[0],gan_metrics[0])) text_file.close() # save GAN loss to plot later avg_GAN_loss.append(gan_metrics[0]) end_time = time.time() diff_time = int(end_time - step_begin_time) print(\u0026#34;Step %dcompleted. Time took: %ssecs.\u0026#34; % (tot_step, diff_time)) # save model at every 500 steps if ((tot_step+1) % 500) == 0: print(\u0026#34;-----------------------------------------------------------------\u0026#34;) print(\u0026#34;Average Disc_fake loss: %f\u0026#34; % (np.mean(avg_disc_fake_loss))) print(\u0026#34;Average Disc_real loss: %f\u0026#34; % (np.mean(avg_disc_real_loss))) print(\u0026#34;Average GAN loss: %f\u0026#34; % (np.mean(avg_GAN_loss))) print(\u0026#34;-----------------------------------------------------------------\u0026#34;) discriminator.trainable = False generator.trainable = False # predict on fixed_noise fixed_noise_generate = generator.predict(noise) step_num = str(tot_step).zfill(4) save_img_batch(fixed_noise_generate,img_save_dir+step_num+\u0026#34;fixed_image.png\u0026#34;) generator.save(save_model_dir+str(tot_step)+\u0026#34;_GENERATOR_weights_and_arch.hdf5\u0026#34;) discriminator.save(save_model_dir+str(tot_step)+\u0026#34;_DISCRIMINATOR_weights_and_arch.hdf5\u0026#34;) Don‚Äôt worry, I will try to break the above code step by step here. The main steps in every training iteration are:\nStep 1: Sample a batch of normalized images from the dataset directory\n# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(\u0026#34;Begin step: \u0026#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) **Step2:**Generate noise for input to the generator\n# Generate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) **Step3:**Generate images using random noise using the generator.\n# Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 100th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir+step_num+\u0026#34;_image.png\u0026#34;) **Step 4:**Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their noisy labels.\n# Create the labels for real and fake data. We don\u0026#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(\u0026#34;Disc: real loss: %ffake loss: %f\u0026#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) **Step 5:**Train the GAN using noise as X and 1\u0026rsquo;s(noisy) as Y while keeping discriminator as untrainable.\n# Train the generator using a random vector of noise and its labels (1\u0026#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(\u0026#34;GAN loss: %f\u0026#34; % (gan_metrics[0])) We repeat the steps using the for loop to end up with a good discriminator and generator.\nResults The final output image looks like the following. As we can see, the GAN can generate pretty good images for our content editor friends to work with.\nThey might be a little crude for your liking, but still, this project was a starter for our GAN journey.\n  \u0026lt;/figure\u0026gt;    Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average and the variance is decreasing too as we do more steps. One might want to train for even more iterations to get better results.\n Image generated at every 1500 steps You can see the output and running code in Colab :\n# Generating GIF from PNGs import imageio # create a list of PNGs generated_images = [img_save_dir+str(x).zfill(4)+\u0026#34;_image.png\u0026#34; for x in range(0,num_steps,100)] images = [] for filename in generated_images: images.append(imageio.imread(filename)) imageio.mimsave(img_save_dir+\u0026#39;movie.gif\u0026#39;, images) from IPython.display import Image with open(img_save_dir+\u0026#39;movie.gif\u0026#39;,\u0026#39;rb\u0026#39;) as f: display(Image(data=f.read(), format=\u0026#39;png\u0026#39;))  Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases the images are getting better.\n# create a list of 20 PNGs to show generated_images = [img_save_dir+str(x).zfill(4)+\u0026#34;fixed_image.png\u0026#34; for x in range(0,num_steps,1500)] print(\u0026#34;Displaying generated images\u0026#34;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(16,20)) gs1 = gridspec.GridSpec(5, 4) gs1.update(wspace=0, hspace=0) for i,image in enumerate(generated_images): ax1 = plt.subplot(gs1[i]) ax1.set_aspect(\u0026#39;equal\u0026#39;) step = image.split(\u0026#34;fixed\u0026#34;)[0] image = Image.open(image) fig = plt.imshow(image) # you might need to change some params here fig = plt.text(20,47,\u0026#34;Step: \u0026#34;+step,bbox=dict(facecolor=\u0026#39;red\u0026#39;, alpha=0.5),fontsize=12) plt.axis(\u0026#39;off\u0026#39;) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.tight_layout() plt.savefig(\u0026#34;GENERATEDimage.png\u0026#34;,bbox_inches=\u0026#39;tight\u0026#39;,pad_inches=0) plt.show() Given below is the result of the GAN at different time steps:\n Conclusion In this post, we learned about the basics of GAN. We also learned about the Generator and Discriminator architecture for DC-GANs, and we built a simple DC-GAN to generate anime images from scratch.\nThis model is not very good at generating fake images, yet we get to understand the basics of GANs with this project, and we are fired up to build more exciting and complex GANs as we go forward.\nThe DC-GAN flavor of GANs is widely applicable not only to generate Faces or new anime characters, but it can also be used to generate new fashion styles, for general content creation and sometimes for data augmentation purposes as well.\nWe can now conjure up realistic textures or characters on demand if we have the training data at hand, and that is no small feat.\nIf you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew NG. Andrew is a great instructor, and this course is great too.\nI am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/06/17/gans/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Awesome Guides","Generative Adversarial NETWORKS"],"title":"An End to End Introduction to GANs using Keras"},{"categories":["Data Science","Awesome Guides"],"contents":"Good Features are the backbone of any machine learning model.\nAnd good feature creation often needs domain knowledge, creativity, and lots of time.\nIn this post, I am going to talk about:\n  Various methods of feature creation- Both Automated and manual\n  Different Ways to handle categorical features\n  Longitude and Latitude features\n  Some kaggle tricks\n  And some other ideas to think about feature creation.\n  TLDR; this post is about useful feature engineering methods and tricks that I have learned and end up using often.\n1. Automatic Feature Creation using featuretools:   Automation is the future    Have you read about featuretools yet? If not, then you are going to be delighted.\nFeaturetools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.\nHow? Let us work with a toy example to show you the power of featuretools.\nLet us say that we have three tables in our database: Customers, Sessions, and Transactions.\n  Datasets and relationships          This is a reasonably good toy dataset to work on since it has time-based columns as well as categorical and numerical columns.\nIf we were to create features on this data, we would need to do a lot of merging and aggregations using Pandas.\nFeaturetools makes it so easy for us. Though there are a few things, we will need to learn before our life gets easier.\nFeaturetools works with entitysets.\nYou can understand an entityset as a bucket for dataframes as well as relationships between them.\n  Entityset = Bucket of dataframes and relationships    So without further ado, let us create an empty entityset. I just gave the name as customers. You can use any name here. It is just an empty bucket right now.\n# Create new entityset es = ft.EntitySet(id = \u0026#39;customers\u0026#39;) Let us add our dataframes to it. The order of adding dataframes is not important. To add a dataframe to an existing entityset, we do the below operation.\n# Create an entity from the customers dataframe es = es.entity_from_dataframe(entity_id = \u0026#39;customers\u0026#39;, dataframe = customers_df, index = \u0026#39;customer_id\u0026#39;, time_index = \u0026#39;join_date\u0026#39; ,variable_types = {\u0026#34;zip_code\u0026#34;: ft.variable_types.ZIPCode}) So here are a few things we did here to add our dataframe to the empty entityset bucket.\n  Provided a entity_id: This is just a name. Put it as customers.\n  dataframe name set as customers_df\n  index : This argument takes as input the primary key in the table\n  time_index : The time index is defined as the first time that any information from a row can be used. For customers, it is the joining date. For transactions, it will be the transaction time.\n  variable_types: This is used to specify if a particular variable must be handled differently. In our Dataframe, we have the zip_code variable, and we want to treat it differently, so we use this. These are the different variable types we could use:\n  [featuretools.variable_types.variable.Datetime, featuretools.variable_types.variable.Numeric, featuretools.variable_types.variable.Timedelta, featuretools.variable_types.variable.Categorical, featuretools.variable_types.variable.Text, featuretools.variable_types.variable.Ordinal, featuretools.variable_types.variable.Boolean, featuretools.variable_types.variable.LatLong, featuretools.variable_types.variable.ZIPCode, featuretools.variable_types.variable.IPAddress, featuretools.variable_types.variable.EmailAddress, featuretools.variable_types.variable.URL, featuretools.variable_types.variable.PhoneNumber, featuretools.variable_types.variable.DateOfBirth, featuretools.variable_types.variable.CountryCode, featuretools.variable_types.variable.SubRegionCode, featuretools.variable_types.variable.FilePath] This is how our entityset bucket looks right now. It has just got one dataframe in it. And no relationships\n  Let us add all our dataframes:\n# adding the transactions_df es = es.entity_from_dataframe(entity_id=\u0026#34;transactions\u0026#34;, dataframe=transactions_df, index=\u0026#34;transaction_id\u0026#34;, time_index=\u0026#34;transaction_time\u0026#34;, variable_types={\u0026#34;product_id\u0026#34;: ft.variable_types.Categorical}) # adding sessions_df es = es.entity_from_dataframe(entity_id=\u0026#34;sessions\u0026#34;, dataframe=sessions_df, index=\u0026#34;session_id\u0026#34;, time_index = \u0026#39;session_start\u0026#39;) This is how our entityset buckets look now.\n  All three dataframes but no relationships. By relationships, I mean that my bucket doesn‚Äôt know that customer_id in customers_df and session_df are the same columns.\nWe can provide this information to our entityset as:\n# adding the customer_id relationship cust_relationship = ft.Relationship(es[\u0026#34;customers\u0026#34;][\u0026#34;customer_id\u0026#34;], es[\u0026#34;sessions\u0026#34;][\u0026#34;customer_id\u0026#34;]) # Add the relationship to the entity set es = es.add_relationship(cust_relationship) # adding the session_id relationship sess_relationship = ft.Relationship(es[\u0026#34;sessions\u0026#34;][\u0026#34;session_id\u0026#34;], es[\u0026#34;transactions\u0026#34;][\u0026#34;session_id\u0026#34;]) # Add the relationship to the entity set es = es.add_relationship(sess_relationship) After this our entityset looks like:\n  We can see the datasets as well as the relationships. Most of our work here is done. We are ready to cook features.\n  Cooking is no different from feature engineering. Think of features as ingredients.    Creating features is as simple as:\nfeature_matrix, feature_defs = ft.dfs(entityset=es, target_entity=\u0026#34;customers\u0026#34;,max_depth = 2) feature_matrix.head()   And we end up with 73 new features. You can see the feature names from feature_defs. Some of the features that we end up creating are:\n[\u0026lt;Feature: NUM_UNIQUE(sessions.device)\u0026gt;, \u0026lt;Feature: MODE(sessions.device)\u0026gt;, \u0026lt;Feature: SUM(transactions.amount)\u0026gt;, \u0026lt;Feature: STD(transactions.amount)\u0026gt;, \u0026lt;Feature: MAX(transactions.amount)\u0026gt;, \u0026lt;Feature: SKEW(transactions.amount)\u0026gt;, \u0026lt;Feature: DAY(join_date)\u0026gt;, \u0026lt;Feature: YEAR(join_date)\u0026gt;, \u0026lt;Feature: MONTH(join_date)\u0026gt;, \u0026lt;Feature: WEEKDAY(join_date)\u0026gt;, \u0026lt;Feature: SUM(sessions.STD(transactions.amount))\u0026gt;, \u0026lt;Feature: SUM(sessions.MAX(transactions.amount))\u0026gt;, \u0026lt;Feature: SUM(sessions.SKEW(transactions.amount))\u0026gt;, \u0026lt;Feature: SUM(sessions.MIN(transactions.amount))\u0026gt;, \u0026lt;Feature: SUM(sessions.MEAN(transactions.amount))\u0026gt;, \u0026lt;Feature: SUM(sessions.NUM_UNIQUE(transactions.product_id))\u0026gt;, \u0026lt;Feature: STD(sessions.SUM(transactions.amount))\u0026gt;, \u0026lt;Feature: STD(sessions.MAX(transactions.amount))\u0026gt;, \u0026lt;Feature: STD(sessions.SKEW(transactions.amount))\u0026gt;, \u0026lt;Feature: STD(sessions.MIN(transactions.amount))\u0026gt;, \u0026lt;Feature: STD(sessions.MEAN(transactions.amount))\u0026gt;, \u0026lt;Feature: STD(sessions.COUNT(transactions))\u0026gt;, \u0026lt;Feature: STD(sessions.NUM_UNIQUE(transactions.product_id))\u0026gt;] You can get features like the Sum of std of amount(SUM(sessions.STD(transactions.amount))) or Std of the sum of amount(STD(sessions.SUM(transactions.amount))) This is what max_depth parameter means in the function call. Here we specify it as 2 to get two level aggregations.\nIf we change max_depth to 3 we can get features like: MAX(sessions.NUM_UNIQUE(transactions.YEAR(transaction_time)))\nJust think of how much time you would have to spend if you had to write code to get such features. Also, a caveat is that increasing the max_depth might take longer times.\n2. Handling Categorical Features: Label/Binary/Hashing and Target/Mean Encoding Creating automated features has its perks. But why would we data scientists be required if a simple library could do all our work?\nThis is the section where I will talk about handling categorical features.\nOne hot encoding   One Hot Coffee    We can use One hot encoding to encode our categorical features. So if we have n levels in a category, we will get n-1 features.\nIn our sessions_df table, we have a column named device, which contains three levels ‚Äî desktop, mobile, or tablet. We can get two columns from such a column using:\npd.get_dummies(sessions_df[\u0026#39;device\u0026#39;],drop_first=True)   This is the most natural thing that comes to mind when talking about categorical features and works well in many cases.\nOrdinalEncoding Sometimes there is an order associated with categories. In such a case, I usually use a simple map/apply function in pandas to create a new ordinal column.\nFor example, if I had a dataframe containing temperature as three levels: high medium and low, I would encode that as:\nmap_dict = {\u0026#39;low\u0026#39;:0,\u0026#39;medium\u0026#39;:1,\u0026#39;high\u0026#39;:2} def map_values(x): return map_dict[x] df[\u0026#39;Temperature_oe\u0026#39;] = df[\u0026#39;Temperature\u0026#39;].apply(lambda x: map_values(x))   Using this I preserve the information that low \u0026lt; medium \u0026lt; high\nLabelEncoder We could also have used LabelEncoder to encode our variable to numbers. What a label encoder essentially does is that it sees the first value in the column and converts it to 0, next value to 1 and so on. This approach works reasonably well with tree models, and I end up using it when I have a lot of levels in the categorical variable. We can use this as:\nfrom sklearn.preprocessing import LabelEncoder # create a labelencoder object le = LabelEncoder() # fit and transform on the data sessions_df[\u0026#39;device_le\u0026#39;] = le.fit_transform(sessions_df[\u0026#39;device\u0026#39;]) sessions_df.head()   BinaryEncoder BinaryEncoder is another method that one can use to encode categorical variables. It is an excellent method to use if you have many levels in a column. While we can encode a column with 1024 levels using 1023 columns using One Hot Encoding, using Binary encoding we can do it by just using ten columns.\nLet us say we have a column in our FIFA 19 player data that contains all club names. This column has 652 unique values. One Hot encoding means creating 651 columns that would mean a lot of memory usage and a lot of sparse columns.\nIf we use Binary encoder, we will only need ten columns as 2‚Åπ\u0026lt;652 \u0026lt;2¬π‚Å∞.\nWe can binaryEncode this variable easily by using BinaryEncoder object from category_encoders:\nfrom category_encoders.binary import BinaryEncoder # create a Binaryencoder object be = BinaryEncoder(cols = [\u0026#39;Club\u0026#39;]) # fit and transform on the data players = be.fit_transform(players)   HashingEncoder   One can think of Hashing Encoder as a black box function that converts a string to a number between 0 to some prespecified value.\nIt differs from binary encoding as in binary encoding two or more of the club parameters could have been 1 while in hashing only one value is 1.\nWe can use hashing as:\nplayers = pd.read_csv(\u0026#34;../input/fifa19/data.csv\u0026#34;) from category_encoders.hashing import HashingEncoder # create a HashingEncoder object he = HashingEncoder(cols = [\u0026#39;Club\u0026#39;]) # fit and transform on the data players = he.fit_transform(players)   There are bound to be collisions(two clubs having the same encoding. For example, Juventus and PSG have the same encoding) but sometimes this technique works well.\nTarget/Mean Encoding   This is a technique that I found works pretty well in Kaggle competitions. If both training/test comes from the same dataset from the same time period(cross-sectional), we can get crafty with features.\nFor example: In the Titanic knowledge challenge, the test data is randomly sampled from the train data. In this case, we can use the target variable averaged over different categorical variable as a feature.\nIn Titanic, we can create a target encoded feature over the PassengerClass variable.\nWe have to be careful when using Target encoding as it might induce overfitting in our models. Thus we use k-fold target encoding when we use it.\n# taken from https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b from sklearn import base from sklearn.model_selection import KFold class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin): def __init__(self,colnames,targetName, n_fold=5, verbosity=True, discardOriginal_col=False): self.colnames = colnames self.targetName = targetName self.n_fold = n_fold self.verbosity = verbosity self.discardOriginal_col = discardOriginal_col def fit(self, X, y=None): return self def transform(self,X): assert(type(self.targetName) == str) assert(type(self.colnames) == str) assert(self.colnames in X.columns) assert(self.targetName in X.columns) mean_of_target = X[self.targetName].mean() kf = KFold(n_splits = self.n_fold, shuffle = True, random_state=2019) col_mean_name = self.colnames + \u0026#39;_\u0026#39; + \u0026#39;Kfold_Target_Enc\u0026#39; X[col_mean_name] = np.nan for tr_ind, val_ind in kf.split(X): X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind] X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames) [self.targetName].mean()) X[col_mean_name].fillna(mean_of_target, inplace = True) if self.verbosity: encoded_feature = X[col_mean_name].values print(\u0026#39;Correlation between the new feature, {} and, {} is {}.\u0026#39;.format(col_mean_name,self.targetName, np.corrcoef(X[self.targetName].values, encoded_feature)[0][1])) if self.discardOriginal_col: X = X.drop(self.targetName, axis=1) return X We can then create a mean encoded feature as:\ntargetc = KFoldTargetEncoderTrain(\u0026#39;Pclass\u0026#39;,\u0026#39;Survived\u0026#39;,n_fold=5) new_train = targetc.fit_transform(train) new_train[[\u0026#39;Pclass_Kfold_Target_Enc\u0026#39;,\u0026#39;Pclass\u0026#39;]]   You can see how the passenger class 3 gets encoded as 0.261538 and 0.230570 based on which fold the average is taken from.\nThis feature is pretty helpful as it encodes the value of the target for the category. Just looking at this feature, we can say that the Passenger in class 1 has a high propensity of surviving compared with Class 3.\n3. Some Kaggle Tricks: While not necessarily feature creation techniques, some postprocessing techniques that you may find useful.\nlog loss clipping Technique: Something that I learned in the Neural Network course by Jeremy Howard. It is based on an elementary Idea.\nLog loss penalizes us a lot if we are very confident and wrong.\nSo in the case of Classification problems where we have to predict probabilities in Kaggle, it would be much better to clip our probabilities between 0.05‚Äì0.95 so that we are never very sure about our prediction. And in turn, get penalized less. Can be done by a simple np.clip\nKaggle submission in gzip format: A small piece of code that will help you save countless hours of uploading. Enjoy.\ndf.to_csv(‚Äòsubmission.csv.gz‚Äô, index=False, compression=‚Äôgzip‚Äô) 4. Using Latitude and Longitude features: This part will tread upon how to use Latitude and Longitude features well.\nFor this task, I will be using Data from the Playground competition: New York City Taxi Trip Duration The train data looks like:\n  Most of the functions I am going to write here are inspired by a Kernel on Kaggle written by Beluga.\nIn this competition, we had to predict the trip duration. We were given many features in which Latitude and Longitude of pickup and Dropoff were also there. We created features like:\nA. Haversine Distance Between the Two Lat/Lons:  The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes\n def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h We could then use the function as:\ntrain[\u0026#39;haversine_distance\u0026#39;] = train.apply(lambda x: haversine_array(x[\u0026#39;pickup_latitude\u0026#39;], x[\u0026#39;pickup_longitude\u0026#39;], x[\u0026#39;dropoff_latitude\u0026#39;], x[\u0026#39;dropoff_longitude\u0026#39;]),axis=1) B. Manhattan Distance Between the two Lat/Lons:   Manhattan Skyline     The distance between two points measured along axes at right angles\n def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a + b We could then use the function as:\ntrain[\u0026#39;manhattan_distance\u0026#39;] = train.apply(lambda x: dummy_manhattan_distance(x[\u0026#39;pickup_latitude\u0026#39;], x[\u0026#39;pickup_longitude\u0026#39;], x[\u0026#39;dropoff_latitude\u0026#39;], x[\u0026#39;dropoff_longitude\u0026#39;]),axis=1) C. Bearing Between the two Lat/Lons: A bearing is used to represent the direction of one point relative to another point.\ndef bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) We could then use the function as:\ntrain[\u0026#39;bearing\u0026#39;] = train.apply(lambda x: bearing_array(x[\u0026#39;pickup_latitude\u0026#39;], x[\u0026#39;pickup_longitude\u0026#39;], x[\u0026#39;dropoff_latitude\u0026#39;], x[\u0026#39;dropoff_longitude\u0026#39;]),axis=1) D. Center Latitude and Longitude between Pickup and Dropoff: train.loc[:, \u0026#39;center_latitude\u0026#39;] = (train[\u0026#39;pickup_latitude\u0026#39;].values + train[\u0026#39;dropoff_latitude\u0026#39;].values) / 2 train.loc[:, \u0026#39;center_longitude\u0026#39;] = (train[\u0026#39;pickup_longitude\u0026#39;].values + train[\u0026#39;dropoff_longitude\u0026#39;].values) / 2 These are the new columns that we create:\n  5. AutoEncoders: Sometimes people use Autoencoders too for creating automatic features.\nWhat are Autoencoders?\nEncoders are deep learning functions which approximate a mapping from X to X, i.e. input=output. They first compress the input features into a lower-dimensional representation/code and then reconstruct the output from this representation.\n  We can use this representation vector as a feature for our models.\n6. Some Normal Things you can do with your features:   Scaling by Max-Min: This is good and often required preprocessing for Linear models, Neural Networks\n  Normalization using Standard Deviation: This is good and often required preprocessing for Linear models, Neural Networks\n  Log-based feature/Target: Use log based features or log-based target function. If one is using a Linear model which assumes that the features are normally distributed, a log transformation could make the feature normal. It is also handy in case of skewed variables like income.\n  Or in our case trip duration. Below is the graph of trip duration without log transformation.\n  And with log transformation:\ntrain[\u0026#39;log_trip_duration\u0026#39;] = train[\u0026#39;trip_duration\u0026#39;].apply(lambda x: np.log(1+x))   A log transformation on trip duration is much less skewed and thus much more helpful for a model.\n7. Some Additional Features based on Intuition: Date time Features: One could create additional Date time features based on domain knowledge and intuition. For example, Time-based Features like ‚ÄúEvening,‚Äù ‚ÄúNoon,‚Äù ‚ÄúNight,‚Äù ‚ÄúPurchases_last_month,‚Äù ‚ÄúPurchases_last_week,‚Äù etc. could work for a particular application.\nDomain Specific Features:   Style matters    Suppose you have got some shopping cart data and you want to categorize the TripType. It was the exact problem in Walmart Recruiting: Trip Type Classification on Kaggle .\nSome examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes.\nTo solve this problem, you could think of creating a feature like ‚ÄúStylish‚Äù where you create this variable by adding together the number of items that belong to category Men‚Äôs Fashion, Women‚Äôs Fashion, Teens Fashion.\nOr you could create a feature like ‚ÄúRare‚Äù which is created by tagging some items as rare, based on the data we have and then counting the number of those rare items in the shopping cart.\nSuch features might work or might not work. From what I have observed, they usually provide a lot of value.\nI feel this is the way that Target‚Äôs ‚ÄúPregnant Teen model‚Äù was made. They would have had a variable in which they kept all the items that a pregnant teen could buy and put them into a classification algorithm.\nInteraction Features: If you have features A and B, you can create features A*B, A+B, A/B, A-B, etc.\nFor example, to predict the price of a house, if we have two features length and breadth, a better idea would be to create an area(length x breadth) feature.\nOr in some case, a ratio might be more valuable than having two features alone. Example: Credit Card utilization ratio is more valuable than having the Credit limit and limit utilized variables.\nConclusion   Creativity is vital!!!    These were just some of the methods I use for creating features.\nBut there is surely no limit when it comes to feature engineering, and it is only your imagination that limits you.\nOn that note, I always think about feature engineering while keeping what model I am going to use in mind. Features that work in a random forest may not work well with Logistic Regression.\nFeature creation is the territory of trial and error. You won‚Äôt be able to know what transformation works or what encoding works best before trying it. It is always a trade-off between time and utility.\nSometimes the feature creation process might take a lot of time. In such cases, you might want to parallelize your Pandas function .\nWhile I have tried to keep this post as exhaustive as possible, I might have missed some of the useful methods. Let me know about them in the comments.\nYou can find all the code for this post and run it yourself in this Kaggle Kernel Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of intuitive ways to improve your model. Definitely recommended.\nI am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/05/19/feature_extraction/","tags":["Machine Learning","Data Science","Awesome Guides","Best Content"],"title":"The Hitchhiker‚Äôs Guide to Feature Extraction"},{"categories":null,"contents":"It is election month in India and a quote by Dr. Rahat Indori sums it up pretty well.\n ‚Äú‡§∏‡§∞‡§π‡§¶‡•ã‡§Ç ‡§™‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§§‡§®‡§æ‡§µ ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ , ‡§™‡§§‡§æ ‡§§‡•ã ‡§ï‡§∞‡•ã ‡§ö‡•Å‡§®‡§æ‡§µ ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ !‚Äù  For English speakers, this means: Is there a lot of tension at the borders? just ask if the elections are on.\nThis election India has talked about a lot of issues. News channels have talked about Patriotism, Socialism, Religion as well as terrorism.\nYou might have heard of the tension brewing between India and Pakistan.\nThis election season has also been marred with a cacophony of insults. Politicians seem to have forgotten basic manners.\nCalling the Prime Minister a Chai-Wala(Tea-Seller) and a Chowkidaar(Watchman). And the Prime Minister accepting both titles with grace and putting a Chowkidaar in front of his name on Twitter too. It was pure genius.\nToday is the final phase of the 2019 Lok Sabha elections. I urge all those voting in this phase to vote in record numbers. Your one vote will shape India‚Äôs development trajectory in the years to come. I also hope first time voters vote enthusiastically.\n\u0026mdash; Chowkidar Narendra Modi (@narendramodi) May 19, 2019  Now finally the elections are over and the news channels are releasing the exit polls. This is pretty good data to look at.\nAnd why waste good data?\nSo, here is an effort to showing some of the election data using some interactive visualizations using Flourish.\nRecap of Politics in India To give a little perspective, in India, the center is held by BJP(Bhartiya Janta Party) and Narendra Modi is the current prime minister of India.\nMost of India‚Äôs political history has been dominated by INC(Indian National Congress). This is the party of Mahatma Gandhi. INC‚Äôs head Rahul Gandhi, who is sometimes called the 50 years old youth, could be the perfect example of dynastic politics.\nThere are a lot of other local state parties, which I will call as OTH(Others)\nFor a perspective to the US audience(very broadly): BJP=Right Wing and INC=Left Wing.\nIndia has 28 states and each state has a number of electoral seats. This is the seat distribution as per states.\n Let us talk a little about the swing states.\nThe first state is Uttar Pradesh. A region of caste-based politics. Holding 8o seats out of 543 seats. A state that could swing the whole election. This state is the home of a large coalition named ‚ÄúMahaGatbandhan‚Äù(Literally a large coalition). This coalition is created by two heavy-weights ‚Äî Akhilesh Yadav and Mayawati. Both have totally opposite views, yet they came together to move the BJP government out of the center.\nThen there is the case of West Bengal having 42 seats, which is held by Mamata Banerjee. Sometimes people say that it might be a dictatorship and there have been a lot of violent incidents in this state over the past few days.She might also form a coalition with INC in the later stage to overthrow the current government.\nAll in all, INC, MahaGatbandhan and all others are out there to overthrow BJP(Modi government)\nExit Polls  Now the elections are over after 7 phases of voting. And today is the exit polls in India.\nThat means all the news channels will come up with their individual surveys across the 542 Loksabha seats across the 28 states.\nLet us not wait further and look at exit poll data of various news channels.\nHere it is:\n Seems like a sweeping victory for BJP and alliance.\nAnd here is a racing bar chart of the same.\n The only thing that doesn‚Äôt seem to move is BJP+ and seems like there will be a second term based on this visualization.\n The parliament seems to look a little bit orange(BJP Color)though not as much as the last time.\nAnd the swing states. I am going to look at poll data by TimesNow.\n While it seems like a defeat for BJP+ in West Bengal, more than 10 seats in WB is actually a lot as per the state politics of left and should be categorized as a win.\nIn Uttar Pradesh too, BJP+ was able to handle a great opposition in form of Mahagatbandhan and that seems a win too.\nConclusion The 2019 Mandate seems to be favoring BJP and its allies.\nAnd it is a sweeping answer to people who promote caste-based politics, dynastic rules, and minority appeasement politics.\nHere is a tweet showing some frustration by the current CM of West Bengal.\nI don‚Äôt trust Exit Poll gossip. The game plan is to manipulate or replace thousands of EVMs through this gossip. I appeal to all Opposition parties to be united, strong and bold. We will fight this battle together\n\u0026mdash; Mamata Banerjee (@MamataOfficial) May 19, 2019  While the data seems to show a lot of promise for BJP+, we still need to see if the predictions by these news channels stand up to the actuals. The Election results will be published by 23 May.\nI would also like to highlight Flourish here, which made creating the visualizations a breeze.\nVisualizations that were Easy to make, Publish and share.\nIf you want to learn about best strategies for creating Visualizations, I would like to call out an excellent course about \u0026lt;strong\u0026gt;Data Visualization and applied plotting\u0026lt;/strong\u0026gt; from the University of Michigan which is a part of a pretty good \u0026lt;strong\u0026gt;Data Science Specialization with Python\u0026lt;/strong\u0026gt; in itself. Do check it out\nI am going to be writing more visualization posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\nPS: I am a BJP Supporter but I have tried to be as objective as possible.\n","permalink":"https://mlwhiz.com/blog/2019/05/19/election/","tags":null,"title":"The Nation of a Billion Votes"},{"categories":["Programming","Data Science"],"contents":"Python has a lot of constructs that are reasonably easy to learn and use in our code. Then there are some constructs which always confuse us when we encounter them in our code.\nThen are some that even seasoned programmers are not able to understand. *args, **kwargs and decorators are some constructs that fall into this category.\nI guess a lot of my data science friends have faced them too.\nMost of the seaborn functions use *args and **kwargs in some way or other.\n  Or what about decorators?\nEvery time you see a warning like some function will be deprecated in the next version. The sklearn package uses decorators for that. You can see the @deprecated in the source code. That is a decorator function.\n  In this series of posts named Python Shorts I will explain some simple constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.\nThis post is about explaining some of the difficult concepts in an easy to understand way.\nWhat are *args? In simple terms,*you can use args to give an arbitrary number of inputs to your function.\nA simple example: Let us say we have to create a function that adds two numbers. We can do this easily in python.\ndef adder(x,y): return x+y What if we want to create a function to add three variables?\ndef adder(x,y,z): return x+y+z What if we want the same function to add an unknown number of variables? Please note that we can use *args or *argv or *anyOtherName to do this. It is the * that matters.\ndef adder(*args): result = 0 for arg in args: result+=arg return result What *args does is that it takes all your passed arguments and provides a variable length argument list to the function which you can use as you want.\nNow you can use the same function as follows:\nadder(1,2) adder(1,2,3) adder(1,2,5,7,8,9,100) and so on.\nNow, have you ever thought how the print function in python could take so many arguments? *args\n What are **kwargs?   In simple terms,you can use **kwargs to give an arbitrary number of Keyworded inputs to your function and access them using a dictionary.\nA simple example: Let‚Äôs say you want to create a print function that can take a name and age as input and print that.\ndef myprint(name,age): print(f\u0026#39;{name} is {age} years old\u0026#39;) Simple. Let us now say you want the same function to take two names and two ages.\ndef myprint(name1,age1,name2,age2): print(f\u0026#39;{name1} is {age1} years old\u0026#39;) print(f\u0026#39;{name2} is {age2} years old\u0026#39;) You guessed right my next question is: What if I don‚Äôt know how many arguments I am going to need?\nCan I use *args? Guess not since name and age order is essential. We don‚Äôt want to write ‚Äú28 is Michael years old‚Äù.\nCome **kwargs in the picture.\ndef myprint(**kwargs): for k,v in kwargs.items(): print(f\u0026#39;{k} is {v} years old\u0026#39;) You can call this function using:\nmyprint(Sansa=20,Tyrion=40,Arya=17) Output: ----------------------------------- Sansa is 20 years old Tyrion is 40 years old Arya is 17 years old Remember we never defined Sansa or Arya or Tyrion as our methods arguments.\nThat is a pretty powerful concept. And many programmers utilize this pretty cleverly when they write wrapper libraries.\nFor example, seaborn.scatterplot function wraps the plt.scatter function from Matplotlib. Essentially, using *args and **kwargs we can provide all the arguments that plt.scatter can take to seaborn.Scatterplot as well.\nThis can save a lot of coding effort and also makes the code future proof. If at any time in the future plt.scatter starts accepting any new arguments the seaborn.Scatterplot function will still work.\n What are Decorators?   In simple terms: Decorators are functions that wrap another function thus modifying its behavior.\nA simple example: Let us say we want to add custom functionality to some of our functions. The functionality is that whenever the function gets called the ‚Äúfunction name begins‚Äù is printed and whenever the function ends the \u0026ldquo;function name ends‚Äù and time taken by the function is printed.\nLet us assume our function is:\ndef somefunc(a,b): output = a+b return output We can add some print lines to all our functions to achieve this.\nimport time def somefunc(a,b): print(\u0026#34;somefunc begins\u0026#34;) start_time = time.time() output = a+b print(\u0026#34;somefunc ends in \u0026#34;,time.time()-start_time, \u0026#34;secs\u0026#34;) return output out = somefunc(4,5) OUTPUT: ------------------------------------------- somefunc begins somefunc ends in 9.5367431640625e-07 secs But, Can we do better?\nThis is where decorators excel. We can use decorators to wrap any function.\nfrom functools import wraps def timer(func): [@wraps](http://twitter.com/wraps)(func) def wrapper(a,b): print(f\u0026#34;{func.__name__!r} begins\u0026#34;) start_time = time.time() func(a,b) print(f\u0026#34;{func.__name__!r} ends in {time.time()-start_time} secs\u0026#34;) return wrapper This is how we can define any decorator. functools helps us create decorators using wraps. In essence, we do something before any function is called and do something after a function is called in the above decorator.\nWe can now use this timer decorator to decorate our function somefunc\n@timer def somefunc(a,b): output = a+b return output Now calling this function, we get:\na = somefunc(4,5) Output --------------------------------------------- 'somefunc' begins 'somefunc' ends in 2.86102294921875e-06 secs Now we can append @timer to each of our function for which we want to have the time printed. And we are done.\nReally?\nConnecting all the pieces   What if our function takes three arguments? Or many arguments?\nThis is where whatever we have learned till now connects. We use *args and **kwargs\nWe change our decorator function as:\nfrom functools import wraps def timer(func): [@wraps](http://twitter.com/wraps)(func) def wrapper(*args,**kwargs): print(f\u0026#34;{func.__name__!r} begins\u0026#34;) start_time = time.time() func(*args,**kwargs) print(f\u0026#34;{func.__name__!r} ends in {time.time()-start_time} secs\u0026#34;) return wrapper Now our function can take any number of arguments, and our decorator will still work.\n Isn‚Äôt Python Beautiful?  In my view, decorators could be pretty helpful. I provided only one use case of decorators, but there are several ways one can use them.\nYou can use a decorator to debug code by checking which arguments go in a function. Or a decorator could be used to count the number of times a particular function has been called. This could help with counting recursive calls.\n Conclusion In this post, I talked about some of the constructs you can find in python source code and how you can understand them.\nIt is not necessary that you end up using them in your code now. But I guess understanding how these things work helps mitigate some of the confusion and panic one faces whenever these constructs come up.\n Understanding is vital when it comes to coding  Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; .\n","permalink":"https://mlwhiz.com/blog/2019/05/14/python_args_kwargs/","tags":["Python"],"title":"A primer on *args, **kwargs, decorators for Data Scientists"},{"categories":["Data Science"],"contents":"I distinctly remember the time when Seaborn came. I was really so fed up with Matplotlib. To create even simple graphs I had to run through so many StackOverflow threads.\nThe time I could have spent in thinking good ideas for presenting my data was being spent in handling Matplotlib. And it was frustrating.\nSeaborn is much better than Matplotlib, yet it also demands a lot of code for a simple ‚Äúgood looking‚Äù graph.\nWhen Plotly came it tried to solve that problem. And when added with Pandas, plotly is a great tool.\nJust using the iplot function, you can do so much with Plotly.\nBut still, it is not very intuitive. At least not for me.\nI still didn‚Äôt switch to Plotly just because I had spent enough time with Seaborn to do things ‚Äúquickly‚Äù enough and I didn‚Äôt want to spend any more time learning a new visualization library. I had created my own functions in Seaborn to create the visualizations I most needed. Yet it was still a workaround. I had given up hope of having anything better.\nComes Plotly Express in the picture. And is it awesome?\nAccording to the creators of Plotly Express (who also created Plotly obviously), Plotly Express is to Plotly what Seaborn is to Matplotlib.\n A terse, consistent, high-level wrapper around Plotly.py for rapid data exploration and figure generation.  I just had to try it out.\nAnd have the creators made it easy to start experimenting with it?\nOne-liners to do everything you want? ‚úÖ\nStandardized functions? Learn to create a scatterplot and you have pretty much learned this tool ‚Äî ‚úÖ\nInteractive graphs? ‚úÖ\nAnimations? Racing Bar plots, Scatter plots with time, Maps ‚úÖ\nFree and Open Source? ‚úÖ\nJust a sneak peek of what we will be able to create(and more) by the end of this post. Using a single line of code.\n  Ok enough of the talk, let‚Äôs get to it.\nFirst the Dataset ‚Äî Interesting, Depressing and Inspiring all at once  We will be working with the Suicide dataset I took from Kaggle. This dataset is compiled from data taken from the UN, World Bank and World Health Organization. The dataset was accumulated with the inspiration for Suicide Prevention. I am always up for such good use of data.\nYou can find all the code for this post and run it yourself in this Kaggle Kernel First I will do some data Cleaning to add continent information and Country ISO codes as they will be helpful later:\nimport pandas as pd import numpy as np import plotly_express as px # Suicide Data suicides = pd.read_csv(\u0026#34;../input/suicide-rates-overview-1985-to-2016/master.csv\u0026#34;) del suicides[\u0026#39;HDI for year\u0026#39;] del suicides[\u0026#39;country-year\u0026#39;] # Country ISO Codes iso_country_map = pd.read_csv(\u0026#34;../input/countries-iso-codes/wikipedia-iso-country-codes.csv\u0026#34;) iso_country_map = iso_country_map.rename(columns = {\u0026#39;English short name lower case\u0026#39;:\u0026#34;country\u0026#34;}) # Load Country Continents file concap =pd.read_csv(\u0026#34;../input/country-to-continent/countryContinent.csv\u0026#34;, encoding=\u0026#39;iso-8859-1\u0026#39;)[[\u0026#39;code_3\u0026#39;, \u0026#39;continent\u0026#39;, \u0026#39;sub_region\u0026#39;]] concap = concap.rename(columns = {\u0026#39;code_3\u0026#39;:\u0026#34;Alpha-3 code\u0026#34;}) correct_names = {\u0026#39;Cabo Verde\u0026#39;: \u0026#39;Cape Verde\u0026#39;, \u0026#39;Macau\u0026#39;: \u0026#39;Macao\u0026#39;, \u0026#39;Republic of Korea\u0026#39;: \u0026#34;Korea, Democratic People\u0026#39;s Republic of\u0026#34; , \u0026#39;Russian Federation\u0026#39;: \u0026#39;Russia\u0026#39;, \u0026#39;Saint Vincent and Grenadines\u0026#39;:\u0026#39;Saint Vincent and the Grenadines\u0026#39; , \u0026#39;United States\u0026#39;: \u0026#39;United States Of America\u0026#39;} def correct_country(x): if x in correct_names: return correct_names[x] else: return x suicides[\u0026#39;country\u0026#39;] = suicides[\u0026#39;country\u0026#39;].apply(lambda x : correct_country(x)) suicides = pd.merge(suicides,iso_country_map,on=\u0026#39;country\u0026#39;,how=\u0026#39;left\u0026#39;) suicides = pd.merge(suicides,concap,on=\u0026#39;Alpha-3 code\u0026#39;,how=\u0026#39;left\u0026#39;) suicides[\u0026#39;gdp\u0026#39;] = suicides[\u0026#39;gdp_per_capita ($)\u0026#39;]*suicides[\u0026#39;population\u0026#39;] Let us look at the suicides data:\n  I will also group the data by continents. Honestly, I am doing this only to show the power of the library as the main objective of this post will still be to create awesome visualizations.\nsuicides_gby_Continent = suicides.groupby([\u0026#39;continent\u0026#39;,\u0026#39;sex\u0026#39;,\u0026#39;year\u0026#39;]).aggregate(np.sum).reset_index() suicides_gby_Continent[\u0026#39;gdp_per_capita ($)\u0026#39;] = suicides_gby_Continent[\u0026#39;gdp\u0026#39;]/suicides_gby_Continent[\u0026#39;population\u0026#39;] suicides_gby_Continent[\u0026#39;suicides/100k pop\u0026#39;] = suicides_gby_Continent[\u0026#39;suicides_no\u0026#39;]*1000/suicides_gby_Continent[\u0026#39;population\u0026#39;] # 2016 data is not full suicides_gby_Continent=suicides_gby_Continent[suicides_gby_Continent[\u0026#39;year\u0026#39;]!=2016] suicides_gby_Continent.head() The final data we created:\n  Simplicity of use We are ready to visualize our data. Comes Plotly Express time. I can install it just by a simple:\npip install plotly_express and import it as:\nimport plotly_express as px Now let us create a simple scatter plot with it.\nsuicides_gby_Continent_2007 = suicides_gby_Continent[suicides_gby_Continent[\u0026#39;year\u0026#39;]==2007] px.scatter(suicides_gby_Continent_2007,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;)   Not very inspiring. Right. Let us make it better step by step. Lets color the points by Continent.\npx.scatter(suicides_gby_Continent_2007,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;,color=\u0026#39;continent\u0026#39;)   Better but not inspiring. YET.\nThe points look so small. Right. Let us increase the point size. How? What could the parameter be‚Ä¶.\npx.scatter(suicides_gby_Continent_2007,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;,color=\u0026#39;ContinentName\u0026#39;,size =\u0026#39;suicides/100k pop\u0026#39;)   Can you see there are two points for every continent? They are for male and female. Let me show that in the graph. We can show this distinction using a couple of ways. We can use different symbol or use different facets for male and female.\nLet me show them both.\npx.scatter(suicides_gby_Continent_2007,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;, size = \u0026#39;suicides/100k pop\u0026#39;, color=\u0026#39;ContinentName\u0026#39;,symbol=\u0026#39;sex\u0026#39;)    We could also create a faceted plot.\npx.scatter(suicides_gby_Continent_2007,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;, size = \u0026#39;suicides/100k pop\u0026#39;, color=\u0026#39;continent\u0026#39;,facet_col=\u0026#39;sex\u0026#39;)   The triangles are for male and the circles are for females in the symbol chart. We are already starting to see some good info from the chart. For example:\n  There is a significant difference between the suicide rates of Male vs Females at least in 2007 data.\n  European Males were highly susceptible to Suicide in 2007?\n  The income disparity doesn‚Äôt seem to play a big role in suicide rates. Asia has a lower GDP per capita and a lower suicide rate than Europe.\n  There doesn‚Äôt seem to be income disparity amongst males and females.\n  Still not inspiring? Umm. Let us add some animation. That shouldn‚Äôt have to be hard. I will just add some more parameters,\n  animation_frame which specifies what will be our animation dimension.\n  range of x and y values using range_y and range_x\n  text which labels all points with continents. Helps in visualizing data better\n  px.scatter(suicides_gby_Continent,x = \u0026#39;suicides/100k pop\u0026#39;, y = \u0026#39;gdp_per_capita ($)\u0026#39;,color=\u0026#39;continent\u0026#39;, size=\u0026#39;suicides/100k pop\u0026#39;,symbol=\u0026#39;sex\u0026#39;,animation_frame=\u0026#39;year\u0026#39;, animation_group=\u0026#39;continent\u0026#39;,range_x = [0,0.6], range_y = [0,70000],text=\u0026#39;continent\u0026#39;)   Wait for the gif plot to show.\nIn the Jupyter notebook, you will be able to stop the visualization, hover over the points, just look at a particular continent and do so much more with interactions.\nSo much information with a single command. We can see that:\n  From 1991‚Äì2001 European Males had a pretty bad Suicide rate.\n  Oceania even after having a pretty high GDP per capita, it is still susceptible to suicides.\n  Africa has lower suicide rates as compared to other countries.\n  For the Americas, the suicide rates have been increasing gradually.\n  All of my above observations would warrant more analysis. But that is the point of having so much information on a single graph. It will help you to come up with a lot of hypotheses.\nThe above style of the plot is known as Hans Rosling plot named after its founder.\nHere I would ask you to see this presentation from Hans Rosling where he uses Gapminder data to explain how income and lifespan emerged in the world through years. See it. It\u0026rsquo;s great.\n Function Standardization So till now, we have learned about scatter plots. So much time to just learn a single class of charts. In the start of my post, I told you that this library has a sort of standardized functions.\nLet us specifically look at European data as we saw that European males have a high Suicide rate.\neuropean_suicide_data = suicides[suicides[\u0026#39;continent\u0026#39;] ==\u0026#39;Europe\u0026#39;] european_suicide_data_gby = european_suicide_data.groupby([\u0026#39;age\u0026#39;,\u0026#39;sex\u0026#39;,\u0026#39;year\u0026#39;]).aggregate(np.sum).reset_index() european_suicide_data_gby[\u0026#39;suicides/100k pop\u0026#39;] = european_suicide_data_gby[\u0026#39;suicides_no\u0026#39;]*1000/european_suicide_data_gby[\u0026#39;population\u0026#39;] # A single line to create an animated Bar chart too. px.bar(european_suicide_data_gby,x=\u0026#39;age\u0026#39;,y=\u0026#39;suicides/100k pop\u0026#39;,facet_col=\u0026#39;sex\u0026#39;,animation_frame=\u0026#39;year\u0026#39;, animation_group=\u0026#39;age\u0026#39;, category_orders={\u0026#39;age\u0026#39;:[\u0026#39;5-14 years\u0026#39;, \u0026#39;15-24 years\u0026#39;, \u0026#39;25-34 years\u0026#39;, \u0026#39;35-54 years\u0026#39;, \u0026#39;55-74 years\u0026#39;, \u0026#39;75+ years\u0026#39;]},range_y=[0,1])  Just like that, we have learned about animating our bar plots too. In the function above I provide a category_order for the axes to force the order of categories since they are ordinal. Rest all is still the same.\nWe can see that from 1991 to 2001 the suicide rate of 75+ males was very high. That might have increased the overall suicide rate for males.\nWant to see how the suicide rates decrease in a country using a map? That is why we got the ISO-codes for the country in the data.\nHow many lines should that take? You guessed right. One.\nsuicides_map = suicides.groupby([\u0026#39;year\u0026#39;,\u0026#39;country\u0026#39;,\u0026#39;Alpha-3 code\u0026#39;]).aggregate(np.sum).reset_index()[[\u0026#39;country\u0026#39;,\u0026#39;Alpha-3 code\u0026#39;,\u0026#39;suicides_no\u0026#39;,\u0026#39;population\u0026#39;,\u0026#39;year\u0026#39;]] suicides_map[\u0026#34;suicides/100k pop\u0026#34;]=suicides_map[\u0026#34;suicides_no\u0026#34;]*1000/suicides_map[\u0026#34;population\u0026#34;] px.choropleth(suicides_map, locations=\u0026#34;Alpha-3 code\u0026#34;, color=\u0026#34;suicides/100k pop\u0026#34;, hover_name=\u0026#34;country\u0026#34;, animation_frame=\u0026#34;year\u0026#34;, color_continuous_scale=px.colors.sequential.Plasma)  The plot above shows how suicide rates have changed over time in different countries and based on the info we get from the plot the coding effort required is minimal. We can see that:\n  A lot of countries are missing\n  Africa has very few countries in data\n  Almost all of Asia is also missing.\n  We can get quite a good understanding of our data just by seeing the above graphs.\nAnimations on the time axis also add up a lot of value as we are able to see all our data using a single graph.\nThis can help us in finding hidden patterns in the data. And you have to agree, it looks cool too.\nConclusion This was just a preview of Plotly Express. You can do a lot of other things using this library.\nThe main thing I liked about this library is the way it has tried to simplify graph creation. And how the graphs look cool out of the box.\nJust think of the lengths one would have to go to to create the same graphs in Seaborn or Matplotlib or even Plotly. And you will be able to appreciate the power the library provides even more.\nThere is a bit of lack of documentation for this project by Plotly, but I found that the functions are pretty much well documented. On that note, you can see function definitions using Shift+Tab in Jupyter.\nAlso as per its announcement article: ‚ÄúPlotly Express is totally free: with its permissive open-source MIT license, you can use it however you like (yes, even in commercial products!).‚Äù\nSo there is no excuse left now to put off that visual. Just get to it‚Ä¶\nYou can find all the code for this post and run it yourself in this Kaggle Kernel If you want to learn about best strategies for creating Visualizations, I would like to call out an excellent course about \u0026lt;strong\u0026gt;Data Visualization and applied plotting\u0026lt;/strong\u0026gt; from the University of Michigan which is a part of a pretty good \u0026lt;strong\u0026gt;Data Science Specialization with Python\u0026lt;/strong\u0026gt; in itself. Do check it out\nI am going to be writing more beginner friendly posts in the future too. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz ","permalink":"https://mlwhiz.com/blog/2019/05/05/plotly_express/","tags":["Visualization","Python","Machine Learning","Data Science"],"title":"Python‚Äôs One Liner graph creation library with animations Hans Rosling Style"},{"categories":["Programming","Data Science"],"contents":"Parallelization is awesome.\nWe data scientists have got laptops with quad-core, octa-core, turbo-boost. We work with servers with even more cores and computing power.\nBut do we really utilize the raw power we have at hand?\nInstead, we wait for time taking processes to finish. Sometimes for hours, when urgent deliverables are at hand.\nCan we do better? Can we get better?\nIn this series of posts named  Python Shorts , I will explain some simple constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.\nThis post is about using the computing power we have at hand and applying it to the data structure we use most.\nProblem Statement We have got a huge pandas data frame, and we want to apply a complex function to it which takes a lot of time.\nFor this post, I will use data from the Quora Insincere Question Classification on Kaggle, and we need to create some numerical features like length, the number of punctuations, etc. on it.\nThe competition was a Kernel-based competition and the code needed to run in 2 hours. So every minute was essential, and there was too much time going in preprocessing.\nCan we use parallelization to get extra performance out of our code?\nYes, we can.\nParallelization using just a single function  Can we make all our cores run?\nLet me first start with defining the function I want to use to create our features. add_features is the toy function we wish to apply to our data.\nimport random import pandas as pd import numpy as np from multiprocessing import Pool def add_features(df): df[\u0026#39;question_text\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda x:str(x)) df[\u0026#34;lower_question_text\u0026#34;] = df[\u0026#34;question_text\u0026#34;].apply(lambda x: x.lower()) df[\u0026#39;total_length\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(len) df[\u0026#39;capitals\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: sum(1 for c in comment if c.isupper())) df[\u0026#39;caps_vs_length\u0026#39;] = df.apply(lambda row: float(row[\u0026#39;capitals\u0026#39;])/float(row[\u0026#39;total_length\u0026#39;]), axis=1) df[\u0026#39;num_words\u0026#39;] = df.question_text.str.count(\u0026#39;\\S+\u0026#39;) df[\u0026#39;num_unique_words\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: len(set(w for w in comment.split()))) df[\u0026#39;words_vs_unique\u0026#39;] = df[\u0026#39;num_unique_words\u0026#39;] / df[\u0026#39;num_words\u0026#39;] df[\u0026#39;num_exclamation_marks\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: comment.count(\u0026#39;!\u0026#39;)) df[\u0026#39;num_question_marks\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: comment.count(\u0026#39;?\u0026#39;)) df[\u0026#39;num_punctuation\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: sum(comment.count(w) for w in \u0026#39;.,;:\u0026#39;)) df[\u0026#39;num_symbols\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: sum(comment.count(w) for w in \u0026#39;*\u0026amp;$%\u0026#39;)) df[\u0026#39;num_smilies\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: sum(comment.count(w) for w in (\u0026#39;:-)\u0026#39;, \u0026#39;:)\u0026#39;, \u0026#39;;-)\u0026#39;, \u0026#39;;)\u0026#39;))) df[\u0026#39;num_sad\u0026#39;] = df[\u0026#39;question_text\u0026#39;].apply(lambda comment: sum(comment.count(w) for w in (\u0026#39;:-\u0026lt;\u0026#39;, \u0026#39;:()\u0026#39;, \u0026#39;;-()\u0026#39;, \u0026#39;;(\u0026#39;))) df[\u0026#34;mean_word_len\u0026#34;] = df[\u0026#34;question_text\u0026#34;].apply(lambda x: np.mean([len(w) for w in str(x).split()])) return df We can use parallelized apply using the below function.\ndef parallelize_dataframe(df, func, n_cores=4): df_split = np.array_split(df, n_cores) pool = Pool(n_cores) df = pd.concat(pool.map(func, df_split)) pool.close() pool.join() return df What does it do? It breaks the dataframe into n_cores parts, and spawns n_cores processes which apply the function to all the pieces.\nOnce it applies the function to all the split dataframes, it just concatenates the split dataframe and returns the full dataframe to us.\nHow can we use it? It is pretty simple to use.\ntrain = parallelize_dataframe(train_df, add_features) Does this work? To check the performance of this parallelize function, I ran %%timeit magic on this function in my Jupyter notebook in a Kaggle Kernel.\n vs. just using the function as it is:\n As you can see I gained some performance just by using the parallelize function. And it was using a kaggle kernel which has only got 2 CPUs.\nIn the actual competition, there was a lot of computation involved, and the add_features function I was using was much more involved. And this parallelize function helped me immensely to reduce processing time and get a Silver medal .\nHere is the kernel with the full code.\nConclusion Parallelization is not a silver bullet; it is buckshot. It won‚Äôt solve all your problems, and you would still have to work on optimizing your functions, but it is a great tool to have in your arsenal.\nTime never comes back, and sometimes we have a shortage of it. At these times we should be able to use parallelization easily.\n Parallelization is not a silver bullet it is a buckshot  Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz .\n","permalink":"https://mlwhiz.com/blog/2019/04/22/python_pandas_multiproc/","tags":["Python","multiprocessing","pandas"],"title":"Make your own Super Pandas using Multiproc"},{"categories":["Programming","Data Science"],"contents":"Python provides us with many styles of coding.\nIn a way, it is pretty inclusive.\nOne can come from any language and start writing Python.\nHowever, learning to write a language and writing a language in an optimized way are two different things.\nIn this series of posts named \u0026lt;strong\u0026gt;Python Shorts\u0026lt;/strong\u0026gt; , I will explain some simple but very useful constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.\nIn this post, I am going to talk about for loops in Python and how you should avoid them whenever possible.\n3 Ways of writing a for loop: Let me explain this with a simple example statement.\nSuppose you want to take the sum of squares in a list.\nThis is a valid problem we all face in machine learning whenever we want to calculate the distance between two points in n dimension.\nYou can do this using loops easily.\nIn fact, I will show you** three ways to do the same task which I have seen people use and let you choose for yourself which you find the best.**\nx = [1,3,5,7,9] sum_squared = 0 for i in range(len(x)): sum_squared+=x[i]**2 Whenever I see the above code in a python codebase, I understand that the person has come from C or Java background.\nA **slightly more pythonic way **of doing the same thing is:\nx = [1,3,5,7,9] sum_squared = 0 for y in x: sum_squared+=y**2 Better.\nI didn‚Äôt index the list. And my code is more readable.\nBut still, the pythonic way to do it is in one line.\nx = [1,3,5,7,9] sum_squared = sum([y**2 for y in x]) This approach is called List Comprehension, and this may very well be one of the reasons that I love Python.\nYou can also use if in a list comprehension.\nLet‚Äôs say we wanted a list of squared numbers for even numbers only.\nx = [1,2,3,4,5,6,7,8,9] even_squared = [y**2 for y in x if y%2==0] -------------------------------------------- [4,16,36,64] if-else?\nWhat if we wanted to have the number squared for even and cubed for odd?\nx = [1,2,3,4,5,6,7,8,9] squared_cubed = [y**2 if y%2==0 else y**3 for y in x] -------------------------------------------- [1, 4, 27, 16, 125, 36, 343, 64, 729] Great!!!\n  So basically follow specific guidelines: Whenever you feel like writing a for statement, you should ask yourself the following questions,\n  Can it be done without a for loop? Most Pythonic\n  Can it be done using list comprehension? If yes, use it.\n  Can I do it without indexing arrays? if not, think about using enumerate\n  What is enumerate?\nSometimes we need both the index in an array as well as the value in an array.\nIn such cases, I prefer to use enumerate rather than indexing the list.\nL = [\u0026#39;blue\u0026#39;, \u0026#39;yellow\u0026#39;, \u0026#39;orange\u0026#39;] for i, val in enumerate(L): print(\u0026#34;index is %dand value is %s\u0026#34; % (i, val)) --------------------------------------------------------------- index is 0 and value is blue index is 1 and value is yellow index is 2 and value is orange The rule is:\n Never index a list, if you can do without it.  Try Using Dictionary Comprehension Also try using dictionary comprehension, which is a relatively new addition in Python. The syntax is pretty similar to List comprehension.\nLet me explain using an example. I want to get a dictionary with (key: squared value) for every value in x.\nx = [1,2,3,4,5,6,7,8,9] {k:k**2 for k in x} --------------------------------------------------------- {1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} What if I want a dict only for even values?\nx = [1,2,3,4,5,6,7,8,9] {k:k**2 for k in x if x%2==0} --------------------------------------------------------- {2: 4, 4: 16, 6: 36, 8: 64} What if we want squared value for even key and cubed number for the odd key?\nx = [1,2,3,4,5,6,7,8,9] {k:k**2 if k%2==0 else k**3 for k in x} --------------------------------------------------------- {1: 1, 2: 4, 3: 27, 4: 16, 5: 125, 6: 36, 7: 343, 8: 64, 9: 729} Conclusion To conclude, I will say that while it might seem easy to transfer the knowledge you acquired from other languages to Python, you won‚Äôt be able to appreciate the beauty of Python if you keep doing that. Python is much more powerful when we use its ways and decidedly much more fun.\nSo, use List Comprehensions and Dict comprehensions when you need afor loop. Use enumerate if you need array index.\n Avoid for loops like plague  Your code will be much more readable and maintainable in the long run.\nAlso if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nI am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; to be informed about them.\nAs always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz .\n","permalink":"https://mlwhiz.com/blog/2019/04/22/python_forloops/","tags":["Python"],"title":"Minimize for loop usage in Python"},{"categories":["Data science","Programming"],"contents":"Learning a language is easy. Whenever I start with a new language, I focus on a few things in below order, and it is a breeze to get started with writing code in any language.\n  Operators and Data Types: +,-,int,float,str\n  Conditional statements: if,else,case,switch\n  Loops: For, while\n  Data structures: List, Array, Dict, Hashmaps\n  Define Function\n  However, learning to write a language and writing a language in an optimized way are two different things.\nEvery Language has some ingredients which make it unique.\nYet, a new programmer to any language will always do some forced overfitting. A Java programmer, new to python, for example, might write this code to add numbers in a list.\nx=[1,2,3,4,5] sum_x = 0 for i in range(len(x)): sum_x+=x[i] While a python programmer will naturally do this:\nsum_x = sum(x) In this series of posts named ‚ÄòPython Shorts‚Äô, I will explain some simple constructs that Python provides, some essential tips and some use cases I come up with regularly in my Data Science work.\nThis series is about efficient and readable code.\nCounter and defaultdict ‚Äî Use Cases   Let‚Äôs say I need to count the number of word occurrences in a piece of text. Maybe for a book like Hamlet. How could I do that?\nPython always provides us with multiple ways to do the same thing. But only one way that I find elegant.\nThis is a Naive Python implementation using the dict object.\ntext = \u0026#34;I need to count the number of word occurrences in a piece of text. How could I do that? Python provides us with multiple ways to do the same thing. But only one way I find beautiful.\u0026#34; word_count_dict = {} for w in text.split(\u0026#34; \u0026#34;): if w in word_count_dict: word_count_dict[w]+=1 else: word_count_dict[w]=1 We could use defaultdict to reduce the number of lines in the code.\nfrom Collections import defaultdict word_count_dict = defaultdict(int) for w in text.split(\u0026#34; \u0026#34;): word_count_dict[w]+=1 We could also have used Counter to do this.\nfrom Collections import Counter word_count_dict = Counter() for w in text.split(\u0026#34; \u0026#34;): word_count_dict[w]+=1 If we use Counter, we can also get the most common words using a simple function.\nword_count_dict.most_common(10) --------------------------------------------------------------- [(\u0026#39;I\u0026#39;, 3), (\u0026#39;to\u0026#39;, 2), (\u0026#39;the\u0026#39;, 2)] Other use cases of Counter:\n# Count Characters Counter(\u0026#39;abccccccddddd\u0026#39;) --------------------------------------------------------------- Counter({\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 1, \u0026#39;c\u0026#39;: 6, \u0026#39;d\u0026#39;: 5}) # Count List elements Counter([1,2,3,4,5,1,2]) --------------------------------------------------------------- Counter({1: 2, 2: 2, 3: 1, 4: 1, 5: 1}) So, why ever use defaultdict ? Notice that in Counter, the value is always an integer.\nWhat if we wanted to parse through a list of tuples and wanted to create a dictionary of key and list of values.\nThe main functionality provided by a defaultdict is that it defaults a key to empty/zero if it is not found in the defaultdict.\ns = [(\u0026#39;color\u0026#39;, \u0026#39;blue\u0026#39;), (\u0026#39;color\u0026#39;, \u0026#39;orange\u0026#39;), (\u0026#39;color\u0026#39;, \u0026#39;yellow\u0026#39;), (\u0026#39;fruit\u0026#39;, \u0026#39;banana\u0026#39;), (\u0026#39;fruit\u0026#39;, \u0026#39;orange\u0026#39;),(\u0026#39;fruit\u0026#39;,\u0026#39;banana\u0026#39;)] d = defaultdict(list) for k, v in s: d[k].append(v) print(d) --------------------------------------------------------------- defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {\u0026#39;color\u0026#39;: [\u0026#39;blue\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;yellow\u0026#39;], \u0026#39;fruit\u0026#39;: [\u0026#39;banana\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;banana\u0026#39;]}) banana comes two times in fruit, we could use set\nd = defaultdict(set) for k, v in s: d[k].add(v) print(d) --------------------------------------------------------------- defaultdict(\u0026lt;class \u0026#39;set\u0026#39;\u0026gt;, {\u0026#39;color\u0026#39;: {\u0026#39;yellow\u0026#39;, \u0026#39;blue\u0026#39;, \u0026#39;orange\u0026#39;}, \u0026#39;fruit\u0026#39;: {\u0026#39;banana\u0026#39;, \u0026#39;orange\u0026#39;}})  Conclusion To conclude, I will say that there is always a beautiful way to do anything in Python. Search for it before you write code. Going to StackOverflow is okay. I go there a lot of times when I get stuck. Always Remember:\n Creating a function for what already is provided is not pythonic.\n Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.\nIf you liked this post do share. It will help increase coverage for this post. I am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at \u0026lt;strong\u0026gt;Medium\u0026lt;/strong\u0026gt; or Subscribe to my \u0026lt;strong\u0026gt;blog\u0026lt;/strong\u0026gt; to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz .\n","permalink":"https://mlwhiz.com/blog/2019/04/22/python_defaultdict/","tags":["Python"],"title":"Python Pro Tip: Start using Python defaultdict and Counter in place of dictionary"},{"categories":["Data Science","Awesome Guides"],"contents":"Visualizations are awesome. However, a good visualization is annoyingly hard to make.\nMoreover, it takes time and effort when it comes to present these visualizations to a bigger audience.\nWe all know how to make Bar-Plots, Scatter Plots, and Histograms , yet we don\u0026rsquo;t pay much attention to beautify them.\nThis hurts us‚Ää-‚Ääour credibility with peers and managers. You won\u0026rsquo;t feel it now, but it happens.\nAlso, I find it essential to reuse my code. Every time I visit a new dataset do I need to start again? Some reusable ideas of graphs that can help us to find information about the data FAST.\nIn this post, I am also going to talk about 3 cool visual tools:\n Categorical Correlation with Graphs, Pairplots, Swarmplots and Graph Annotations using Seaborn.  In short, this post is about useful and presentable graphs.\n I will be using data from FIFA 19 complete player dataset on kaggle‚Ää-‚ÄäDetailed attributes for every player registered in the latest edition of FIFA 19 database.\nSince the Dataset has many columns, we will only focus on a subset of categorical and continuous columns.\nimport numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=\u0026#34;ticks\u0026#34;) player_df = pd.read_csv(\u0026#34;../input/data.csv\u0026#34;) numcols = [ \u0026#39;Overall\u0026#39;, \u0026#39;Potential\u0026#39;, \u0026#39;Crossing\u0026#39;,\u0026#39;Finishing\u0026#39;, \u0026#39;ShortPassing\u0026#39;, \u0026#39;Dribbling\u0026#39;,\u0026#39;LongPassing\u0026#39;, \u0026#39;BallControl\u0026#39;, \u0026#39;Acceleration\u0026#39;, \u0026#39;SprintSpeed\u0026#39;, \u0026#39;Agility\u0026#39;, \u0026#39;Stamina\u0026#39;, \u0026#39;Value\u0026#39;,\u0026#39;Wage\u0026#39;] catcols = [\u0026#39;Name\u0026#39;,\u0026#39;Club\u0026#39;,\u0026#39;Nationality\u0026#39;,\u0026#39;Preferred Foot\u0026#39;,\u0026#39;Position\u0026#39;,\u0026#39;Body Type\u0026#39;] # Subset the columns player_df = player_df[numcols+ catcols] # Few rows of data player_df.head(5)   Player Data    This is a nicely formatted data, yet we need to do some preprocessing to the Wage and Value columns(as they are in Euro and contain strings) to make them numeric for our subsequent analysis.\ndef wage_split(x): try: return int(x.split(\u0026#34;K\u0026#34;)[0][1:]) except: return 0 player_df[\u0026#39;Wage\u0026#39;] = player_df[\u0026#39;Wage\u0026#39;].apply(lambda x : wage_split(x)) def value_split(x): try: if \u0026#39;M\u0026#39; in x: return float(x.split(\u0026#34;M\u0026#34;)[0][1:]) elif \u0026#39;K\u0026#39; in x: return float(x.split(\u0026#34;K\u0026#34;)[0][1:])/1000 except: return 0 player_df[\u0026#39;Value\u0026#39;] = player_df[\u0026#39;Value\u0026#39;].apply(lambda x : value_split(x))  Categorical Correlation with¬†Graphs: In Simple terms, Correlation is a measure of how two variables move together.\nFor example, In the real world, Income and Spend are positively correlated. If one increases the other also increases.\nAcademic Performance and Video Games Usage is negatively correlated. Increase in one predicts a decrease in another.\nSo if our predictor variable is positively or negatively correlated with our target variable, it is valuable.\nI feel that Correlations among different variables are a pretty good thing to do when we try to understand our data.\nWe can create a pretty good correlation plot using Seaborn easily.\ncorr = player_df.corr() g = sns.heatmap(corr, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\u0026#34;shrink\u0026#34;: .5}, annot=True, fmt=\u0026#39;.2f\u0026#39;, cmap=\u0026#39;coolwarm\u0026#39;) sns.despine() g.figure.set_size_inches(14,10) plt.show()   Where did all the categorical variables go?    But do you notice any problem?\nYes, this graph only calculates Correlation between Numerical columns. What if my target variable is Club or Position?\nI want to be able to get a correlation among three different cases, and we use the following metrics of correlation to calculate these:\n1. Numerical Variables We already have this in the form of Pearson\u0026amp;rsquo;s Correlation which is a measure of how two variables move together. This Ranges from [-1,1]\n2. Categorical Variables We will use Cramer\u0026amp;rsquo;s V for categorical-categorical cases. It is the intercorrelation of two discrete variables and used with variables having two or more levels. It is a symmetrical measure as in the order of variable does not matter. Cramer(A,B) == Cramer(B,A).\nFor Example: In our dataset, Club and Nationality must be somehow correlated.\nLet us check this using a stacked graph which is an excellent way to understand distribution between categorical vs. categorical variables. Note that we use a subset of data since there are a lot of nationalities and club in this data.\nWe keep only the best teams(Kept FC Porto just for more diversity in the sample)and the most common nationalities.\n  Note that Club preference says quite a bit about Nationality: knowing the former helps a lot in predicting the latter.\nWe can see that if a player belongs to England, it is more probable that he plays in Chelsea or Manchester United and not in FC Barcelona or Bayern Munchen or Porto.\nSo there is some information present here. Cramer\u0026rsquo;s V captures the same information.\nIf all clubs have the same proportion of players from every nationality, Cramer\u0026rsquo;s V is 0.\nIf Every club prefers a single nationality Cramer\u0026rsquo;s V ==1, for example, all England player play in Manchester United, All Germans in Bayern Munchen and so on.\nIn all other cases, it ranges from [0,1]\n3. Numerical and Categorical variables We will use the Correlation Ratio for categorical-continuous cases.\nWithout getting into too much Maths, it is a measure of Dispersion.\n Given a number can we find out which category it belongs¬†to?\n For Example:\nSuppose we have two columns from our dataset: SprintSpeed and Position:\n GK: 58(De Gea),52(T. Courtois), 58(M. Neuer), 43(G. Buffon) CB: 68(D. Godin), 59(V. Kompany), 73(S. Umtiti), 75(M. Benatia) ST: 91(C.Ronaldo), 94(G. Bale), 80(S.Aguero), 76(R. Lewandowski)  As you can see these numbers are pretty predictive of the bucket they fall into and thus high Correlation Ratio.\nIf I know the sprint speed is more than 85, I can definitely say this player plays at ST.\nThis ratio also ranges from [0,1]\n The code to do this is taken from the dython package. I won\u0026rsquo;t write too much into code which you can anyway find in my Kaggle Kernel . The final result looks something like:\nplayer_df = player_df.fillna(0) results = associations(player_df,nominal_columns=catcols,return_results=True)   Categorical vs. Categorical, Categorical vs. Numeric, Numeric vs. Numeric. Much more interesting    Isn\u0026rsquo;t it Beautiful?\nWe can understand so much about Football just by looking at this data. For Example:\n  The position of a player is highly correlated with dribbling ability. You won\u0026rsquo;t play Messi at the back. Right?\n  Value is more highly correlated with passing and ball control than dribbling. The rule is to pass the ball always. Neymar I am looking at you.\n  Club and Wage have high Correlation. To be expected.\n  Body Type and Preferred Foot is correlated highly. Does that mean if you are Lean, you are most likely left-footed? Doesn\u0026rsquo;t make much sense. One can investigate further.\n  Moreover, so much info we could find with this simple graph which was not visible in the typical correlation plot without Categorical Variables.\nI leave it here at that. One can look more into the chart and find more meaningful results, but the point is that this makes life so much easier to find patterns.\n Pairplots While I talked a lot about correlation, it is a fickle metric.\nTo understand what I mean let us see one example.\nAnscombe\u0026rsquo;s quartet comprises four datasets that have nearly identical Correlation of 1, yet have very different distributions and appear very different when graphed.\n  Anscombe Quartet‚Ää-‚ÄäCorrelations can be fickle.    Thus sometimes it becomes crucial to plot correlated data. And see the distributions individually.\nNow we have many columns in our dataset. Graphing them all would be so much effort.\nNo, it is a single line of code.\nfiltered_player_df = player_df[(player_df[\u0026#39;Club\u0026#39;].isin([\u0026#39;FC Barcelona\u0026#39;, \u0026#39;Paris Saint-Germain\u0026#39;, \u0026#39;Manchester United\u0026#39;, \u0026#39;Manchester City\u0026#39;, \u0026#39;Chelsea\u0026#39;, \u0026#39;Real Madrid\u0026#39;,\u0026#39;FC Porto\u0026#39;,\u0026#39;FC Bayern M√ºnchen\u0026#39;])) \u0026amp; (player_df[\u0026#39;Nationality\u0026#39;].isin([\u0026#39;England\u0026#39;, \u0026#39;Brazil\u0026#39;, \u0026#39;Argentina\u0026#39;, \u0026#39;Brazil\u0026#39;, \u0026#39;Italy\u0026#39;,\u0026#39;Spain\u0026#39;,\u0026#39;Germany\u0026#39;])) ] # Single line to create pairplot g = sns.pairplot(filtered_player_df[[\u0026#39;Value\u0026#39;,\u0026#39;SprintSpeed\u0026#39;,\u0026#39;Potential\u0026#39;,\u0026#39;Wage\u0026#39;]])   Pretty Good. We can see so much in this graph.\n  Wage and Value are highly correlated.\n  Most of the other values are correlated too. However, the trend of potential vs. value is unusual. We can see how the value increases exponentially as we reach a particular potential threshold. This information can be helpful in modeling. Can use some transformation on Potential to make it more correlated?\n  Caveat: No categorical columns.\nCan we do better? We always can.\ng = sns.pairplot(filtered_player_df[[\u0026#39;Value\u0026#39;,\u0026#39;SprintSpeed\u0026#39;,\u0026#39;Potential\u0026#39;,\u0026#39;Wage\u0026#39;,\u0026#39;Club\u0026#39;]],hue = \u0026#39;Club\u0026#39;)   So much more info. Just by adding the hue parameter as a categorical variable Club.\n Porto\u0026rsquo;s Wage distribution is too much towards the lower side. I don\u0026rsquo;t see that steep distribution in value of Porto players. Porto\u0026rsquo;s players would always be looking out for an opportunity. See how a lot of pink points(Chelsea) form sort of a cluster on Potential vs. wage graph. Chelsea has a lot of high potential players with lower wages. Needs more attention.  I already know some of the points on the Wage/Value Subplot.\nThe blue point for wage 500k is Messi. Also, the orange point having more value than Messi is Neymar.\nAlthough this hack still doesn\u0026rsquo;t solve the Categorical problem, I have something cool to look into categorical variables distribution. Though individually.\n SwarmPlots How to see the relationship between categorical and numerical data?\nEnter into picture Swarmplots, just like their name. A swarm of points plotted for each category with a little dispersion on the y-axis to make them easier to see.\nThey are my current favorite for plotting such relationships.\ng = sns.swarmplot(y = \u0026#34;Club\u0026#34;, x = \u0026#39;Wage\u0026#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(14,10) plt.show()   Swarmplot...    Why don\u0026rsquo;t I use Boxplots? Where are the median values? Can I plot that? Obviously. Overlay a bar plot on top, and we have a great looking graph.\ng = sns.boxplot(y = \u0026#34;Club\u0026#34;, x = \u0026#39;Wage\u0026#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = \u0026#34;Club\u0026#34;, x = \u0026#39;Wage\u0026#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7,color = \u0026#39;black\u0026#39;) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(12,8) plt.show()   Swarmplot+Boxplot, Interesting    Pretty good. We can see the individual points on the graph, see some statistics and understand the wage difference categorically.\nThe far right point is Messi. However, I should not have to tell you that in a text below the chart. Right?\nThis graph is going to go in a presentation. Your boss says. I want to write Messi on this graph. Comes into picture annotations.\nmax_wage = filtered_player_df.Wage.max() max_wage_player = filtered_player_df[(player_df[\u0026#39;Wage\u0026#39;] == max_wage)][\u0026#39;Name\u0026#39;].values[0] g = sns.boxplot(y = \u0026#34;Club\u0026#34;, x = \u0026#39;Wage\u0026#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = \u0026#34;Club\u0026#34;, x = \u0026#39;Wage\u0026#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7,color=\u0026#39;black\u0026#39;) # remove the top and right line in graph sns.despine() # Annotate. xy for coordinate. max_wage is x and 0 is y. In this plot y ranges from 0 to 7 for each level # xytext for coordinates of where I want to put my text plt.annotate(s = max_wage_player, xy = (max_wage,0), xytext = (500,1), # Shrink the arrow to avoid occlusion arrowprops = {\u0026#39;facecolor\u0026#39;:\u0026#39;gray\u0026#39;, \u0026#39;width\u0026#39;: 3, \u0026#39;shrink\u0026#39;: 0.03}, backgroundcolor = \u0026#39;white\u0026#39;) g.figure.set_size_inches(12,8) plt.show()   Annotated, Statistical Info and point swarm. To the presentation, I go.     See Porto Down there. Competing with the giants with such a small wage budget. So many Highly paid players in Real and Barcelona. Manchester City has the highest median Wage. Manchester United and Chelsea believes in equality. Many players clustered in around the same wage scale. I am happy that while Neymar is more valued than Messi, Messi and Neymar have a huge Wage difference.  A semblance of normalcy in this crazy world.\n So to recap, in this post, we talked about calculating and reading correlations between different variable types, plotting correlations between numerical data and Plotting categorical data with Numerical data using Swarmplots. I love how we can overlay chart elements on top of each other in Seaborn.\n Also if you want to learn more about Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out\nIf you liked this post, do look at my other post on Seaborn too where I have created some more straightforward reusable graphs. I am going to be writing more beginner friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz Code for this post in this kaggle kernel .\n References:   The Search for Categorical Correlation   Seaborn Swarmplot Documentation   Seaborn Pairplot Documentation   ","permalink":"https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/","tags":["Visualization","Python","Machine Learning","Data Science"],"title":"3 Awesome Visualization Techniques for every¬†dataset"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them. Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.\n  Let me assure you this is not going to be \u0026ldquo;that kind of a post\u0026rdquo;.\nI will try to distill some of the knowledge I acquired while working through a project in the Natural Language Processing course in the Advanced machine learning specialization .\nSo before I start, let me first say it for once that don\u0026rsquo;t be intimidated by the hype and the enigma surrounding Chatbots. They are pretty much using pretty simple NLP techniques which most of us already know. If you don\u0026rsquo;t, you are welcome to check out my NLP Learning Series , where I go through the problem of text classification in fair detail using Conventional , Deep Learning and Transfer Learning methods.\nA Very brief Intro to Chatbots We can logically divide of Chatbots in the following two categories.\n  Database/FAQ based - We have a database with some questions and answers, and we would like that a user can query that using Natural Language. This is the sort of Chatbots you find at most of the Banking websites for answering FAQs.\n  Chit-Chat Based - Simulate dialogue with the user. These are the kind of chatbots that bring the cool in chatbots. We can use Seq-2-Seq models to create such bots.\n   The Chatbot we will be creating We will be creating a dialogue chat bot, which will be able to:\n Answer programming-related questions (using StackOverflow dataset) Chit-Chat and simulate dialogue on all non-programming related questions  Once we will have it up and running our final chatbot should look like this.\n  Seems quite fun.\nWe will be taking help of resources like Telegram and Chatterbot to build our Chatbot. So before we start, I think I should get you up and running with these two tools.\n 1. Telegram: From the website :\n Telegram is a messaging app with a focus on speed and security, it‚Äôs super-fast, simple and free. You can use Telegram on all your devices at the same time ‚Äî your messages sync seamlessly across any number of your phones, tablets or computers.\n For us, Telegram provides us with an easy way to create a Chatbot UI. It provides us with an access token which we will use to connect to the Telegram App backend and run our chatbot logic. Naturally, we need to have a window where we will write our questions to the chatbot, for us that is provided by Telegram. Also, telegram powers the chatbot by communicating with our chatbot logic. The above screenshot is taken from the telegram app only.\nSet up Telegram: Don\u0026rsquo;t worry if you don\u0026rsquo;t understand how it works yet; I will try to give step by step instructions as we go forward.\n  Step 1: Download and Install Telegram App on your Laptop.\n  Step 2: Talk with BotFather by opening this link in Chrome and subsequently your Telegram App.\n  Step 3: The above steps will take you to a Chatbot called Botfather which can help you create a new bot. Inception Anyone? It will look something like this.\n Set up a new bot using command \u0026ldquo;/newbot\u0026rdquo; Create a name for Your bot. Create a username for your bot.       Step 4: You will get an access token for the bot. Copy the Token at a safe place. Step 5: Click on the \u0026ldquo;t.me/MLWhizbot\u0026rdquo; link to open Chat with your chatbot in a new window.  Right now if you try to communicate with the chatbot, you won\u0026rsquo;t receive any answers. And that is how it should be.\n  But that\u0026rsquo;s not at all fun. Is it? Let\u0026rsquo;s do some python magic to make it responsive.\nMaking our Telegram Chatbot responsive Create a file main.py and put the following code in it. Don\u0026rsquo;t worry most of the code here is Boilerplate code to make our Chatbot communicate with Telegram using the Access token. We need to worry about implementing the class SimpleDialogueManager. This class contains a function called generate_answer which is where we will write our bot logic.\n#!/usr/bin/env python3 import requests import time import argparse import os import json from requests.compat import urljoin class BotHandler(object): \u0026#34;\u0026#34;\u0026#34; BotHandler is a class which implements all back-end of the bot. It has three main functions: \u0026#39;get_updates\u0026#39; ‚Äî checks for new messages \u0026#39;send_message\u0026#39; ‚Äì posts new message to user \u0026#39;get_answer\u0026#39; ‚Äî computes the most relevant on a user\u0026#39;s question \u0026#34;\u0026#34;\u0026#34; def __init__(self, token, dialogue_manager): self.token = token self.api_url = \u0026#34;https://api.telegram.org/bot{}/\u0026#34;.format(token) self.dialogue_manager = dialogue_manager def get_updates(self, offset=None, timeout=30): params = {\u0026#34;timeout\u0026#34;: timeout, \u0026#34;offset\u0026#34;: offset} raw_resp = requests.get(urljoin(self.api_url, \u0026#34;getUpdates\u0026#34;), params) try: resp = raw_resp.json() except json.decoder.JSONDecodeError as e: print(\u0026#34;Failed to parse response {}: {}.\u0026#34;.format(raw_resp.content, e)) return [] if \u0026#34;result\u0026#34; not in resp: return [] return resp[\u0026#34;result\u0026#34;] def send_message(self, chat_id, text): params = {\u0026#34;chat_id\u0026#34;: chat_id, \u0026#34;text\u0026#34;: text} return requests.post(urljoin(self.api_url, \u0026#34;sendMessage\u0026#34;), params) def get_answer(self, question): if question == \u0026#39;/start\u0026#39;: return \u0026#34;Hi, I am your project bot. How can I help you today?\u0026#34; return self.dialogue_manager.generate_answer(question) def is_unicode(text): return len(text) == len(text.encode()) class SimpleDialogueManager(object): \u0026#34;\u0026#34;\u0026#34; This is a simple dialogue manager to test the telegram bot. The main part of our bot will be written here. \u0026#34;\u0026#34;\u0026#34; def generate_answer(self, question): if \u0026#34;Hi\u0026#34; in question: return \u0026#34;Hello, You\u0026#34; else: return \u0026#34;Don\u0026#39;t be rude. Say Hi first.\u0026#34; def main(): # Put your own Telegram Access token here... token = \u0026#39;839585958:AAEfTDo2X6PgHb9IEdb62ueS4SmdpCkhtmc\u0026#39; simple_manager = SimpleDialogueManager() bot = BotHandler(token, simple_manager) ############################################################### print(\u0026#34;Ready to talk!\u0026#34;) offset = 0 while True: updates = bot.get_updates(offset=offset) for update in updates: print(\u0026#34;An update received.\u0026#34;) if \u0026#34;message\u0026#34; in update: chat_id = update[\u0026#34;message\u0026#34;][\u0026#34;chat\u0026#34;][\u0026#34;id\u0026#34;] if \u0026#34;text\u0026#34; in update[\u0026#34;message\u0026#34;]: text = update[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;] if is_unicode(text): print(\u0026#34;Update content: {}\u0026#34;.format(update)) bot.send_message(chat_id, bot.get_answer(update[\u0026#34;message\u0026#34;][\u0026#34;text\u0026#34;])) else: bot.send_message(chat_id, \u0026#34;Hmm, you are sending some weird characters to me...\u0026#34;) offset = max(offset, update[\u0026#39;update_id\u0026#39;] + 1) time.sleep(1) if __name__ == \u0026#34;__main__\u0026#34;: main() You can run the file main.py in the terminal window to make your bot responsive.\n$ python main.py   Nice. It is following simple logic. But the good thing is that our bot now does something.\nAlso, take a look at the terminal window where we have run our main.py File. Whenever a user asks a question, we get the sort of dictionary below containing Unique Chat ID, Chat Text, User Information, etc.\nUpdate content: {'update_id': 484689748, 'message': {'message_id': 115, 'from': {'id': 844474950, 'is_bot': False, 'first_name': 'Rahul', 'last_name': 'Agarwal', 'language_code': 'en'}, 'chat': {'id': 844474950, 'first_name': 'Rahul', 'last_name': 'Agarwal', 'type': 'private'}, 'date': 1555266010, 'text': 'What is 2+2'}} Until now whatever we had done was sort of setting up and engineering sort of work.\nOnly if we can write some sound Data Science logic in the generate_answer function in our main.py we should have a decent chatbot.\n 2. ChatterBot From the Documentation:\n ChatterBot is a Python library that makes it easy to generate automated responses to a user‚Äôs input. ChatterBot uses a selection of machine learning algorithms to produce different types of reactions. This makes it easy for developers to create chat bots and automate conversations with users.\n Simply. It is a Blackbox system which can provide us with responses for Chitchat type questions for our Chatbot. And the best part about it is that it is pretty easy to integrate with our current flow. We could also have trained a SeqtoSeq model to do the same thing. Might be I will do it in a later post. I digress.\nSo, install it with:\n$ pip install chatterbot And change the SimpleDialogueManager Class in main.py to the following. We can have a bot that can talk to the user and answer random queries.\nclass SimpleDialogueManager(object): \u0026#34;\u0026#34;\u0026#34; This is a simple dialogue manager to test the telegram bot. The main part of our bot will be written here. \u0026#34;\u0026#34;\u0026#34; def __init__(self): from chatterbot import ChatBot from chatterbot.trainers import ChatterBotCorpusTrainer chatbot = ChatBot(\u0026#39;MLWhizChatterbot\u0026#39;) trainer = ChatterBotCorpusTrainer(chatbot) trainer.train(\u0026#39;chatterbot.corpus.english\u0026#39;) self.chitchat_bot = chatbot def generate_answer(self, question): response = self.chitchat_bot.get_response(question) return response The code in init instantiates a chatbot using chatterbot and trains it on the provided english corpus data. The data is pretty small, but you can always train it on your dataset too. Just see the documentation . We can then give our responses using the Chatterbot chatbot in the generate_answer function.\n  Not too \u0026ldquo;ba a a a a a d\u0026rdquo; , I must say.\n Creating our StackOverFlow ChatBot Ok, so finally we are at a stage where we can do something we love. Use Data Science to power our Application/Chatbot. Let us start with creating a rough architecture of what we are going to do next.\n  We will need to create two classifiers and save them as .pkl files.\n  Intent-Classifier: This classifier will predict if it a question is a Stack-Overflow question or not. If it is not a Stack-overflow question, we let Chatterbot handle it.\n  Programming-Language(Tag) Classifier: This classifier will predict which language a question belongs to if the question is a Stack-Overflow question. We do this so that we can search for those language questions in our database only.\n  To keep it simple we will create simple TFIDF models. We will need to save these TFIDF vectorizers.\nWe will also need to store word vectors for every question for similarity calculations later.\nLet us go through the process step by step. You can get the full code in this jupyter notebook in my project repository .\nStep 1. Reading and Visualizing the Data dialogues = pd.read_csv(\u0026#34;data/dialogues.tsv\u0026#34;,sep=\u0026#34;\\t\u0026#34;) posts = pd.read_csv(\u0026#34;data/tagged_posts.tsv\u0026#34;,sep=\u0026#34;\\t\u0026#34;) dialogues.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: left; }  \n  text tag     0 Okay -- you're gonna need to learn how to lie. dialogue   1 I'm kidding. You know how sometimes you just ... dialogue   2 Like my fear of wearing pastels? dialogue   3 I figured you'd get to the good stuff eventually. dialogue   4 Thank God! If I had to hear one more story ab... dialogue     posts.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  post_id title tag     0 9 Calculate age in C# c#   1 16 Filling a DataSet or DataTable from a LINQ que... c#   2 39 Reliable timer in a console application c#   3 42 Best way to allow plugins for a PHP application php   4 59 How do I get a distinct, ordered list of names... c#     print(\u0026#34;Num Posts:\u0026#34;,len(posts)) print(\u0026#34;Num Dialogues:\u0026#34;,len(dialogues)) Num Posts: 2171575 Num Dialogues: 218609  Step 2: Create training data for intent classifier - Chitchat/StackOverflow Question We will be creating a TFIDF model with Logistic regression to do this. If you want to know about the TFIDF model you can read it here.\nWe could also have used one of the Deep Learning models or transfer learning approaches to do this, but since the main objective of this post is to get a chatbot up and running and not worry too much about the accuracy we sort of work with the TFIDF based model only.\ntexts = list(dialogues[:200000].text.values) + list(posts[:200000].title.values) labels = [\u0026#39;dialogue\u0026#39;]*200000 + [\u0026#39;stackoverflow\u0026#39;]*200000 data = pd.DataFrame({\u0026#39;text\u0026#39;:texts,\u0026#39;target\u0026#39;:labels}) def text_prepare(text): \u0026#34;\u0026#34;\u0026#34;Performs tokenization and simple preprocessing.\u0026#34;\u0026#34;\u0026#34; replace_by_space_re = re.compile(\u0026#39;[/(){}\\[\\]\\|@,;]\u0026#39;) bad_symbols_re = re.compile(\u0026#39;[^0-9a-z #+_]\u0026#39;) stopwords_set = set(stopwords.words(\u0026#39;english\u0026#39;)) text = text.lower() text = replace_by_space_re.sub(\u0026#39; \u0026#39;, text) text = bad_symbols_re.sub(\u0026#39;\u0026#39;, text) text = \u0026#39; \u0026#39;.join([x for x in text.split() if x and x not in stopwords_set]) return text.strip() # Doing some data cleaning data[\u0026#39;text\u0026#39;] = data[\u0026#39;text\u0026#39;].apply(lambda x : text_prepare(x)) X_train, X_test, y_train, y_test = train_test_split(data[\u0026#39;text\u0026#39;],data[\u0026#39;target\u0026#39;],test_size = .1 , random_state=0) print(\u0026#39;Train size = {}, test size = {}\u0026#39;.format(len(X_train), len(X_test))) Train size = 360000, test size = 40000  Step 3. Create Intent classifier Here we Create a TFIDF Vectorizer to create features and also train a Logistic regression model to create the intent_classifier. Please note how we are saving TFIDF Vectorizer to resources/tfidf.pkl and intent_classifier to resources/intent_clf.pkl. We will need these files once we are going to write the SimpleDialogueManager class for our final Chatbot.\n# We will keep our models and vectorizers in this folder !mkdir resources def tfidf_features(X_train, X_test, vectorizer_path): \u0026#34;\u0026#34;\u0026#34;Performs TF-IDF transformation and dumps the model.\u0026#34;\u0026#34;\u0026#34; tfv = TfidfVectorizer(dtype=np.float32, min_df=3, max_features=None, strip_accents=\u0026#39;unicode\u0026#39;, analyzer=\u0026#39;word\u0026#39;,token_pattern=r\u0026#39;\\w{1,}\u0026#39;, ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = \u0026#39;english\u0026#39;) X_train = tfv.fit_transform(X_train) X_test = tfv.transform(X_test) pickle.dump(tfv,vectorizer_path) return X_train, X_test X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, open(\u0026#34;resources/tfidf.pkl\u0026#34;,\u0026#39;wb\u0026#39;)) intent_recognizer = LogisticRegression(C=10,random_state=0) intent_recognizer.fit(X_train_tfidf,y_train) pickle.dump(intent_recognizer, open(\u0026#34;resources/intent_clf.pkl\u0026#34; , \u0026#39;wb\u0026#39;)) # Check test accuracy. y_test_pred = intent_recognizer.predict(X_test_tfidf) test_accuracy = accuracy_score(y_test, y_test_pred) print(\u0026#39;Test accuracy = {}\u0026#39;.format(test_accuracy)) Test accuracy = 0.989825  The Intent Classifier has a pretty good test accuracy of 98%. TFIDF is not so bad.\nStep 4: Create Programming Language classifier Let us first create the data for Programming Language classifier and then train a Logistic Regression model using TFIDF features. We save this tag Classifier at the location resources/tag_clf.pkl. We do this step mostly because we don\u0026rsquo;t want to do similarity calculations over the whole database of questions but only on the subset of questions by the language tag.\n# creating the data for Programming Language classifier X = posts[\u0026#39;title\u0026#39;].values y = posts[\u0026#39;tag\u0026#39;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) print(\u0026#39;Train size = {}, test size = {}\u0026#39;.format(len(X_train), len(X_test))) Train size = 1737260, test size = 434315  vectorizer = pickle.load(open(\u0026#34;resources/tfidf.pkl\u0026#34;, \u0026#39;rb\u0026#39;)) X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test) tag_classifier = OneVsRestClassifier(LogisticRegression(C=5,random_state=0)) tag_classifier.fit(X_train_tfidf,y_train) pickle.dump(tag_classifier, open(\u0026#34;resources/tag_clf.pkl\u0026#34;, \u0026#39;wb\u0026#39;)) # Check test accuracy. y_test_pred = tag_classifier.predict(X_test_tfidf) test_accuracy = accuracy_score(y_test, y_test_pred) print(\u0026#39;Test accuracy = {}\u0026#39;.format(test_accuracy)) Test accuracy = 0.8043816124241622  Not Bad again.\nStep 5: Store Question database Embeddings One can use pre-trained word vectors from Google or get a better result by training their embeddings using their data. Since again accuracy and precision is not the primary goal of this post, we will use pretrained vectors.\n# Load Google\u0026#39;s pre-trained Word2Vec model. model = gensim.models.KeyedVectors.load_word2vec_format(\u0026#39;GoogleNews-vectors-negative300.bin\u0026#39;, binary=True) We want to convert every question to an embedding and store them so that we don\u0026rsquo;t calculate the embeddings for the whole dataset every time. In essence, whenever the user asks a Stack Overflow question, we want to use some distance similarity measure to get the most similar question.\ndef question_to_vec(question, embeddings, dim=300): \u0026#34;\u0026#34;\u0026#34; question: a string embeddings: dict where the key is a word and a value is its\u0026#39; embedding dim: size of the representation result: vector representation for the question \u0026#34;\u0026#34;\u0026#34; word_tokens = question.split(\u0026#34; \u0026#34;) question_len = len(word_tokens) question_mat = np.zeros((question_len,dim), dtype = np.float32) for idx, word in enumerate(word_tokens): if word in embeddings: question_mat[idx,:] = embeddings[word] # remove zero-rows which stand for OOV words  question_mat = question_mat[~np.all(question_mat == 0, axis = 1)] # Compute the mean of each word along the sentence if question_mat.shape[0] \u0026gt; 0: vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim)) else: vec = np.zeros((1,dim), dtype = np.float32) return vec counts_by_tag = posts.groupby(by=[\u0026#39;tag\u0026#39;])[\u0026#34;tag\u0026#34;].count().reset_index(name = \u0026#39;count\u0026#39;).sort_values([\u0026#39;count\u0026#39;], ascending = False) counts_by_tag = list(zip(counts_by_tag[\u0026#39;tag\u0026#39;],counts_by_tag[\u0026#39;count\u0026#39;])) print(counts_by_tag) [('c#', 394451), ('java', 383456), ('javascript', 375867), ('php', 321752), ('c_cpp', 281300), ('python', 208607), ('ruby', 99930), ('r', 36359), ('vb', 35044), ('swift', 34809)]  We save the embeddings in a folder aptly named resources/embeddings_folder. This folder will contain a .pkl file for every tag. For example one of the files will be python.pkl.\n! mkdir resources/embeddings_folder for tag, count in counts_by_tag: tag_posts = posts[posts[\u0026#39;tag\u0026#39;] == tag] tag_post_ids = tag_posts[\u0026#39;post_id\u0026#39;].values tag_vectors = np.zeros((count, 300), dtype=np.float32) for i, title in enumerate(tag_posts[\u0026#39;title\u0026#39;]): tag_vectors[i, :] = question_to_vec(title, model, 300) # Dump post ids and vectors to a file. filename = \u0026#39;resources/embeddings_folder/\u0026#39;+ tag + \u0026#39;.pkl\u0026#39; pickle.dump((tag_post_ids, tag_vectors), open(filename, \u0026#39;wb\u0026#39;)) We are nearing the end now. We need to have a function to get most similar question\u0026rsquo;s post id in the dataset given we know the programming Language of the question. Here it is:\ndef get_similar_question(question,tag): # get the path where all question embeddings are kept and load the post_ids and post_embeddings embeddings_path = \u0026#39;resources/embeddings_folder/\u0026#39; + tag + \u0026#34;.pkl\u0026#34; post_ids, post_embeddings = pickle.load(open(embeddings_path, \u0026#39;rb\u0026#39;)) # Get the embeddings for the question question_vec = question_to_vec(question, model, 300) # find index of most similar post best_post_index = pairwise_distances_argmin(question_vec, post_embeddings) # return best post id return post_ids[best_post_index] get_similar_question(\u0026#34;how to use list comprehension in python?\u0026#34;,\u0026#39;python\u0026#39;) array([5947137])  we can use this post ID and find this question at https://stackoverflow.com/questions/5947137\nThe question the similarity checker suggested has the actual text: \u0026ldquo;How can I use a list comprehension to extend a list in python? [duplicate]\u0026rdquo;\nNot too bad. It could have been better if we train our embeddings or use starspace embeddings.\nAssemble the Puzzle - SimpleDialogueManager Class Finally, we have reached the end of the whole exercise, and we have to fit all the pieces in the puzzle in our SimpleDialogueManager Class. Here is the code for that. Go in the main.py file again to paste this code and see if it works or not.\nGo through the comments to understand how the pieces are fitting together to build one wholesome logic.\nimport gensim import pickle import re import nltk from nltk.corpus import stopwords import numpy as np from sklearn.metrics.pairwise import pairwise_distances_argmin # We will need this function to prepare text at prediction time def text_prepare(text): \u0026#34;\u0026#34;\u0026#34;Performs tokenization and simple preprocessing.\u0026#34;\u0026#34;\u0026#34; replace_by_space_re = re.compile(\u0026#39;[/(){}\\[\\]\\|@,;]\u0026#39;) bad_symbols_re = re.compile(\u0026#39;[^0-9a-z #+_]\u0026#39;) stopwords_set = set(stopwords.words(\u0026#39;english\u0026#39;)) text = text.lower() text = replace_by_space_re.sub(\u0026#39; \u0026#39;, text) text = bad_symbols_re.sub(\u0026#39;\u0026#39;, text) text = \u0026#39; \u0026#39;.join([x for x in text.split() if x and x not in stopwords_set]) return text.strip() # need this to convert questions asked by user to vectors def question_to_vec(question, embeddings, dim=300): \u0026#34;\u0026#34;\u0026#34; question: a string embeddings: dict where the key is a word and a value is its\u0026#39; embedding dim: size of the representation result: vector representation for the question \u0026#34;\u0026#34;\u0026#34; word_tokens = question.split(\u0026#34; \u0026#34;) question_len = len(word_tokens) question_mat = np.zeros((question_len,dim), dtype = np.float32) for idx, word in enumerate(word_tokens): if word in embeddings: question_mat[idx,:] = embeddings[word] # remove zero-rows which stand for OOV words  question_mat = question_mat[~np.all(question_mat == 0, axis = 1)] # Compute the mean of each word along the sentence if question_mat.shape[0] \u0026gt; 0: vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim)) else: vec = np.zeros((1,dim), dtype = np.float32) return vec class SimpleDialogueManager(object): \u0026#34;\u0026#34;\u0026#34; This is a simple dialogue manager to test the telegram bot. The main part of our bot will be written here. \u0026#34;\u0026#34;\u0026#34; def __init__(self): # Instantiate all the models and TFIDF Objects. print(\u0026#34;Loading resources...\u0026#34;) # Instantiate a Chatterbot for Chitchat type questions from chatterbot import ChatBot from chatterbot.trainers import ChatterBotCorpusTrainer chatbot = ChatBot(\u0026#39;MLWhizChatterbot\u0026#39;) trainer = ChatterBotCorpusTrainer(chatbot) trainer.train(\u0026#39;chatterbot.corpus.english\u0026#39;) self.chitchat_bot = chatbot print(\u0026#34;Loading Word2vec model...\u0026#34;) # Instantiate the Google\u0026#39;s pre-trained Word2Vec model. self.model = gensim.models.KeyedVectors.load_word2vec_format(\u0026#39;GoogleNews-vectors-negative300.bin\u0026#39;, binary=True) print(\u0026#34;Loading Classifier objects...\u0026#34;) # Load the intent classifier and tag classifier self.intent_recognizer = pickle.load(open(\u0026#39;resources/intent_clf.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) self.tag_classifier = pickle.load(open(\u0026#39;resources/tag_clf.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) # Load the TFIDF vectorizer object self.tfidf_vectorizer = pickle.load(open(\u0026#39;resources/tfidf.pkl\u0026#39;, \u0026#39;rb\u0026#39;)) print(\u0026#34;Finished Loading Resources\u0026#34;) # We created this function just above. We just need to have a function to get most similar question\u0026#39;s *post id* in the dataset given we know the programming Language of the question. Here it is: def get_similar_question(self,question,tag): # get the path where all question embeddings are kept and load the post_ids and post_embeddings embeddings_path = \u0026#39;resources/embeddings_folder/\u0026#39; + tag + \u0026#34;.pkl\u0026#34; post_ids, post_embeddings = pickle.load(open(embeddings_path, \u0026#39;rb\u0026#39;)) # Get the embeddings for the question question_vec = question_to_vec(question, self.model, 300) # find index of most similar post best_post_index = pairwise_distances_argmin(question_vec, post_embeddings) # return best post id return post_ids[best_post_index] def generate_answer(self, question): prepared_question = text_prepare(question) features = self.tfidf_vectorizer.transform([prepared_question]) # find intent intent = self.intent_recognizer.predict(features)[0] # Chit-chat part:  if intent == \u0026#39;dialogue\u0026#39;: response = self.chitchat_bot.get_response(question) # Stack Overflow Question else: # find programming language tag = self.tag_classifier.predict(features)[0] # find most similar question post id post_id = self.get_similar_question(question,tag)[0] # respond with response = \u0026#39;I think its about %s\\nThis thread might help you: https://stackoverflow.com/questions/%s\u0026#39; % (tag, post_id) return response Here is the code for the whole \u0026lt;code\u0026gt;main.py\u0026lt;/code\u0026gt; for you to use and see. Just run the whole main.py using\n$ python main.py And we will have our bot up and running.\nAgain, here is the link to the github repository The possibilities are really endless This is just a small demo project of what you can do with the chatbots. You can do a whole lot more once you recognize that the backend is just python.\n  One idea is to run a chatbot script on all the servers I have to run system commands straight from telegram. We can use os.system to run any system command. Bye Bye SSH.\n  You can make chatbots to do some daily tasks by using simple keyword-based intents. It is just simple logic. Find out the weather, find out cricket scores or maybe newly released movies. Whatever floats your boat.\n  Or maybe try to integrate Telegram based Chatbot in your website. See livechatbot   Or maybe just try to have fun with it.\n    Conclusion Here we learned how to create a simple chatbot. And it works well. We can improve a whole lot on this present chatbot by increasing classifier accuracy, handling edge cases, making it respond faster or maybe adding more logic to handle more use cases. But the fact remains the same. The AI in chatbots is just simple human logic and nothing magic.\nIn this post, I closely followed one of the projects from this course to create this chatbot. Do check out this course if you get confused, or tell me your problems in the comments I will certainly try to help.\nFollow me up at Medium or Subscribe to my blog to be informed about my next posts.\nTill then Ciao!!\n","permalink":"https://mlwhiz.com/blog/2019/04/15/chatbot/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Best Content"],"title":"Chatbots  aren't as difficult to make as You Think"},{"categories":["Data Science"],"contents":"Just Kidding, Nothing is hotter than Jennifer Lawrence. But as you are here, let\u0026rsquo;s proceed.\nFor a practitioner in any field, they turn out as good as the tools they use. Data Scientists are no different. But sometimes we don\u0026rsquo;t even know which tools we need and also if we need them. We are not able to fathom if there could be a more natural way to solve the problem we face. We could learn about Data Science using awesome MOOCs like Machine Learning by Andrew Ng but no one teaches the spanky tools of the trade. This motivated me to write about the tools and skills that one is not taught in any course in my new series of short posts‚Ää-‚ÄäTools For Data Science. As it is rightly said:\n We shape our tools and afterward our tools shape us.\n In this post, I will try to talk about the Sublime Text Editor in the context of Data Science.\nSublime Text is such a lifesaver, and we as data scientists don\u0026rsquo;t even realize that we need it. We are generally so happy with our Jupyter Notebooks and R studio that we never try to use another editor.\nSo, let me try to sway you a little bit from your Jupyter notebooks into integrating another editor in your workflow. I will try to provide some use cases below. On that note, these use cases are not at all exhaustive and are here just to demonstrate the functionality and Sublime power.\n 1. Create A Dictionary/List or Whatever: How many times does it happen that we want to make a list or dictionary for our Python code from a list we got in an email text? I bet numerous times.¬†How do we do this? We haggle in Excel by loading that Text in Excel and then trying out concatenating operations. For those of us on a Mac, it is even more troublesome since Mac\u0026rsquo;s Excel is not as good as windows(to put it mildly)\nSo, for example, if you had information about State Name and State Short Name and you had to create a dictionary for Python, you would end up doing something like this in Excel. Or maybe you will load the CSV in pandas and then play with it in Python itself.\n  Here is how you would do the same in Sublime. And see just how wonderful it looks. We ended up getting the Dictionary in one single line. It took me around 27 seconds to do.¬†I still remember the first time I saw one of my developer friends doing this, and I was amazed. On that note, We should always learn from other domains\n  So how I did this?\nHere is a step by step idea. You might want to get some data in Sublime and try it out yourself. The command that you will be using most frequently is Cmd+Shift+L\n Select all the text in the sublime window using Cmd+A Cmd+Shift+L to get the cursor on all lines Use Cmd and Opt with arrow keys to move these cursors to required locations. Cmd takes to beginning and end. Opt takes you token by token Do your Magic and write. Press Delete key to getting everything in one line Press Esc to get out from Multiple cursor mode Enjoy!   2. Select Selectively and Look Good while doing¬†it: Another functionality in Sublime that I love. We all have used Replace functionality in many text editors. This functionality is Find and Replace with a twist.\nSo, without further ado, let me demonstrate it with an example. Let\u0026rsquo;s say we have a code snippet written in Python and we want to replace some word. We can very well do it with Find and Replace Functionality. We will find and replace each word and would end up clicking a lot of times. Sublime makes it so much easier. And it looks impressive too. You look like you know what you are doing, which will get a few brownie points in my book.\n  So how I did this?\n Select the word you want to replace Press Cmd+D multiple times to only select instances of the word you want to remove. When all words are selected, write the new word And that\u0026rsquo;s all    This concludes my post about one of the most efficient editors I have ever known. You can try to do a lot of things with Sublime but the above use cases are the ones which I find most useful. These simple commands will make your work much more efficient and remove the manual drudgery which is sometimes a big part of our jobs. Hope you end up using this in your Workflow. Trust me you will end up loving it.\n Let me know if you liked this post. I will continue writing such Tips and Tricks in a series of posts. Also, do follow me on Medium to get notified about my future posts.\n PS1: All the things above will also work with Atom text editor using the exact same commands on Mac.\nPS2: For Window Users, Replace Cmd by Ctrl and Opt with Alt to get the same functionality.\n","permalink":"https://mlwhiz.com/blog/2019/03/31/sublime_ds_post/","tags":["Data Science","Tools"],"title":"Why Sublime Text for Data Science is Hotter than Jennifer Lawrence?"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post , I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post , I delved deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.\nAs a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\nBefore introducing the notion of transfer learning to NLP applications, we will first need to understand a little bit about Language models.\n Language Models And NLP Transfer Learning Intuition: In very basic terms the objective of the language model is to predict the next word given a stream of input words. In the past, many different approaches have been used to solve this particular problem. Probabilistic models using Markov assumption is one example of this sort of models.\n$$ P(W_n) = P(W_n|W_{n-1}) $$\nIn the recent era, people have been using RNNs/LSTMs to create such language models. They take as input a word embedding and at each time state return the probability distribution of next word probability over the dictionary words. An example of this is shown below in which the below Neural Network uses multiple stacked layers of RNN cells to learn a language model to predict the next word.\n  Now why do we need the concept of Language Modeling? Or How does predicting the next word tie with the current task of text classification? The intuition ties to the way that the neural network gets trained. The neural network that can predict the next word after being trained on a massive corpus like Wikipedia already has learned a lot of structure in a particular language. Can we use this knowledge in the weights of the network for our advantage? Yes, we can, and that is where the idea of Transfer Learning in NLP stems from. So to make this intuition more concrete, Let us think that our neural network is divided into two parts -\n Language Specific: The lower part of the neural network is language specific. That is it learns the features of the language. This part could be used to transfer our knowledge from a language corpus to our current task Task Specific: I will call the upper part of our network as task specific. The weights in these layers are trained so that it learns to predict the next word.    Now as it goes in a lot of transfer learning models for Image, we stack the Language Specific part with some dense and softmax layers(Our new task) and train on our new task to achieve what we want to do.\n ULMFit: Now the concept of Transfer learning in NLP is not entirely new and people already used Language models for transfer learning back in 2015-16 without good result. So what has changed now?\nThe thing that has changed is that people like Jeremy Howard and Sebastian Ruder have done a lot of research on how to train these networks. And so we have achieved state of the art results on many text datasets with Transfer Learning approaches.\nLet\u0026rsquo;s follow up with the key research findings in the ULMFit paper written by them along with the code.\n Change in the way Transfer Learning networks are trained:   Training a model as per ULMFiT we need to take these three steps:\na) Create a Base Language Model: Training the language model on a general-domain corpus that captures high-level natural language features\nb) Finetune Base Language Model on Task Specific Data: Fine-tuning the pre-trained language model on target task data\nc) Finetune Base Language Model Layers + Task Specific Layers on Task Specific Data: Fine-tuning the classifier on target task data\nSo let us go through these three steps one by one along with the code that is provided to us with the FastAI library.\na) Create a Base Language Model: This task might be the most time-consuming task. This model is analogous to resnet50 or Inception for the vision task. In the paper, they use the language model AWD-LSTM, a regular LSTM architecture trained with various tuned dropout hyperparameters. This model was trained on Wikitext-103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words. We won\u0026rsquo;t perform this task ourselves and will use the fabulous FastAI library to use this model as below. The code below will take our data and preprocess it for usage in the AWD_LSTM model as well as load the model.\n# Language model data : We use test_df as validation for language model data_lm = TextLMDataBunch.from_df(path = \u0026#34;\u0026#34;,train_df= train_df ,valid_df = test_df) learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5) It is also where we preprocess the data as per the required usage for the FastAI models. For example:\nprint(train_df)   print(data_lm) TextLMDataBunch; Train: LabelList (1306122 items) x: LMTextList xxbos xxmaj how did xxmaj quebec nationalists see their province as a nation in the 1960s ?,xxbos xxmaj do you have an adopted dog , how would you encourage people to adopt and not shop ?,xxbos xxmaj why does velocity affect time ? xxmaj does velocity affect space geometry ?,xxbos xxmaj how did xxmaj otto von xxmaj guericke used the xxmaj magdeburg hemispheres ?,xxbos xxmaj can i convert montra xxunk d to a mountain bike by just changing the tyres ? y: LMLabelList ,,,, Path: .; Valid: LabelList (375806 items) x: LMTextList xxbos xxmaj why do so many women become so rude and arrogant when they get just a little bit of wealth and power ?,xxbos xxmaj when should i apply for xxup rv college of engineering and xxup bms college of engineering ? xxmaj should i wait for the xxup comedk result or am i supposed to apply before the result ?,xxbos xxmaj what is it really like to be a nurse practitioner ?,xxbos xxmaj who are entrepreneurs ?,xxbos xxmaj is education really making good people nowadays ? y: LMLabelList ,,,, Path: .; Test: None The tokenized prepared data is based on a lot of research from the FastAI developers. To make this post a little bit complete, I am sharing some of the tokens definition as well.\n xxunk is for an unknown word (one that isn\u0026rsquo;t present in the current vocabulary) xxpad is the token used for padding, if we need to regroup several texts of different lengths in a batch xxbos represents the beginning of a text in your dataset xxmaj is used to indicate the next word begins with a capital in the original text xxup is used to indicate the next word is written in all caps in the original text  b) Finetune Base Language Model on Task Specific Data This task is also pretty easy when we look at the code. The specific details of how we do the training is what holds the essence.\n# Learning with Discriminative fine tuning learn.fit_one_cycle(1, 1e-2) learn.unfreeze() learn.fit_one_cycle(1, 1e-3) # Save encoder Object learn.save_encoder(\u0026#39;ft_enc\u0026#39;) The paper introduced two general concepts for this learning stage:\n Discriminative fine-tuning:  The Main Idea is: As different layers capture different types of information, they should be fine-tuned to different extents. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. In the paper, the authors suggest first to finetune only the last layer, and then unfreeze all the layers with a learning rate lowered by a factor of 2.6.\n Slanted triangular learning rates:    According to the authors: \u0026ldquo;For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters\u0026rdquo; The Main Idea is to use a high learning rate at the starting stage for increased learning and low learning rates to finetune at later stages in an epoch.\nAfter training our Language model on the Quora dataset, we should be able to see how our model performs on the Language Model task itself. FastAI library provides us with a simple function to do that.\n# check how the language model performs learn.predict(\u0026#34;What should\u0026#34;, n_words=10) 'What should be the likelihood of a tourist visiting Mumbai for'  c) Finetune Base Language Model Layers + Task Specific Layers on Task Specific Data This is the stage where task-specific learning takes place that is we add the classification layers and fine tune them.\nThe authors augment the pretrained language model with two additional linear blocks. Each block uses batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer. The params of these task-specific layers are the only ones that are learned from scratch.\n#Creating Classification Data data_clas = TextClasDataBunch.from_df(path =\u0026#34;\u0026#34;, train_df=train, valid_df =valid, test_df=test_df, vocab=data_lm.train_ds.vocab, bs=32,label_cols = \u0026#39;target\u0026#39;) # Creating Classifier Object learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) # Add weights of finetuned Language model learn.load_encoder(\u0026#39;ft_enc\u0026#39;) # Fitting Classifier Object learn.fit_one_cycle(1, 1e-2) # Fitting Classifier Object after freezing all but last 2 layers learn.freeze_to(-2) learn.fit_one_cycle(1, slice(5e-3/2., 5e-3)) # Fitting Classifier Object - discriminative learning learn.unfreeze() learn.fit_one_cycle(1, slice(2e-3/100, 2e-3)) Here also the Authors have derived a few novel methods:\n Concat Pooling:  The authors use not only the concatenation of all the hidden state but also the Maxpool and Meanpool representation of all hidden states as input to the linear layers.\n$$ H = [h_1, . . . , h_T ] $$\n$$ h_c = [h_T , maxpool(H), meanpool(H)] $$\n Gradual Unfreezing:  Rather than fine-tuning all layers at once, which risks catastrophic forgetting(Forgetting everything we have learned so far from language models), the authors propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge. The Authors first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. They then unfreeze the next lower frozen layer and repeat, until they finetune all layers until convergence at the last iteration. The function slice(2e-3/100, 2e-3) means that we train every layer with different learning rates ranging from max to min value.\nOne can get the predictions for the test data at once using:\ntest_preds = np.array(learn.get_preds(DatasetType.Test, ordered=True)[0])[:,1] I am a big fan of Kaggle Kernels. One could not have imagined having all that compute for free. You can find a running version of the above code in this kaggle kernel . Do try to experiment with it after forking and running the code. Also please upvote the kernel if you find it helpful.\n Results: Here are the final results of all the different approaches I have tried on the Kaggle Dataset. I ran a 5 fold Stratified CV.\na. Conventional Methods:   b. Deep Learning Methods:   c. Transfer Learning Methods(ULMFIT):   The results achieved were not very good compared to deep learning methods, but I still liked the idea of the transfer learning approach, and it was so easy to implement it using fastAI. Also running the code took a lot of time at 9 hours, compared to other methods which got over in 2 hours.\nEven if this approach didn\u0026rsquo;t work well for this dataset, it is a valid approach for other datasets, as the Authors of the paper have achieved pretty good results on different datasets ‚Äî definitely a genuine method to try out.\nPS: Note that I didn\u0026rsquo;t work on tuning the above models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning using hyperopt or just old fashioned Grid-search.\n Conclusion: Finally, this post concludes my NLP Learning series. It took a lot of time to write, but the effort was well worth it. I hope you found it helpful in your work. I will try to write some more on this topic when I get some time. Follow me up at Medium or Subscribe to my blog to be informed about my next posts.\nAlso if you want to \u0026lt;strong\u0026gt;learn more about NLP\u0026lt;/strong\u0026gt; here is an excellent course. You can start for free with the 7-day Free Trial.\nLet me know if you think I can add something more to the post; I will try to incorporate it.\nCheers!!!\n","permalink":"https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle"],"title":"NLP  Learning Series: Part 4 - Transfer Learning Intuition for Text Classification"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post , I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like ULMFit transfer learning approaches in the fourth post in the series.\nAs a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\nSo let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them ‚Äî also, some code in Keras and Pytorch. So you can try them out for yourself.\n 1. TextCNN The idea of using a CNN to classify text was first presented in the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim.\nRepresentation: The central intuition about this idea is to see our documents as images. How? Let us say we have a sentence and we have maxlen = 70 and embedding size = 300. We can create a matrix of numbers with the shape 70x300 to represent this sentence. For images, we also have a matrix where individual elements are pixel values. Instead of image pixels, the input to the tasks is sentences or documents represented as a matrix. Each row of the matrix corresponds to one-word vector.\n  Convolution Idea: While for an image we move our conv filter horizontally as well as vertically, for text we fix kernel size to filter_size x embed_size, i.e. (3,300) we are just going to move vertically down for the convolution taking look at three words at once since our filter size is 3 in this case. This idea seems right since our convolution filter is not splitting word embedding. It gets to look at the full embedding of each word. Also one can think of filter sizes as unigrams, bigrams, trigrams, etc. Since we are looking at a context window of 1,2,3, and 5 words respectively.\nHere is the text classification network coded in Pytorch:\nimport torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable class CNN_Text(nn.Module): def __init__(self): super(CNN_Text, self).__init__() filter_sizes = [1,2,3,5] num_filters = 36 self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes]) self.dropout = nn.Dropout(0.1) self.fc1 = nn.Linear(len(Ks)*num_filters, 1) def forward(self, x): x = self.embedding(x) x = x.unsqueeze(1) x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] x = torch.cat(x, 1) x = self.dropout(x) logit = self.fc1(x) return logit And for the Keras enthusiasts:\n# https://www.kaggle.com/yekenot/2dcnn-textclassifier def model_cnn(embedding_matrix): filter_sizes = [1,2,3,5] num_filters = 36 inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) x = Reshape((maxlen, embed_size, 1))(x) maxpool_pool = [] for i in range(len(filter_sizes)): conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size), kernel_initializer=\u0026#39;he_normal\u0026#39;, activation=\u0026#39;relu\u0026#39;)(x) maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv)) z = Concatenate(axis=1)(maxpool_pool) z = Flatten()(z) z = Dropout(0.1)(z) outp = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(z) model = Model(inputs=inp, outputs=outp) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model I am a big fan of Kaggle Kernels. One could not have imagined having all that compute for free. You can find a running version of the above two code snippets in this kaggle kernel . Do try to experiment with it after forking and running the code. Also please upvote the kernel if you find it helpful.\nThe Keras model and Pytorch model performed similarly with Pytorch model beating the keras model by a small margin. The Out-Of-Fold CV F1 score for the Pytorch model came out to be 0.6609 while for Keras model the same score came out to be 0.6559. I used the same preprocessing in both the models to be better able to compare the platforms.\n 2. BiDirectional RNN(LSTM/GRU): TextCNN works well for Text Classification. It takes care of words in close range. It can see \u0026ldquo;new york\u0026rdquo; together. However, it still can\u0026rsquo;t take care of all the context provided in a particular text sequence. It still does not learn the sequential structure of the data, where every word is dependent on the previous word. Or a word in the previous sentence.\nRNN help us with that. They can remember previous information using hidden states and connect it to the current task.\nLong Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for an extended period. Moreover, the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won\u0026rsquo;t work for a time series prediction task as we don\u0026rsquo;t have visibility into the future in this case).\n  For a most simplistic explanation of Bidirectional RNN, think of RNN cell as a black box taking as input a hidden state(a vector) and a word vector and giving out an output vector and the next hidden state. This box has some weights which are to be tuned using Backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.\n   Hidden state, Word vector -\u0026gt;(RNN Cell) -\u0026gt; Output Vector , Next Hidden state  For a sequence of length 4 like \u0026ldquo;you will never believe\u0026rdquo;, The RNN cell gives 4 output vectors, which can be concatenated and then used as part of a dense feedforward architecture.\nIn the Bidirectional RNN, the only change is that we read the text in the usual fashion as well in reverse. So we stack two RNNs in parallel, and hence we get 8 output vectors to append.\nOnce we get the output vectors, we send them through a series of dense layers and finally a softmax layer to build a text classifier.\nIn most cases, you need to understand how to stack some layers in a neural network to get the best results. We can try out multiple bidirectional GRU/LSTM layers in the network if it performs better.\nDue to the limitations of RNNs like not remembering long term dependencies, in practice, we almost always use LSTM/GRU to model long term dependencies. In such a case you can think of the RNN cell being replaced by an LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangeably with CuDNNLSTM when you build models. (CuDNNGRU/LSTM are just implementations of LSTM/GRU that are created to run faster on GPUs. In most cases always use them instead of the vanilla LSTM/GRU implementations)\nSo here is some code in Pytorch for this network.\nclass BiLSTM(nn.Module): def __init__(self): super(BiLSTM, self).__init__() self.hidden_size = 64 drp = 0.1 self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True) self.linear = nn.Linear(self.hidden_size*4 , 64) self.relu = nn.ReLU() self.dropout = nn.Dropout(drp) self.out = nn.Linear(64, 1) def forward(self, x): h_embedding = self.embedding(x) h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)) h_lstm, _ = self.lstm(h_embedding) avg_pool = torch.mean(h_lstm, 1) max_pool, _ = torch.max(h_lstm, 1) conc = torch.cat(( avg_pool, max_pool), 1) conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) return out Also, here is the same code in Keras.\n# BiDirectional LSTM def model_lstm_du(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) \u0026#39;\u0026#39;\u0026#39; Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer? 64*70(maxlen)*2(bidirection concat) CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU \u0026#39;\u0026#39;\u0026#39; x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) conc = concatenate([avg_pool, max_pool]) conc = Dense(64, activation=\u0026#34;relu\u0026#34;)(conc) conc = Dropout(0.1)(conc) outp = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(conc) model = Model(inputs=inp, outputs=outp) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model You can run this code in my BiLSTM with Pytorch and Keras kaggle kernel for this competition. Please do upvote the kernel if you find it helpful.\nIn the BiLSTM case also, Pytorch model beats the keras model by a small margin. The Out-Of-Fold CV F1 score for the Pytorch model came out to be 0.6741 while for Keras model the same score came out to be 0.6727. This score is around a 1-2% increase from the TextCNN performance which is pretty good. Also, note that it is around 6-7% better than conventional methods.\n 3. Attention Models Dzmitry Bahdanau et al first presented attention in their paper Neural Machine Translation by Jointly Learning to Align and Translate but I find that the paper on Hierarchical Attention Networks for Document Classification written jointly by CMU and Microsoft in 2016 is a much easier read and provides more intuition.\nSo let us talk about the intuition first. In the past conventional methods like TFIDF/CountVectorizer etc. we used to find features from the text by doing a keyword extraction. Some word is more helpful in determining the category of a text than others. However, in this method we sort of lost the sequential structure of the text. With LSTM and deep learning methods, while we can take care of the sequence structure, we lose the ability to give higher weight to more important words. Can we have the best of both worlds?\nThe answer is Yes. Actually, Attention is all you need. In the author\u0026rsquo;s words:\n Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector\n   In essence, we want to create scores for every word in the text, which is the attention similarity score for a word.\nTo do this, we start with a weight matrix(W), a bias vector(b) and a context vector u. The optimization algorithm learns all of these weights. On this note I would like to highlight something I like a lot about neural networks - If you don\u0026rsquo;t know some params, let the network learn them. We only have to worry about creating architectures and params to tune.\nThen there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as nonlinearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v‚Äôs to get the Final Scores,s\nThese final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.\nHere is the code in Pytorch. Do try to read through the pytorch code for attention layer. It just does what I have explained above.\nclass Attention(nn.Module): def __init__(self, feature_dim, step_dim, bias=True, **kwargs): super(Attention, self).__init__(**kwargs) self.supports_masking = True self.bias = bias self.feature_dim = feature_dim self.step_dim = step_dim self.features_dim = 0 weight = torch.zeros(feature_dim, 1) nn.init.kaiming_uniform_(weight) self.weight = nn.Parameter(weight) if bias: self.b = nn.Parameter(torch.zeros(step_dim)) def forward(self, x, mask=None): feature_dim = self.feature_dim step_dim = self.step_dim eij = torch.mm( x.contiguous().view(-1, feature_dim), self.weight ).view(-1, step_dim) if self.bias: eij = eij + self.b eij = torch.tanh(eij) a = torch.exp(eij) if mask is not None: a = a * mask a = a / (torch.sum(a, 1, keepdim=True) + 1e-10) weighted_input = x * torch.unsqueeze(a, -1) return torch.sum(weighted_input, 1) class Attention_Net(nn.Module): def __init__(self): super(Attention_Net, self).__init__() drp = 0.1 self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False self.embedding_dropout = nn.Dropout2d(0.1) self.lstm = nn.LSTM(embed_size, 128, bidirectional=True, batch_first=True) self.lstm2 = nn.GRU(128*2, 64, bidirectional=True, batch_first=True) self.attention_layer = Attention(128, maxlen) self.linear = nn.Linear(64*2 , 64) self.relu = nn.ReLU() self.out = nn.Linear(64, 1) def forward(self, x): h_embedding = self.embedding(x) h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)) h_lstm, _ = self.lstm(h_embedding) h_lstm, _ = self.lstm2(h_lstm) h_lstm_atten = self.attention_layer(h_lstm) conc = self.relu(self.linear(h_lstm_atten)) out = self.out(conc) return out Same code for Keras.\ndef dot_product(x, kernel): \u0026#34;\u0026#34;\u0026#34; Wrapper for dot product operation, in order to be compatible with both Theano and Tensorflow Args: x (): input kernel (): weights Returns: \u0026#34;\u0026#34;\u0026#34; if K.backend() == \u0026#39;tensorflow\u0026#39;: return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1) else: return K.dot(x, kernel) class AttentionWithContext(Layer): \u0026#34;\u0026#34;\u0026#34; Attention operation, with a context/query vector, for temporal data. Supports Masking. Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] \u0026#34;Hierarchical Attention Networks for Document Classification\u0026#34; by using a context vector to assist the attention # Input shape 3D tensor with shape: `(samples, steps, features)`. # Output shape 2D tensor with shape: `(samples, features)`. How to use: Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. The dimensions are inferred based on the output shape of the RNN. Note: The layer has been tested with Keras 2.0.6 Example: model.add(LSTM(64, return_sequences=True)) model.add(AttentionWithContext()) # next add a Dense layer (for classification/regression) or whatever... \u0026#34;\u0026#34;\u0026#34; def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None, W_constraint=None, u_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get(\u0026#39;glorot_uniform\u0026#39;) self.W_regularizer = regularizers.get(W_regularizer) self.u_regularizer = regularizers.get(u_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.u_constraint = constraints.get(u_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias super(AttentionWithContext, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1], input_shape[-1],), initializer=self.init, name=\u0026#39;{}_W\u0026#39;.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) if self.bias: self.b = self.add_weight((input_shape[-1],), initializer=\u0026#39;zero\u0026#39;, name=\u0026#39;{}_b\u0026#39;.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) self.u = self.add_weight((input_shape[-1],), initializer=self.init, name=\u0026#39;{}_u\u0026#39;.format(self.name), regularizer=self.u_regularizer, constraint=self.u_constraint) super(AttentionWithContext, self).build(input_shape) def compute_mask(self, input, input_mask=None): # do not pass the mask to the next layers return None def call(self, x, mask=None): uit = dot_product(x, self.W) if self.bias: uit += self.b uit = K.tanh(uit) ait = dot_product(uit, self.u) a = K.exp(ait) # apply mask after the exp. will be re-normalized next if mask is not None: # Cast the mask to floatX to avoid float64 upcasting in theano a *= K.cast(mask, K.floatx()) # in some cases especially in the early stages of training the sum may be almost zero # and this results in NaN\u0026#39;s. A workaround is to add a very small positive number Œµ to the sum. # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], input_shape[-1] def model_lstm_atten(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x) x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) x = AttentionWithContext()(x) x = Dense(64, activation=\u0026#34;relu\u0026#34;)(x) x = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(x) model = Model(inputs=inp, outputs=x) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model Again, my Attention with Pytorch and Keras Kaggle kernel contains the working versions for this code. Please do upvote the kernel if you find it useful.\nThis method performed well with Pytorch CV scores reaching around 0.6758 and Keras CV scores reaching around 0.678. This score is more than what we were able to achieve with BiLSTM and TextCNN. However, please note that we didn\u0026rsquo;t work on tuning any of the given methods yet and so the scores might be different.\nWith this, I leave you to experiment with new architectures and playing around with stacking multiple GRU/LSTM layers to improve your network performance. You can also look at including more techniques in these network like Bucketing, handmade features, etc. Some of the tips and new techniques are mentioned here on my blog post: What my first Silver Medal taught me about Text Classification and Kaggle in general? . Also, here is another Kaggle kernel which is my silver-winning entry for this competition.\n Results Here are the final results of all the different approaches I have tried on the Kaggle Dataset. I ran a 5 fold Stratified CV.\na. Conventional Methods:   b. Deep Learning Methods:   PS: Note that I didn\u0026rsquo;t work on tuning the above models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning using hyperopt or just old fashioned Grid-search.\n Conclusion In this post, I went through with the explanations of various deep learning architectures people are using for Text classification tasks. In the next post, we will delve further into the next new phenomenon in NLP space - Transfer Learning with BERT and ULMFit. Follow me up at Medium or Subscribe to my blog to be informed about my next post.\nAlso if you want to \u0026lt;strong\u0026gt;learn more about NLP\u0026lt;/strong\u0026gt; here is an excellent course. You can start for free with the 7-day Free Trial.\nLet me know if you think I can add something more to the post; I will try to incorporate it.\nCheers!!!\n","permalink":"https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle","Best Content"],"title":"NLP  Learning Series: Part 3 - Attention, CNN and what not for Text Classification"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\nSo first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. The challenge was not only a test for performance but also a test of efficient code writing skills. As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.\nSome Kaggle Learnings: There were a couple of learnings about kaggle as a whole that I would like to share before jumping into my final solution:\n1. Always trust your CV   One of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was small test dataset(only 65k rows) in the first stage(around 15% of total test data).\nA common theme on discussion forums was focussing on which submissions we should select as the final submission:\n The one having the best local CV? or The one having the best LB?  And while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.\nLuckily I didn\u0026rsquo;t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around \u0026gt;1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.\nWhile this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score \u0026gt;= 0.70\n2. Use the code from public kernels but check for errors  This Pytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn\u0026rsquo;t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post here or on my kernel . Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.\nNonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.\n3. Don\u0026rsquo;t trust everything that goes on the discussion forums   I will talk about two things here:\n  Seed tuning: While in the middle of the competition, everyone was trying to get the best possible rank on the public LB. It is just human nature. A lot of discussions was around good seeds and bad seeds for neural network initialization. While it seems okay in the first look, the conversation went a stage further where people started tuning seeds in the kernel as a hyper param. Some discussions even went on to say that it was a valid strategy. And that is where a large amount of overfitting to public LB started happening. The same submission would score 0.704 from 0.699 just by changing the seed. For a reference, that meant you could go from anywhere near 400-500 rank to top 50 only by changing seed in a public kernel. And that spelled disaster. Some people did that. They went up the public LB. Went crashing out at the private stage.\n  CV score disclosure on discussion forums: We always try to gauge our performance against other people. In a lot of discussions, people provided their CV scores and corresponding Public LB scores. The scores were all over the place and not comparable due to Different CV schemes, No of folds in CV, Metric reported, Overfitting or just plain Wrong implementation of Cross-Validation. But they ended up influencing a lot of starters and newcomers.\n  4. On that note, be active on Discussion forums and check public kernels regularly You can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by SRK , Models by Shujian , and Preprocessing by Theo Viel which gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.\nEven after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very ** vital to check out the winning solutions.**\n5. Share a lot Sharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post , I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like ULMFit transfer learning approaches in the fourth post in the series.\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts too: What Kagglers are using for Text Classification , which talks about various deep learning models in use in NLP and how to switch from Keras to Pytorch .\n6. Beware of trolls :) We were going along happily towards the end of the competition with two weeks left. Scores were increasing slowly. The top players were somewhat stagnant. And then came Pavel and team with a Public LB score of 0.782. The next group had an LB score of 0.713. Such a huge difference. I was so sure that there was some leakage in the data which nobody has caught yet except for Pavel. I spent nearly half a day to do EDA again.\nIn the end, it turned out that what they did was scraping ‚Äî nicely played!\nThey also have some pretty awesome ideas around including additional data, which could have worked but did not in this competition.\n My Final Solution:   My main focus was on meta-feature engineering and on increasing embedding coverage and quality. That means I did not play much with various Neural Net architectures. Here are the things that I included in my final submission:\n I noticed that Glove embeddings were doing good on the local CV but not on LB, while meta embeddings(mean of glove and paragram) were doing good on LB but not that good on the CV. I took a mixed approach so some of my models are trained with only glove embedding and some on meta embeddings. Included four more features in embedding. Thus my embedding was a 304-dimensional vector. The four new values corresponded to title case flag, uppercase flag, Textblob word polarity, textblob word subjectivity Found out NER tokens from the whole train and test data using spacy and kept the tokens and the entities in a dict. I used this dict to create extra features like counts of GPE, PERSON, ORG, NORP, WORK_OF_ART.Added some value and were highly correlated with the target. Other features that I used include total_length,capitals,words_vs_unique as well as some engineered features like sum_feat(sum of expletives), question_start_with_why, question_start_with_how_or_what, question_start_with_do_or_are. Might not have added much value but still kept them. My final solution consisted of a stacked ensemble for four models. I stacked the four models using Logistic regression(with positive weights and 0 intercept) and gave the weights as a list in the final kernel.  You can find the kernel for my final submission here .\n Tips and Tricks used in other solutions: 1. Increasing Embeddings Coverage: In the third place solution kernel , wowfattie uses stemming, lemmatization, capitalize, lower, uppercase, as well as embedding of the nearest word using a spell checker to get embeddings for all words in his vocab. Such a great idea. I liked this solution the best as it can do what I was trying to do and finished at a pretty good place. Also, the code is very clean.\nfrom nltk.stem import PorterStemmer ps = PorterStemmer() from nltk.stem.lancaster import LancasterStemmer lc = LancasterStemmer() from nltk.stem import SnowballStemmer sb = SnowballStemmer(\u0026#34;english\u0026#34;) def load_glove(word_dict, lemma_dict): EMBEDDING_FILE = \u0026#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt\u0026#39; def get_coefs(word,*arr): return word, np.asarray(arr, dtype=\u0026#39;float32\u0026#39;) embeddings_index = dict(get_coefs(*o.split(\u0026#34; \u0026#34;)) for o in open(EMBEDDING_FILE)) embed_size = 300 nb_words = len(word_dict)+1 embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32) unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1. print(unknown_vector[:5]) for key in tqdm(word_dict): word = key embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = key.lower() embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = key.upper() embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = key.capitalize() embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = ps.stem(key) embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = lc.stem(key) embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = sb.stem(key) embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue word = lemma_dict[key] embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue if len(key) \u0026gt; 1: word = correction(key) embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[word_dict[key]] = embedding_vector continue embedding_matrix[word_dict[key]] = unknown_vector return embedding_matrix, nb_words 2. Checkpoint Ensembling: Get a lot of models at no cost. Most of the winning solutions have some version of checkpoint ensembling. For the third place solution, the predictions are a weighted average of predictions after the 4th epoch and predictions after the 5th epoch. I got this idea but forgot to implement it in my ensemble based kernel submission.\n3. Meta Embeddings: A lot of winning solutions ended up using weighted meta embeddings where they provided a higher weight to the Glove embedding. Some solutions also used concatenated embeddings.\n4. Model Architecture: One surprising thing I saw people doing was to use a 1Dconv layer just after the Bidirectional layer. For example, This is the architecture used by the team that placed first in the competition.\n  5. Bucketing/Variable Sequence Length and increased hidden units: Another thing I noticed is the increased number of hidden units as compared to many public kernels. Most of the public kernels used a hidden unit size of 60 due to time constraints. I used 80 units at the cost of training one less network. A lot of high scoring kernels were able to use a higher number of hidden units owing to variable sequence length idea or bucketing. From the 1st place kernel discussion:\n We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.\n Also from 7th place discussion :\n Bucketing is to make a minibatch from instances that have similar lengths to alleviate the cost of padding. This makes the training speed more than 3x faster, and thus I can run 9 epochs for each split of 5-fold.\n Thus the use of this technique also allowed some competitors to fit many more epochs in less time and run more models at the same time. Pretty Neat!\n6. For those winners who didn\u0026rsquo;t use bucketing, Maxlen = 72 was too large: Most of us saw a distribution of question length and took the length that covered maximum questions fully as the maxlen parameter. I never tried to tune it, but it seems like it could have been tuned. One of the tricks was to use maxlen ranging from 35 to 60. This made the kernels run a lot faster.\n7. Time taking models/complex architectures like Capsule were mostly not used: Most of the winning solutions didn\u0026rsquo;t use capsule networks as they took a lot of time to train.\n8. Backprop errors on embeddings weights in last few epochs: Another thing I saw was in the 18th place kernel which uses a single model\nif epoch \u0026gt;= 3: model.embedding.embeddings.weight.requires_grad = True  Conclusion: It was a good and long 2-month competition, and I learned a lot about Text and NLP during this time. I want to emphasize here is that I ended up trying a lot of things that didn\u0026rsquo;t work before reaching my final solution. It was a little frustrating at times, but in the end, I was happy that I ended up with the best data science practices. Would also like to thank Kaggle master Kazanova who along with some of his friends released a ‚ÄúHow to win a data science competition‚Äù Coursera course. I learned a lot from this course.\nLet me know in the comments if you think something is missing/wrong or if I could add more tips/tricks for this competition.\n","permalink":"https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle","Best Content"],"title":"What my first Silver Medal taught me about Text Classification and Kaggle in general?"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the various preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into Deep learning models in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like ULMFit transfer learning approaches in the fourth post in the series.\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts too: What Kagglers are using for Text Classification , which talks about various deep learning models in use in NLP and how to switch from Keras to Pytorch .\nSo again we start with the first step: Preprocessing.\n Basic Preprocessing Techniques for text data(Continued) So in the last post, we talked about various preprocessing methods for text for deep learning purpose. Most of the preprocessing for conventional methods remains the same. We will still remove special characters, punctuations, and contractions. But We also may want to do stemming/lemmatization when it comes to conventional methods. Let us talk about them.\n For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n Since we are going to create features for words in the feature creation step, it makes sense to reduce words to a common denominator so that \u0026lsquo;organize\u0026rsquo;,\u0026lsquo;organizes\u0026rsquo; and \u0026lsquo;organizing\u0026rsquo; could be referred to by a single word \u0026lsquo;organize\u0026rsquo;\n a) Stemming Stemming is the process of converting words to their base forms using crude Heuristic rules. For example, one rule could be to remove \u0026rsquo;s' from the end of any word, so that \u0026lsquo;cats\u0026rsquo; becomes \u0026lsquo;cat\u0026rsquo;. or another rule could be to replace \u0026lsquo;ies\u0026rsquo; with \u0026lsquo;i\u0026rsquo; so that \u0026lsquo;ponies becomes \u0026lsquo;poni\u0026rsquo;. One of the main point to note here is that when we stem the word we might get a nonsense word like \u0026lsquo;poni\u0026rsquo;. But it will still work for our use case as we count the number of occurrences of a particular word and not focus on the meanings of these words in conventional methods. It doesn\u0026rsquo;t work with deep learning for precisely the same reason.\n  We can do this pretty simply by using this function in python.\nfrom nltk.stem import SnowballStemmer from nltk.tokenize.toktok import ToktokTokenizer def stem_text(text): tokenizer = ToktokTokenizer() stemmer = SnowballStemmer(\u0026#39;english\u0026#39;) tokens = tokenizer.tokenize(text) tokens = [token.strip() for token in tokens] tokens = [stemmer.stem(token) for token in tokens] return \u0026#39; \u0026#39;.join(tokens)  b) Lemmatization Lemmatization is very similar to stemming but it aims to remove endings only if the base form is present in a dictionary.\nfrom nltk.stem import WordNetLemmatizer from nltk.tokenize.toktok import ToktokTokenizer def lemma_text(text): tokenizer = ToktokTokenizer() tokens = tokenizer.tokenize(text) tokens = [token.strip() for token in tokens] tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens] return \u0026#39; \u0026#39;.join(tokens) Once we are done with processing a text, our text will necessarily go through these following steps.\ndef clean_sentence(x): x = x.lower() x = clean_text(x) x = clean_numbers(x) x = replace_typical_misspell(x) x = remove_stopwords(x) x = replace_contractions(x) x = lemma_text(x) x = x.replace(\u0026#34;\u0026#39;\u0026#34;,\u0026#34;\u0026#34;) return x  Text Representation In Conventional Machine learning methods, we ought to create features for a text. There are a lot of representations that are present to achieve this. Let us talk about them one by one.\na) Bag of Words - Countvectorizer Features Suppose we have a series of sentences(documents)\nX = [ \u0026#39;This is good\u0026#39;, \u0026#39;This is bad\u0026#39;, \u0026#39;This is awesome\u0026#39; ]   Bag of words will create a dictionary of the most common words in all the sentences. For the example above the dictionary would look like:\nword_index {\u0026#39;this\u0026#39;:0,\u0026#39;is\u0026#39;:1,\u0026#39;good\u0026#39;:2,\u0026#39;bad\u0026#39;:3,\u0026#39;awesome\u0026#39;:4} And then encode the sentences using the above dict.\nThis is good - [1,1,1,0,0] This is bad - [1,1,0,1,0] This is awesome - [1,1,0,0,1] We could do this pretty simply in Python by using the CountVectorizer class from Python. Don\u0026rsquo;t worry much about the heavy name, it just does what I explained above. It has a lot of parameters most significant of which are:\n ngram_range: I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features. min_df: Minimum no of time an ngram should appear in a corpus to be used as a feature.  cnt_vectorizer = CountVectorizer(dtype=np.float32, strip_accents=\u0026#39;unicode\u0026#39;, analyzer=\u0026#39;word\u0026#39;,token_pattern=r\u0026#39;\\w{1,}\u0026#39;, ngram_range=(1, 3),min_df=3) # we fit count vectorizer to get ngrams from both train and test data. cnt_vectorizer.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values)) xtrain_cntv = cnt_vectorizer.transform(train_df.cleaned_text.values) xtest_cntv = cnt_vectorizer.transform(test_df.cleaned_text.values) We could then use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like. For example:\n# Fitting a simple Logistic Regression on CV Feats clf = LogisticRegression(C=1.0) clf.fit(xtrain_cntv,y_train)  Here is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don\u0026rsquo;t forget to upvote.\n b) TFIDF Features TFIDF is a simple technique to find features from sentences. While in Count features we take count of all the words/ngrams present in a document, with TFIDF we take features only for the significant words. How do we do that? If you think of a document in a corpus, we will consider two things about any word in that document:\n   Term Frequency: How important is the word in the document?  $$TF(word\\ in\\ a\\ document) = \\dfrac{No\\ of\\ occurances\\ of\\ that\\ word\\ in\\ document}{No\\ of\\ words\\ in\\ document}$$\n Inverse Document Frequency: How important the term is in the whole corpus?  $$IDF(word\\ in\\ a\\ corpus) = -log(ratio\\ of\\ documents\\ that\\ include\\ the\\ word)$$\nTFIDF then is just multiplication of these two scores.\nIntuitively, One can understand that a word is important if it occurs many times in a document. But that creates a problem. Words like \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo; occur many times in sentence. Their TF score will always be high. We solve that by using Inverse Document frequency, which is high if the word is rare, and low if the word is common across the corpus.\nIn essence, we want to find important words in a document which are also not very common.\nWe could do this pretty simply in Python by using the TFIDFVectorizer class from Python. It has a lot of parameters most significant of which are:\n ngram_range: I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features. min_df: Minimum no of time an ngram should appear in a corpus to be used as a feature.  # Always start with these features. They work (almost) everytime! tfv = TfidfVectorizer(dtype=np.float32, min_df=3, max_features=None, strip_accents=\u0026#39;unicode\u0026#39;, analyzer=\u0026#39;word\u0026#39;,token_pattern=r\u0026#39;\\w{1,}\u0026#39;, ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = \u0026#39;english\u0026#39;) # Fitting TF-IDF to both training and test sets (semi-supervised learning) tfv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values)) xtrain_tfv = tfv.transform(train_df.cleaned_text.values) xvalid_tfv = tfv.transform(test_df.cleaned_text.values) Again, we could use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like. Here is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don\u0026rsquo;t forget to upvote.\n c) Hashing Features Normally there will be a lot of ngrams in a document corpus. The number of features that our TFIDFVectorizer generated was in excess of 2,00,000 features. This might lead to a problem on very large datasets as we have to hold a very large vocabulary dictionary in memory. One way to counter this is to use the Hash Trick.\n  One can think of hashing as a single function which maps any ngram to a number range for example between 0 to 1024. Now we don\u0026rsquo;t have to store our ngrams in a dictionary. We can just use the function to get the index of any word, rather than getting the index from a dictionary.\nSince there can be more than 1024 ngrams, different ngrams might map to the same number, and this is called collision. The larger the range we provide our Hashing function, the less is the chance of collisions.\nWe could do this pretty simply in Python by using the HashingVectorizer class from Python. It has a lot of parameters most significant of which are:\n ngram_range: I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features. n_features: No of features you want to consider. The range I gave above.  # Always start with these features. They work (almost) everytime! hv = HashingVectorizer(dtype=np.float32, strip_accents=\u0026#39;unicode\u0026#39;, analyzer=\u0026#39;word\u0026#39;, ngram_range=(1, 4),n_features=2**12,non_negative=True) # Fitting Hash Vectorizer to both training and test sets (semi-supervised learning) hv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values)) xtrain_hv = hv.transform(train_df.cleaned_text.values) xvalid_hv = hv.transform(test_df.cleaned_text.values) y_train = train_df.target.values  Here is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don\u0026rsquo;t forget to upvote.\n d) Word2vec Features We already talked a little about word2vec in the previous post. We can use the word to vec features to create sentence level feats also. We want to create a d dimensional vector for sentence. For doing this, we will simply average the word embedding of all the words in a sentence.\n  We can do this in Python using the following functions.\n# load the GloVe vectors in a dictionary: def load_glove_index(): EMBEDDING_FILE = \u0026#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt\u0026#39; def get_coefs(word,*arr): return word, np.asarray(arr, dtype=\u0026#39;float32\u0026#39;)[:300] embeddings_index = dict(get_coefs(*o.split(\u0026#34; \u0026#34;)) for o in open(EMBEDDING_FILE)) return embeddings_index embeddings_index = load_glove_index() print(\u0026#39;Found %sword vectors.\u0026#39; % len(embeddings_index)) from nltk.corpus import stopwords stop_words = stopwords.words(\u0026#39;english\u0026#39;) def sent2vec(s): words = str(s).lower() words = word_tokenize(words) words = [w for w in words if not w in stop_words] words = [w for w in words if w.isalpha()] M = [] for w in words: try: M.append(embeddings_index[w]) except: continue M = np.array(M) v = M.sum(axis=0) if type(v) != np.ndarray: return np.zeros(300) return v / np.sqrt((v ** 2).sum()) # create glove features xtrain_glove = np.array([sent2vec(x) for x in tqdm(train_df.cleaned_text.values)]) xtest_glove = np.array([sent2vec(x) for x in tqdm(test_df.cleaned_text.values)])  Here is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don\u0026rsquo;t forget to upvote.\n Results Here are the results of different approaches on the Kaggle Dataset. I ran a 5 fold Stratified CV.\n   Here is the code. If you like it please don\u0026rsquo;t forget to upvote. Also note that I didn\u0026rsquo;t work on tuning the models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning using hyperopt or just old fashioned Grid-search and the performance of models may change after that substantially.\n Conclusion While Deep Learning works a lot better for NLP classification task, it still makes sense to have an understanding of how these problems were solved in the past, so that we can appreciate the nature of the problem. I have tried to provide a perspective on the conventional methods and one should experiment with them too to create baselines before moving to Deep Learning methods. If you want to learn more about NLP here is an awesome course. You can start for free with the 7-day Free Trial. If you think I can add something to the flow, do mention it in the comments.\n Endnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.\n  Approaching (Almost) Any NLP Problem on Kaggle   How to: Preprocessing when using embeddings    ","permalink":"https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle","Best Content"],"title":"NLP  Learning Series: Part 2 - Conventional Methods for Text Classification"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.\nSince we have a large amount of material to cover, I am splitting this post into a series of posts. The first post i.e. this one will be based on preprocessing techniques that work with Deep learning models and we will also talk about increasing embeddings coverage. In the second post , I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into Deep learning models in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like ULMFit transfer learning approaches in the fourth post in the series.\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.\nIt might take me a little time to write the whole series. Till then you can take a look at my other posts: What Kagglers are using for Text Classification , which talks about various deep learning models in use in NLP and how to switch from Keras to Pytorch .\nSo first let me start with explaining a little more about the text classification problem. Text classification is a common task in natural language processing, which transforms a sequence of a text of indefinite length into a category of text. How could you use that?\n To find the sentiment of a review. Find toxic comments on a platform like Facebook Find Insincere questions on Quora. A current ongoing competition on kaggle Find fake reviews on websites Will a text advert get clicked or not?  Now each of these problems has something in common. From a Machine Learning perspective, these are essentially the same problem with just the target labels changing and nothing else. With that said, the addition of business knowledge can help make these models more robust and that is what we want to incorporate while preprocessing the data for test classification. While the preprocessing pipeline I am focussing on in this post is mainly centered around Deep Learning but most of it will also be applicable to conventional machine learning models too.\nBut let me first go through the flow of a deep learning pipeline for text data before going through all the steps to get a higher level perspective about the whole process.\n  We normally start with cleaning up the text data and performing basic EDA. Here we try to improve our data quality by cleaning up the data. We also try to improve the quality of our word2vec embeddings by removing OOV(Out-of-Vocabulary) words. These first two steps normally don\u0026rsquo;t have much order between them and I generally go back and forth between these two steps. Next, we create a representation for text that could be fed into a deep learning model. We then start with creating our models and training them. Finally, we evaluate the models using appropriate metrics and get approval from respective shareholders to deploy our models. Don\u0026rsquo;t worry if these terms don\u0026rsquo;t make much sense now. I will try to explain them through the course of this article.\nHere at this junction, let us take a little detour to talk a little about word embeddings. We will have to think about them while preprocessing data for our Deep Learning models.\nA Primer on word2vec embeddings: We need to have a way to represent words in a vocab. One way to do that could be to use One hot encoding of word vectors but that is not really a good choice. One of the major reasons is that the one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity.\n$$\\frac{\\boldsymbol{x}^\\top \\boldsymbol{y}}{|\\boldsymbol{x}| |\\boldsymbol{y}|} \\in [-1, 1].$$\nGiven the structure of one hot encoded vectors, the similarity is always going to come as 0 between different words. Another reason is that as the size of vocabulary increases these one hot encoded vectors become very large.\nWord2Vec overcomes the above difficulties by providing us with a fixed length vector representation of words and by capturing the similarity and analogy relationships between different words.\n  Word2vec vectors of words are learned in such a way that they allow us to learn different analogies. It enables us to do algebraic manipulations on words which were not possible before. For example: What is king - man + woman? It comes out to be Queen.\nWord2Vec vectors also help us to find out the similarity between words. If we try to find similar words to \u0026ldquo;good\u0026rdquo;, we will find awesome, great etc. It is this property of word2vec that makes it invaluable for text classification. Now our deep learning network understands that \u0026ldquo;good\u0026rdquo; and \u0026ldquo;great\u0026rdquo; are essentially words with similar meaning.\nThus in very simple terms, word2vec creates vectors for words. Thus we have a d dimensional vector for every word(common bigrams too) in a dictionary. We normally use pretrained word vectors which are provided to us by others after training on large corpora of texts like Wikipedia, twitter etc. The most commonly used pretrained word vectors are Glove and Fasttext with 300-dimensional word vectors. We are going to use Glove in this post.\nBasic Preprocessing Techniques for text data: In most of the cases, we observe that text data is not entirely clean. Data coming from different sources have different characteristics and that makes Text Preprocessing as one of the most important steps in the classification pipeline. For example, Text data from Twitter is totally different from text data on Quora, or some news/blogging platform, and thus would need to be treated differently. Helpfully, the techniques I am going to talk about in this post are generic enough for any kind of data you might encounter in the jungles of NLP.\na) Cleaning Special Characters and Removing Punctuations: Our preprocessing pipeline depends a lot on the word2vec embeddings we are going to use for our classification task. In principle our preprocessing should match the preprocessing that was used before training the word embedding. Since most of the embeddings don\u0026rsquo;t provide vector values for punctuations and other special chars, the first thing you want to do is to get rid of is the special characters in your text data. These are some of the special chars that were there in the Quora Question data and we use replace function to get rid of these special chars.\n# Some preprocesssing that will be common to all the text classification methods you will see. puncts = [\u0026#39;,\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;\u0026#34;\u0026#39;, \u0026#39;:\u0026#39;, \u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;, \u0026#39;-\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;?\u0026#39;, \u0026#39;|\u0026#39;, \u0026#39;;\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;$\u0026#39;, \u0026#39;\u0026amp;\u0026#39;, \u0026#39;/\u0026#39;, \u0026#39;[\u0026#39;, \u0026#39;]\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, \u0026#39;%\u0026#39;, \u0026#39;=\u0026#39;, \u0026#39;#\u0026#39;, \u0026#39;*\u0026#39;, \u0026#39;+\u0026#39;, \u0026#39;\\\\\u0026#39;, \u0026#39;‚Ä¢\u0026#39;, \u0026#39;~\u0026#39;, \u0026#39;@\u0026#39;, \u0026#39;¬£\u0026#39;, \u0026#39;¬∑\u0026#39;, \u0026#39;_\u0026#39;, \u0026#39;{\u0026#39;, \u0026#39;}\u0026#39;, \u0026#39;¬©\u0026#39;, \u0026#39;^\u0026#39;, \u0026#39;¬Æ\u0026#39;, \u0026#39;`\u0026#39;, \u0026#39;\u0026lt;\u0026#39;, \u0026#39;‚Üí\u0026#39;, \u0026#39;¬∞\u0026#39;, \u0026#39;‚Ç¨\u0026#39;, \u0026#39;‚Ñ¢\u0026#39;, \u0026#39;‚Ä∫\u0026#39;, \u0026#39;‚ô•\u0026#39;, \u0026#39;‚Üê\u0026#39;, \u0026#39;√ó\u0026#39;, \u0026#39;¬ß\u0026#39;, \u0026#39;‚Ä≥\u0026#39;, \u0026#39;‚Ä≤\u0026#39;, \u0026#39;√Ç\u0026#39;, \u0026#39;‚ñà\u0026#39;, \u0026#39;¬Ω\u0026#39;, \u0026#39;√†\u0026#39;, \u0026#39;‚Ä¶\u0026#39;, \u0026#39;‚Äú\u0026#39;, \u0026#39;‚òÖ\u0026#39;, \u0026#39;‚Äù\u0026#39;, \u0026#39;‚Äì\u0026#39;, \u0026#39;‚óè\u0026#39;, \u0026#39;√¢\u0026#39;, \u0026#39;‚ñ∫\u0026#39;, \u0026#39;‚àí\u0026#39;, \u0026#39;¬¢\u0026#39;, \u0026#39;¬≤\u0026#39;, \u0026#39;¬¨\u0026#39;, \u0026#39;‚ñë\u0026#39;, \u0026#39;¬∂\u0026#39;, \u0026#39;‚Üë\u0026#39;, \u0026#39;¬±\u0026#39;, \u0026#39;¬ø\u0026#39;, \u0026#39;‚ñæ\u0026#39;, \u0026#39;‚ïê\u0026#39;, \u0026#39;¬¶\u0026#39;, \u0026#39;‚ïë\u0026#39;, \u0026#39;‚Äï\u0026#39;, \u0026#39;¬•\u0026#39;, \u0026#39;‚ñì\u0026#39;, \u0026#39;‚Äî\u0026#39;, \u0026#39;‚Äπ\u0026#39;, \u0026#39;‚îÄ\u0026#39;, \u0026#39;‚ñí\u0026#39;, \u0026#39;Ôºö\u0026#39;, \u0026#39;¬º\u0026#39;, \u0026#39;‚äï\u0026#39;, \u0026#39;‚ñº\u0026#39;, \u0026#39;‚ñ™\u0026#39;, \u0026#39;‚Ä†\u0026#39;, \u0026#39;‚ñ†\u0026#39;, \u0026#39;‚Äô\u0026#39;, \u0026#39;‚ñÄ\u0026#39;, \u0026#39;¬®\u0026#39;, \u0026#39;‚ñÑ\u0026#39;, \u0026#39;‚ô´\u0026#39;, \u0026#39;‚òÜ\u0026#39;, \u0026#39;√©\u0026#39;, \u0026#39;¬Ø\u0026#39;, \u0026#39;‚ô¶\u0026#39;, \u0026#39;¬§\u0026#39;, \u0026#39;‚ñ≤\u0026#39;, \u0026#39;√®\u0026#39;, \u0026#39;¬∏\u0026#39;, \u0026#39;¬æ\u0026#39;, \u0026#39;√É\u0026#39;, \u0026#39;‚ãÖ\u0026#39;, \u0026#39;‚Äò\u0026#39;, \u0026#39;‚àû\u0026#39;, \u0026#39;‚àô\u0026#39;, \u0026#39;Ôºâ\u0026#39;, \u0026#39;‚Üì\u0026#39;, \u0026#39;„ÄÅ\u0026#39;, \u0026#39;‚îÇ\u0026#39;, \u0026#39;Ôºà\u0026#39;, \u0026#39;¬ª\u0026#39;, \u0026#39;Ôºå\u0026#39;, \u0026#39;‚ô™\u0026#39;, \u0026#39;‚ï©\u0026#39;, \u0026#39;‚ïö\u0026#39;, \u0026#39;¬≥\u0026#39;, \u0026#39;„Éª\u0026#39;, \u0026#39;‚ï¶\u0026#39;, \u0026#39;‚ï£\u0026#39;, \u0026#39;‚ïî\u0026#39;, \u0026#39;‚ïó\u0026#39;, \u0026#39;‚ñ¨\u0026#39;, \u0026#39;‚ù§\u0026#39;, \u0026#39;√Ø\u0026#39;, \u0026#39;√ò\u0026#39;, \u0026#39;¬π\u0026#39;, \u0026#39;‚â§\u0026#39;, \u0026#39;‚Ä°\u0026#39;, \u0026#39;‚àö\u0026#39;, ] def clean_text(x): x = str(x) for punct in puncts: if punct in x: x = x.replace(punct, f\u0026#39; {punct} \u0026#39;) return x This could also have been done with the help of a simple regex. But I normally like the above way of doing things as it helps to understand the sort of characters we are removing from our data.\ndef clean_text(x): pattern = r\u0026#39;[^a-zA-z0-9\\s]\u0026#39; text = re.sub(pattern, \u0026#39;\u0026#39;, x) return x b) Cleaning Numbers: Why do we want to replace numbers with #s? Because most embeddings have preprocessed their text like this.\nSmall Python Trick: We use an if statement in the code below to check beforehand if a number exists in a text. It is as an if is always fast than a re.sub command and most of our text doesn\u0026rsquo;t contain numbers.\ndef clean_numbers(x): if bool(re.search(r\u0026#39;\\d\u0026#39;, x)): x = re.sub(\u0026#39;[0-9]{5,}\u0026#39;, \u0026#39;#####\u0026#39;, x) x = re.sub(\u0026#39;[0-9]{4}\u0026#39;, \u0026#39;####\u0026#39;, x) x = re.sub(\u0026#39;[0-9]{3}\u0026#39;, \u0026#39;###\u0026#39;, x) x = re.sub(\u0026#39;[0-9]{2}\u0026#39;, \u0026#39;##\u0026#39;, x) return x c) Removing Misspells: It always helps to find out misspells in the data. As those word embeddings are not present in the word2vec, we should replace words with their correct spellings to get better embedding coverage. The following code artifact is an adaptation of Peter Norvig\u0026rsquo;s spell checker. It uses word2vec ordering of words to approximate word probabilities. As Google word2vec apparently orders words in decreasing order of frequency in the training corpus. You can use this to find out some misspelled words in the data you have.\n# This comes from CPMP script in the Quora questions similarity challenge. import re from collections import Counter import gensim import heapq from operator import itemgetter from multiprocessing import Pool model = gensim.models.KeyedVectors.load_word2vec_format(\u0026#39;../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\u0026#39;, binary=True) words = model.index2word w_rank = {} for i,word in enumerate(words): w_rank[word] = i WORDS = w_rank def words(text): return re.findall(r\u0026#39;\\w+\u0026#39;, text.lower()) def P(word): \u0026#34;Probability of `word`.\u0026#34; # use inverse of rank as proxy # returns 0 if the word isn\u0026#39;t in the dictionary return - WORDS.get(word, 0) def correction(word): \u0026#34;Most probable spelling correction for word.\u0026#34; return max(candidates(word), key=P) def candidates(word): \u0026#34;Generate possible spelling corrections for word.\u0026#34; return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word]) def known(words): \u0026#34;The subset of `words` that appear in the dictionary of WORDS.\u0026#34; return set(w for w in words if w in WORDS) def edits1(word): \u0026#34;All edits that are one edit away from `word`.\u0026#34; letters = \u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; splits = [(word[:i], word[i:]) for i in range(len(word) + 1)] deletes = [L + R[1:] for L, R in splits if R] transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)\u0026gt;1] replaces = [L + c + R[1:] for L, R in splits if R for c in letters] inserts = [L + c + R for L, R in splits for c in letters] return set(deletes + transposes + replaces + inserts) def edits2(word): \u0026#34;All edits that are two edits away from `word`.\u0026#34; return (e2 for e1 in edits1(word) for e2 in edits1(e1)) def build_vocab(texts): sentences = texts.apply(lambda x: x.split()).values vocab = {} for sentence in sentences: for word in sentence: try: vocab[word] += 1 except KeyError: vocab[word] = 1 return vocab vocab = build_vocab(train.question_text) top_90k_words = dict(heapq.nlargest(90000, vocab.items(), key=itemgetter(1))) pool = Pool(4) corrected_words = pool.map(correction,list(top_90k_words.keys())) for word,corrected_word in zip(top_90k_words,corrected_words): if word!=corrected_word: print(word,\u0026#34;:\u0026#34;,corrected_word) Once we are through with finding misspelled data, the next thing remains to replace them using a misspell mapping and regex functions.\nmispell_dict = {\u0026#39;colour\u0026#39;: \u0026#39;color\u0026#39;, \u0026#39;centre\u0026#39;: \u0026#39;center\u0026#39;, \u0026#39;favourite\u0026#39;: \u0026#39;favorite\u0026#39;, \u0026#39;travelling\u0026#39;: \u0026#39;traveling\u0026#39;, \u0026#39;counselling\u0026#39;: \u0026#39;counseling\u0026#39;, \u0026#39;theatre\u0026#39;: \u0026#39;theater\u0026#39;, \u0026#39;cancelled\u0026#39;: \u0026#39;canceled\u0026#39;, \u0026#39;labour\u0026#39;: \u0026#39;labor\u0026#39;, \u0026#39;organisation\u0026#39;: \u0026#39;organization\u0026#39;, \u0026#39;wwii\u0026#39;: \u0026#39;world war 2\u0026#39;, \u0026#39;citicise\u0026#39;: \u0026#39;criticize\u0026#39;, \u0026#39;youtu \u0026#39;: \u0026#39;youtube \u0026#39;, \u0026#39;Qoura\u0026#39;: \u0026#39;Quora\u0026#39;, \u0026#39;sallary\u0026#39;: \u0026#39;salary\u0026#39;, \u0026#39;Whta\u0026#39;: \u0026#39;What\u0026#39;, \u0026#39;narcisist\u0026#39;: \u0026#39;narcissist\u0026#39;, \u0026#39;howdo\u0026#39;: \u0026#39;how do\u0026#39;, \u0026#39;whatare\u0026#39;: \u0026#39;what are\u0026#39;, \u0026#39;howcan\u0026#39;: \u0026#39;how can\u0026#39;, \u0026#39;howmuch\u0026#39;: \u0026#39;how much\u0026#39;, \u0026#39;howmany\u0026#39;: \u0026#39;how many\u0026#39;, \u0026#39;whydo\u0026#39;: \u0026#39;why do\u0026#39;, \u0026#39;doI\u0026#39;: \u0026#39;do I\u0026#39;, \u0026#39;theBest\u0026#39;: \u0026#39;the best\u0026#39;, \u0026#39;howdoes\u0026#39;: \u0026#39;how does\u0026#39;, \u0026#39;mastrubation\u0026#39;: \u0026#39;masturbation\u0026#39;, \u0026#39;mastrubate\u0026#39;: \u0026#39;masturbate\u0026#39;, \u0026#34;mastrubating\u0026#34;: \u0026#39;masturbating\u0026#39;, \u0026#39;pennis\u0026#39;: \u0026#39;penis\u0026#39;, \u0026#39;Etherium\u0026#39;: \u0026#39;Ethereum\u0026#39;, \u0026#39;narcissit\u0026#39;: \u0026#39;narcissist\u0026#39;, \u0026#39;bigdata\u0026#39;: \u0026#39;big data\u0026#39;, \u0026#39;2k17\u0026#39;: \u0026#39;2017\u0026#39;, \u0026#39;2k18\u0026#39;: \u0026#39;2018\u0026#39;, \u0026#39;qouta\u0026#39;: \u0026#39;quota\u0026#39;, \u0026#39;exboyfriend\u0026#39;: \u0026#39;ex boyfriend\u0026#39;, \u0026#39;airhostess\u0026#39;: \u0026#39;air hostess\u0026#39;, \u0026#34;whst\u0026#34;: \u0026#39;what\u0026#39;, \u0026#39;watsapp\u0026#39;: \u0026#39;whatsapp\u0026#39;, \u0026#39;demonitisation\u0026#39;: \u0026#39;demonetization\u0026#39;, \u0026#39;demonitization\u0026#39;: \u0026#39;demonetization\u0026#39;, \u0026#39;demonetisation\u0026#39;: \u0026#39;demonetization\u0026#39;} def _get_mispell(mispell_dict): mispell_re = re.compile(\u0026#39;(%s)\u0026#39; % \u0026#39;|\u0026#39;.join(mispell_dict.keys())) return mispell_dict, mispell_re mispellings, mispellings_re = _get_mispell(mispell_dict) def replace_typical_misspell(text): def replace(match): return mispellings[match.group(0)] return mispellings_re.sub(replace, text) # Usage replace_typical_misspell(\u0026#34;Whta is demonitisation\u0026#34;) d) Removing Contractions: Contractions are words that we write with an apostrophe. Examples of contractions are words like \u0026ldquo;ain\u0026rsquo;t\u0026rdquo; or \u0026ldquo;aren\u0026rsquo;t\u0026rdquo;. Since we want to standardize our text, it makes sense to expand these contractions. Below we have done this using a contraction mapping and regex functions.\ncontraction_dict = {\u0026#34;ain\u0026#39;t\u0026#34;: \u0026#34;is not\u0026#34;, \u0026#34;aren\u0026#39;t\u0026#34;: \u0026#34;are not\u0026#34;,\u0026#34;can\u0026#39;t\u0026#34;: \u0026#34;cannot\u0026#34;, \u0026#34;\u0026#39;cause\u0026#34;: \u0026#34;because\u0026#34;, \u0026#34;could\u0026#39;ve\u0026#34;: \u0026#34;could have\u0026#34;, \u0026#34;couldn\u0026#39;t\u0026#34;: \u0026#34;could not\u0026#34;, \u0026#34;didn\u0026#39;t\u0026#34;: \u0026#34;did not\u0026#34;, \u0026#34;doesn\u0026#39;t\u0026#34;: \u0026#34;does not\u0026#34;, \u0026#34;don\u0026#39;t\u0026#34;: \u0026#34;do not\u0026#34;, \u0026#34;hadn\u0026#39;t\u0026#34;: \u0026#34;had not\u0026#34;, \u0026#34;hasn\u0026#39;t\u0026#34;: \u0026#34;has not\u0026#34;, \u0026#34;haven\u0026#39;t\u0026#34;: \u0026#34;have not\u0026#34;, \u0026#34;he\u0026#39;d\u0026#34;: \u0026#34;he would\u0026#34;,\u0026#34;he\u0026#39;ll\u0026#34;: \u0026#34;he will\u0026#34;, \u0026#34;he\u0026#39;s\u0026#34;: \u0026#34;he is\u0026#34;, \u0026#34;how\u0026#39;d\u0026#34;: \u0026#34;how did\u0026#34;, \u0026#34;how\u0026#39;d\u0026#39;y\u0026#34;: \u0026#34;how do you\u0026#34;, \u0026#34;how\u0026#39;ll\u0026#34;: \u0026#34;how will\u0026#34;, \u0026#34;how\u0026#39;s\u0026#34;: \u0026#34;how is\u0026#34;, \u0026#34;I\u0026#39;d\u0026#34;: \u0026#34;I would\u0026#34;, \u0026#34;I\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;I would have\u0026#34;, \u0026#34;I\u0026#39;ll\u0026#34;: \u0026#34;I will\u0026#34;, \u0026#34;I\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;I will have\u0026#34;,\u0026#34;I\u0026#39;m\u0026#34;: \u0026#34;I am\u0026#34;, \u0026#34;I\u0026#39;ve\u0026#34;: \u0026#34;I have\u0026#34;, \u0026#34;i\u0026#39;d\u0026#34;: \u0026#34;i would\u0026#34;, \u0026#34;i\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;i would have\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#34;: \u0026#34;i will\u0026#34;, \u0026#34;i\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;i will have\u0026#34;,\u0026#34;i\u0026#39;m\u0026#34;: \u0026#34;i am\u0026#34;, \u0026#34;i\u0026#39;ve\u0026#34;: \u0026#34;i have\u0026#34;, \u0026#34;isn\u0026#39;t\u0026#34;: \u0026#34;is not\u0026#34;, \u0026#34;it\u0026#39;d\u0026#34;: \u0026#34;it would\u0026#34;, \u0026#34;it\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;it would have\u0026#34;, \u0026#34;it\u0026#39;ll\u0026#34;: \u0026#34;it will\u0026#34;, \u0026#34;it\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;it will have\u0026#34;,\u0026#34;it\u0026#39;s\u0026#34;: \u0026#34;it is\u0026#34;, \u0026#34;let\u0026#39;s\u0026#34;: \u0026#34;let us\u0026#34;, \u0026#34;ma\u0026#39;am\u0026#34;: \u0026#34;madam\u0026#34;, \u0026#34;mayn\u0026#39;t\u0026#34;: \u0026#34;may not\u0026#34;, \u0026#34;might\u0026#39;ve\u0026#34;: \u0026#34;might have\u0026#34;,\u0026#34;mightn\u0026#39;t\u0026#34;: \u0026#34;might not\u0026#34;,\u0026#34;mightn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;might not have\u0026#34;, \u0026#34;must\u0026#39;ve\u0026#34;: \u0026#34;must have\u0026#34;, \u0026#34;mustn\u0026#39;t\u0026#34;: \u0026#34;must not\u0026#34;, \u0026#34;mustn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;must not have\u0026#34;, \u0026#34;needn\u0026#39;t\u0026#34;: \u0026#34;need not\u0026#34;, \u0026#34;needn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;need not have\u0026#34;,\u0026#34;o\u0026#39;clock\u0026#34;: \u0026#34;of the clock\u0026#34;, \u0026#34;oughtn\u0026#39;t\u0026#34;: \u0026#34;ought not\u0026#34;, \u0026#34;oughtn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;ought not have\u0026#34;, \u0026#34;shan\u0026#39;t\u0026#34;: \u0026#34;shall not\u0026#34;, \u0026#34;sha\u0026#39;n\u0026#39;t\u0026#34;: \u0026#34;shall not\u0026#34;, \u0026#34;shan\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;shall not have\u0026#34;, \u0026#34;she\u0026#39;d\u0026#34;: \u0026#34;she would\u0026#34;, \u0026#34;she\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;she would have\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#34;: \u0026#34;she will\u0026#34;, \u0026#34;she\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;she will have\u0026#34;, \u0026#34;she\u0026#39;s\u0026#34;: \u0026#34;she is\u0026#34;, \u0026#34;should\u0026#39;ve\u0026#34;: \u0026#34;should have\u0026#34;, \u0026#34;shouldn\u0026#39;t\u0026#34;: \u0026#34;should not\u0026#34;, \u0026#34;shouldn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;should not have\u0026#34;, \u0026#34;so\u0026#39;ve\u0026#34;: \u0026#34;so have\u0026#34;,\u0026#34;so\u0026#39;s\u0026#34;: \u0026#34;so as\u0026#34;, \u0026#34;this\u0026#39;s\u0026#34;: \u0026#34;this is\u0026#34;,\u0026#34;that\u0026#39;d\u0026#34;: \u0026#34;that would\u0026#34;, \u0026#34;that\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;that would have\u0026#34;, \u0026#34;that\u0026#39;s\u0026#34;: \u0026#34;that is\u0026#34;, \u0026#34;there\u0026#39;d\u0026#34;: \u0026#34;there would\u0026#34;, \u0026#34;there\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;there would have\u0026#34;, \u0026#34;there\u0026#39;s\u0026#34;: \u0026#34;there is\u0026#34;, \u0026#34;here\u0026#39;s\u0026#34;: \u0026#34;here is\u0026#34;,\u0026#34;they\u0026#39;d\u0026#34;: \u0026#34;they would\u0026#34;, \u0026#34;they\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;they would have\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#34;: \u0026#34;they will\u0026#34;, \u0026#34;they\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;they will have\u0026#34;, \u0026#34;they\u0026#39;re\u0026#34;: \u0026#34;they are\u0026#34;, \u0026#34;they\u0026#39;ve\u0026#34;: \u0026#34;they have\u0026#34;, \u0026#34;to\u0026#39;ve\u0026#34;: \u0026#34;to have\u0026#34;, \u0026#34;wasn\u0026#39;t\u0026#34;: \u0026#34;was not\u0026#34;, \u0026#34;we\u0026#39;d\u0026#34;: \u0026#34;we would\u0026#34;, \u0026#34;we\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;we would have\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#34;: \u0026#34;we will\u0026#34;, \u0026#34;we\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;we will have\u0026#34;, \u0026#34;we\u0026#39;re\u0026#34;: \u0026#34;we are\u0026#34;, \u0026#34;we\u0026#39;ve\u0026#34;: \u0026#34;we have\u0026#34;, \u0026#34;weren\u0026#39;t\u0026#34;: \u0026#34;were not\u0026#34;, \u0026#34;what\u0026#39;ll\u0026#34;: \u0026#34;what will\u0026#34;, \u0026#34;what\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;what will have\u0026#34;, \u0026#34;what\u0026#39;re\u0026#34;: \u0026#34;what are\u0026#34;, \u0026#34;what\u0026#39;s\u0026#34;: \u0026#34;what is\u0026#34;, \u0026#34;what\u0026#39;ve\u0026#34;: \u0026#34;what have\u0026#34;, \u0026#34;when\u0026#39;s\u0026#34;: \u0026#34;when is\u0026#34;, \u0026#34;when\u0026#39;ve\u0026#34;: \u0026#34;when have\u0026#34;, \u0026#34;where\u0026#39;d\u0026#34;: \u0026#34;where did\u0026#34;, \u0026#34;where\u0026#39;s\u0026#34;: \u0026#34;where is\u0026#34;, \u0026#34;where\u0026#39;ve\u0026#34;: \u0026#34;where have\u0026#34;, \u0026#34;who\u0026#39;ll\u0026#34;: \u0026#34;who will\u0026#34;, \u0026#34;who\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;who will have\u0026#34;, \u0026#34;who\u0026#39;s\u0026#34;: \u0026#34;who is\u0026#34;, \u0026#34;who\u0026#39;ve\u0026#34;: \u0026#34;who have\u0026#34;, \u0026#34;why\u0026#39;s\u0026#34;: \u0026#34;why is\u0026#34;, \u0026#34;why\u0026#39;ve\u0026#34;: \u0026#34;why have\u0026#34;, \u0026#34;will\u0026#39;ve\u0026#34;: \u0026#34;will have\u0026#34;, \u0026#34;won\u0026#39;t\u0026#34;: \u0026#34;will not\u0026#34;, \u0026#34;won\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;will not have\u0026#34;, \u0026#34;would\u0026#39;ve\u0026#34;: \u0026#34;would have\u0026#34;, \u0026#34;wouldn\u0026#39;t\u0026#34;: \u0026#34;would not\u0026#34;, \u0026#34;wouldn\u0026#39;t\u0026#39;ve\u0026#34;: \u0026#34;would not have\u0026#34;, \u0026#34;y\u0026#39;all\u0026#34;: \u0026#34;you all\u0026#34;, \u0026#34;y\u0026#39;all\u0026#39;d\u0026#34;: \u0026#34;you all would\u0026#34;,\u0026#34;y\u0026#39;all\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;you all would have\u0026#34;,\u0026#34;y\u0026#39;all\u0026#39;re\u0026#34;: \u0026#34;you all are\u0026#34;,\u0026#34;y\u0026#39;all\u0026#39;ve\u0026#34;: \u0026#34;you all have\u0026#34;,\u0026#34;you\u0026#39;d\u0026#34;: \u0026#34;you would\u0026#34;, \u0026#34;you\u0026#39;d\u0026#39;ve\u0026#34;: \u0026#34;you would have\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#34;: \u0026#34;you will\u0026#34;, \u0026#34;you\u0026#39;ll\u0026#39;ve\u0026#34;: \u0026#34;you will have\u0026#34;, \u0026#34;you\u0026#39;re\u0026#34;: \u0026#34;you are\u0026#34;, \u0026#34;you\u0026#39;ve\u0026#34;: \u0026#34;you have\u0026#34;} def _get_contractions(contraction_dict): contraction_re = re.compile(\u0026#39;(%s)\u0026#39; % \u0026#39;|\u0026#39;.join(contraction_dict.keys())) return contraction_dict, contraction_re contractions, contractions_re = _get_contractions(contraction_dict) def replace_contractions(text): def replace(match): return contractions[match.group(0)] return contractions_re.sub(replace, text) # Usage replace_contractions(\u0026#34;this\u0026#39;s a text with contraction\u0026#34;) Apart from the above techniques, there are other preprocessing techniques of text like Stemming, Lemmatization and Stopword Removal. Since these techniques are not used along with Deep Learning NLP models, we won\u0026rsquo;t talk about them.\nRepresentation: Sequence Creation One of the things that have made Deep Learning the goto choice for NLP is the fact that we don\u0026rsquo;t really have to hand-engineer features from the text data. The deep learning algorithms take as input a sequence of text to learn the structure of text just like a human does. Since Machine cannot understand words they expect their data in numerical form. So we would like to represent out text data as a series of numbers. To understand how this is done we need to understand a little about the Keras Tokenizer function. One can use any other tokenizer also but keras tokenizer seems like a good choice for me.\na) Tokenizer: In simple words, a tokenizer is a utility function to split a sentence into words. keras.preprocessing.text.Tokenizer tokenizes(splits) the texts into tokens(words) while keeping only the most occurring words in the text corpus.\n#Signature: Tokenizer(num_words=None, filters=\u0026#39;!\u0026#34;#$%\u0026amp;()*+,-./:;\u0026lt;=\u0026gt;?@[\\\\]^_`{|}~\\t\\n\u0026#39;, lower=True, split=\u0026#39; \u0026#39;, char_level=False, oov_token=None, document_count=0, **kwargs) The num_words parameter keeps a prespecified number of words in the text only. This is helpful as we don\u0026rsquo;t want our models to get a lot of noise by considering words that occur very infrequently. In real-world data, most of the words we leave using num_words param are normally misspells. The tokenizer also filters some non-wanted tokens by default and converts the text into lowercase.\nThe tokenizer once fitted to the data also keeps an index of words(dictionary of words which we can use to assign a unique number to a word) which can be accessed by tokenizer.word_index. The words in the indexed dictionary are ranked in order of frequencies.\n  So the whole code to use tokenizer is as follows:\nfrom keras.preprocessing.text import Tokenizer ## Tokenize the sentences tokenizer = Tokenizer(num_words=max_features) tokenizer.fit_on_texts(list(train_X)+list(test_X)) train_X = tokenizer.texts_to_sequences(train_X) test_X = tokenizer.texts_to_sequences(test_X) where train_X and test_X are lists of documents in the corpus.\nb) Pad Sequence: Normally our model expects that each sequence(each training example) will be of the same length(same number of words/tokens). We can control this using the maxlen parameter.\nFor example:\n  train_X = pad_sequences(train_X, maxlen=maxlen) test_X = pad_sequences(test_X, maxlen=maxlen) Now our train data contains a list of list of numbers. Each list has the same length. And we also have the word_index which is a dictionary of most occuring words in the text corpus.\nEmbedding Enrichment: As I said I will be using GLoVE Word2Vec embeddings to explain the enrichment. GLoVE pretrained vectors are trained on the Wikipedia corpus. (You can download them here ). That means some of the words that might be present in your data might not be present in the embeddings. How could we deal with that? Let\u0026rsquo;s first load the Glove Embeddings first.\ndef load_glove_index(): EMBEDDING_FILE = \u0026#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt\u0026#39; def get_coefs(word,*arr): return word, np.asarray(arr, dtype=\u0026#39;float32\u0026#39;)[:300] embeddings_index = dict(get_coefs(*o.split(\u0026#34; \u0026#34;)) for o in open(EMBEDDING_FILE)) return embeddings_index glove_embedding_index = load_glove_index() Be sure to put the path of the folder where you download these GLoVE vectors. What does this glove_embedding_index contain? It is just a dictionary in which the key is the word and the value is the word vector, a np.array of length 300. The length of this dictionary is somewhere around a billion. Since we only want the embeddings of words that are in our word_index, we will create a matrix which just contains required embeddings.\n  def create_glove(word_index,embeddings_index): emb_mean,emb_std = -0.005838499,0.48782197 all_embs = np.stack(embeddings_index.values()) embed_size = all_embs.shape[1] nb_words = min(max_features, len(word_index)) embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) count_found = nb_words for word, i in tqdm(word_index.items()): if i \u0026gt;= max_features: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector else: count_found-=1 print(\u0026#34;Got embedding for \u0026#34;,count_found,\u0026#34; words.\u0026#34;) return embedding_matrix The above code works fine but is there a way that we can use the preprocessing in GLoVE to our advantage? Yes. When preprocessing was done for glove, the creators didn\u0026rsquo;t convert the words to lowercase. That means that it contains multiple variations of a word like \u0026lsquo;USA\u0026rsquo;, \u0026lsquo;usa\u0026rsquo; and \u0026lsquo;Usa\u0026rsquo;. That also means that in some cases while a word like \u0026lsquo;Word\u0026rsquo; is present, its analog in lowercase i.e. \u0026lsquo;word\u0026rsquo; is not present. We can get through this situation by using the below code.\ndef create_glove(word_index,embeddings_index): emb_mean,emb_std = -0.005838499,0.48782197 all_embs = np.stack(embeddings_index.values()) embed_size = all_embs.shape[1] nb_words = min(max_features, len(word_index)) embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) count_found = nb_words for word, i in tqdm(word_index.items()): if i \u0026gt;= max_features: continue embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector else: if word.islower(): # try to get the embedding of word in titlecase if lowercase is not present embedding_vector = embeddings_index.get(word.capitalize()) if embedding_vector is not None: embedding_matrix[i] = embedding_vector else: count_found-=1 else: count_found-=1 print(\u0026#34;Got embedding for \u0026#34;,count_found,\u0026#34; words.\u0026#34;) return embedding_matrix The above was just an example of how we can use our knowledge of an embedding to get better coverage. Sometimes depending on the problem, one might also derive value by adding extra information to the embeddings using some domain knowledge and NLP skills. For example, we can add external knowledge to the embeddings themselves by adding polarity and subjectivity of a word from the TextBlob package in Python.\nfrom textblob import TextBlob word_sent = TextBlob(\u0026#34;good\u0026#34;).sentiment print(word_sent.polarity,word_sent.subjectivity) # 0.7 0.6 We can get the polarity and subjectivity of any word using TextBlob. Pretty neat. So let us try to add this extra information to our embeddings.\ndef create_glove(word_index,embeddings_index): emb_mean,emb_std = -0.005838499,0.48782197 all_embs = np.stack(embeddings_index.values()) embed_size = all_embs.shape[1] nb_words = min(max_features, len(word_index)) embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size+4)) count_found = nb_words for word, i in tqdm(word_index.items()): if i \u0026gt;= max_features: continue embedding_vector = embeddings_index.get(word) word_sent = TextBlob(word).sentiment # Extra information we are passing to our embeddings extra_embed = [word_sent.polarity,word_sent.subjectivity] if embedding_vector is not None: embedding_matrix[i] = np.append(embedding_vector,extra_embed) else: if word.islower(): embedding_vector = embeddings_index.get(word.capitalize()) if embedding_vector is not None: embedding_matrix[i] = np.append(embedding_vector,extra_embed) else: embedding_matrix[i,300:] = extra_embed count_found-=1 else: embedding_matrix[i,300:] = extra_embed count_found-=1 print(\u0026#34;Got embedding for \u0026#34;,count_found,\u0026#34; words.\u0026#34;) return embedding_matrix Engineering embeddings is an essential part of getting better performance from the Deep learning models at a later stage. Generally, I revisit this part of code multiple times during the stage of a project while trying to improve my models even further. You can show up a lot of creativity here to improve coverage over your word_index and to include extra features in your embedding.\nMore Engineered Features   One can always add sentence specific features like sentence length, number of unique words etc. as another input layer to give extra information to the Deep Neural Network. For example: I created these extra features as part of a feature engineering pipeline for Quora Insincerity Classification Challenge. def add_features(df): df[\u0026#39;question_text\u0026#39;] = df[\u0026#39;question_text\u0026#39;].progress_apply(lambda x:str(x)) df[\u0026#34;lower_question_text\u0026#34;] = df[\u0026#34;question_text\u0026#34;].apply(lambda x: x.lower()) df[\u0026#39;total_length\u0026#39;] = df[\u0026#39;question_text\u0026#39;].progress_apply(len) df[\u0026#39;capitals\u0026#39;] = df[\u0026#39;question_text\u0026#39;].progress_apply(lambda comment: sum(1 for c in comment if c.isupper())) df[\u0026#39;caps_vs_length\u0026#39;] = df.progress_apply(lambda row: float(row[\u0026#39;capitals\u0026#39;])/float(row[\u0026#39;total_length\u0026#39;]), axis=1) df[\u0026#39;num_words\u0026#39;] = df.question_text.str.count(\u0026#39;\\S+\u0026#39;) df[\u0026#39;num_unique_words\u0026#39;] = df[\u0026#39;question_text\u0026#39;].progress_apply(lambda comment: len(set(w for w in comment.split()))) df[\u0026#39;words_vs_unique\u0026#39;] = df[\u0026#39;num_unique_words\u0026#39;] / df[\u0026#39;num_words\u0026#39;] return df Conclusion: NLP is still a very interesting problem in Deep Learning space and thus I would encourage you to do a lot of experimentation to see what works and what doesn\u0026rsquo;t. I have tried to provide a wholesome perspective of the preprocessing steps for a Deep Learning Neural network for any NLP problem. But that doesn\u0026rsquo;t mean it is definitive. If you want to learn more about NLP here is an awesome course. You can start for free with the 7-day Free Trial. If you think we can add something to the flow, do mention it in the comments.\nEndnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.\n  How to: Preprocessing when using embeddings   Improve your Score with some Text Preprocessing   Pytorch baseline   Pytorch starter   ","permalink":"https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle","Best Content"],"title":"NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning"},{"categories":["Natural Language Processing","Deep Learning","Computer Vision","Awesome Guides"],"contents":"  Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome course on  Natural Language Processing  in the  Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: Sentiment Analysis, summarization, dialogue state tracking, to name a few.\nAlso take a look at my other post: Text Preprocessing Methods for Deep Learning , which talks about different preprocessing techniques you can use for your NLP task and What Kagglers are using for Text Classification , which talks about various deep learning models in use in NLP.\nOk back to the task at hand. While Keras is great to start with deep learning, with time you are going to resent some of its limitations. I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole session.run commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.\nPytorch helps in that since it seems like the python way to do things. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:\nI\u0026#39;ve been using PyTorch a few months now and I\u0026#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.\n\u0026mdash; Andrej Karpathy (@karpathy) May 26, 2017  So without further ado let me translate Keras to Pytorch for you.\nThe Classy way to write your network?   Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch take any network you have and try porting it to Pytorch. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post here .\ndef get_model(features,clipvalue=1.,num_filters=40,dropout=0.1,embed_size=501): features_input = Input(shape=(features.shape[1],)) inp = Input(shape=(maxlen, )) # Layer 1: Word2Vec Embeddings. x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) # Layer 2: SpatialDropout1D(0.1) x = SpatialDropout1D(dropout)(x) # Layer 3: Bidirectional CuDNNLSTM x = Bidirectional(LSTM(num_filters, return_sequences=True))(x) # Layer 4: Bidirectional CuDNNGRU x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x) # Layer 5: some pooling operations avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features x = concatenate([avg_pool, x_h, max_pool,features_input]) # Layer 7: A dense layer x = Dense(16, activation=\u0026#34;relu\u0026#34;)(x) # Layer 8: A dropout layer x = Dropout(0.1)(x) # Layer 9: Output dense layer with one output for our Binary Classification problem. outp = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(x) # Some keras model creation and compiling model = Model(inputs=[inp,features_input], outputs=outp) adam = optimizers.adam(clipvalue=clipvalue) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=adam, metrics=[\u0026#39;accuracy\u0026#39;]) return model So a model in pytorch is defined as a class(therefore a little more classy) which inherits from nn.module . Every class necessarily contains an __init__ procedure block and a block for the forward pass.\n  In the __init__ part the user defines all the layers the network is going to have but doesn\u0026rsquo;t yet define how those layers would be connected to each other\n  In the forward pass block, the user defines how data flows from one layer to another inside the network.\n  Why is this Classy? Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:\n  It gives you a lot of control on how your network is built.\n  You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)\n  Easy to debug networks. Any time you find any problem with the network just use something like print(\u0026quot;avg_pool\u0026quot;, avg_pool.size()) in the forward pass to check the sizes of the layer and you will debug the network easily\n  You can return multiple outputs from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.\n  Pytorch tensors work in a very similar manner to numpy arrays. For example, I could have used Pytorch Maxpool function to write the maxpool layer but max_pool, _ = torch.max(h_gru, 1) will also work.\n  You can set up different layers with different initialization schemes. Something you won\u0026rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.\n  Wait until you see the training loop in Pytorch You will be amazed at the sort of control it provides.\n  Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.\nclass Alex_NeuralNet_Meta(nn.Module): def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix): super(Alex_NeuralNet_Meta, self).__init__() # Initialize some parameters for your model self.hidden_size = hidden_size drp = 0.1 # Layer 1: Word2Vec Embeddings. self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False # Layer 2: Dropout1D(0.1) self.embedding_dropout = nn.Dropout2d(0.1) # Layer 3: Bidirectional CuDNNLSTM self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True) for name, param in self.lstm.named_parameters(): if \u0026#39;bias\u0026#39; in name: nn.init.constant_(param, 0.0) elif \u0026#39;weight_ih\u0026#39; in name: nn.init.kaiming_normal_(param) elif \u0026#39;weight_hh\u0026#39; in name: nn.init.orthogonal_(param) # Layer 4: Bidirectional CuDNNGRU self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True) for name, param in self.gru.named_parameters(): if \u0026#39;bias\u0026#39; in name: nn.init.constant_(param, 0.0) elif \u0026#39;weight_ih\u0026#39; in name: nn.init.kaiming_normal_(param) elif \u0026#39;weight_hh\u0026#39; in name: nn.init.orthogonal_(param) # Layer 7: A dense layer self.linear = nn.Linear(hidden_size*6 + features.shape[1], lin_size) self.relu = nn.ReLU() # Layer 8: A dropout layer self.dropout = nn.Dropout(drp) # Layer 9: Output dense layer with one output for our Binary Classification problem. self.out = nn.Linear(lin_size, 1) def forward(self, x): \u0026#39;\u0026#39;\u0026#39; here x[0] represents the first element of the input that is going to be passed. We are going to pass a tuple where first one contains the sequences(x[0]) and the second one is a additional feature vector(x[1]) \u0026#39;\u0026#39;\u0026#39; h_embedding = self.embedding(x[0]) # Based on comment by Ivank to integrate spatial dropout. embeddings = h_embedding.unsqueeze(2) # (N, T, 1, K) embeddings = embeddings.permute(0, 3, 2, 1) # (N, K, 1, T) embeddings = self.embedding_dropout(embeddings) # (N, K, 1, T), some features are masked embeddings = embeddings.permute(0, 3, 2, 1) # (N, T, 1, K) h_embedding = embeddings.squeeze(2) # (N, T, K) #h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0))) #print(\u0026#34;emb\u0026#34;, h_embedding.size()) h_lstm, _ = self.lstm(h_embedding) #print(\u0026#34;lst\u0026#34;,h_lstm.size()) h_gru, hh_gru = self.gru(h_lstm) hh_gru = hh_gru.view(-1, 2*self.hidden_size ) #print(\u0026#34;gru\u0026#34;, h_gru.size()) #print(\u0026#34;h_gru\u0026#34;, hh_gru.size()) # Layer 5: is defined dynamically as an operation on tensors. avg_pool = torch.mean(h_gru, 1) max_pool, _ = torch.max(h_gru, 1) #print(\u0026#34;avg_pool\u0026#34;, avg_pool.size()) #print(\u0026#34;max_pool\u0026#34;, max_pool.size()) # the extra features you want to give to the model f = torch.tensor(x[1], dtype=torch.float).cuda() #print(\u0026#34;f\u0026#34;, f.size()) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features conc = torch.cat(( hh_gru, avg_pool, max_pool,f), 1) #print(\u0026#34;conc\u0026#34;, conc.size()) # passing conc through linear and relu ops conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) # return the final output return out Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.\nTailored or Readymade: The Best Fit with a highly customizable Training Loop   In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.\nThis training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.\nSo a brief summary of this loop are as follows:\n Create stratified splits using train data Loop through the splits.  Convert your train and CV data to tensor and load your data to the GPU using the X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() command Load the model onto the GPU using the model.cuda() command Define Loss function, Scheduler and Optimizer create train_loader and valid_loader` to iterate through batches. Start running epochs. In each epoch  Set the model mode to train using model.train(). Go through the batches in train_loader and run the forward pass Run a scheduler step to change the learning rate Compute loss Set the existing gradients in the optimizer to zero Backpropagate the losses through the network Clip the gradients Take an optimizer step to change the weights in the whole network Set the model mode to eval using model.eval(). Get predictions for the validation data using valid_loader and store in variable valid_preds_fold Calculate Loss and print   After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final test_preds Get Out-of-fold(OOF) predictions for train set using train_preds[valid_idx] = valid_preds_fold These OOF predictions can then be used to calculate the Local CV score for your model.    def pytorch_model_run_cv(x_train,y_train,features,x_test, model_obj, feats = False,clip = True): seed_everything() avg_losses_f = [] avg_val_losses_f = [] # matrix for the out-of-fold predictions train_preds = np.zeros((len(x_train))) # matrix for the predictions on the test set test_preds = np.zeros((len(x_test))) splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train)) for i, (train_idx, valid_idx) in enumerate(splits): seed_everything(i*1000+i) x_train = np.array(x_train) y_train = np.array(y_train) if feats: features = np.array(features) x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() if feats: kfold_X_features = features[train_idx.astype(int)] kfold_X_valid_features = features[valid_idx.astype(int)] x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda() y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() model = copy.deepcopy(model_obj) model.cuda() loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\u0026#39;sum\u0026#39;) step_size = 300 base_lr, max_lr = 0.001, 0.003 optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr) ################################################################################################ scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode=\u0026#39;exp_range\u0026#39;, gamma=0.99994) ############################################################################################### train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold)) valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold)) train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False) print(f\u0026#39;Fold {i + 1}\u0026#39;) for epoch in range(n_epochs): start_time = time.time() model.train() avg_loss = 0. for i, (x_batch, y_batch, index) in enumerate(train_loader): if feats: f = kfold_X_features[index] y_pred = model([x_batch,f]) else: y_pred = model(x_batch) if scheduler: scheduler.batch_step() # Compute and print loss. loss = loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() if clip: nn.utils.clip_grad_norm_(model.parameters(),1) optimizer.step() avg_loss += loss.item() / len(train_loader) model.eval() valid_preds_fold = np.zeros((x_val_fold.size(0))) test_preds_fold = np.zeros((len(x_test))) avg_val_loss = 0. for i, (x_batch, y_batch,index) in enumerate(valid_loader): if feats: f = kfold_X_valid_features[index] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader) valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0] elapsed_time = time.time() - start_time print(\u0026#39;Epoch {}/{} \\tloss={:.4f} \\tval_loss={:.4f} \\ttime={:.2f}s\u0026#39;.format( epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time)) avg_losses_f.append(avg_loss) avg_val_losses_f.append(avg_val_loss) # predict all samples in the test set batch per batch for i, (x_batch,) in enumerate(test_loader): if feats: f = test_features[i * batch_size:(i+1) * batch_size] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0] train_preds[valid_idx] = valid_preds_fold test_preds += test_preds_fold / len(splits) print(\u0026#39;All \\tloss={:.4f} \\tval_loss={:.4f} \\t\u0026#39;.format(np.average(avg_losses_f),np.average(avg_val_losses_f))) return train_preds, test_preds But Why? Why so much code? Okay. I get it. That was probably a handful. What you could have done with a simple.fit in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:\n While in Keras you have prespecified schedulers like ReduceLROnPlateau (and it is a task to write them), in Pytorch you can experiment like crazy. If you know how to write Python you are going to get along just fine Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly. And much more. It is only your imagination that will stop you.  Wanna Run it Yourself?   So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the seed_everything, MyDataset and CyclicLR (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a Kaggle Kernel with the whole running code. You can see the code here and include it in your projects.\nIf you liked this post, please don\u0026rsquo;t forget to upvote the Kernel too. I will be obliged.\nEndnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.\n  Discussion on 3rd Place winner model in Toxic comment   3rd Place model in Keras by Larry Freeman   Pytorch starter Capsule model   How to: Preprocessing when using embeddings   Improve your Score with some Text Preprocessing   Pytorch baseline   Pytorch starter   ","permalink":"https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Computer Vision","Kaggle","Python","Awesome Guides","Best Content"],"title":"A Layman guide to moving from Keras to Pytorch"},{"categories":["Natural Language Processing","Deep Learning","Awesome Guides"],"contents":"With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning. For those who don\u0026rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. How could you use that?\n To find sentiment of a review. Find toxic comments in a platform like Facebook Find Insincere questions on Quora. A current ongoing competition on kaggle Find fake reviews on websites Will a text advert get clicked or not  And much more. The whole internet is filled with text and to categorise that information algorithmically will only give us incremental benefits to say the least in the field of AI.\nHere I am going to use the data from Quora\u0026rsquo;s Insincere questions to talk about the different models that people are building and sharing to perform this task. Obviously these standalone models are not going to put you on the top of the leaderboard, yet I hope that this ensuing discussion would be helpful for people who want to learn more about text classification. This is going to be a long post in that regard.\nAs a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.\nAlso take a look at my other post: Text Preprocessing Methods for Deep Learning , which talks about different preprocessing techniques you can use for your NLP task and how to switch from Keras to Pytorch .\nSo let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them.\n1. TextCNN: The idea of using a CNN to classify text was first presented in the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim. Instead of image pixels, the input to the tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one word vector. That is, each row is word-vector that represents a word. Thus a sequence of max length 70 gives us a image of 70(max sequence length)x300(embedding size)\n  Now for some intuition. While for a image we move our conv filter horizontally also since here we have fixed our kernel size to filter_size x embed_size i.e. (3,300) we are just going to move down for the convolution taking look at three words at once since our filter size is 3 in this case.Also one can think of filter sizes as unigrams, bigrams, trigrams etc. Since we are looking at a context window of 1,2,3, and 5 words respectively. Here is the text classification network coded in Keras:\n# https://www.kaggle.com/yekenot/2dcnn-textclassifier def model_cnn(embedding_matrix): filter_sizes = [1,2,3,5] num_filters = 36 inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) x = Reshape((maxlen, embed_size, 1))(x) maxpool_pool = [] for i in range(len(filter_sizes)): conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size), kernel_initializer=\u0026#39;he_normal\u0026#39;, activation=\u0026#39;elu\u0026#39;)(x) maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv)) z = Concatenate(axis=1)(maxpool_pool) z = Flatten()(z) z = Dropout(0.1)(z) outp = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(z) model = Model(inputs=inp, outputs=outp) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.661 on the public leaderboard.\n2. BiDirectional RNN(LSTM/GRU): TextCNN takes care of a lot of things. For example it takes care of words in close range. It is able to see \u0026ldquo;new york\u0026rdquo; together. But it still can\u0026rsquo;t take care of all the context provided in a particular text sequence. It still does not learn the seem to learn the sequential structure of the data, where every word is dependednt on the previous word. Or a word in the previous sentence.\nRNN help us with that. They are able to remember previous information using hidden states and connect it to the current task.\nLong Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time. More over the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won\u0026rsquo;t work for a time sweries prediction task).\n  For a most simplistic explanation of Bidirectional RNN, think of RNN cell as taking as input a hidden state(a vector) and the word vector and giving out an output vector and the next hidden state.\n Hidden state, Word vector -\u0026gt;(RNN Cell) -\u0026gt; Output Vector , Next Hidden state  For a sequence of length 4 like \u0026lsquo;you will never believe\u0026rsquo;, The RNN cell will give 4 output vectors. Which can be concatenated and then used as part of a dense feedforward architecture.\nIn the Bidirectional RNN the only change is that we read the text in the normal fashion as well in reverse. So we stack two RNNs in parallel and hence we get 8 output vectors to append.\nOnce we get the output vectors we send them through a series of dense layers and finally a softmax layer to build a text classifier.\nDue to the limitations of RNNs like not remembering long term dependencies, in practice we almost always use LSTM/GRU to model long term dependencies. In such a case you can just think of the RNN cell being replaced by a LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangably with CuDNNLSTM, when you build models.\n# BiDirectional LSTM def model_lstm_du(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) \u0026#39;\u0026#39;\u0026#39; Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer? 64*70(maxlen)*2(bidirection concat) CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU \u0026#39;\u0026#39;\u0026#39; x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) conc = concatenate([avg_pool, max_pool]) conc = Dense(64, activation=\u0026#34;relu\u0026#34;)(conc) conc = Dropout(0.1)(conc) outp = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(conc) model = Model(inputs=inp, outputs=outp) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.671 on the public leaderboard.\n3. Attention Models The concept of Attention is relatively new as it comes from Hierarchical Attention Networks for Document Classification paper written jointly by CMU and Microsoft guys in 2016.\nSo in the past we used to find features from text by doing a keyword extraction. Some word are more helpful in determining the category of a text than others. But in this method we sort of lost the sequential structure of text. With LSTM and deep learning methods while we are able to take case of the sequence structure we lose the ability to give higher weightage to more important words. Can we have the best of both worlds?\nAnd that is attention for you. In the author\u0026rsquo;s words:\n Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector\n   In essense we want to create scores for every word in the text, which are the attention similarity score for a word.\nTo do this we start with a weight matrix(W), a bias vector(b) and a context vector u. All of them will be learned by the optimmization algorithm.\nThen there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as non linearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to an exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v‚Äôs to get the Final Scores,s\nThese final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.\ndef dot_product(x, kernel): \u0026#34;\u0026#34;\u0026#34; Wrapper for dot product operation, in order to be compatible with both Theano and Tensorflow Args: x (): input kernel (): weights Returns: \u0026#34;\u0026#34;\u0026#34; if K.backend() == \u0026#39;tensorflow\u0026#39;: return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1) else: return K.dot(x, kernel) class AttentionWithContext(Layer): \u0026#34;\u0026#34;\u0026#34; Attention operation, with a context/query vector, for temporal data. Supports Masking. Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] \u0026#34;Hierarchical Attention Networks for Document Classification\u0026#34; by using a context vector to assist the attention # Input shape 3D tensor with shape: `(samples, steps, features)`. # Output shape 2D tensor with shape: `(samples, features)`. How to use: Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. The dimensions are inferred based on the output shape of the RNN. Note: The layer has been tested with Keras 2.0.6 Example: model.add(LSTM(64, return_sequences=True)) model.add(AttentionWithContext()) # next add a Dense layer (for classification/regression) or whatever... \u0026#34;\u0026#34;\u0026#34; def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None, W_constraint=None, u_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get(\u0026#39;glorot_uniform\u0026#39;) self.W_regularizer = regularizers.get(W_regularizer) self.u_regularizer = regularizers.get(u_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.u_constraint = constraints.get(u_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias super(AttentionWithContext, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1], input_shape[-1],), initializer=self.init, name=\u0026#39;{}_W\u0026#39;.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) if self.bias: self.b = self.add_weight((input_shape[-1],), initializer=\u0026#39;zero\u0026#39;, name=\u0026#39;{}_b\u0026#39;.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) self.u = self.add_weight((input_shape[-1],), initializer=self.init, name=\u0026#39;{}_u\u0026#39;.format(self.name), regularizer=self.u_regularizer, constraint=self.u_constraint) super(AttentionWithContext, self).build(input_shape) def compute_mask(self, input, input_mask=None): # do not pass the mask to the next layers return None def call(self, x, mask=None): uit = dot_product(x, self.W) if self.bias: uit += self.b uit = K.tanh(uit) ait = dot_product(uit, self.u) a = K.exp(ait) # apply mask after the exp. will be re-normalized next if mask is not None: # Cast the mask to floatX to avoid float64 upcasting in theano a *= K.cast(mask, K.floatx()) # in some cases especially in the early stages of training the sum may be almost zero # and this results in NaN\u0026#39;s. A workaround is to add a very small positive number Œµ to the sum. # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], input_shape[-1] def model_lstm_atten(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x) x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) x = AttentionWithContext()(x) x = Dense(64, activation=\u0026#34;relu\u0026#34;)(x) x = Dense(1, activation=\u0026#34;sigmoid\u0026#34;)(x) model = Model(inputs=inp, outputs=x) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.682 on the public leaderboard.\nHope that Helps! Do checkout the kernels for all the networks and see the comments too. I will try to write a part 2 of this post where I would like to talk about capsule networks and more techniques as they get used in this competition.\nHere are the kernel links again: TextCNN , BiLSTM/GRU , Attention Do upvote the kenels if you find them helpful.\nReferences:   CNN for NLP  https://en.diveintodeeplearning.org/d2l-en.pdf https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 http://univagora.ro/jour/index.php/ijccc/article/view/3142  Shujian\u0026amp;rsquo;s kernel on Kaggle   ","permalink":"https://mlwhiz.com/blog/2018/12/17/text_classification/","tags":["Natural Language Processing","Deep Learning","Artificial Intelligence","Kaggle"],"title":"What Kagglers are using for Text Classification"},{"categories":["Data Science","Big Data"],"contents":"Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that.\nHere is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory. You can start for free with the 7-day Free Trial.\nOne of the algorithms I am going to focus in the current post is called Connected Components. Why it is important. We all know clustering.\nYou can think of Connected Components in very layman\u0026rsquo;s terms as sort of a hard clustering algorithm which finds clusters/islands in related/connected data. As a concrete example: Say you have data about roads joining any two cities in the world. And you need to find out all the continents in the world and which city they contain.\nHow will you achieve that? Come on give some thought.\nTo put a Retail Perspective: Lets say, we have a lot of customers using a lot of accounts. One way in which we can use the Connected components algorithm is to find out distinct families in our dataset. We can assume edges(roads) between CustomerIDs based on same credit card usage, or same address or same mobile number etc. Once we have those connections, we can then run the connected component algorithm on the same to create individual clusters to which we can then assign a family ID. We can use these family IDs to provide personalized recommendations based on a family needs. We can also use this family ID to fuel our classification algorithms by creating grouped features based on family.\nIn Finance Perspective: Another use case would be to capture fraud using these family IDs. If an account has done fraud in past, it is highly probable that the connected accounts are also susceptible to fraud.\nSo enough of use cases. Lets start with a simple graph class written in Python to start up our exploits with code.\nThis post will revolve more around code from here onwards.\n\u0026#34;\u0026#34;\u0026#34; A Python Class A simple Python graph class, demonstrating the essential facts and functionalities of graphs. Taken from https://www.python-course.eu/graphs_python.php Changed the implementation a little bit to include weighted edges \u0026#34;\u0026#34;\u0026#34; class Graph(object): def __init__(self, graph_dict=None): \u0026#34;\u0026#34;\u0026#34; initializes a graph object If no dictionary or None is given, an empty dictionary will be used \u0026#34;\u0026#34;\u0026#34; if graph_dict == None: graph_dict = {} self.__graph_dict = graph_dict def vertices(self): \u0026#34;\u0026#34;\u0026#34; returns the vertices of a graph \u0026#34;\u0026#34;\u0026#34; return list(self.__graph_dict.keys()) def edges(self): \u0026#34;\u0026#34;\u0026#34; returns the edges of a graph \u0026#34;\u0026#34;\u0026#34; return self.__generate_edges() def add_vertex(self, vertex): \u0026#34;\u0026#34;\u0026#34; If the vertex \u0026#34;vertex\u0026#34; is not in self.__graph_dict, a key \u0026#34;vertex\u0026#34; with an empty dict as a value is added to the dictionary. Otherwise nothing has to be done. \u0026#34;\u0026#34;\u0026#34; if vertex not in self.__graph_dict: self.__graph_dict[vertex] = {} def add_edge(self, edge,weight=1): \u0026#34;\u0026#34;\u0026#34; assumes that edge is of type set, tuple or list \u0026#34;\u0026#34;\u0026#34; edge = set(edge) (vertex1, vertex2) = tuple(edge) if vertex1 in self.__graph_dict: self.__graph_dict[vertex1][vertex2] = weight else: self.__graph_dict[vertex1] = {vertex2:weight} if vertex2 in self.__graph_dict: self.__graph_dict[vertex2][vertex1] = weight else: self.__graph_dict[vertex2] = {vertex1:weight} def __generate_edges(self): \u0026#34;\u0026#34;\u0026#34; A static method generating the edges of the graph \u0026#34;graph\u0026#34;. Edges are represented as sets with one (a loop back to the vertex) or two vertices \u0026#34;\u0026#34;\u0026#34; edges = [] for vertex in self.__graph_dict: for neighbour,weight in self.__graph_dict[vertex].iteritems(): if (neighbour, vertex, weight) not in edges: edges.append([vertex, neighbour, weight]) return edges def __str__(self): res = \u0026#34;vertices: \u0026#34; for k in self.__graph_dict: res += str(k) + \u0026#34; \u0026#34; res += \u0026#34;\\nedges: \u0026#34; for edge in self.__generate_edges(): res += str(edge) + \u0026#34; \u0026#34; return res def adj_mat(self): return self.__graph_dict You can certainly play with our new graph class.Here we try to build some graphs.\ng = { \u0026#34;a\u0026#34; : {\u0026#34;d\u0026#34;:2}, \u0026#34;b\u0026#34; : {\u0026#34;c\u0026#34;:2}, \u0026#34;c\u0026#34; : {\u0026#34;b\u0026#34;:5, \u0026#34;d\u0026#34;:3, \u0026#34;e\u0026#34;:5} } graph = Graph(g) print(\u0026#34;Vertices of graph:\u0026#34;) print(graph.vertices()) print(\u0026#34;Edges of graph:\u0026#34;) print(graph.edges()) print(\u0026#34;Add vertex:\u0026#34;) graph.add_vertex(\u0026#34;z\u0026#34;) print(\u0026#34;Vertices of graph:\u0026#34;) print(graph.vertices()) print(\u0026#34;Add an edge:\u0026#34;) graph.add_edge({\u0026#34;a\u0026#34;,\u0026#34;z\u0026#34;}) print(\u0026#34;Vertices of graph:\u0026#34;) print(graph.vertices()) print(\u0026#34;Edges of graph:\u0026#34;) print(graph.edges()) print(\u0026#39;Adding an edge {\u0026#34;x\u0026#34;,\u0026#34;y\u0026#34;} with new vertices:\u0026#39;) graph.add_edge({\u0026#34;x\u0026#34;,\u0026#34;y\u0026#34;}) print(\u0026#34;Vertices of graph:\u0026#34;) print(graph.vertices()) print(\u0026#34;Edges of graph:\u0026#34;) print(graph.edges()) Vertices of graph: ['a', 'c', 'b'] Edges of graph: [['a', 'd', 2], ['c', 'b', 5], ['c', 'e', 5], ['c', 'd', 3], ['b', 'c', 2]] Add vertex: Vertices of graph: ['a', 'c', 'b', 'z'] Add an edge: Vertices of graph: ['a', 'c', 'b', 'z'] Edges of graph: [['a', 'z', 1], ['a', 'd', 2], ['c', 'b', 5], ['c', 'e', 5], ['c', 'd', 3], ['b', 'c', 2], ['z', 'a', 1]] Adding an edge {\"x\",\"y\"} with new vertices: Vertices of graph: ['a', 'c', 'b', 'y', 'x', 'z'] Edges of graph: [['a', 'z', 1], ['a', 'd', 2], ['c', 'b', 5], ['c', 'e', 5], ['c', 'd', 3], ['b', 'c', 2], ['y', 'x', 1], ['x', 'y', 1], ['z', 'a', 1]]  Lets do something interesting now.\nWe will use the above graph class for our understanding purpose. There are many Modules in python which we can use to do whatever I am going to do next,but to understand the methods we will write everything from scratch. Lets start with an example graph which we can use for our purpose.\n  g = {\u0026#39;Frankfurt\u0026#39;: {\u0026#39;Mannheim\u0026#39;:85, \u0026#39;Wurzburg\u0026#39;:217, \u0026#39;Kassel\u0026#39;:173}, \u0026#39;Mannheim\u0026#39;: {\u0026#39;Frankfurt\u0026#39;:85, \u0026#39;Karlsruhe\u0026#39;:80}, \u0026#39;Karlsruhe\u0026#39;: {\u0026#39;Augsburg\u0026#39;:250, \u0026#39;Mannheim\u0026#39;:80}, \u0026#39;Augsburg\u0026#39;: {\u0026#39;Karlsruhe\u0026#39;:250, \u0026#39;Munchen\u0026#39;:84}, \u0026#39;Wurzburg\u0026#39;: {\u0026#39;Erfurt\u0026#39;:186, \u0026#39;Numberg\u0026#39;:103,\u0026#39;Frankfurt\u0026#39;:217}, \u0026#39;Erfurt\u0026#39;: {\u0026#39;Wurzburg\u0026#39;:186}, \u0026#39;Numberg\u0026#39;: {\u0026#39;Wurzburg\u0026#39;:103, \u0026#39;Stuttgart\u0026#39;:183,\u0026#39;Munchen\u0026#39;:167}, \u0026#39;Munchen\u0026#39;: {\u0026#39;Numberg\u0026#39;:167, \u0026#39;Augsburg\u0026#39;:84,\u0026#39;Kassel\u0026#39;:502}, \u0026#39;Kassel\u0026#39;: {\u0026#39;Frankfurt\u0026#39;:173, \u0026#39;Munchen\u0026#39;:502}, \u0026#39;Stuttgart\u0026#39;: {\u0026#39;Numberg\u0026#39;:183} } graph = Graph(g) print(\u0026#34;Vertices of graph:\u0026#34;) print(graph.vertices()) print(\u0026#34;Edges of graph:\u0026#34;) print(graph.edges()) Vertices of graph: ['Mannheim', 'Erfurt', 'Munchen', 'Numberg', 'Stuttgart', 'Augsburg', 'Kassel', 'Frankfurt', 'Wurzburg', 'Karlsruhe'] Edges of graph: [['Mannheim', 'Frankfurt', 85], ['Mannheim', 'Karlsruhe', 80], ['Erfurt', 'Wurzburg', 186], ['Munchen', 'Numberg', 167], ['Munchen', 'Augsburg', 84], ['Munchen', 'Kassel', 502], ['Numberg', 'Stuttgart', 183], ['Numberg', 'Wurzburg', 103], ['Numberg', 'Munchen', 167], ['Stuttgart', 'Numberg', 183], ['Augsburg', 'Munchen', 84], ['Augsburg', 'Karlsruhe', 250], ['Kassel', 'Munchen', 502], ['Kassel', 'Frankfurt', 173], ['Frankfurt', 'Mannheim', 85], ['Frankfurt', 'Wurzburg', 217], ['Frankfurt', 'Kassel', 173], ['Wurzburg', 'Numberg', 103], ['Wurzburg', 'Erfurt', 186], ['Wurzburg', 'Frankfurt', 217], ['Karlsruhe', 'Mannheim', 80], ['Karlsruhe', 'Augsburg', 250]]  Lets say we are given a graph with the cities of Germany and respective distance between them. You want to find out how to go from Frankfurt (The starting node) to Munchen. There might be many ways in which you can traverse the graph but you need to find how many cities you will need to visit on a minimum to go from frankfurt to Munchen) This problem is analogous to finding out distance between nodes in an unweighted graph.\nThe algorithm that we use here is called as Breadth First Search.\ndef min_num_edges_between_nodes(graph,start_node): distance = 0 shortest_path = [] queue = [start_node] #FIFO levels = {} levels[start_node] = 0 shortest_paths = {} shortest_paths[start_node] = \u0026#34;:\u0026#34; visited = [start_node] while len(queue)!=0: start = queue.pop(0) neighbours = graph[start] for neighbour,_ in neighbours.iteritems(): if neighbour not in visited: queue.append(neighbour) visited.append(neighbour) levels[neighbour] = levels[start]+1 shortest_paths[neighbour] = shortest_paths[start] +\u0026#34;-\u0026gt;\u0026#34;+ start return levels, shortest_paths What we do in the above piece of code is create a queue and traverse it based on levels. We start with Frankfurt as starting node. We loop through its neighbouring cities(Menheim, Wurzburg and Kassel) and push them into the queue. We keep track of what level they are at and also the path through which we reached them. Since we are popping a first element of a queue we are sure we will visit cities in the order of their level.\nCheckout this good post about BFS to understand more about queues and BFS.\nmin_num_edges_between_nodes(g,\u0026#39;Frankfurt\u0026#39;) ({'Augsburg': 3, 'Erfurt': 2, 'Frankfurt': 0, 'Karlsruhe': 2, 'Kassel': 1, 'Mannheim': 1, 'Munchen': 2, 'Numberg': 2, 'Stuttgart': 3, 'Wurzburg': 1}, {'Augsburg': ':-Frankfurt-Mannheim-Karlsruhe', 'Erfurt': ':-Frankfurt-Wurzburg', 'Frankfurt': ':', 'Karlsruhe': ':-Frankfurt-Mannheim', 'Kassel': ':-Frankfurt', 'Mannheim': ':-Frankfurt', 'Munchen': ':-Frankfurt-Kassel', 'Numberg': ':-Frankfurt-Wurzburg', 'Stuttgart': ':-Frankfurt-Wurzburg-Numberg', 'Wurzburg': ':-Frankfurt'})  I did this example to show how BFS algorithm works. We can extend this algorithm to find out connected components in an unconnected graph. Lets say we need to find groups of unconnected vertices in the graph.\nFor example: the below graph has 3 unconnected sub-graphs. Can we find what nodes belong to a particular subgraph?\n  #We add another countries in the loop graph = Graph(g) graph.add_edge((\u0026#34;Mumbai\u0026#34;, \u0026#34;Delhi\u0026#34;),400) graph.add_edge((\u0026#34;Delhi\u0026#34;, \u0026#34;Kolkata\u0026#34;),500) graph.add_edge((\u0026#34;Kolkata\u0026#34;, \u0026#34;Bangalore\u0026#34;),600) graph.add_edge((\u0026#34;TX\u0026#34;, \u0026#34;NY\u0026#34;),1200) graph.add_edge((\u0026#34;ALB\u0026#34;, \u0026#34;NY\u0026#34;),800) g = graph.adj_mat() def bfs_connected_components(graph): connected_components = [] nodes = graph.keys() while len(nodes)!=0: start_node = nodes.pop() queue = [start_node] #FIFO visited = [start_node] while len(queue)!=0: start = queue[0] queue.remove(start) neighbours = graph[start] for neighbour,_ in neighbours.iteritems(): if neighbour not in visited: queue.append(neighbour) visited.append(neighbour) nodes.remove(neighbour) connected_components.append(visited) return connected_components print bfs_connected_components(g) The above code is similar to the previous BFS code. We keep all the vertices of the graph in the nodes list. We take a node from the nodes list and start BFS on it. as we visit a node we remove that node from the nodes list. Whenever the BFS completes we start again with another node in the nodes list until the nodes list is empty.\n[['Kassel', 'Munchen', 'Frankfurt', 'Numberg', 'Augsburg', 'Mannheim', 'Wurzburg', 'Stuttgart', 'Karlsruhe', 'Erfurt'], ['Bangalore', 'Kolkata', 'Delhi', 'Mumbai'], ['NY', 'ALB', 'TX']]  As you can see we are able to find distinct components in our data. Just by using Edges and Vertices. This algorithm could be run on different data to satisfy any use case I presented above.\nBut Normally using Connected Components for a retail case will involve a lot of data and you will need to scale this algorithm.\nConnected Components in PySpark Below is an implementation from this paper on Connected Components in MapReduce and Beyond from Google Research. Read the PPT to understand the implementation better. Some ready to use code for you.\ndef create_edges(line): a = [int(x) for x in line.split(\u0026#34; \u0026#34;)] edges_list=[] for i in range(0, len(a)-1): for j in range(i+1 ,len(a)): edges_list.append((a[i],a[j])) edges_list.append((a[j],a[i])) return edges_list # adj_list.txt is a txt file containing adjacency list of the graph. adjacency_list = sc.textFile(\u0026#34;adj_list.txt\u0026#34;) edges_rdd = adjacency_list.flatMap(lambda line : create_edges(line)).distinct() def largeStarInit(record): a, b = record yield (a,b) yield (b,a) def largeStar(record): a, b = record t_list = list(b) t_list.append(a) list_min = min(t_list) for x in b: if a \u0026lt; x: yield (x,list_min) def smallStarInit(record): a, b = record if b\u0026lt;=a: yield (a,b) else: yield (b,a) def smallStar(record): a, b = record t_list = list(b) t_list.append(a) list_min = min(t_list) for x in t_list: if x!=list_min: yield (x,list_min) #Handle case for single nodes def single_vertex(line): a = [int(x) for x in line.split(\u0026#34; \u0026#34;)] edges_list=[] if len(a)==1: edges_list.append((a[0],a[0])) return edges_list iteration_num =0 while 1==1: if iteration_num==0: print \u0026#34;iter\u0026#34;, iteration_num large_star_rdd = edges_rdd.groupByKey().flatMap(lambda x : largeStar(x)) small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct() iteration_num += 1 else: print \u0026#34;iter\u0026#34;, iteration_num large_star_rdd = small_star_rdd.flatMap(lambda x: largeStarInit(x)).groupByKey().flatMap(lambda x : largeStar(x)).distinct() small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct() iteration_num += 1 #check Convergence changes = (large_star_rdd.subtract(small_star_rdd).union(small_star_rdd.subtract(large_star_rdd))).collect() if len(changes) == 0 : break single_vertex_rdd = adjacency_list.flatMap(lambda line : single_vertex(line)).distinct() answer = single_vertex_rdd.collect() + large_star_rdd.collect() print answer[:10] Or Use GraphFrames in PySpark To Install graphframes:\nI ran on command line: pyspark \u0026ndash;packages graphframes:graphframes:0.5.0-spark2.1-s_2.11 which opened up my notebook and installed graphframes after i try to import in my notebook.\nThe string to be formatted as : graphframes:(latest version)-spark(your spark version)-s_(your scala version).\nCheckout this guide on how to use GraphFrames for more information.\nfrom graphframes import * def vertices(line): vert = [int(x) for x in line.split(\u0026#34; \u0026#34;)] return vert vertices = adjacency_list.flatMap(lambda x: vertices(x)).distinct().collect() vertices = sqlContext.createDataFrame([[x] for x in vertices], [\u0026#34;id\u0026#34;]) def create_edges(line): a = [int(x) for x in line.split(\u0026#34; \u0026#34;)] edges_list=[] if len(a)==1: edges_list.append((a[0],a[0])) for i in range(0, len(a)-1): for j in range(i+1 ,len(a)): edges_list.append((a[i],a[j])) edges_list.append((a[j],a[i])) return edges_list edges = adjacency_list.flatMap(lambda x: create_edges(x)).distinct().collect() edges = sqlContext.createDataFrame(edges, [\u0026#34;src\u0026#34;, \u0026#34;dst\u0026#34;]) g = GraphFrame(vertices, edges) sc.setCheckpointDir(\u0026#34;.\u0026#34;) # graphframes uses the same paper we referenced apparently cc = g.connectedComponents() print cc.show() The GraphFrames library implements the CC algorithm as well as a variety of other graph algorithms.\nThe above post was a lot of code but hope it was helpful. It took me a lot of time to implement the algorithm so wanted to make it easy for the folks.\nIf you want to read up more on Graph Algorithms here is an Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.\nReferences   Graphs in Python   A Gentle Intoduction to Graph Theory Blog   Graph Analytics for Big Data course on Coursera by UCSanDiego   ","permalink":"https://mlwhiz.com/blog/2018/12/07/connected_components/","tags":["Data Science","Graphs","Machine Learning","Big Data"],"title":"To all Data Scientists - The one Graph Algorithm you need to know"},{"categories":["Deep Learning","Computer Vision"],"contents":"We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets. ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer's outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.  As a side note: if you want to know more about convnets and Transfer Learning I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization . You can start for free with the 7-day Free Trial. This course talks about various CNN architetures and covers a wide variety of problems in the image domain including detection and segmentation.\nBut there are a lot many interesting problems in the Image domain. The one which we are going to focus on today is the Segmentation, Localization and Detection problem. So what are these problems?\n  So these problems are divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:\n Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class? Classification+Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number(commonly 1) in the image. Object Detection: A More general case of the Classification+Localization problem. In a real-world setting, we don\u0026rsquo;t know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them? Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won\u0026rsquo;t be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.  In this post, we will focus mainly on Object Detection.\nClassification+Localization So lets first try to understand how we can solve the problem when we have a single object in the image. The Classification+Localization case. Pretty neatly said in the CS231n notes:\nTreat localization as a regression problem!    Input Data: Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting we used to have data in the form (X,y) where X is the image and y used to be the class labels. In the Classification+Localization setting we will have data normally in the form (X,y), where X is still the image and y is a array containing (class_label, x,y,w,h) where,\nx = bounding box top left corner x-coordinate\ny = bounding box top left corner y-coordinate\nw = width of bounding box in pixel\nh = height of bounding box in pixel\nModel: So in this setting we create a multi-output model which takes an image as the input and has (n_labels + 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).\nLoss: In such a setting setting up the loss is pretty important. Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).\n$$Loss = alpha*SoftmaxLoss + (1-alpha)*L2Loss$$\nSince these two losses would be on a different scale, the alpha hyper-parameter needs to be tuned.\nThere is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function. And here in lies the essence of the whole Deep Learning framework - Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem. And that is what we are going to see a lot going forward.\nObject Detection So how does this idea of localization using regression get mapped to Object Detection? It doesn\u0026rsquo;t. We don\u0026rsquo;t have a fixed number of objects. So we can\u0026rsquo;t have 4 outputs denoting, the bounding box coordinates.\nOne naive idea could be to apply a CNN to many different crops of the image, CNN classifies each crop as object class or background class. This is intractable. There could be a lot of such crops that you can create.\nRegion Proposals: If just there was a method(Normally called Region Proposal Network)which could find some cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is what selective search (Uijlings et al, \u0026ldquo; Selective Search for Object Recognition \u0026rdquo;, IJCV 2013) provided for RCNN.\nSo what are Region Proposals:\n Find \u0026ldquo;blobby\u0026rdquo; image regions that are likely to contain objects Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU  How the region proposals are being made?\nSelective Search for Object Recognition: So this paper starts with a set of some initial regions using [13] (P. F. Felzenszwalb and D. P. Huttenlocher. Efficient GraphBased Image Segmentation . IJCV, 59:167‚Äì181, 2004. 1, 3, 4, 5, 7) Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ‚àà V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels. A weight is associated with each edge based on some property of the pixels that it connects, such as their image intensities. Depending on the method, there may or may not be an edge connecting each pair of vertices.  In this paper they take an approach: Each edge (vi¬†, vj )‚àà E has a corresponding weight w((vi¬†, vj )), which is a non-negative measure of the dissimilarity between neighboring elements vi and vj¬†. In the case of image segmentation, the elements in V are pixels and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute). In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ‚àà S corresponds to a connected component in a graph.    As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. For that the authors of Selective Search for Object Recognition apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill.\n    RCNN The above selective search is the region proposal they used in RCNN paper. But what is RCNN and how does it use region proposals?\n  Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.  Along with this, the authors have also used a class specific bounding box regressor, that takes: Input¬†: (Px,Py,Ph,Pw)‚Ää-‚Ääthe location of the proposed region. Target: (Gx,Gy,Gh,Gw)‚Ää-‚ÄäGround truth labels for the region. The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G) Training RCNN What is the input to an RCNN? So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes) Next we treat all region proposals with ‚â• 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box\u0026rsquo;s class and the rest as negative. We train class specific SVM\u0026rsquo;s\nSo every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class specific data.\nTest Time¬†RCNN At test time we predict detection boxes using class specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes\n  Problems with¬†RCNN: Training is slow. Inference (detection) is slow. 47s / image with VGG16‚Ää-‚ÄäSince the Convnet needs to be run many times.\nNeed for speed. Hence comes in picture by the same authors:\nFast RCNN So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM to classify those¬†crops.  Something like:   From Paper: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all \"background\" class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.  This idea depends a little upon the architecture of the model that get used too. Do we take the 4096 bottleneck layer from VGG16? So the architecture that the authors have proposed is: We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net's first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network's last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.  This obviously is a little confusing and \"hairy\", let us break this down. But for that, we need to see the VGG16 architecture.   The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).\n  Now the location of ROI coordinates are in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512. As we are using VGG we will transform image (224 x 224 x 3) into (14 x 14 x 512)‚Ää-‚Ääheight and width is divided by 16. we can map ROIs coordinates onto the feature map just by dividing them by 16.\nIn its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the \"things\" it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature¬†map.  How the ROI pooling is done?   In the above image our region proposal is (0,3,5,7) and we divide that area into 4 regions since we want to have a ROI pooling layer of 2x2.\n How do you do ROI-Pooling on Areas smaller than the target size? if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.\nAfter replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K + 1 categories(+1 for Background) and category-specific bounding-box regressors.\nTraining Fast-RCNN What is the input to an Fast- RCNN?\nPretty much similar: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)\nNext we treat all region proposals with ‚â• 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box\u0026rsquo;s class and the rest as negative. This time we have a dense layer on top, and we use multi task loss.\nSo every ROI becomes a training example. The main difference is that there is concept of multi-task loss:\nA Fast R-CNN network has two sibling output layers. The first outputs a discrete probability distribution (per RoI), p = (p0,¬†.¬†.¬†.¬†, pK), over K + 1 categories. As usual, p is computed by a softmax over the K+1 outputs of a fully connected layer. The second sibling layer outputs bounding-box regression offsets, t= (tx¬†, ty¬†, tw, th), for each of the K object classes. Each training RoI is labeled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labeled RoI to jointly train for classification and bounding-box regression\n  Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class. Further:\n  Problem:   Faster-RCNN The next question that got asked was¬†: Can the network itself do region proposals?\nThe intuition is that: With FastRCNN we're already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes + bbox coordinates?    How does the Region Proposal Network¬†work? One of the main idea in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.\nSo first of all we define anchor centers on the image.\n  The anchor centers are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224/14). This is how anchors look like:\n   So we start with some predefined regions we think our objects could be with Anchors. Our RPN Classifies which regions have the object and the offset of the object bounding box. 1 if IOU for anchor with bounding box\u0026gt;0.5 0 otherwise. Non-Maximum suppression to reduce region proposals Fast RCNN detection network on top of proposals  Faster-RCNN Loss:  The whole network is then jointly trained with 4 losses:\n RPN classify object / not object RPN regress box coordinates offset Final classification score (object classes) Final box coordinates offset  Results:   Disclaimer: This is my own understanding of these papers with inputs from many blogs and slides on the internet. Let me know if you find something wrong with my understanding. I will be sure to correct myself and post.\nReferences:   Transfer Learning   CS231 Object detection Lecture Slides   Efficient Graph-Based Image Segmentation   Rich feature hierarchies for accurate object detection and semantic segmentation(RCNN Paper)   Selective Search for Object Recognition   ROI Pooling Explanation   Faster RCNN Blog   StackOverflow   Faster RCNN Blog   Faster RCNN Blog   Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks   https://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection   ","permalink":"https://mlwhiz.com/blog/2018/09/22/object_detection/","tags":["Deep Learning","Artificial Intelligence","Computer Vision","Object Detection","Math"],"title":"Object Detection: An End to End Theoretical Perspective"},{"categories":["Data Science"],"contents":"Recently I was working on a in-class competition from the \u0026amp;ldquo;How to win a data science competition\u0026amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using XGBoost for time series prediction tasks.\nThe one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed. Left the machine with hyperopt in the night. And in the morning I had my results. It was really awesome and I did avoid a lot of hit and trial.\nWhat really is Hyperopt? From the site:\n Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.\n What the above means is that it is a optimizer that could minimize/maximize the loss function/accuracy(or whatever metric) for you.\nAll of us are fairly known to cross-grid search or random-grid search. Hyperopt takes as an input a space of hyperparams in which it will search, and moves according to the result of past trials.\nTo know more about how it does this, take a look at this paper by J Bergstra. Here is the documentation from github.\nHow? Let me just put the code first. This is how I define the objective function. The objective function takes space(the hyperparam space) as the input and returns the loss(The thing you want to minimize.Or negative of the thing you want to maximize)\n(X,y) and (Xcv,ycv) are the train and cross validation dataframes respectively.\nWe have defined a hyperparam space by using the variable space which is actually just a dictionary. We could choose different distributions for different parameter values.\nWe use the fmin function from the hyperopt package to minimize our fn through the space.\nfrom sklearn.metrics import mean_squared_error import xgboost as xgb from hyperopt import hp, fmin, tpe, STATUS_OK, Trials import numpy as np def objective(space): print(space) clf = xgb.XGBRegressor(n_estimators =1000,colsample_bytree=space[\u0026#39;colsample_bytree\u0026#39;], learning_rate = .3, max_depth = int(space[\u0026#39;max_depth\u0026#39;]), min_child_weight = space[\u0026#39;min_child_weight\u0026#39;], subsample = space[\u0026#39;subsample\u0026#39;], gamma = space[\u0026#39;gamma\u0026#39;], reg_lambda = space[\u0026#39;reg_lambda\u0026#39;],) eval_set = [( X, y), ( Xcv, ycv)] clf.fit(X, y, eval_set=eval_set, eval_metric=\u0026#34;rmse\u0026#34;, early_stopping_rounds=10,verbose=False) pred = clf.predict(Xcv) mse_scr = mean_squared_error(ycv, pred) print \u0026#34;SCORE:\u0026#34;, np.sqrt(mse_scr) #change the metric if you like return {\u0026#39;loss\u0026#39;:mse_scr, \u0026#39;status\u0026#39;: STATUS_OK } space ={\u0026#39;max_depth\u0026#39;: hp.quniform(\u0026#34;x_max_depth\u0026#34;, 4, 16, 1), \u0026#39;min_child_weight\u0026#39;: hp.quniform (\u0026#39;x_min_child\u0026#39;, 1, 10, 1), \u0026#39;subsample\u0026#39;: hp.uniform (\u0026#39;x_subsample\u0026#39;, 0.7, 1), \u0026#39;gamma\u0026#39; : hp.uniform (\u0026#39;x_gamma\u0026#39;, 0.1,0.5), \u0026#39;colsample_bytree\u0026#39; : hp.uniform (\u0026#39;x_colsample_bytree\u0026#39;, 0.7,1), \u0026#39;reg_lambda\u0026#39; : hp.uniform (\u0026#39;x_reg_lambda\u0026#39;, 0,1) } trials = Trials() best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials) print best Finally: Running the above gives us pretty good hyperparams for our learning algorithm. In fact I bagged up the results from multiple hyperparam settings and it gave me the best score on the LB. If you like this and would like to get more information about such things, subscribe to the mailing list on the right hand side. Also I would definitely recommend this course about winning Kaggle competitions by Kazanova, Kaggle rank 3 . Do take a look.\n","permalink":"https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/","tags":["Machine Learning","Data Science","Python"],"title":"Hyperopt - A bayesian Parameter Tuning Framework"},{"categories":["Data Science","Awesome Guides"],"contents":"Recently Kaggle master Kazanova along with some of his friends released a \u0026amp;ldquo;How to win a data science competition\u0026amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.\n  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.\nWe were asked you to predict total sales for every product and store in the next month.\nThe evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.\nThe main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.\n1. Created a dataframe of all Date_block_num, Store and Item combinations: This is important because in the months we don\u0026rsquo;t have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.\nfrom itertools import product # Create \u0026#34;grid\u0026#34; with columns index_cols = [\u0026#39;shop_id\u0026#39;, \u0026#39;item_id\u0026#39;, \u0026#39;date_block_num\u0026#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[\u0026#39;date_block_num\u0026#39;].unique(): cur_shops = sales.loc[sales[\u0026#39;date_block_num\u0026#39;] == block_num, \u0026#39;shop_id\u0026#39;].unique() cur_items = sales.loc[sales[\u0026#39;date_block_num\u0026#39;] == block_num, \u0026#39;item_id\u0026#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=\u0026#39;int32\u0026#39;)) grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) 2. Cleaned up a little of sales data after some basic EDA: sales = sales[sales.item_price\u0026lt;100000] sales = sales[sales.item_cnt_day\u0026lt;=1000] 3. Created Mean Encodings: sales_m = sales.groupby([\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]).agg({\u0026#39;item_cnt_day\u0026#39;: \u0026#39;sum\u0026#39;,\u0026#39;item_price\u0026#39;: np.mean}).reset_index() sales_m = pd.merge(grid,sales_m,on=[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;],how=\u0026#39;left\u0026#39;).fillna(0) # adding the category id too sales_m = pd.merge(sales_m,items,on=[\u0026#39;item_id\u0026#39;],how=\u0026#39;left\u0026#39;) for type_id in [\u0026#39;item_id\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_category_id\u0026#39;]: for column_id,aggregator,aggtype in [(\u0026#39;item_price\u0026#39;,np.mean,\u0026#39;avg\u0026#39;),(\u0026#39;item_cnt_day\u0026#39;,np.sum,\u0026#39;sum\u0026#39;),(\u0026#39;item_cnt_day\u0026#39;,np.mean,\u0026#39;avg\u0026#39;)]: mean_df = sales.groupby([type_id,\u0026#39;date_block_num\u0026#39;]).aggregate(aggregator).reset_index()[[column_id,type_id,\u0026#39;date_block_num\u0026#39;]] mean_df.columns = [type_id+\u0026#39;_\u0026#39;+aggtype+\u0026#39;_\u0026#39;+column_id,type_id,\u0026#39;date_block_num\u0026#39;] sales_m = pd.merge(sales_m,mean_df,on=[\u0026#39;date_block_num\u0026#39;,type_id],how=\u0026#39;left\u0026#39;) These above lines add the following 9 features :\n \u0026lsquo;item_id_avg_item_price\u0026rsquo; \u0026lsquo;item_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;item_id_avg_item_cnt_day\u0026rsquo; \u0026lsquo;shop_id_avg_item_price\u0026rsquo;, \u0026lsquo;shop_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;shop_id_avg_item_cnt_day\u0026rsquo; \u0026lsquo;item_category_id_avg_item_price\u0026rsquo; \u0026lsquo;item_category_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;item_category_id_avg_item_cnt_day\u0026rsquo;  4. Create Lag Features: Next we create lag features with diferent lag periods on the following features:\n \u0026lsquo;item_id_avg_item_price\u0026rsquo;, \u0026lsquo;item_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;item_id_avg_item_cnt_day\u0026rsquo; \u0026lsquo;shop_id_avg_item_price\u0026rsquo; \u0026lsquo;shop_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;shop_id_avg_item_cnt_day\u0026rsquo; \u0026lsquo;item_category_id_avg_item_price\u0026rsquo; \u0026lsquo;item_category_id_sum_item_cnt_day\u0026rsquo; \u0026lsquo;item_category_id_avg_item_cnt_day\u0026rsquo; \u0026lsquo;item_cnt_day\u0026rsquo;  lag_variables = list(sales_m.columns[7:])+[\u0026#39;item_cnt_day\u0026#39;] lags = [1 ,2 ,3 ,4, 5, 12] for lag in lags: sales_new_df = sales_m.copy() sales_new_df.date_block_num+=lag sales_new_df = sales_new_df[[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]+lag_variables] sales_new_df.columns = [\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;]+ [lag_feat+\u0026#39;_lag_\u0026#39;+str(lag) for lag_feat in lag_variables] sales_means = pd.merge(sales_means, sales_new_df,on=[\u0026#39;date_block_num\u0026#39;,\u0026#39;shop_id\u0026#39;,\u0026#39;item_id\u0026#39;] ,how=\u0026#39;left\u0026#39;) 5. Fill NA with zeros: for feat in sales_means.columns: if \u0026#39;item_cnt\u0026#39; in feat: sales_means[feat]=sales_means[feat].fillna(0) elif \u0026#39;item_price\u0026#39; in feat: sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median()) 6. Drop the columns that we are not going to use in training: cols_to_drop = lag_variables[:-1] + [\u0026#39;item_name\u0026#39;,\u0026#39;item_price\u0026#39;] 7. Take a recent bit of data only: sales_means = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]\u0026gt;12] 8. Split in train and CV : X_train = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]\u0026lt;33].drop(cols_to_drop, axis=1) X_cv = sales_means[sales_means[\u0026#39;date_block_num\u0026#39;]==33].drop(cols_to_drop, axis=1) 9. THE MAGIC SAUCE: In the start I told that the clipping aspect of [0,20] will be important. In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.\ndef clip(x): if x\u0026gt;40: return 40 elif x\u0026lt;0: return 0 else: return x train[\u0026#39;item_cnt_day\u0026#39;] = train.apply(lambda x: clip(x[\u0026#39;item_cnt_day\u0026#39;]),axis=1) cv[\u0026#39;item_cnt_day\u0026#39;] = cv.apply(lambda x: clip(x[\u0026#39;item_cnt_day\u0026#39;]),axis=1) 10: Modelling:  Created a XGBoost model to get the most important features(Top 42 features) Use hyperopt to tune xgboost Used top 10 models from tuned XGBoosts to generate predictions. clipped the predictions to [0,20] range Final solution was the average of these 10 predictions.  Learned a lot of new things from this awesome course . Most recommended.\n","permalink":"https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/","tags":["Machine Learning","Data Science","Python","Kaggle","Timeseries"],"title":"Using XGBoost for time series prediction tasks"},{"categories":["Data Science"],"contents":"Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.\nTake a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing). You can start for free with the 7-day Free Trial.\nThis post is inspired by a Kernel on Kaggle written by Beluga, one of the top Kagglers, for a knowledge based competition .\nSome of the techniques/tricks I am sharing have been taken directly from that kernel so you could take a look yourself. Otherwise stay here and read on.\n1. Don\u0026rsquo;t try predicting the future when you don\u0026rsquo;t have to: If both training/test comes from the same timeline, we can get really crafty with features. Although this is a case with Kaggle only, we can use this to our advantage. For example: In the Taxi Trip duration challenge the test data is randomly sampled from the train data. In this case we can use the target variable averaged over different categorical variable as a feature. Like in this case Beluga actually used the averaged the target variable over different weekdays. He then mapped the same averaged value as a variable by mapping it to test data too.\n2. logloss clipping Technique: Something that I learned in the Neural Network course by Jeremy Howard. Its based on a very simple Idea. Logloss penalises a lot if we are very confident and wrong. So in case of Classification problems where we have to predict probabilities, it would be much better to clip our probabilities between 0.05-0.95 so that we are never very sure about our prediction.\n3. kaggle submission in gzip format: A small piece of code that will help you save countless hours of uploading. Enjoy. df.to_csv(\u0026lsquo;submission.csv.gz\u0026rsquo;, index=False, compression=\u0026lsquo;gzip\u0026rsquo;)\n4. How best to use Latitude and Longitude features - Part 1: One of the best things that I liked about the Beluga Kernel is how he used the Lat/Lon Data. So in the example we had pickup Lat/Lon and Dropoff Lat/Lon. We created features like:\nA. Haversine Distance Between the Two Lat/Lons: def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h B. Manhattan Distance Between the two Lat/Lons: def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a + b C. Bearing Between the two Lat/Lons: def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) D. Center Latitude and Longitude between Pickup and Dropoff: train.loc[:, \u0026#39;center_latitude\u0026#39;] = (train[\u0026#39;pickup_latitude\u0026#39;].values + train[\u0026#39;dropoff_latitude\u0026#39;].values) / 2 train.loc[:, \u0026#39;center_longitude\u0026#39;] = (train[\u0026#39;pickup_longitude\u0026#39;].values + train[\u0026#39;dropoff_longitude\u0026#39;].values) / 2 5. How best to use Latitude and Longitude features - Part 2: The Second way he used the Lat/Lon Feats was to create clusters for Pickup and Dropoff Lat/Lons. The way it worked was it created sort of Boroughs in the data by design.\nfrom sklearn.cluster import MiniBatchKMeans coords = np.vstack((train[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]].values, train[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]].values, test[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]].values, test[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]].values)) sample_ind = np.random.permutation(len(coords))[:500000] kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind]) train.loc[:, \u0026#39;pickup_cluster\u0026#39;] = kmeans.predict(train[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]]) train.loc[:, \u0026#39;dropoff_cluster\u0026#39;] = kmeans.predict(train[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]]) test.loc[:, \u0026#39;pickup_cluster\u0026#39;] = kmeans.predict(test[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]]) test.loc[:, \u0026#39;dropoff_cluster\u0026#39;] = kmeans.predict(test[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]]) He then used these Clusters to create features like counting no of trips going out and coming in on a particular day.\n6. How best to use Latitude and Longitude features - Part 3 He used PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since he transformed 2D-\u0026gt; 2D. The rotation could help for decision tree splits, and it did actually.\npca = PCA().fit(coords) train[\u0026#39;pickup_pca0\u0026#39;] = pca.transform(train[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]])[:, 0] train[\u0026#39;pickup_pca1\u0026#39;] = pca.transform(train[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]])[:, 1] train[\u0026#39;dropoff_pca0\u0026#39;] = pca.transform(train[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]])[:, 0] train[\u0026#39;dropoff_pca1\u0026#39;] = pca.transform(train[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]])[:, 1] test[\u0026#39;pickup_pca0\u0026#39;] = pca.transform(test[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]])[:, 0] test[\u0026#39;pickup_pca1\u0026#39;] = pca.transform(test[[\u0026#39;pickup_latitude\u0026#39;, \u0026#39;pickup_longitude\u0026#39;]])[:, 1] test[\u0026#39;dropoff_pca0\u0026#39;] = pca.transform(test[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]])[:, 0] test[\u0026#39;dropoff_pca1\u0026#39;] = pca.transform(test[[\u0026#39;dropoff_latitude\u0026#39;, \u0026#39;dropoff_longitude\u0026#39;]])[:, 1] 7. Lets not forget the Normal Things you can do with your features:  Scaling by Max-Min Normalization using Standard Deviation Log based feature/Target: use log based features or log based target function. One Hot Encoding  8. Creating Intuitive Additional Features: A) Date time Features: Time based Features like \u0026ldquo;Evening\u0026rdquo;, \u0026ldquo;Noon\u0026rdquo;, \u0026ldquo;Night\u0026rdquo;, \u0026ldquo;Purchases_last_month\u0026rdquo;, \u0026ldquo;Purchases_last_week\u0026rdquo; etc.\nB) Thought Features: Suppose you have shopping cart data and you want to categorize TripType (See Walmart Recruiting: Trip Type Classification on Kaggle for some background).\nYou could think of creating a feature like \u0026ldquo;Stylish\u0026rdquo; where you create this variable by adding together number of items that belong to category Men\u0026rsquo;s Fashion, Women\u0026rsquo;s Fashion, Teens Fashion.\nYou could create a feature like \u0026ldquo;Rare\u0026rdquo; which is created by tagging some items as rare, based on the data we have and then counting the number of those rare items in the shopping cart. Such features might work or might not work. From what I have observed they normally provide a lot of value.\nI feel this is the way that Target\u0026rsquo;s \u0026ldquo;Pregnant Teen model\u0026rdquo; was made. They would have had a variable in which they kept all the items that a pregnant teen could buy and put it into a classification algorithm.\n9 . The not so Normal Things which people do: These features are highly unintuitive and should not be created where the machine learning model needs to be interpretable.\nA) Interaction Features: If you have features A and B create features A*B, A+B, A/B, A-B. This explodes the feature space. If you have 10 features and you are creating two variable interactions you will be adding 10C2 * 4 features = 180 features to your model. And most of us have a lot more than 10 features.\nB) Bucket Feature Using Hashing: Suppose you have a lot of features. In the order of Thousands but you don\u0026rsquo;t want to use all the thousand features because of the training times of algorithms involved. People bucket their features using some hashing algorithm to achieve this.Mostly done for text classification tasks. For example: If we have 6 features A,B,C,D,E,F. And the row of data is: A:1,B:1,C:1,D:0,E:1,F:0 I may decide to use a hashing function so that these 6 features correspond to 3 buckets and create the data using this feature hashing vector. After processing my data might look like: Bucket1:2,Bucket2:2,Bucket3:0 Which happened because A and B fell in bucket1, C and E fell in bucket2 and D and F fell in bucket 3. I summed up the observations here, but you could substitute addition with any math function you like. Now i would use Bucket1,Bucket2,Bucket3 as my variables for machine learning.\nWill try to keep on expanding. Wait for more\u0026hellip;.\n","permalink":"https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/","tags":["Machine Learning","Data Science","Kaggle"],"title":"Good Feature Building Techniques - Tricks for Kaggle -  My Kaggle Code Repository"},{"categories":["Data Science"],"contents":"Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track. One of the most helpful way to learn more about these is the STAT110 course by Joe Blitzstein and his book . You can check out this Coursera course too. Hope it could be useful to someone else too. So here goes:\n1. Bernoulli Distribution: Perhaps the most simple discrete distribution of all.\nStory: A Coin is tossed with probability p of heads.\nPMF of Bernoulli Distribution is given by:\n$$P(X=k) = \\begin{cases}1-p \u0026 k = 0\\\\p \u0026 k = 1\\end{cases}$$ CDF of Bernoulli Distribution is given by:\n$$P(X \\leq k) = \\begin{cases}0 \u0026 k \\lt 0\\\\1-p \u0026 0 \\leq k \\lt 1 \\\\1 \u0026 k \\geq 1\\end{cases}$$ Expected Value:\n$$E[X] = \\sum kP(X=k)$$ $$E[X] = 0P(X=0)+1P(X=1) = p$$\nVariance:\n$$Var[X] = E[X^2] - E[X]^2$$ Now we find, $$E[X]^2 = p^2$$ and $$E[X^2] = \\sum k^2P(X=k)$$ $$E[X^2] = 0^2P(X=0) + 1^2P(X=1) = p $$ Thus, $$Var[X] = p(1-p)$$\n2. Binomial Distribution:   One of the most basic distribution in the Statistician toolkit. The parameters of this distribution is n(number of trials) and p(probability of success).\nStory: Probability of getting exactly k successes in n trials\nPMF of binomial Distribution is given by:\n$$P(X=k) = \\left(\\begin{array}{c}n\\ k\\end{array}\\right) p^{k}(1-p)^{n-k}$$\nCDF of binomial Distribution is given by:\n$$ P(X\\leq k) = \\sum_{i=0}^k \\left(\\begin{array}{c}n\\ i\\end{array}\\right) p^i(1-p)^{n-i} $$\nExpected Value:\n$$E[X] = \\sum kP(X=k)$$ $$E[X] = \\sum_{k=0}^n k \\left(\\begin{array}{c}n\\ k\\end{array}\\right) * p^{k}(1-p)^{n-k} = np $$\nA better way to solve this:\n$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$ X is the sum on n Indicator Bernoulli random variables.\nThus,\n $$E[X] = E[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$ $$E[X] = E[I_{1}] + E[I_{2}] + ....+ E[I_{n-1}]+ E[I_{n}]$$ $$E[X] = \\underbrace{p + p + ....+ p + p}_{n} = np$$ Variance:\n$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$ X is the sum on n Indicator Bernoulli random variables. $$Var[X] = Var[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$ $$Var[X] = Var[I_{1}] + Var[I_{2}] + ....+ Var[I_{n-1}]+ Var[I_{n}]$$ $$Var[X] = \\underbrace{p(1-p) + p(1-p) + ....+ p(1-p) + p(1-p)}_{n} = np(1-p)$$ 3. Geometric Distribution: The parameters of this distribution is p(probability of success).\nStory: The number of failures before the first success(Heads) when a coin with probability p is tossed\nPMF of Geometric Distribution is given by:\n$$P(X=k) = (1-p)^kp$$\nCDF of Geometric Distribution is given by:\n$$ P(X\\leq k) = \\sum_{i=0}^k (1-p)^{i}p$$ $$ P(X\\leq k) = p(1+q+q^2\u0026hellip;+q^k)= p(1-q^k)/(1-q) = 1-(1-p)^k $$\nExpected Value:\n$$E[X] = \\sum kP(X=k)$$ $$E[X] = \\sum_{k=0}^{inf} k (1-p)^kp$$ $$E[X] = qp +2q^2p +3q^3p +4q^4p \u0026hellip;. $$ $$E[X] = qp(1+2q+3q^2+4q^3+\u0026hellip;.)$$ $$E[X] = qp/(1-q)^2 = q/p $$\nVariance:\n$$Var[X] = E[X^2] - E[X]^2$$ Now we find, $$E[X]^2 = q^2/p^2$$ and $$E[X^2] = \\sum_{k=0}^{inf} k^2q^kp= qp + 4q^2p + 9q^3p +16q^4p \u0026hellip; = qp(1+4q+9q^2+16q^3\u0026hellip;.)$$ $$E[X^2] = qp^{-2}(1+q)$$\nThus, $$Var[X] =q/p^2$$\nCheck Math appendix at bottom of this post for Geometric Series Proofs.\nExample:\nQ. A doctor is seeking an anti-depressant for a newly diagnosed patient. Suppose that, of the available anti-depressant drugs, the probability that any particular drug will be effective for a particular patient is p=0.6. What is the probability that the first drug found to be effective for this patient is the first drug tried, the second drug tried, and so on? What is the expected number of drugs that will be tried to find one that is effective?\nA. Expected number of drugs that will be tried to find one that is effective = q/p = .4/.6 =.67\n4. Negative Binomial Distribution: The parameters of this distribution is p(probability of success) and r(number of success).\nStory: The number of failures of independent Bernoulli(p) trials before the rth success.\nPMF of Negative Binomial Distribution is given by:\nr successes , k failures , last attempt needs to be a success: $$P(X=k) = \\left(\\begin{array}{c}k+r-1\\ k\\end{array}\\right) p^r(1-p)^k$$\nExpected Value:\nThe negative binomial RV could be stated as the sum of r Geometric RVs $$X = X^1+X^2\u0026hellip;. X^{r-1} +X^r$$ Thus, $$E[X] = E[X^1]+E[X^2]\u0026hellip;. E[X^{r-1}] +E[X^r]$$\n$$E[X] = rq/p$$\nVariance:\nThe negative binomial RV could be stated as the sum of r independent Geometric RVs $$X = X^1+X^2\u0026hellip;. X^{r-1} +X^r$$ Thus, $$Var[X] = Var[X^1]+Var[X^2]\u0026hellip;. Var[X^{r-1}] +Var[X^r]$$\n$$Var[X] = rq/p^2$$\nExample:\nQ. Pat is required to sell candy bars to raise money for the 6th grade field trip. There are thirty houses in the neighborhood, and Pat is not supposed to return home until five candy bars have been sold. So the child goes door to door, selling candy bars. At each house, there is a 0.4 probability of selling one candy bar and a 0.6 probability of selling nothing. What\u0026rsquo;s the probability of selling the last candy bar at the nth house?\nA. r = 5 ; k = n - r\nProbability of selling the last candy bar at the nth house = $$P(X=k) = \\left(\\begin{array}{c}k+r-1\\ k\\end{array}\\right) p^r(1-p)^k$$ $$P(X=k) = \\left(\\begin{array}{c}n-1\\ n-5\\end{array}\\right) .4^5(.6)^{n-5}$$\n5. Poisson Distribution: The parameters of this distribution is $\\lambda$ the rate parameter.\nMotivation: There is as such no story to this distribution but only motivation for using this distribution. The Poisson distribution is often used for applications where we count the successes of a large number of trials where the per-trial success rate is small. For example, the Poisson distribution is a good starting point for counting the number of people who email you over the course of an hour.The number of chocolate chips in a chocolate chip cookie is another good candidate for a Poisson distribution, or the number of earthquakes in a year in some particular region\nPMF of Poisson Distribution is given by: $$ P(X=k) = \\frac{e^{-\\lambda}\\lambda^k} {k!}$$\nExpected Value:\n$$E[X] = \\sum kP(X=k)$$ $$ E[X] = \\sum_{k=0}^{inf} k \\frac{e^{-\\lambda}\\lambda^k} {k!}$$ $$ E[X] = \\lambda e^{-\\lambda}\\sum_{k=0}^{inf} \\frac{\\lambda^{k-1}} {(k-1)!}$$ $$ E[X] = \\lambda e^{-\\lambda} e^{\\lambda} = \\lambda $$ Variance:\n$$Var[X] = E[X^2] - E[X]^2$$ Now we find,\n$$E[X^2] = \\lambda + \\lambda^2$$ Thus, $$Var[X] = \\lambda$$ Example:\nQ. If electricity power failures occur according to a Poisson distribution with an average of 3 failures every twenty weeks, calculate the probability that there will not be more than one failure during a particular week?\nA. Probability = P(X=0)+P(X=1) =\n$$e^{-3/20} + e^{-3/20}3/20 = 23/20*e^{-3/20} $$ Probability of selling the last candy bar at the nth house = $$P(X=k) = \\left(\\begin{array}{c}k+r-1\\ k\\end{array}\\right) p^r(1-p)^k$$ $$P(X=k) = \\left(\\begin{array}{c}n-1\\ n-5\\end{array}\\right) .4^5(.6)^{n-5}$$\nMath Appendix: Some Math (For Geometric Distribution) :\n$$a+ar+ar^2+ar^3+‚ãØ=a/(1‚àír)=a(1‚àír)^{‚àí1}$$ Taking the derivatives of both sides, the first derivative with respect to r must be: $$a+2ar+3ar^2+4ar^3‚ãØ=a(1‚àír)^{‚àí2}$$ Multiplying above with r: $$ar+2ar^2+3ar^3+4ar^4‚ãØ=ar(1‚àír)^{‚àí2}$$ Taking the derivatives of both sides, the first derivative with respect to r must be: $$a+4ar+9ar^2+16ar^3‚ãØ=a(1‚àír)^{-3}(1+r)$$\nBonus - Python Graphs and Functions: # Useful Function to create graph def chart_creator(x,y,title): import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes %matplotlib inline # Create a list of 100 Normal RVs data = pd.DataFrame(zip(x,y)) data.columns = [\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;] # We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=\u0026#34;ticks\u0026#34;) # Here we create a matplotlib axes object. The extra parameters we use # \u0026#34;ci\u0026#34; to remove confidence interval # \u0026#34;marker\u0026#34; to have a x as marker. # \u0026#34;scatter_kws\u0026#34; to provide style info for the points.[s for size] # \u0026#34;line_kws\u0026#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=\u0026#39;x\u0026#39;, y=\u0026#39;y\u0026#39;, data=data, ci = False, scatter_kws={\u0026#34;color\u0026#34;:\u0026#34;darkred\u0026#34;,\u0026#34;alpha\u0026#34;:0.3,\u0026#34;s\u0026#34;:90}, line_kws={\u0026#34;color\u0026#34;:\u0026#34;g\u0026#34;,\u0026#34;alpha\u0026#34;:0.5,\u0026#34;lw\u0026#34;:0},marker=\u0026#34;x\u0026#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(title, fontsize=34,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(\u0026#34;k\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(\u0026#34;pmf\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;) And here I will generate the PMFs of the discrete distributions we just discussed above using Pythons built in functions. For more details on the upper function, please see my previous post - Create basic graph visualizations with SeaBorn . Also take a look at the documentation guide for the below functions\n# Binomial : from scipy.stats import binom n=30 p=0.5 k = range(0,n) pmf = binom.pmf(k, n, p) chart_creator(k,pmf,\u0026#34;Binomial PMF\u0026#34;)   # Geometric : from scipy.stats import geom n=30 p=0.5 k = range(0,n) # -1 here is the location parameter for generating the PMF we want. pmf = geom.pmf(k, p,-1) chart_creator(k,pmf,\u0026#34;Geometric PMF\u0026#34;)   # Negative Binomial : from scipy.stats import nbinom r=5 # number of successes p=0.5 # probability of Success k = range(0,25) # number of failures # -1 here is the location parameter for generating the PMF we want. pmf = nbinom.pmf(k, r, p) chart_creator(k,pmf,\u0026#34;Nbinom PMF\u0026#34;)   #Poisson from scipy.stats import poisson lamb = .3 # Rate k = range(0,5) pmf = poisson.pmf(k, lamb) chart_creator(k,pmf,\u0026#34;Poisson PMF\u0026#34;)   References:   Introduction to Probability by Joe Blitzstein   Wikipedia   Next thing I want to come up with is a same sort of post for continuous distributions too. Keep checking for the same. Till then Ciao.\n","permalink":"https://mlwhiz.com/blog/2017/09/14/discrete_distributions/","tags":["Statistics","Data Science"],"title":"The story of every distribution - Discrete Distributions"},{"categories":["Deep Learning","Computer Vision"],"contents":"Deeplearning is the buzz word right now. I was working on the course for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the Dogs vs Cats competition on Kaggle to achieve very good results.\nWhat are pretrained Neural Networks? So let me tell you about the background a little bit. There is a challenge that happens every year in the visual recognition community - The Imagenet Challenge. The task there is to classify the images in 1000 categories using Image training data. People train big convolutional deep learning models for this challenge.\nNow what does training a neural model actually mean? It just means that they learn the weights for a NN. What if we can get the weights they learn? We can use those weights to load them into our own NN model and predict on the test dataset. Right?\nBut actually we can go further than that. We can add an extra layer on top of the NN they have prepared to classify our own dataset.\nIn a way you can think of the intermediate features created by the Pretrained neural networks to be the features for the next layer.\nWhy it works? We are essentially doing the image classification task only. We need to find out edges, shapes, intensities and other features from the images that are given to us. The pretrained model is already pretty good at finding these sort of features. Forget neural nets, if we plug these features into a machine learning algorithm we should be good.\nWhat we actually do here is replace the last layer of the neural network with a new prediction/output layer and train while keeping the weights for all the layers before the second last layer constant.\nCode: I assume that you understand Keras a little. If not you can look at the docs. Let us get into coding now. First of all we will create the architecture of the neural network the VGG Team created in 2014. Then we will load the weights.\nImport some stuff\nimport numpy as np from numpy.random import random, permutation from scipy import misc, ndimage from scipy.ndimage.interpolation import zoom import keras from keras import backend as K from keras.utils.data_utils import get_file from keras.models import Sequential, Model from keras.layers.core import Flatten, Dense, Dropout, Lambda from keras.layers import Input from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D from keras.optimizers import SGD, RMSprop, Adam from keras.preprocessing import image VGG has just one type of convolutional block, and one type of fully connected (\u0026lsquo;dense\u0026rsquo;) block. We start by defining the building blocks of our Deep learning model.\ndef ConvBlock(layers, model, filters): for i in range(layers): model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(filters, 3, 3, activation=\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D((2,2), strides=(2,2))) def FCBlock(model): model.add(Dense(4096, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.5))  Now the input of the VGG Model was images. When the VGG model was trained in 2014, the creators subtracted the average of each of the three (R,G,B) channels first, so that the data for each channel had a mean of zero. Furthermore, their software that expected the channels to be in B,G,R order, whereas Python by default uses R,G,B. We need to preprocess our data to make these two changes, so that it is compatible with the VGG model. We also add some helper functions.\n#Mean of each channel as provided by VGG researchers vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1)) def vgg_preprocess(x): x = x - vgg_mean # subtract mean return x[:, ::-1] # reverse axis bgr-\u0026gt;rgb def VGG_16(): model = Sequential() model.add(Lambda(vgg_preprocess, input_shape=(3,224,224))) ConvBlock(2, model, 64) ConvBlock(2, model, 128) ConvBlock(3, model, 256) ConvBlock(3, model, 512) ConvBlock(3, model, 512) model.add(Flatten()) FCBlock(model) FCBlock(model) model.add(Dense(1000, activation=\u0026#39;softmax\u0026#39;)) return model def finetune(model, num_classes): # Drop last layer model.pop() # Make all layers untrainable. i.e fix all weights for layer in model.layers: layer.trainable=False # Add a new layer which is the new output layer model.add(Dense(num_classes, activation=\u0026#39;softmax\u0026#39;)) model.compile(optimizer=Adam(lr=0.001), loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) return model # A way to generate batches of images def get_batches(path, dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=64, class_mode=\u0026#39;categorical\u0026#39;): return gen.flow_from_directory(path+dirname, target_size=(224,224), class_mode=class_mode, shuffle=shuffle, batch_size=batch_size) The hard part is done now. Just create a VGG object and load the weights.We will need to load pretrained weights into the model too. You can download the \u0026ldquo;VGG16_weights.h5\u0026rdquo; file here model = VGG_16() model.load_weights(\u0026#39;VGG16_weights.h5\u0026#39;) # Since our dogs vs cat dataset is binary classification model ftmodel = finetune(model,2) print ftmodel.summary()   Showing a little bit of output here. This is how the last layers of our Neural net look after training. Now we have got a architecture which we got to train. Here we are only training to get the last layer weights. As you can see from the trainable params. path = \u0026#34;dogscats/\u0026#34; batch_size=64 # Iterators to get our images from our datasets. The datasets are folders named train and valid. Both folder contain two directories \u0026#39;dogs\u0026#39; and \u0026#39;cats\u0026#39;. In each directory the corresponding images are kept. batches = get_batches(path,\u0026#39;train\u0026#39;, batch_size=batch_size) val_batches = get_batches(path,\u0026#39;valid\u0026#39;, batch_size=batch_size) # Now run for some epochs till the validation loss stops decreasing. no_of_epochs=1 for epoch in range(no_of_epochs): print \u0026#34;Running epoch: %d\u0026#34; % epoch ftmodel.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample) latest_weights_filename = \u0026#39;ft%d.h5\u0026#39; % epoch ftmodel.save_weights(latest_weights_filename) #Create Predictions on test set. The test images should be in the folder dogscats/test/test_images/ , which is a single directory containing all images. test_batches = get_batches(path, \u0026#39;test\u0026#39;, batch_size=2*batch_size, class_mode=None) preds = ftmodel.predict_generator(test_batches, test_batches.nb_sample) isdog = preds[:,1] image_id = batches.filenames final_submission = np.stack([ids,isdog], axis=1) And we are done!\n","permalink":"https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/","tags":["Deep Learning","Artificial Intelligence","Computer Vision"],"title":"Today I Learned This Part 2: Pretrained Neural Networks What are they?"},{"categories":["Data Science"],"contents":"Newton once said that \u0026ldquo;God does not play dice with the universe\u0026rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.\nOne of such ways intuition plays with us is with the Birthday problem.\nProblem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?\nBy the pigeonhole principle , the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).\nHowever, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.\nMathematical Proof: Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event. We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement) Since there are k ways in which birthdays can be chosen with replacement.\n$P(birthday Match) = 1 - \\dfrac{(365).364\u0026hellip;(365‚àík+1)}{365^k}$\nSimulation: Lets try to build around this result some more by trying to simulate this result:\n%matplotlib inline import matplotlib import numpy as np import matplotlib.pyplot as plt import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes import random def sim_bithday_problem(num_people_room, trials =1000): \u0026#39;\u0026#39;\u0026#39;This function takes as input the number of people in the room. Runs 1000 trials by default and returns (number of times same brthday found)/(no of trials) \u0026#39;\u0026#39;\u0026#39; same_birthdays_found = 0 for i in range(trials): # randomly sample from the birthday space which could be any of a number from 1 to 365 birthdays = [random.randint(1,365) for x in range(num_people_room)] if len(birthdays) - len(set(birthdays))\u0026gt;0: same_birthdays_found+=1 return same_birthdays_found/float(trials) num_people = range(2,100) probs = [sim_bithday_problem(i) for i in num_people] data = pd.DataFrame() data[\u0026#39;num_peeps\u0026#39;] = num_people data[\u0026#39;probs\u0026#39;] = probs sns.set(style=\u0026#34;ticks\u0026#34;) g = sns.regplot(x=\u0026#34;num_peeps\u0026#34;, y=\u0026#34;probs\u0026#34;, data=data, ci = False, scatter_kws={\u0026#34;color\u0026#34;:\u0026#34;darkred\u0026#34;,\u0026#34;alpha\u0026#34;:0.3,\u0026#34;s\u0026#34;:90}, marker=\u0026#34;x\u0026#34;,fit_reg=False) sns.despine() g.figure.set_size_inches(10,6) g.axes.set_title(\u0026#39;As the Number of people in room reaches 23 the probability reaches ~0.5\\nAt more than 50 people the probability is reaching 1\u0026#39;, fontsize=15,color=\u0026#34;g\u0026#34;,alpha=0.5) g.set_xlabel(\u0026#34;# of people in room\u0026#34;,size = 30,color=\u0026#34;r\u0026#34;,alpha=0.5) g.set_ylabel(\u0026#34;Probability\u0026#34;,size = 30,color=\u0026#34;r\u0026#34;,alpha=0.5) g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   We can see from the graph that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.\nIntuition: To understand it we need to think of this problem in terms of pairs. There are ${{23}\\choose{2}} = 253$ pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at ${{70}\\choose{2}} = 2450$ pairs.\nSo thats it for now. To learn more about this go to Wikipedia which has an awesome page on this topic.\nReferences:   Introduction to Probability by Joseph K. Blitzstein   Birthday Problem on Wikipedia   ","permalink":"https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/","tags":["Data Science","Statistics","Math"],"title":"Maths Beats Intuition probably every damn time"},{"categories":["Natural Language Processing","Deep Learning"],"contents":"Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.\nTill now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.\nFor those who don\u0026rsquo;t know bag of words: If we have a series of sentences(documents)\n This is good - [1,1,1,0,0] This is bad - [1,1,0,1,0] This is awesome - [1,1,0,0,1]  Bag of words would encode it using 0:This 1:is 2:good 3:bad 4:awesome\nBut it is much more powerful than that.\nWhat word2vec does is that it creates vectors for words. What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.\nHow does that help? We can use this for multiple scenarios but the most common are:\nA. Using word2vec embeddings we can find out similarity between words. Assume you have to answer if these two statements signify the same thing:\n President greets press in Chicago Obama speaks to media in Illinois.  If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.\n  But with a word encoding we can say that\n President is similar to Obama greets is similar to speaks press is similar to media Chicago is similar to Illinois  B. Encode Sentences: I read a post from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.\nHis Approach: Lets say the sentence is \u0026ldquo;What is this\u0026rdquo; And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)\n what : [.25 ,.25 ,.25 ,.25] is : [ 1 , 0 , 0 , 0] this : [ .5 , 0 , 0 , .5]  Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.\nElementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75] divided by math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5 gives:[1.16, .17, .17, 0.5]  Thus I can convert any sentence to a vector of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.\nC. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?\nGuess what it comes out to be : Queen\nApplication/Coding: Now lets get down to the coding part as we know a little bit of fundamentals.\nFirst of all we download a custom word embedding from Google. There are many other embeddings too.\nwget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz The above file is pretty big. Might take some time. Then moving on to coding.\nfrom gensim.models import word2vec model = gensim.models.KeyedVectors.load_word2vec_format(\u0026#39;data/GoogleNews-vectors-negative300.bin.gz\u0026#39;, binary=True) 1. Starting simple, lets find out similar words. Want to find similar words to python? model.most_similar(\u0026#39;python\u0026#39;) [(u'pythons', 0.6688377261161804),\n(u'Burmese_python', 0.6680364608764648),\n(u'snake', 0.6606293320655823),\n(u'crocodile', 0.6591362953186035),\n(u'boa_constrictor', 0.6443519592285156),\n(u'alligator', 0.6421656608581543),\n(u'reptile', 0.6387745141983032),\n(u'albino_python', 0.6158879995346069),\n(u'croc', 0.6083582639694214),\n(u'lizard', 0.601341724395752)]\n 2. Now we can use this model to find the solution to the equation: What is king - man + woman?\nmodel.most_similar(positive = [\u0026#39;king\u0026#39;,\u0026#39;woman\u0026#39;],negative = [\u0026#39;man\u0026#39;]) [(u'queen', 0.7118192315101624),\n(u'monarch', 0.6189674139022827),\n(u'princess', 0.5902431011199951),\n(u'crown_prince', 0.5499460697174072),\n(u'prince', 0.5377321839332581),\n(u'kings', 0.5236844420433044),\n(u'Queen_Consort', 0.5235946178436279),\n(u'queens', 0.5181134343147278),\n(u'sultan', 0.5098593235015869),\n(u'monarchy', 0.5087412595748901)]\n You can do plenty of freaky/cool things using this:\n3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma? model.most_similar(positive = [\u0026#39;emma\u0026#39;,\u0026#39;he\u0026#39;,\u0026#39;male\u0026#39;,\u0026#39;mr\u0026#39;],negative = [\u0026#39;she\u0026#39;,\u0026#39;mrs\u0026#39;,\u0026#39;female\u0026#39;]) [(u'sanchez', 0.4920658469200134),\n(u'kenny', 0.48300960659980774),\n(u'alves', 0.4684845209121704),\n(u'gareth', 0.4530612826347351),\n(u'bellamy', 0.44884198904037476),\n(u'gibbs', 0.445194810628891),\n(u'dos_santos', 0.44508373737335205),\n(u'gasol', 0.44387346506118774),\n(u'silva', 0.4424275755882263),\n(u'shaun', 0.44144102931022644)]\n 4. Find which word doesn\u0026rsquo;t belong to a list ? model.doesnt_match(\u0026#34;math shopping reading science\u0026#34;.split(\u0026#34; \u0026#34;)) I think staple doesn\u0026rsquo;t belong in this list!\nOther Cool Things 1. Recommendations:   In this paper , the authors have shown that itembased CF can be cast in the same framework of word embedding.\n2. Some other examples that people have seen after using their own embeddings: Library - Books = Hall\nObama + Russia - USA = Putin\nIraq - Violence = Jordan\nPresident - Power = Prime Minister (Not in India Though)\n3.Seeing the above I started playing with it a little. Is this model sexist?\nmodel.most_similar(positive = [\u0026#34;donald_trump\u0026#34;],negative = [\u0026#39;brain\u0026#39;]) [(u'novak', 0.40405112504959106),\n(u'ozzie', 0.39440611004829407),\n(u'democrate', 0.39187556505203247),\n(u'clinton', 0.390536367893219),\n(u'hillary_clinton', 0.3862358033657074),\n(u'bnp', 0.38295692205429077),\n(u'klaar', 0.38228923082351685),\n(u'geithner', 0.380607008934021),\n(u'bafana_bafana', 0.3801495432853699),\n(u'whitman', 0.3790769875049591)]\n Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.\n","permalink":"https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/","tags":["Deep Learning","Artificial Intelligence","Natural Language Processing"],"title":"Today I Learned This Part I: What are word2vec Embeddings?"},{"categories":["Data Science","Learning Resources"],"contents":"I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.\nTo be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn\u0026rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?\nBut it is not magic. And this is a way an aspiring guy could take to become a self-trained data scientist. Follow in order. I have tried to include everything that comes to my mind. So here goes:\n1. Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University  The one stat course you gotta take. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.\nThis was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.\nOne of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gambler‚Äôs Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.\nIt will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.\nHe got a textbook out based on this course which is clearly a great text:\n 2. Data Science CS109 : - Again by Professor Blitzstein. Again an awesome course. Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts. You will learn about Python Libraries like Numpy,Pandas for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:\nLearning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.  3. CS229: Andrew Ng  After doing these two above courses you will gain the status of what I would like to call a \u0026ldquo;Beginner\u0026rdquo;. Congrats!!!. You know stuff, you know how to implement stuff. Yet you do not fully understand all the math and grind that goes behind all this.\nHere comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms. I will put this course as the one course you gotta take as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.\nAlso recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website here .\nYou are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D\u0026rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.\n4. Intro to Hadoop \u0026amp;amp; Mapreduce - Udacity  Let us first focus on the literal elephant in the room - Hadoop. Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.\nOnce you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem. Read this amazing Blog Post from Michael Noll: Writing An Hadoop MapReduce Program In Python - Michael G. Noll . Just read the basic mapreduce codes. Don\u0026rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.\nNow try to solve these two problems from the CS109 Harvard course from 2013:\nA. First, grab the file word_list.txt from here . This contains a list of six-letter words. To keep things simple, all of the words consist of lower-case letters only.Write a mapreduce job that finds all anagrams in word_list.txt.\nB. For the next problem, download the file baseball_friends.csv . Each row of this csv file contains the following:\n A person\u0026rsquo;s name The team that person is rooting for \u0026ndash; either \u0026ldquo;Cardinals\u0026rdquo; or \u0026ldquo;Red Sox\u0026rdquo; A list of that person\u0026rsquo;s friends, which could have arbitrary length  For example: The first line tells us that Aaden is a Red Sox friend and he has 65 friends, who are all listed here. For this problem, it\u0026rsquo;s safe to assume that all of the names are unique and that the friendship structure is symmetric (i.e. if Alannah shows up in Aaden\u0026rsquo;s friends list, then Aaden will show up in Alannah\u0026rsquo;s friends list). Write an mr job that lists each person\u0026rsquo;s name, their favorite team, the number of Red Sox fans they are friends with, and the number of Cardinals fans they are friends with.\nTry to do this yourself. Don\u0026rsquo;t use the mrjob (pronounced Mr. Job) way that they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.\nIf you are done with these, you can safely call yourself as someone who could \u0026ldquo;think in Mapreduce\u0026rdquo; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:\nHadoop Mapreduce Streaming Tricks and Techniques If you are someone who likes learning from a book you can get:\n 5. Spark - In memory Big Data tool. Now comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn\u0026rsquo;t.\nNow Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.\n Data Science and Engineering with Apache¬Æ Spark‚Ñ¢ I have written a little bit about Basic data processing with Spark here. Take a look: Learning Spark using Python: Basics and Applications Also take a look at some of the projects I did as part of course at github If you would like a book to read:\n If you don\u0026rsquo;t go through the courses, try solving the same two problems above that you solved by Hadoop using Spark too. Otherwise the problem sets in the courses are more than enough.\n6. Understand Linux Shell: Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn\u0026rsquo;t emphasize how much time shell saves for me everyday.\nRead these tutorials by me for doing that:\nShell Basics every Data Scientist Should know -Part I Shell Basics every Data Scientist Should know - Part II(AWK) If you would like a course you can go for this course on edX .\nIf you want a book, go for:\n Congrats you are an \u0026ldquo;Hacker\u0026rdquo; now. You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. \u0026ldquo;All Roads lead to Rome\u0026rdquo; as long as you are running.\n7. Learn Statistical Inference and Bayesian Statistics  I took the previous version of the specialization which was a single course taught by Mine √áetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data. You can also use these books:\n  8. Deep Learning  Intro - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.\n Advanced - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.\n Bonus - A free online book by Michael Nielsen.\n Advanced Math Book - A math intensive book by Yoshua Bengio \u0026amp; Ian Goodfellow\n9. Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More  This course used to be there on Coursera but now only video links on youtube available. You can learn from this book too:\n Apart from that if you want to learn about Python and the basic intricacies of the language you can take the  Computer Science Mini Specialization from RICE university  too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.\n10. Advanced Maths: Couldn\u0026rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.\n Linear Algebra By Gilbert Strang - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.\n Multivariate Calculus - MIT OCW  Convex Optimization - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.\nThe Machine learning field is evolving and new advancements are made every day. That\u0026rsquo;s why I didn\u0026rsquo;t put a third tier. The maximum I can call myself is a \u0026ldquo;Hacker\u0026rdquo; and my learning continues. Hope you do the same.\nHope you like this list. Please provide your inputs in comments on more learning resources as you see fit.\nTill then. Ciao!!!\n","permalink":"https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/","tags":["Big Data","Machine Learning","Data Science","Python","Statistics","Visualization","Production","Curated Resources"],"title":"Top Data Science Resources on the Internet right now"},{"categories":["Data Science"],"contents":"Today we will look into the basics of linear regression. Here we go :\nContents  Simple Linear Regression (SLR) Multiple Linear Regression (MLR) Assumptions  1. Simple Linear Regression Regression is the process of building a relationship between a dependent variable and set of independent variables. Linear Regression restricts this relationship to be linear in terms of coefficients. In SLR, we consider only one independent variable.\nExample: The Waist Circumference ‚Äì Adipose Tissue data   Studies have shown that individuals with excess Adipose tissue (AT) in the abdominal region have a higher risk of cardio-vascular diseases\n  Computed Tomography, commonly called the CT Scan is the only technique that allows for the precise and reliable measurement of the AT (at any site in the body)\n  The problems with using the CT scan are:\n Many physicians do not have access to this technology Irradiation of the patient (suppresses the immune system) Expensive    Is there a simpler yet reasonably accurate way to predict the AT area? i.e.\n Easily available Risk free Inexpensive    A group of researchers conducted a study with the aim of predicting abdominal AT area using simple anthropometric measurements i.e. measurements on the human body\n  The Waist Circumference ‚Äì Adipose Tissue data is a part of this study wherein the aim is to study how well waist circumference(WC) predicts the AT area\n  # Setting working directory filepath \u0026lt;- c(\u0026#34;/Users/nkaveti/Documents/Work_Material/Statistics Learning/\u0026#34;) setwd(filepath) # Reading data Waist_AT \u0026lt;- read.csv(\u0026#34;adipose_tissue.csv\u0026#34;) cat(\u0026#34;Number of rows: \u0026#34;, nrow(Waist_AT), \u0026#34;\\n\u0026#34;) head(Waist_AT) Number of rows: 109   WaistAT  74.7525.72 72.6025.89 81.8042.60 83.9542.80 74.6529.84 71.8521.68   Let\u0026rsquo;s start with a scatter plot of Waist Vs AT, to understand the relationship between these two variables.\nplot(AT ~ Waist, data = Waist_AT)   Any observations from above plot?\nNow the objective is to find a linear relation between Waist and AT. In otherwords, finding the amount of change in AT per one unit change (increment/decrement) in Waist.\nIn SLR, it is equivalent to finding an optimal straight line equation such that the sum of squares of differences between straight line and the points will be minimum. This method of estimation is called as Ordiany Least Squares (OLS) .\n$$AT = \\beta_0 + \\beta_1 \\ Waist + \\epsilon$$\n$$Min_{\\beta_0 , \\beta_1} \\ \\ \\epsilon^\\intercal \\epsilon \\implies Min_{\\beta_0 , \\beta_1} \\ \\ (AT - \\beta_0 - \\beta_1 \\ Waist)^\\intercal (AT - \\beta_0 - \\beta_1 \\ Waist)$$ Where, $\\beta_1$ represents the amount of change in AT per one unit change in Waist.\nNow our problem becomes an unconstrained optimization problem. We can find optimal values for $\\beta_0$ and $\\beta_1$ using basic calculus.\nLets re-write above regression equation in matrix form\n$$ AT = X \\beta + \\epsilon$$\nWhere, $ X = [1 \\ \\ Waist]$ 1 is a vector of ones and $\\beta = (\\beta_0, \\ \\beta_1)$\n$$ \\begin{equation} \\begin{split} \\epsilon^\\intercal \\epsilon \u0026amp; = {(AT - X \\beta)}^\\intercal {(AT - X \\beta)} \\\n\u0026amp; = AT^\\intercal AT - AT^\\intercal X \\beta - {(X \\beta)}^\\intercal AT + {(X \\beta)}^\\intercal (X \\beta) \\end{split} \\end{equation} $$\nNow differentiate this w.r.t to $\\beta$ and equate it to zero. Then we have, $$\\hat{\\beta} = (X^\\intercal X)^{-1} X^\\intercal AT $$\nNow we can find the fitted values of model by substituting $\\hat{\\beta}$ in above regression equation $$\\hat{AT} = X \\hat{\\beta}=X(X^\\intercal X)^{-1} X^\\intercal AT$$\nNote: We are arriving to above equation through an assumption $^1$ of $E(\\epsilon)=0$. What happens if this assumption violates?\nLet, $X(X^\\intercal X)^{-1} X^\\intercal = H$ $$\\hat{AT} = H \\ AT$$\nWe call H as an hat matrix, because it transforms $AT$ into $\\hat{AT}$ :D\n# Lets compute the hat matrix X = cbind(1, Waist_AT$Waist) temp = solve(t(X) %*% X) %*% t(X) betahat = temp %*% Waist_AT$AT # Estimated coefficients cat(\u0026#34;Let\u0026#39;s compare the computed values with lm() output: \\n \\n\u0026#34;) cat(\u0026#34;Computed Coefficients: \\n \\n\u0026#34;) print(data.frame(Intercept = betahat[1], Waist = betahat[2])) cat(\u0026#34;======================================================================= \\n\u0026#34;) #cat(\u0026#34;Optimal value for beta_0 is: \u0026#34;, betahat[1], \u0026#34;and for beta_1 is: \u0026#34;, betahat[2], \u0026#34;\\n \\n\u0026#34;) fit_lm = lm(AT ~ Waist, data = Waist_AT) #cat(\u0026#34;Compare our computed estimates with lm() estimates\u0026#34;, \u0026#34;\\n\u0026#34;) print(fit_lm) cat(\u0026#34;======================================================================= \\n \\n\u0026#34;) H = X %*% temp # Computing hat matrix AThat = H %*% Waist_AT$AT # Computing predicted values cat(\u0026#34;Therefore, there is a\u0026#34;, betahat[2], \u0026#34;increment in AT per one unit change in Waist \\n\u0026#34; ) Let's compare the computed values with lm() output: Computed Coefficients: Intercept Waist 1 -215.9815 3.458859 ======================================================================= Call: lm(formula = AT ~ Waist, data = Waist_AT) Coefficients: (Intercept) Waist -215.981 3.459 ======================================================================= Therefore, there is a 3.458859 increment in AT per one unit change in Waist  What\u0026rsquo;s next? We succesfully computed estimates for regression coefficients and fitted values.\n  We are working on only one sample, how can we generalise these results to population?\n  How to measure model\u0026rsquo;s performance quantitatively?\n  ** We are working on only one sample, how can we generalise these results to population? **\nLet\u0026rsquo;s focus on question 1. Our regression coefficients are computed using only one sample and these values will change, if we change the sample. But how much they vary? We need to estimate the variation for each beta coefficient to check whether the corresponding regressor is consistently explaining the same behaviour even if we change the sample.\nNow the big problem is collecting multiple samples to check the above hypothesis. Hence, we use distributions to check statistical significance of regressors.\nFor our example, we need to test below two hypotheses.\n$$ Null \\ Hypothesis: \\beta_{0} = 0 $$\n$$ Alternative \\ Hypothesis: \\beta_{0} \\neq 0$$\n$$ Null \\ Hypothesis: \\beta_{1} = 0 $$\n$$ Alternative \\ Hypothesis: \\beta_{1} \\neq 0$$\nTest Statistic for these hypotheses is,\n$$t = \\frac{\\hat{\\beta_{i}}}{\\sqrt{Var(\\hat{\\beta_{i}})}}$$\nTest statistic t follows t-distribution, assuming $^2$ dependent variable follows normal distribution\nSuggestion: If your not aware of testing of hypothesis , probability distributions and p-values please browse through the Google.\nLet\u0026rsquo;s recall that, $\\hat{\\beta} = (X^\\intercal X)^{-1} X^\\intercal AT$\n$$\\begin{equation} \\begin{split} Var(\\hat{\\beta}) \u0026amp; = Var((X^\\intercal X)^{-1} X^\\intercal AT) \\\n\u0026amp; = (X^\\intercal X)^{-1} X^\\intercal \\ Var(AT) \\ X(X^\\intercal X)^{-1} \\\n\u0026amp; = (X^\\intercal X)^{-1} X^\\intercal \\ X(X^\\intercal X)^{-1} \\ \\sigma^2 \\\n\u0026amp; = (X^\\intercal X)^{-1} \\sigma^2 \\end{split} \\end{equation} $$\nNote: In the above calculations we assumed $^3$ $Var(AT) = \\sigma^2$ (Constant). Where, $\\sigma^2$ is variation in population AT.\nSuggestion: Try solving $(X^\\intercal X)^{-1}$ with $X = [1, \\ x]$ where $x = (x_1, x_2, x_3 \u0026hellip; x_n)$. You will get the following expression.\n$$ \\ Var(\\hat{\\beta}) = \\frac{1}{n \\sum x_i^2 - (\\sum x_i)^2} \\begin{bmatrix} \\sum_{i=1}^n x_i^2 \u0026 -\\sum x_i \\\\ -\\sum x_i \u0026 n \\end{bmatrix} \\sigma^2 \\ $$ Diagonal elements of above matrix are varinaces of $\\beta_0$ and $\\beta_1$ respectively. Off-diagonal element is covariance between $\\beta_0$ and $\\beta_1$.\nHence,\n $$Var(\\hat{\\beta_0}) = \\frac{\\sigma^2 \\sum_{i = 1}^n x_i^2}{n \\sum_{i = 1}^n (x_i - \\bar{x})^2}$$   $$Var(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2}$$  One important observation from $Var(\\hat{\\beta})$ expressions is, $Var(x)$ is inversely proportional to $Var(\\hat{\\beta})$. That is, we will get more consistent estimators if there is high variation in corresponding predictors.\nRecall that, $\\sigma^2$ in above expression is the population variance, not the sample. Hence, we need to estimate this using the sample that we have.\n$$\\hat{\\sigma^2} = \\frac{1}{n-2} \\sum_{i = 1}^n e_i^2$$\nWhere, $e_i = AT_i - \\hat{AT}_i$\n# Let\u0026#39;s compute variances of beta hat and test statistic \u0026#39;t\u0026#39; sigmasq = (1/length(AThat[-c(1:2)]))*sum((AThat - Waist_AT$AT)^2) VarBeta0 = (sigmasq * sum(Waist_AT$Waist^2))/(length(AThat) * sum((Waist_AT$Waist - mean(Waist_AT$Waist))^2)) VarBeta1 = sigmasq/sum((Waist_AT$Waist - mean(Waist_AT$Waist))^2) cat(\u0026#34;Let\u0026#39;s compare the computed values with lm() output: \\n \\n\u0026#34;) cat(\u0026#34;======================================================================= \\n\u0026#34;) cat(\u0026#34;Computed Coefficients: \\n \\n\u0026#34;) res = data.frame(Estimate = betahat, Std.Error = c(sqrt(VarBeta0), sqrt(VarBeta1)), t_value = c(betahat[1]/sqrt(VarBeta0), betahat[2]/sqrt(VarBeta1))) row.names(res) = c(\u0026#34;(Intercept)\u0026#34;, \u0026#34;Waist\u0026#34;) res$p_value = 2*pt(abs(res$t_value), nrow(Waist_AT)-1, lower.tail = FALSE) print(res) cat(\u0026#34;=======================================================================\u0026#34;) summary(fit_lm) cat(\u0026#34;=======================================================================\u0026#34;) Let's compare the computed values with lm() output: ======================================================================= Computed Coefficients: Estimate Std.Error t_value p_value (Intercept) -215.981488 21.7962708 -9.909103 7.507198e-17 Waist 3.458859 0.2346521 14.740376 1.297124e-27 ======================================================================= Call: lm(formula = AT ~ Waist, data = Waist_AT) Residuals: Min 1Q Median 3Q Max -107.288 -19.143 -2.939 16.376 90.342 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -215.9815 21.7963 -9.909 \u0026lt;2e-16 *** Waist 3.4589 0.2347 14.740 \u0026lt;2e-16 *** --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 33.06 on 107 degrees of freedom Multiple R-squared: 0.67, Adjusted R-squared: 0.667 F-statistic: 217.3 on 1 and 107 DF, p-value: \u0026lt; 2.2e-16 =======================================================================  Note: Residual standard error = $\\sqrt{sigmasq}$\nHow to measure model\u0026rsquo;s performance quantitatively?\nLet\u0026rsquo;s focus on question 2 (How to measure model\u0026rsquo;s performance quantitatively?). Recall that, our objective of building model is to explain the variation in AT using the variation in Waist.\nTotal variation in AT is, $\\sum_{i=1}^n (AT - mean(AT))^2$ this can be splitted into two parts as follows:\n $$ \\begin{equation} \\begin{split} \\sum_{i=1}^n (AT_i - \\bar{AT})^2 \u0026 = \\sum_{i=1}^n (AT - \\hat{AT_i} + \\hat{AT_i} - \\bar{AT})^2 \\\\ \u0026 = \\sum_{i = 1}^n (\\hat{AT_i} - \\bar{AT})^2 + \\sum_{i=1}^n (AT_i - \\hat{AT_i})^2 \\end{split} \\end{equation} $$  Where, $\\sum_{i=1}^n (AT_i - \\bar{AT})^2$ is the total variation in AT, $\\sum_{i = 1}^n (\\hat{AT_i} - \\bar{AT})^2$ is the explained variation in AT, this is also called as Regression Sum of Squares and $\\sum_{i=1}^n (AT_i - \\hat{AT_i})^2$ is the unexplained variation in AT, this is also called as Error Sum of Squares\nWe can measure our model using the proportion of total variation explained by independent variable(s). That is, $\\frac{Regression \\ Sum \\ of \\ Squares}{Total \\ Sum \\ of \\ Squares}$\nThe above measure is called as Multiple R-squared:\n $$Multiple \\ R-squared = \\frac{\\sum_{i = 1}^n (\\hat{AT_i} - \\bar{AT})^2}{\\sum_{i=1}^n (AT_i - \\bar{AT})^2}$$  Interesting facts: Multiple R-squared value in SLR is equals to $r^2$ and (1 - Multiple R-squared) is equals to the variance in residuals.\nWhere, r is pearson\u0026amp;rsquo;s correlation coefficient between dependent and independent variable.\n# Let\u0026#39;s compute Multiple R-squared measure for our example SSR = sum((AThat - mean(Waist_AT$AT))^2) SST = sum((Waist_AT$AT - mean(Waist_AT$AT))^2) MulRSq = SSR/SST cat(\u0026#34;Compute Multiple R-squared: \u0026#34;, MulRSq, \u0026#34;\\n \\n\u0026#34;) cat(\u0026#34;Note that computed R squared value is matching with lm() Multiple R-squared value in above output \\n \\n\u0026#34;) cat(\u0026#34;======================================================================= \\n \\n\u0026#34;) Compute Multiple R-squared: 0.6700369 Note that computed R squared value is matching with lm() Multiple R-squared value in above output =======================================================================  What happens to the Multiple R-squared value when you add an irrelevant variable to the model?\nIn the below model, I am generating a random sample of uniform numbers between 1 to 100 and considering this as one of indepedent variable.\nset.seed(1234) fit_lm2 = lm(AT ~ Waist + runif(nrow(Waist_AT), 1, 100), data = Waist_AT) summary(fit_lm2) cat(\u0026#34;======================================================================= \\n \\n\u0026#34;) Call: lm(formula = AT ~ Waist + runif(nrow(Waist_AT), 1, 100), data = Waist_AT) Residuals: Min 1Q Median 3Q Max -106.06 -17.53 -3.63 13.70 91.36 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -226.2894 23.4350 -9.656 3.33e-16 *** Waist 3.5060 0.2376 14.757 \u0026lt; 2e-16 *** runif(nrow(Waist_AT), 1, 100) 0.1397 0.1181 1.183 0.239 --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 33 on 106 degrees of freedom Multiple R-squared: 0.6743, Adjusted R-squared: 0.6682 F-statistic: 109.7 on 2 and 106 DF, p-value: \u0026lt; 2.2e-16 =======================================================================  Multiple R-squared value increases irrespective of quality of explanation, which is incorrect. We should penalize our model performance if the quality of explanation is poor, that is why we need to adjust our R-squared value.\nTo penalize the explained part of AT, we inflate the unexplained part of AT with $\\frac{Total \\ degrees \\ of \\ freedom}{Error \\ degrees \\ of \\ freedom}$. That is,\n$$Adjusted \\ R-squared = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$$\nWhere, n = Total number of observations; p = Total number of predictors (excluding intercept)\nAdding a new independent variable will increase $\\frac{n-1}{n-p-1}$ and $R^2$. If the amount of increment in $R^2$ is less than the amount of increment in $\\frac{n-1}{n-p-1}$ than it will decrease the Adjusted R-squared value.\nIn fit_lm2 model Adjusted R-squared decreases when we add randomly generated variable into the model.\n# Let\u0026#39;s compute adjusted R-squared for our example TDF = nrow(Waist_AT[-1, ]) # Total degrees of freedom EDF = nrow(Waist_AT[-1, ]) - 1 # Error degrees of freedom, where 1 is the number of predictors AdjRSq = 1 - (1 - MulRSq) * (TDF/EDF) # Adjusted R square cat(\u0026#34;Compute Multiple R-squared: \u0026#34;, AdjRSq, \u0026#34;\\n \\n\u0026#34;) cat(\u0026#34;Note that computed Adjusted R-squared value is matching with lm() Adjusted R-squared value in the above output \\n \\n\u0026#34;) cat(\u0026#34;Note: We are comparing with fit_lm model, not fit_lm2 \\n\u0026#34;) cat(\u0026#34;======================================================================= \\n\u0026#34;) Compute Multiple R-squared: 0.6669531 Note that computed Adjusted R-squared value is matching with lm() Adjusted R-squared value in the above output Note: We are comparing with fit_lm model, not fit_lm2 =======================================================================  Aforementioned measures (Multiple R-squared \u0026amp; Adjusted R-squared) for Goodness of fit are functions of sample and these will vary as sample changes. Similar to t-test for regression coefficeints we need some statistical test to test model\u0026rsquo;s performance for population.\nObjective is to compare the Mean sum of squares due to regression and Mean sum of squares due to error. F-test is very helpful to compare the variations.\n$$ F-test = \\frac{\\frac{1}{p-1}\\sum_{i=1}^n (\\hat{AT_i} - \\bar{AT})^2}{\\frac{1}{n-p-1} \\sum_{i=1}^n (\\hat{AT_i} - AT_i)^2}$$  Note: Above expression follows F distribution only if, AT follows Normal Distribution\nRDF = TDF - EDF SSE = SST - SSR MSR = (1/RDF)*SSR MSE = (1/EDF)*SSE F_value = MSR/MSE cat(\u0026#34;Compute F statistic: \u0026#34;, F_value, \u0026#34;\\n \\n\u0026#34;) cat(\u0026#34;Note that computed F-statistic is matching with lm() F-statistic value in the above output \\n \\n\u0026#34;) cat(\u0026#34;Note: We are comparing with fit_lm model, not fit_lm2 \\n\u0026#34;) cat(\u0026#34;======================================================================= \\n\u0026#34;) Compute F statistic: 217.2787 Note that computed F-statistic is matching with lm() F-statistic value in the above output Note: We are comparing with fit_lm model, not fit_lm2 =======================================================================  2. Multiple Linear Regression (MLR) In multiple linear regression we consider more than one predictor and one dependent variable. Most of the above explanation is valid for MLR too.\nExample: Car\u0026rsquo;s MPG (Miles Per Gallon) prediction Our interest is to model the MPG of a car based on the other variables.\nVariable Description:\n VOL = cubic feet of cab space HP = engine horsepower MPG = average miles per gallon SP = top speed, miles per hour WT = vehicle weight, hundreds of pounds  # Reading Boston housing prices data car = read.csv(\u0026#34;Cars.csv\u0026#34;) cat(\u0026#34;Number of rows: \u0026#34;, nrow(car), \u0026#34;\\n\u0026#34;, \u0026#34;Number of variables: \u0026#34;, ncol(car), \u0026#34;\\n\u0026#34;) head(car) Number of rows: 81 Number of variables: 5   HPMPGVOLSPWT  49 53.7006889 104.185428.76206 55 50.0134092 105.461330.46683 55 50.0134092 105.461330.19360 70 45.6963292 113.461330.63211 53 50.5042392 104.461329.88915 70 45.6963289 113.185429.59177   Our objective is to model the variation in MPG using other independent variables. That is,\n$$MPG = \\beta_0 + \\beta_1 VOL + \\beta_2 HP + \\beta_3 SP + \\beta_4 WT + \\epsilon$$\nWhere, $\\beta_1$ represents the amount of change in MPG per one unit change in VOL provided other variables are fixed. Let\u0026rsquo;s consider below two cases,\nCase1: HP = 49; VOL = 89; SP = 104.1854; WT = 28.76206 =\u0026gt; MPG = 104.1854\nCase2: HP = 49; VOL = 90; SP = 104.1854; WT = 28.76206 =\u0026gt; MPG = 105.2453\nthen $\\beta_1 = 105.2453 - 104.1854 = 1.0599$. Similarly, $\\beta_2, \\beta_3, \\beta_4$\nThe above effect is called as \u0026lt;code\u0026gt;Ceteris Paribus Effect\u0026lt;/code\u0026gt; .\nBut in real world it is very difficult to collect records in above manner. That\u0026rsquo;s why we compute (function of) partial correlation coefficients to quantify the effect of one variable, keeping others constant.\n# Let\u0026#39;s build MLR model to predict MPG based using other variables fit_mlr_actual = lm(MPG ~ ., data = car) summary(fit_mlr_actual) Call: lm(formula = MPG ~ ., data = car) Residuals: Min 1Q Median 3Q Max -0.94530 -0.32792 -0.04058 0.24256 1.71034 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 7.100e-17 5.461e-02 0.000 1.0000 HP -1.285e+00 2.453e-01 -5.239 1.4e-06 *** VOL -8.207e-01 1.389e+00 -0.591 0.5563 SP 6.144e-01 2.458e-01 2.500 0.0146 * WT 3.287e-01 1.390e+00 0.237 0.8136 --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 0.4915 on 76 degrees of freedom Multiple R-squared: 0.7705, Adjusted R-squared: 0.7585 F-statistic: 63.8 on 4 and 76 DF, p-value: \u0026lt; 2.2e-16  One key observation from above output is, Std. Error for VOL and WT is very huge comparing to others and this inflates t values and p value. Hence, these two variables becomes very insignificant for the model.\nLet\u0026rsquo;s go into deep, what happened to $Var(\\hat{\\beta_{VOL}})$ and $Var(\\hat{\\beta_{WT}})$?\nAnalogy for $Var(\\hat{\\beta})$ in MLR is as follows:\n $$Var(\\hat{\\beta_{VOL}}) = \\frac{\\sigma^2}{n\\sum_{i=1}^n (VOL_i - \\bar{VOL})^2 (1 - R_{VOL}^2)}$$  Where, $R_{VOL}^2$ = Multiple R-squared value obtained by regressing VOL on all other independent variables\nTask: To understand it more clearly, take few random samples from cars data and run the MLR model and observe the variation in $\\hat{\\beta_{VOL}}$ and $\\hat{\\beta_{WT}}$.\n# Let\u0026#39;s regress VOL on all other independent variables\u0026#39; fit_mlr = lm(VOL ~ HP + SP + WT, data = car) summary(fit_mlr) Call: lm(formula = VOL ~ HP + SP + WT, data = car) Residuals: Min 1Q Median 3Q Max -0.068938 -0.031641 -0.008794 0.032018 0.077931 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -6.155e-18 4.481e-03 0.000 1.000 HP 2.331e-02 1.995e-02 1.168 0.246 SP -2.294e-02 2.000e-02 -1.147 0.255 WT 9.998e-01 4.557e-03 219.396 \u0026lt;2e-16 *** --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 0.04033 on 77 degrees of freedom Multiple R-squared: 0.9984, Adjusted R-squared: 0.9984 F-statistic: 1.637e+04 on 3 and 77 DF, p-value: \u0026lt; 2.2e-16  It\u0026rsquo;s surprising that, $R_{VOL}^2$ is 0.9984 and also only WT is significant. That is, these two predictors (VOL and WT) are highly correlated. This inflates $Var(\\hat{\\beta_{VOL}})$ and thus t value. We might be missing some of the important information because of high correlation between predictors. This problem is called as Multicollinearity .\nOne quick solution for this problem is to remove either VOL or WT from the model. Let\u0026rsquo;s compute partial correlation coeficient between MPG and VOL by removing the effect of WT (say, $r_{MV.W}$) and partial correlation coeficient between MPG and WT by removing the effect of VOL (say, $r_{MW.V}$).\nTo compute $r_{MV.W}$ we need to compute the correlation between (a) part of VOL which cannot be explained by WT (regress VOL on WT and take the residuals) and (b) the part of MPG which cannot be explained by WT (regress MPG on WT and take the residuals)\nfit_partial = lm(VOL ~ WT, data = car) fit_partial2 = lm(MPG ~ WT, data = car) res1 = fit_partial$residual res2 = fit_partial2$residual cat(\u0026#34;Partial correlation coefficient between MPG and VOL by removing the effect of WT is: \u0026#34;, cor(res1, res2)) Partial correlation coefficient between MPG and VOL by removing the effect of WT is: -0.08008873  fit_partial3 = lm(WT ~ VOL, data = car) fit_partial4 = lm(MPG ~ VOL, data = car) res1 = fit_partia3$residual res2 = fit_partial4$residual cat(\u0026#34;Partial correlation coefficient between MPG and WT by removing the effect of VOL is: \u0026#34;, cor(res1, res2)) Partial correlation coefficient between MPG and WT by removing the effect of VOL is: 0.05538241  Since, $abs(r_{MV.W}) \u0026gt;= abs(r_{MW.V})$ we may remove WT from the model.\n# Remove WT and rerun the model fit_mlr_actual2 = lm(MPG ~ .-WT, data = car) summary(fit_mlr_actual2) Call: lm(formula = MPG ~ . - WT, data = car) Residuals: Min 1Q Median 3Q Max -0.94036 -0.31695 -0.03457 0.23316 1.71570 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 7.910e-17 5.427e-02 0.000 1.0000 HP -1.293e+00 2.415e-01 -5.353 8.64e-07 *** VOL -4.925e-01 5.516e-02 -8.928 1.65e-13 *** SP 6.222e-01 2.421e-01 2.571 0.0121 * --- Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1 Residual standard error: 0.4884 on 77 degrees of freedom Multiple R-squared: 0.7704, Adjusted R-squared: 0.7614 F-statistic: 86.11 on 3 and 77 DF, p-value: \u0026lt; 2.2e-16  After eliminating WT from the model there is an increment of ~0.3% in Adjusted R-squared and more importantly, VOL becomes significant at 0 los (level of significance)\n\n3. Assumptions Linear in Parameters: We assume that there is a linear relation between dependent and set of independent variables\nZero conditional mean: $E(\\epsilon \\mid X) = 0$\nHomoskedasticity: $Var(\\epsilon \\mid X) = \\sigma^2$ (Constant)\nNo perfect Collinearity: All predecitors must be independent among themselves\nNo serial correlation in errors: Erros must be uncorrelated among themselves. In otherwords, observations or records must be independent of each other.\nWe discussed first 4 assumptions in section 1 and 2.\nHere is a book that I recommend to learn more about this:\n","permalink":"https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/","tags":["Data Science","Statistics"],"title":"Basics Of Linear Regression"},{"categories":["Data Science"],"contents":"A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:\n1. Beware of the Clean Data Syndrome You need to ask yourself questions even before you start working on the data. Does this data make sense? Falsely assuming that the data is clean could lead you towards wrong Hypotheses. Apart from that, you can discern a lot of important patterns by looking at discrepancies in the data. For example, if you notice that a particular column has more than 50% values missing, you might think about not using the column. Or you may think that some of the data collection instrument has some error.\nOr let\u0026rsquo;s say you have a distribution of Male vs Female as 90:10 in a Female Cosmetic business. You may assume clean data and show the results as it is or you can use common sense and ask if the labels are switched.\n2. Manage Outliers wisely Outliers can help you understand more about the people who are using your website/product 24 hours a day. But including them while building models will skew the models a lot.\n3. Keep an eye out for the Abnormal Be on the lookout for something out of the obvious. If you find something you may have hit gold.\nFor example, Flickr started up as a Multiplayer game . Only when the founders noticed that people were using it as a photo upload service, did they pivot.\nAnother example: fab.com started up as fabulis.com, a site to help gay men meet people. One of the site\u0026rsquo;s popular features was the \u0026ldquo;Gay deal of the Day\u0026rdquo;. One day the deal was for Hamburgers - and half of the buyers were women. This caused the team to realize that there was a market for selling goods to women. So Fabulis pivoted to fab as a flash sale site for designer products.\n4. Start Focussing on the right metrics  Beware of Vanity metrics For example, # of active users by itself doesn\u0026rsquo;t divulge a lot of information. I would rather say \u0026ldquo;5% MoM increase in active users\u0026rdquo; rather than saying \u0026quot; 10000 active users\u0026quot;. Even that is a vanity metric as active users would always increase. I would rather keep a track of percentage of users that are active to know how my product is performing. Try to find out a metric that ties with the business goal. For example, Average Sales/User for a particular month.   5. Statistics may lie too Be critical of everything that gets quoted to you. Statistics has been used to lie in advertisements, in workplaces and a lot of other marketing venues in the past. People will do anything to get sales or promotions.\nFor example: Do you remember Colgate‚Äôs claim that 80% of dentists recommended their brand? This statistic seems pretty good at first. It turns out that at the time of surveying the dentists, they could choose several brands ‚Äî not just one. So other brands could be just as popular as Colgate.\nAnother Example: \u0026ldquo;99 percent Accurate\u0026rdquo; doesn\u0026rsquo;t mean shit. Ask me to create a cancer prediction model and I could give you a 99 percent accurate model in a single line of code. How? Just predict \u0026ldquo;No Cancer\u0026rdquo; for each one. I will be accurate may be more than 99% of the time as Cancer is a pretty rare disease. Yet I have achieved nothing.\n6. Understand how probability works It happened during the summer of 1913 in a Casino in Monaco. Gamblers watched in amazement as a casino\u0026rsquo;s roulette wheel landed on black 26 times in a row. And since the probability of a Red vs Black is exactly half, they were certain that red was \u0026ldquo;due\u0026rdquo;. It was a field day for the Casino. A perfect example of Gambler\u0026amp;rsquo;s fallacy , aka the Monte Carlo fallacy.\nAnd This happens in real life. People tend to avoid long strings of the same answer . Sometimes sacrificing accuracy of judgment for the sake of getting a pattern of decisions that looks fairer or probable.\nFor example, An admissions officer may reject the next application if he has approved three applications in a row, even if the application should have been accepted on merit.\n7. Correlation Does Not Equal Causation   The Holy Grail of a Data scientist toolbox. To see something for what it is. Just because two variables move together in tandem doesn\u0026rsquo;t necessarily mean that one causes the another. There have been hilarious examples for this in the past. Some of my favorites are:\n  Looking at the firehouse department data you infer that the more firemen are sent to a fire, the more damage is done.\n  When investigating the cause of crime in New York City in the 80s, an academic found a strong correlation between the amount of serious crime committed and the amount of ice cream sold by street vendors! Obviously, there was an unobserved variable causing both. Summers are when the crime is the greatest and when the most ice cream is sold. So Ice cream sales don\u0026rsquo;t cause crime. Neither crime increases ice cream sales.\n  8. More data may help Sometimes getting extra data may work wonders. You might be able to model the real world more closely by looking at the problem from all angles. Look for extra data sources.\nFor example, Crime data in a city might help banks provide a better credit line to a person living in a troubled neighborhood and in turn increase the bottom line.\n","permalink":"https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/","tags":["Data Science","Opinion"],"title":"Top advice for a Data Scientist"},{"categories":["Data Science"],"contents":"As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:\n1. Sampling Algorithms. In case you want to work with a sample of data.  Simple Random Sampling : Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen. Stratified Sampling: Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns : Town A has 1 million factory workers, Town B has 2 million workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample. Reservoir Sampling :Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.  2. Map-Reduce. If you want to work with the whole data. Can be used for feature creation. For Example: I had a use case where I had a graph of 60 Million customers and 130 Million accounts. Each account was connected to other account if they had the Same SSN or Same Name+DOB+Address. I had to find customer ID‚Äôs for each of the accounts. On a single node parsing such a graph took more than 2 days. On a Hadoop cluster of 80 nodes running a Connected Component Algorithm took less than 24 minutes. On Spark it is even faster.\n3. Graph Algorithms. Recently I was working on an optimization problem which was focussed on finding shortest distance and routes between two points in a store layout. Routes which don‚Äôt pass through different aisles, so we cannot use euclidean distances. We solved this problem by considering turning points in the store layout and the djikstra‚Äôs Algorithm.\n 4. Feature Selection.  Univariate Selection. Statistical tests can be used to select those features that have the strongest relationship with the output variable. VarianceThreshold. Feature selector that removes all low-variance features. Recursive Feature Elimination. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Feature Importance: Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.  5. Algorithms to work efficiently. Apart from these above algorithms sometimes you may need to write your own algorithms. Now I think of big algorithms as a combination of small but powerful algorithms. You just need to have idea of these algorithms to make a more better/efficient product. So some of these powerful algorithms which can help you are:\n **Recursive Algorithms:**Binary search algorithm. Divide and Conquer Algorithms: Merge-Sort. **Dynamic Programming:**Solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.  6. Classification/Regression Algorithms. The usual suspects. Minimum you must know:  Linear Regression - Ridge Regression, Lasso Regression, ElasticNet Logistic Regression From there you can build upon:  Decision Trees - ID3, CART, C4.5, C5.0 KNN SVM ANN - Back Propogation, CNN   And then on to Ensemble based algorithms:  Boosting: Gradient Boosted Trees Bagging: Random Forests Blending: Prediction outputs of different learning algorithms are fed into another learning algorithm.    7 . **Clustering Methods.**For unsupervised learning.  k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering  8. Other algorithms you can learn about:  Apriori algorithm- Association Rule Mining Eclat algorithm - Association Rule Mining Item/User Based Similarity - Recommender Systems Reinforcement learning - Build your own robot. Graphical Models Bayesian Algorithms NLP - For language based models. Chatbots.  Hope this has been helpful\u0026hellip;..\n","permalink":"https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/","tags":["Algorithms","Statistics","Machine Learning"],"title":"Machine Learning Algorithms for Data Scientists"},{"categories":null,"contents":"This is a post which deviates from my pattern fo blogs that I have wrote till now but I found that Finance also uses up a lot of Statistics. So it won\u0026rsquo;t be a far cry to put this on my blog here. I recently started investing in Mutual funds so thought of rersearching the area before going all in. Here is the result of some of my research.\n1. Load/No-Load: Always Buy No Load Mutual Funds\n2. Regular/Direct: There are many differenct sites from where you can buy Mutual funds. Most of these sites take a commision to let you the investor buy and sell from their platform. To overcome this commision you can buy direct Mutual funds from the fund houses themselves. But that would be difficult as their are a lot of fund houses and mmanaging all of that could be quite painful. But with the advent of MFUtility you can buy direct plans from the same platform.\n3. Expense Ratios: The expense ratio is a measure of what it costs an investment company to operate a mutual fund. To see how expense ratios can affect your investments over time, let‚Äôs compare the returns of several hypothetical investments that differ only in expense ratio. The following table depicts the returns on a 10,000 initial investment, assuming an average annualized gain of 10%, with different expense ratios (0.5%, 1%, 1.5%, 2% and 2.5%):\n  As the table illustrates, even a small difference in expense ratio can cost you a lot of money in the long run. If you had invested 10,000 in the fund with a 2.5% expense ratio, the value of your fund would be 46,022 after 20 years. Had you instead invested your 10,000 in the fund with a lower, 0.5% expense ratio, your investment would be worth $61,159 after two decades, a 0.33% improvement over the more expensive fund. Keep in mind, this hypothetical example examines funds whose only differences are the expense ratios: all other variables, including initial investment and annualized gains, remain constant (for the example, we must assume identical taxation as well). While two funds are not likely to have the exact same performance over a 20-year period, the table illustrates the effects that small changes in expense ratio can have on your long-term returns.\n 4. Avoid Mutual Funds With High Turnover Ratios: Mutual fund turnover is calculated as the value of all transactions (buying, selling) divided by two, then divided by a fund\u0026rsquo;s total holdings. In simpler terms, mutual fund turnover typically measures the replacement of holdings in a mutual fund, and is commonly presented to investors as a percentage over a one year period. If a fund has 100% turnover, the fund replaces all of its holdings over a 12-month period and that bears cost to the investment company in terms of brokerage etc.\n5. Look for Ample Diversification of Assets: Simply owning four different mutual funds specializing in the financial sector (shares of banks, insurance companies, etc.) is not diversification. Don‚Äôt own funds that make heavy sector or industry bets. If you choose to despite this warning, make sure that you don‚Äôt have a huge portion of your funds invested in them. If it‚Äôs a bond fund, you typically want to avoid bets on the direction of interest rates as this is rank speculation.\n6. Not Same Fund Family: Don‚Äôt keep all of your funds within the same fund family. Witness the mutual fund scandal of a few years ago where portfolio management at many firms allowed big traders to market time the funds, essentially stealing money from smaller investors. By spreading your assets out at different companies, you can mitigate the risk of internal turmoil, ethics breaches, and other localized problems.\n7. Keep Track of various Risk Ratios: a. Standard deviation: Standard deviation (SD) measures the volatility the fund\u0026rsquo;s returns in relation to its average. It tells you how much the fund\u0026rsquo;s return can deviate from the historical mean return of the scheme. If a fund has a 12% average rate of return and a standard deviation of 4%, its return will range from 8-16%\nComputation:\nStandard Deviation (SD) = Square root of Variance (V)\nVariance = (Sum of squared difference between each monthly return and its mean / number of monthly return data ‚Äì 1)\nb. R-Squared: R-Squared measures the relationship between a portfolio and its benchmark. It can be thought of as a percentage from 1 to 100. R-squared is not a measure of the performance of a portfolio. A great portfolio can have a very low R-squared. It is simply a measure of the correlation of the portfolio\u0026rsquo;s returns to the benchmark\u0026rsquo;s returns.\nComputation:\nR-Squared = Square of Correlation\nCorrelation(xy)= Covariance between index and portfolio/(Standard deviation of portfolio * standard deviation of index)\nSignificance:\n  If you want a portfolio that moves like the benchmark, you\u0026rsquo;d want a portfolio with a high Rsquared.\n  If you want a portfolio that doesn\u0026rsquo;t move at all like the benchmark, you\u0026rsquo;d want a low R-squared.\n  General Range for R-Squared:\n  70-100% = good correlation between the portfolio\u0026rsquo;s returns and the benchmark\u0026rsquo;s returns\n  40-70% = average correlation between the portfolio\u0026rsquo;s returns and the benchmark\u0026rsquo;s returns\n  1-40% = low correlation between the portfolio\u0026rsquo;s returns and the benchmark\u0026rsquo;s returns\n  Index funds will have an R-squared very close to 100.\n  R-squared can be used to ascertain the significance of a particular beta or alpha. Generally, a higher R-squared will indicate a more useful beta figure. If the R-squared is lower, then the beta is less relevant to the fund\u0026rsquo;s performance\n  Values range from 1 (returns are explained 100% by the market) to 0 (returns bear no association with the market)\n  c. Beta: A beta of 1.0 indicates that the investment\u0026rsquo;s price will move in lock-step with the market.\nA beta of less than 1.0 indicates that the investment will be less volatile than the market, and, correspondingly, a beta of more than 1.0 indicates that the investment\u0026rsquo;s price will be more volatile than the market.\nFor example, if a fund portfolio\u0026rsquo;s beta is 1.2, it\u0026rsquo;s theoretically 20% more volatile than the market. Conservative investors looking to preserve capital should focus on securities and fund portfolios with low betas, whereas those investors willing to take on more risk in search of higher returns should look for high beta investments.\nComputation:\nBeta = (Standard Deviation of Fund x R-Square) / Standard Deviation of Benchmark\nIf a fund has a beta of 1.5, it means that for every 10% upside or downside, the fund\u0026rsquo;s NAV would be 15% in the respective direction.\nd. Jensens Alpha: Alpha is a measure of an investment\u0026rsquo;s performance on a risk-adjusted basis.\nSimply stated, alpha is often considered to represent the value that a portfolio manager adds or subtracts from a fund portfolio\u0026rsquo;s return.\nA positive alpha of 1.0 means the fund has outperformed its benchmark index by 1%. Correspondingly, a similar negative alpha would indicate an underperformance of 1%.\nComputation:\nAlpha = {(Fund return-Risk free return) ‚Äì (Funds beta) *(Benchmark return- risk free return)}\nFor example, assume a mutual fund realized a return of 15% last year. The appropriate market index for this fund returned 12%. The beta of the fund versus that same index is 1.2 and the risk-free rate is 3%. The fund\u0026rsquo;s alpha is calculated as:\nAlpha = {(15 -3) ‚Äì (1.2) *(12- 3)} = 12 - 9 x 1.2 = 12-10.8 = 1.2\nGiven a beta of 1.2, the mutual fund is expected to be riskier than the index, and thus earn more. A positive alpha in this example shows that the mutual fund manager earned more than enough return to be compensated for the risk he took over the course of the year. If the mutual fund only returned 13%, the calculated alpha would be -0.8. With a negative alpha, the mutual fund manager would not have earned enough return given the amount of risk he was taking.\ne. Sharpe Ratio: Sharpe Ratio measures how well the fund has performed vis-a vis the risk taken by it. It is the excess return over risk-free return (usually return from treasury bills or government securities) divided by the standard deviation. The higher the Sharpe Ratio, the better the fund has performed in proportion to the risk taken by it. The Sharpe ratio is also known as Reward-to-Variability ratio and it is named after William Forsyth Sharpe.\nComputation:\nSR = (Total Return ‚Äì Risk Free Rate) / SD Of Fund\nFor example: Your investor gets 7 per cent return on her investment in a scheme with a standard deviation/volatility of 0.5. We assume risk free rate is 5 per cent. Sharpe Ratio is 7-5/0.5 = 4 in this case\n8. And Finally Always Dollar-Cost Average: Dollar cost averaging is a technique designed to reduce market risk through the systematic purchase of securities at predetermined intervals and set amounts.Instead of investing assets in a lump sum, the investor works his way into a position by slowly buying smaller amounts over a longer period of time. This spreads the cost basis out over several years, providing insulation against changes in market price.\nEvery investor investment strategy differs. These are just some common guidelines to work your way through the market and making informed decisions while buying Mutual Funds. Normally I work through points 1-6 and get my list to a few mutual funds after which I generally use risk ratios to determine which of the funds I selected might be a winner. I have a bias towards long term investing when it comes to investing so whatever I wrote here must be taken with a grain of salt just as everything related to investment must be. Some of you who are doing this for a longer time than I can also tell me about the various other things I can do. I will try to include those ideas in this post as well.\nTo Learn more about Mutual funds and investing in general, take a look at the following two gems:\n  The Editorial review of The intelligent Investor says \u0026ldquo;Among the library of investment books promising no-fail strategies for riches, Benjamin Graham\u0026rsquo;s classic, The Intelligent Investor, offers no guarantees or gimmicks but overflows with the wisdom at the core of all good portfolio management\u0026rdquo; and it rings true in every sense. A must read for everyone looking to invest seriously.\nCommon Sense on Mutual Funds focusses on Mutual funds exclusively. Lets you understand that investing is not difficult. For the not so involved reader.\nTill than Ciao!!!\nReferences:  https://www.thebalance.com/picking-winning-mutual-funds-357957 http://www.miraeassetmf.co.in/uploads/TermofWeek/Sharpe_Ratio.pdf http://www.miraeassetmf.co.in/uploads/TermofWeek/Beta_SD_RSquared.pdf http://www.investopedia.com  ","permalink":"https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/","tags":["Statistics"],"title":"Things to see while buying a Mutual Fund"},{"categories":["Data Science"],"contents":"It has been quite a few days I have been working with Pandas and apparently I feel I have gotten quite good at it. (Quite a Braggard I know) So thought about adding a post about Pandas usage here. I intend to make this post quite practical and since I find the pandas syntax quite self explanatory, I won\u0026rsquo;t be explaining much of the codes. Just the use cases and the code to achieve them.\n1. Import Pandas We Start by importing the libraries that we will need to use.\nimport pandas as pd 2. Read a Datasource: # Read from csv data files # With Header df = pd.read_csv(\u0026#34;/Users/ragarw5/Downloads/SalesJan2009.csv\u0026#34;) # Without Header. sep param to provide the delimiter df = pd.read_csv(\u0026#34;/Users/ragarw5/Downloads/SalesJan2009.csv\u0026#34;, header=None, sep= \u0026#34;,\u0026#34;) # Reading from SQL Datasource import MySQLdb from pandas import DataFrame from pandas.io.sql import read_sql db = MySQLdb.connect(host=\u0026#34;localhost\u0026#34;, # your host, usually localhost user=\u0026#34;root\u0026#34;, # your username passwd=\u0026#34;password\u0026#34;, # your password db=\u0026#34;dbname\u0026#34;) # name of the data base query = \u0026#34;SELECT * FROM tablename\u0026#34; data = read_sql(query, db) # Reading from ExcelFile data = pd.read_excel(filename) For now, we will be working with the file at http://samplecsvs.s3.amazonaws.com/SalesJan2009.csv. The Sales Jan 2009 file contains some ‚Äúsanitized‚Äù sales transactions during the month of January. If you want to work along you can download this file from that location.\ndf = pd.read_csv(\u0026#34;/Users/ragarw5/Downloads/SalesJan2009.csv\u0026#34;) 3. See few rows of data: # top 5 rows df.head() # top 50 rows df.head(50) # last 5 rows df.tail() # last 50 rows df.tail(50) 4. Getting Column Names in a list: columnnames = df.columns 5. Specifying user defined Column Names: Sometimes you want to change the column names:\ndf.columns = [\u0026#39;Transdate\u0026#39;, \u0026#39;Product\u0026#39;, \u0026#39;Price\u0026#39;, \u0026#39;PaymentType\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;City\u0026#39;, \u0026#39;State\u0026#39;, \u0026#39;Country\u0026#39;, \u0026#39;AccountCreated\u0026#39;, \u0026#39;LastLogin\u0026#39;, \u0026#39;Latitude\u0026#39;, \u0026#39;Longitude\u0026#39;] 6. Subsetting specific columns: Sometimes you only need to work with specific columns in a dataframe only. You can subset the columns in the dataframe using\nnewDf = df[[\u0026#39;Product\u0026#39;, \u0026#39;Price\u0026#39;, \u0026#39;PaymentType\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;City\u0026#39;, \u0026#39;State\u0026#39;, \u0026#39;Country\u0026#39;]] 7. Seeing column types: newDf.dtypes 8. Change type of a column First thing i try is this.\nnewDf[\u0026#39;Price\u0026#39;] = newDf[\u0026#39;Price\u0026#39;].astype(\u0026#39;int\u0026#39;) It gives error : ValueError: invalid literal for long() with base 10: \u0026lsquo;13,000\u0026rsquo;. That is you cannot cast a string with \u0026ldquo;,\u0026rdquo; to an int. To do that we first have to get rid of the comma. For that we use a particular lambda-apply functionality which lets us apply functions to each row in the data.\nnewDf[\u0026#39;Price\u0026#39;] = newDf.apply(lambda x: int(x[\u0026#39;Price\u0026#39;].replace(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;)),axis=1)  9. Simple Dataframe Statistics: # To get statistics of numerical columns newDf.describe() # To get maximum value of a column. When you take a single column you can think of it as a list and apply functions you would apply to a list max(newDf[\u0026#39;Price\u0026#39;]) # no of rows in dataframe len(newDf) # Shape of Dataframe newDf.shape 10. Creating a new column: # Create a column Address containing City,State and Country. Simply concat the columns. newDf[\u0026#39;Address\u0026#39;] = newDf[\u0026#39;City\u0026#39;] +\u0026#34;,\u0026#34;+ newDf[\u0026#39;State\u0026#39;] +\u0026#34;,\u0026#34;+ newDf[\u0026#39;Country\u0026#39;] # I like to use a function defined approach with lambda-apply as it gives me more flexibility and more options. Like if i want to create a column which is 1 if the price is greater than 1200 and 0 otherwise. def gt(x): if x\u0026gt;1200: return 1 else: return 0 newDf[\u0026#39;Pricegt1200\u0026#39;] = newDf.apply(lambda x: gt(x[\u0026#39;Price\u0026#39;]),axis=1) 11. Subset a DataFrame: # Single condition: dataframe with all entries priced greater than 1500 df_gt_1500 = newDf[newDf[\u0026#39;Price\u0026#39;]\u0026gt;1500] # Multiple conditions: AND - dataframe with all entries priced greater than 1500 and from London And_df = newDf[(newDf[\u0026#39;Price\u0026#39;]\u0026gt;1500) \u0026amp; (newDf[\u0026#39;City\u0026#39;]==\u0026#39;London\u0026#39;)] # Multiple conditions: OR - dataframe with all entries priced greater than 1500 or from London Or_df = newDf[(newDf[\u0026#39;Price\u0026#39;]\u0026gt;1500) | (newDf[\u0026#39;City\u0026#39;]==\u0026#39;London\u0026#39;)] # Multiple conditions: NOT - dataframe with all entries priced greater than 1500 or from London have to be excluded Not_df = newDf[~((newDf[\u0026#39;Price\u0026#39;]\u0026gt;1500) | (newDf[\u0026#39;City\u0026#39;]==\u0026#39;London\u0026#39;))] 12. Change the Column at particular places or impute: # In the state column the state is abbreviated as \u0026#39;TX\u0026#39;. We want the whole name \u0026#39;Texas\u0026#39; in there newDf.loc[newDf[\u0026#39;State\u0026#39;]==\u0026#39;TX\u0026#39;,\u0026#39;State\u0026#39;] = \u0026#39;Texas\u0026#39; # When City is Monaco State is not given. You want to impute \u0026#39;Monaco State\u0026#39; as state also. newDf.loc[newDf[\u0026#39;City\u0026#39;]==\u0026#39;Monaco\u0026#39;,\u0026#39;State\u0026#39;] = \u0026#39;Monaco State\u0026#39; 13. GroupBy: One of the most used functionality. One simple example\n# Find out the sum of transactions by a state. reset_index() is a function that resets the index of a dataframe. I apply this function ALWAYS whenever I do a groupby and you might think of it as a default syntax for groupby operations import numpy as np newDf.groupby([\u0026#39;State\u0026#39;]).aggregate(np.sum).reset_index() # You might get a few extra columns that you dont need. Just subset the columns in the dataframe. You could just chain the commands to subset for the columns you need. newDf.groupby([\u0026#39;State\u0026#39;]).aggregate(np.sum).reset_index()[[\u0026#39;State\u0026#39;,\u0026#39;Price\u0026#39;]] # Find minimum transaction in each state newDf.groupby([\u0026#39;State\u0026#39;]).aggregate(np.min).reset_index()[[\u0026#39;State\u0026#39;,\u0026#39;Price\u0026#39;]] # You might want to groupby more than one column newDf.groupby([\u0026#39;State\u0026#39;,\u0026#39;City\u0026#39;]).aggregate(np.sum).reset_index()[[\u0026#39;State\u0026#39;,\u0026#39;City\u0026#39;,\u0026#39;Price\u0026#39;]] 14. Concat: You have two datarames df1 and df2 you need to concat. Means append one below the other you can do it using:\npd.concat([df1,df2]) 15. Merge: #Suppose in the start, you had two dataframes. One which contains city and price information: City_Price = newwDf[[\u0026#39;City\u0026#39;,\u0026#39;Price\u0026#39;]] #And another which contains \u0026#39;City\u0026#39; and \u0026#39;State\u0026#39; insformation City_State = newDf[[\u0026#39;City\u0026#39;,\u0026#39;State\u0026#39;]].drop_duplicates(keep=False).reset_index() #You need to merge these datatframes on basis of city. You need to do: City_Price_State_df = pd.merge(City_Price,City_State,on=[\u0026#39;City\u0026#39;],how=\u0026#39;left\u0026#39;) 16. Save a Dataframe to external File: # To Csv file newDf.to_csv(\u0026#34;NewDfData.csv\u0026#34;,index=False) # To Excel File from pandas import ExcelWriter writer = ExcelWriter(\u0026#39;NewDfData.xlsx\u0026#39;) newDf.to_excel(writer,\u0026#39;Sheet1\u0026#39;) writer.save() 17. Pushing Pandas Df to a sql database: from pandas.io import sql import MySQLdb db = MySQLdb.connect(host=\u0026#34;localhost\u0026#34;, # your host, usually localhost user=\u0026#34;root\u0026#34;, # your username passwd=\u0026#34;password\u0026#34;, # your password db=\u0026#34;dbname\u0026#34;) # name of the data base newDf.to_sql(con = db, name=\u0026#39;tablename\u0026#39;,if_exists=\u0026#39;append\u0026#39;,flavor=\u0026#39;mysql\u0026#39;, chunksize=10000,index=False) Hope you found this post useful and worth your time. I tried to make this as simple as possible but You may always ask me or see the documentation for doubts.\nIf you have any more ideas on how to use Pandas or other usecases, please suggest in the comments section.\nTill then ciao!!\nReferences   Intro to Pandas By Greg Rada What I have written is in a condensed form, If you want to get a detailed description visit Greg Rada\u0026rsquo;s 3 posts series.  Pandas Documentation   ","permalink":"https://mlwhiz.com/blog/2016/10/27/baby_panda/","tags":["Python","Data Science","Machine Learning"],"title":"Pandas For All - Some Basic Pandas Functions"},{"categories":["Data Science"],"contents":"It has been a long time since I wrote anything on my blog. So thought about giving everyone a treat this time. Or so I think it is.\nRecently I was thinking about a way to deploy all these machine learning models I create in python. I searched through the web but couldn\u0026rsquo;t find anything nice and easy. Then I fell upon this book by Sebastian Rashcka and I knew that it was what I was looking for. To tell you the truth I did had some experience in Flask earlier but this book made it a whole lot easier to deploy a machine learning model in flask.\nSo today I am going to give a brief intro about Flask Apps and how to deploy them using a service called Openshift.\nSo What is flask? Flask is a Python Web Framework that makes it easier to create webapps from python.\nAnd Openshift? Openshift is a free service(if we only use 1 small instance) which lets us use their services to deploy our flask web-apps.\nSo that we don\u0026rsquo;t get lost, let me tell you the flow of this post.\n First of all we will learn about the installation* of Openshift and Flask. We will create a Hello World application using Flask. We will work on creating a very simple calculator App that operates on two numbers provided by the user. This will help us in understanding how user forms work with Flask by implementing a barebones app.  Installation:  Create your FREE OpenShift account Here Very simple sign-up email + password only Install the OpenShift Client Tools . Use these directions for your particular Operating System these tools have a command line interface and allow more control over your app. The OpenShift tool requires an installation of Ruby.  Now once you do this you have installed Openshift Client tools on your system.\nHelloworld So now I am going to do a lot of things in this post. But don\u0026rsquo;t get bothered much it is just code and HTML quirks. I will try to provide enough details on which parts are necessary. First of all, you will need to create a domain on Openshift platform. This can be done by using:\nrhc domain create -n DomainName -l EmailAddress -p password For this example I created:\nrhc domain create -n mlwhiz -l MyEmailAddress -p Mypassword In the free version for Openshift you can run 3 web-apps with a single domain. For example I can create a maximum of 3 webapps whose web address would be:\n myappname1-mlwhiz.rhcloud.com myappname2-mlwhiz.rhcloud.com myappname3-mlwhiz.rhcloud.com  Once we create a domain we need to create a webapp:\nrhc app create HelloWorld python-2.7 This creates the app named helloworld for us. The app currently resides at this address on web: http://helloworld-mlwhiz.rhcloud.com/ This command also creates a folder where our app resides. cd into this folder.\ncd helloworld Now get a basic template to work upon in this directory. You can think of this as a starter code for flask. We can do this by pulling and merging from Github using the following commands.\ngit remote add upstream -m master git://github.com/openshift/flask-example.git git pull -s recursive -X theirs upstream master Use Virtualenv to isolate Python development environments. It‚Äôs a tool that allows you setup an isolated, self-contained Python environment in a folder on your dev box. This way you can experiment with various versions of Python without affecting your system wide configurations:\nbrew install python-virtualenv cd helloworld/wsgi/ virtualenv venv --python=python2.7 #Activate the virtual environment . venv/bin/activate # Install all these into your virtual python environment. pip install flask flask-wtf flask-babel markdown flup Now Change the name of flaskapp.py in wsgi to run.py\nput this code in run.py\nimport os from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def home(): \u0026#34;\u0026#34;\u0026#34;Render website\u0026#39;s home page.\u0026#34;\u0026#34;\u0026#34; return \u0026#39;Hello World!\u0026#39; if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=\u0026#34;True\u0026#34;) Also change the file named application to:\n#!/usr/bin/python import os import sys sys.path.insert(0, os.path.dirname(__file__) or \u0026#39;.\u0026#39;) PY_DIR = os.path.join(os.environ[\u0026#39;OPENSHIFT_HOMEDIR\u0026#39;], \u0026#34;python\u0026#34;) virtenv = PY_DIR + \u0026#39;/virtenv/\u0026#39; PY_CACHE = os.path.join(virtenv, \u0026#39;lib\u0026#39;, os.environ[\u0026#39;OPENSHIFT_PYTHON_VERSION\u0026#39;], \u0026#39;site-packages\u0026#39;) os.environ[\u0026#39;PYTHON_EGG_CACHE\u0026#39;] = os.path.join(PY_CACHE) virtualenv = os.path.join(virtenv, \u0026#39;bin/activate_this.py\u0026#39;) try: exec(open(virtualenv).read(), dict(__file__=virtualenv)) except IOError: pass from run import app as application Run this to host your app:\ncd helloworld/wsgi python run.py  You should be able to see your app on: http://127.0.0.1:5000/ You can deploy this webapp to Openshift using:\ncd helloworld git add . git commit -a -m \u0026#34;Initial deployment of this app to the web\u0026#34; git push Open http://helloworld-mlwhiz.rhcloud.com/ in your browser. You would see Hello World! there. Now we have got a very basic structure complete.\nOur Simple Calculator App: We will now work on creating a app that operates on two numbers provided by the user. The functions possible are +,- and *. You can see this web app in action here before moving on. This app will help us in understanding how user forms work with Flask and how to manage user inputs in Flask. First of all change the code in run.py to\nimport os from flask import Flask,render_template, request from wtforms import Form, TextAreaField, validators,SelectField app = Flask(__name__) # Code to create a WTForm with three fields. 2 text fields and 1 dropdown menu. class OutputForm(Form): myChoices=[(\u0026#39;+\u0026#39;, \u0026#39;+\u0026#39;), (\u0026#39;-\u0026#39;, \u0026#39;-\u0026#39;), (\u0026#39;*\u0026#39;, \u0026#39;*\u0026#39;)] num1 = TextAreaField(\u0026#39;\u0026#39;,[validators.DataRequired()]) num2 = TextAreaField(\u0026#39;\u0026#39;,[validators.DataRequired()]) Operator = SelectField(u\u0026#39;\u0026#39;, choices = myChoices, validators = [validators.DataRequired()]) # This uses the render_template method in flask to use a template first_app.html. # This html contains placeholders for the form that is provided in the kwargs argument to the function call. @app.route(\u0026#39;/\u0026#39;) def index(): #return \u0026#39;Hello World!\u0026#39; form = OutputForm(request.form) return render_template(\u0026#39;first_app.html\u0026#39;,form = form) # This is the output that is displayed. It checks if the form is validated and POST request is made. # If true it renders the output.html else renders the main index page. # Most of the work is done here. Gets the user inputs using the request.form method. @app.route(\u0026#39;/output\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def output(): form = OutputForm(request.form) if request.method == \u0026#39;POST\u0026#39; and form.validate(): num1 = request.form[\u0026#39;num1\u0026#39;] num2 = request.form[\u0026#39;num2\u0026#39;] op = request.form[\u0026#39;Operator\u0026#39;] if op==\u0026#34;+\u0026#34;: name=str(int(num1)+int(num2)) elif op==\u0026#34;-\u0026#34;: name=str(int(num1)-int(num2)) elif op==\u0026#34;*\u0026#34;: name=str(int(num1)*int(num2)) return render_template(\u0026#39;output.html\u0026#39;, name=name) return render_template(\u0026#39;first_app.html\u0026#39;, form=form) if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=\u0026#34;True\u0026#34;) We use WTF forms here to create a form object. We pass this form object to the HTML render_template method. We have accessed these again in the output function so that we can show them in output.html where all the major work is done for creating the app.\nNow Create a folder named template in helloworld/wsgi and create a file named _formhelpers.html with this content. You really don\u0026rsquo;t need to see the content in this file.\n{% macro render_field(field) %} \u0026lt;dt\u0026gt;{{ field.label }} \u0026lt;dd\u0026gt;{{ field(**kwargs)|safe }} {% if field.errors %} \u0026lt;ul class=errors\u0026gt; {% for error in field.errors %} \u0026lt;li\u0026gt;{{ error }}\u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; {% endif %} \u0026lt;/dd\u0026gt; {% endmacro %} Also add another file named first_app.html with this content. Notice how we access the wtform here.\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;First app\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;,filename=\u0026#39;style.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% from \u0026#34;_formhelpers.html\u0026#34; import render_field %} \u0026lt;div\u0026gt;Calculator: Please enter two numbers and a function you want to apply\u0026lt;/div\u0026gt; \u0026lt;form method=post action=\u0026#34;/output\u0026#34;\u0026gt; {{ render_field(form.num1) }}{{ render_field(form.Operator) }}{{ render_field(form.num2) }} \u0026lt;input type=submit value=\u0026#39;Result\u0026#39; name=\u0026#39;submit_btn\u0026#39;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Create a file named output.html where the final output will be shown.\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;First app\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;,filename=\u0026#39;style.css\u0026#39;) }}\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt;The output is: {{ name }}\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Also add a style.css file in the static folder. You can put this in it for right now or any other thing you want.\nh1 { color: blue; font-family: verdana; font-size: 300%; } p { color: red; font-family: courier; font-size: 160%; } And we are mostly done. Run run.py in the wsgi directory and you would be able to access the app at : http://127.0.0.1:5000/. Again deploy this webapp to Openshift using:\ncd helloworld git add . git commit -a -m \u0026#34;Initial deployment of this app to the web\u0026#34; git push Endnotes So here we took inputs from the user and show the output using the flask App. The final app is hosted at http://helloworld-mlwhiz.rhcloud.com/ for you to see. This code provides us with a code skeletn which will be valuable when we will deploy a whole ML model, which is the main motive of this series.\nReferences  Most of the code here is taken from this awesome book by Sebastian Raschka: Python Machine Learning https://blog.openshift.com/beginners-guide-to-writing-flask-apps-on-openshift/  ","permalink":"https://mlwhiz.com/blog/2016/01/10/deploying_ml_apps_using_python_flask/","tags":["Machine Learning","Production"],"title":"Deploying ML Apps using Python and Flask- Learning about Flask"},{"categories":["Data Science"],"contents":"Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it\u0026rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.\nAnd it is easy to learn.\nIn this post, I will try to give you a brief intro about how you could add awk to your daily work-flow.\nPlease see my previous post if you want some background or some basic to intermediate understanding of shell commands.\nBasics/ Fundamentals So let me start with an example first. Say you wanted to sum a column in a comma delimited file. How would you do that in shell?\nHere is the command. The great thing about awk is that it took me nearly 5 sec to write this command. I did not have to open any text editor to write a python script.\nIt lets you do adhoc work quickly.\nawk \u0026#39;BEGIN{ sum=0; FS=\u0026#34;,\u0026#34;} { sum += $5 } END { print sum }\u0026#39; data.txt 44662539172  See the command one more time. There is a basic structure to the awk command\nBEGIN {action} pattern {action} pattern {action} . . pattern { action} END {action}   An awk program consists of:\n  An optional BEGIN segment : In the begin part we initialize our variables before we even start reading from the file or the standard input.\n  pattern - action pairs: In the middle part we Process the input data. You put multiple pattern action pairs when you want to do multiple things with the same line.\n  An optional END segment: In the end part we do something we want to do when we have reached the end of file.\n  An awk command is called on a file using:\nawk \u0026#39;BEGIN{SOMETHING HERE} {SOMETHING HERE: could put Multiple Blocks Like this} END {SOMETHING HERE}\u0026#39; file.txt You also need to know about these preinitialized variables that awk keeps track of.:\n FS : field separator. Default is whitespace (1 or more spaces or tabs). If you are using any other seperator in the file you should specify it in the Begin Part. RS : record separator. Default record separator is newline. Can be changed in BEGIN action. NR : NR is the variable whose value is the number of the current record. You normally use it in the action blocks in the middle. NF : The Number of Fields after the single line has been split up using FS. Dollar variables : awk splits up the line which is coming to it by using the given FS and keeps the split parts in the $ variables. For example column 1 is in $1, column 2 is in $2. $0 is the string representation of the whole line. Note that if you want to access last column you don\u0026rsquo;t have to count. You can just use $NF. For second last column you can use $(NF-1). Pretty handy. Right.  So If you are with me till here, the hard part is done. Now the fun part starts. Lets look at the first awk command again and try to understand it.\nawk \u0026#39;BEGIN{ sum=0; FS=\u0026#34;,\u0026#34;} { sum += $5 } END { print sum }\u0026#39; data.txt So there is a begin block. Remember before we read any line. We initialize sum to 0 and FS to \u0026ldquo;,\u0026rdquo;.\nNow as awk reads its input line by line it increments sum by the value in column 5(as specified by $5).\nNote that there is no pattern specified here so awk will do the action for every line.\nWhen awk has completed reading the file it prints out the sum.\nWhat if you wanted mean?\nWe could create a cnt Variable:\nawk \u0026#39;BEGIN{ sum=0;cnt=0; FS=\u0026#34;,\u0026#34;} { sum += $5; cnt+=1 } END { print sum/cnt }\u0026#39; data.txt 1.86436e+06  or better yet, use our friend NR which bash is already keeping track of:\nawk \u0026#39;BEGIN{ sum=0; FS=\u0026#34;,\u0026#34;} { sum += $5 } END { print sum/NR }\u0026#39; data.txt 1.86436e+06  Filter a file In the mean and sum awk commands we did not put any pattern in our middle commands. Let us use a simple pattern now. Suppose we have a file Salaries.csv which contains:\nhead salaries.txt yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000 1985,BAL,AL,lacyle01,725000 1985,BAL,AL,flanami01,641667 1985,BAL,AL,boddimi01,625000 1985,BAL,AL,stewasa01,581250 1985,BAL,AL,martide01,560000 1985,BAL,AL,roeniga01,558333  I want to filter records for players who who earn more than 22 M in 2013 just because I want to. You just do:\nawk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;} $5\u0026gt;=22000000 \u0026amp;\u0026amp; $1==2013{print $0}\u0026#39; Salaries.csv 2013,DET,AL,fieldpr01,23000000 2013,MIN,AL,mauerjo01,23000000 2013,NYA,AL,rodrial01,29000000 2013,NYA,AL,wellsve01,24642857 2013,NYA,AL,sabatcc01,24285714 2013,NYA,AL,teixema01,23125000 2013,PHI,NL,leecl02,25000000 2013,SFN,NL,linceti01,22250000  Cool right. Now let me explain it a little bit. The part in the command \u0026ldquo;$5\u0026gt;=22000000 \u0026amp;\u0026amp; $1==2013\u0026rdquo; is called a pattern. It says that print this line($0) if and only if the Salary($5) is more than 22M and(\u0026amp;\u0026amp;) year($1) is equal to 2013. If the incoming record(line) does not satisfy this pattern it never reaches the inner block.\nSo Now you could do basic Select SQL at the command line only if you had:\nThe logic Operators:\n  == equality operator; returns TRUE is both sides are equal\n  != inverse equality operator\n  \u0026amp;\u0026amp; logical AND\n  || logical OR\n  ! logical NOT\n  \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;= relational operators\n  Normal Arithmetic Operators: +, -, /, *, %, ^\nSome String Functions: length, substr, split\nGroupBy Now you will say: \u0026ldquo;Hey Dude SQL without groupby is incomplete\u0026rdquo;. You are right and for that we can use the associative array. Lets just see the command first and then I will explain. So lets create another useless use case(or may be something useful to someone :)) We want to find out the number of records for each year in the file. i.e we want to find the distribution of years in the file. Here is the command:\nawk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;} {my_array[$1]=my_array[$1]+1} END{ for (k in my_array){if(k!=\u0026#34;yearID\u0026#34;)print k\u0026#34;|\u0026#34;my_array[k]}; }\u0026#39; Salaries.csv 1990|867 1991|685 1996|931 1997|925 ...  Now I would like to tell you a secret. You don\u0026rsquo;t really need to declare the variables you want to use in awk. So you did not really needed to define sum, cnt variables before. I only did that because it is good practice. If you don\u0026rsquo;t declare a user defined variable in awk, awk assumes it to be null or zero depending on the context. So in the command above we don\u0026rsquo;t declare our myarray in the begin block and that is fine.\nAssociative Array: The variable myarray is actually an associative array. i.e. It stores data in a key value format.(Python dictionaries anyone). The same array could keep integer keys and String keys. For example, I can do this in a single code.\nmyarray[1]=\"key\" myarray['mlwhiz'] = 1   For Loop for associative arrays: I could use a for loop to read associative array\nfor (k in array) { DO SOMETHING } # Assigns to k each Key of array (unordered) # Element is array[k]   If Statement:Uses a syntax like C for the if statement. the else block is optional:\nif (n  0){ DO SOMETHING } else{ DO SOMETHING }   So lets dissect the above command now.\nI set the File separator to \u0026ldquo;,\u0026rdquo; in the beginning. I use the first column as the key of myarray. If the key exists I increment the value by 1.\nAt the end, I loop through all the keys and print out key value pairs separated by \u0026ldquo;|\u0026rdquo;\nI know that the header line in my file contains \u0026ldquo;yearID\u0026rdquo; in column 1 and I don\u0026rsquo;t want \u0026lsquo;yearID|1\u0026rsquo; in the output. So I only print when Key is not equal to \u0026lsquo;yearID\u0026rsquo;.\nGroupBy with case statement: cat Salaries.csv | awk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;} $5\u0026lt;100000{array5[\u0026#34;[0-100000)\u0026#34;]+=1} $5\u0026gt;=100000\u0026amp;\u0026amp;$5\u0026lt;250000{array5[\u0026#34;[100000,250000)\u0026#34;]=array5[\u0026#34;[100000,250000)\u0026#34;]+1} $5\u0026gt;=250000\u0026amp;\u0026amp;$5\u0026lt;500000{array5[\u0026#34;[250000-500000)\u0026#34;]=array5[\u0026#34;[250000-500000)\u0026#34;]+1} $5\u0026gt;=500000\u0026amp;\u0026amp;$5\u0026lt;1000000{array5[\u0026#34;[500000-1000000)\u0026#34;]=array5[\u0026#34;[500000-1000000)\u0026#34;]+1} $5\u0026gt;=1000000{array5[\u0026#34;[1000000)\u0026#34;]=array5[\u0026#34;[1000000)\u0026#34;]+1} END{ print \u0026#34;VAR Distrib:\u0026#34;; for (v in array5){print v\u0026#34;|\u0026#34;array5[v]} }\u0026#39; VAR Distrib: [250000-500000)|8326 [0-100000)|2 [1000000)|23661 [100000,250000)|9480  Here we used multiple pattern-action blocks to create a case statement.\nFor The Brave: This is a awk code that I wrote to calculate the Mean,Median,min,max and sum of a column simultaneously. Try to go through the code and understand it.I have added comments too. Think of this as an exercise. Try to run this code and play with it. You may learn some new tricks in the process. If you don\u0026rsquo;t understand it do not worry. Just get started writing your own awk codes, you will be able to understand it in very little time.\n# Create a New file named A.txt to keep only the salary column. cat Salaries.csv | cut -d \u0026#34;,\u0026#34; -f 5 \u0026gt; A.txt FILENAME=\u0026#34;A.txt\u0026#34; # The first awk counts the number of lines which are numeric. We use a regex here to check if the column is numeric or not. # \u0026#39;;\u0026#39; stands for Synchronous execution i.e sort only runs after the awk is over. # The output of both commands are given to awk command which does the whole work. # So Now the first line going to the second awk is the number of lines in the file which are numeric. # and from the second to the end line the file is sorted. (awk \u0026#39;BEGIN {c=0} $1 ~ /^[-0-9]*(\\.[0-9]*)?$/ {c=c+1;} END {print c;}\u0026#39; \u0026#34;$FILENAME\u0026#34;; \\  sort -n \u0026#34;$FILENAME\u0026#34;) | awk \u0026#39; BEGIN { c = 0; sum = 0; med1_loc = 0; med2_loc = 0; med1_val = 0; med2_val = 0; min = 0; max = 0; } NR==1 { LINES = $1 # We check whether numlines is even or odd so that we keep only # the locations in the array where the median might be. if (LINES%2==0) {med1_loc = LINES/2-1; med2_loc = med1_loc+1;} if (LINES%2!=0) {med1_loc = med2_loc = (LINES-1)/2;} } $1 ~ /^[-0-9]*(\\.[0-9]*)?$/ \u0026amp;\u0026amp; NR!=1 { # setting min value if (c==0) {min = $1;} # middle two values in array if (c==med1_loc) {med1_val = $1;} if (c==med2_loc) {med2_val = $1;} c++ sum += $1 max = $1 } END { ave = sum / c median = (med1_val + med2_val ) / 2 print \u0026#34;sum:\u0026#34; sum print \u0026#34;count:\u0026#34; c print \u0026#34;mean:\u0026#34; ave print \u0026#34;median:\u0026#34; median print \u0026#34;min:\u0026#34; min print \u0026#34;max:\u0026#34; max } \u0026#39; \u0026lt;pre style=\u0026#34;font-size:50%; padding:7px; margin:0em; background-color:#FFF112\u0026#34;\u0026gt;sum:44662539172 count:23956 mean:1.86436e+06 median:507950 min:0 max:33000000 \u0026lt;/pre\u0026gt; Endnote: awk is an awesome tool and there are a lot of use-cases where it can make your life simple. There is a sort of a learning curve, but I think that it would be worth it in the long term. I have tried to give you a taste of awk and I have covered a lot of ground here in this post. To tell you a bit more there, awk is a full programming language. There are for loops, while loops, conditionals, booleans, functions and everything else that you would expect from a programming language. So you could look more still.\nTo learn more about awk you can use this book . This book is a free resource and you could learn more about awk and use cases.\nOr if you like to have your book binded and in paper like me you can buy this book, which is a gem:\n \u0026lt;img src=\u0026#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8\u0026amp;amp;ASIN=1565922255\u0026amp;amp;Format=_SL250_\u0026amp;amp;ID=AsinImage\u0026amp;amp;MarketPlace=US\u0026amp;amp;ServiceVersion=20070822\u0026amp;amp;WS=1\u0026amp;amp;tag=mlwhizcon-20\u0026#34; alt=\u0026#34;MLWhiz: Data Science, Machine Learning, Artificial Intelligence\u0026#34;\u0026gt;  Do leave comments in case you find more use-cases for awk or if you want me to write on new use-cases. Or just comment weather you liked it or not and how I could improve as I am also new and trying to learn more of this.\nTill then Ciao !!!\n","permalink":"https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/","tags":["Machine Learning","Tools","Awesome Guides"],"title":"Shell Basics every Data Scientist Should know - Part II(AWK)"},{"categories":["Data Science"],"contents":"Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).\nConsider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.\nProbably whatever you do it won\u0026rsquo;t be simpler/less time consuming than this\ncat data.txt | cut -d \u0026#34;|\u0026#34; -f 1 | sort | uniq | wc -l 30  And this will run way faster than whatever you do with perl/python script.\nNow this command says\n Use the cat command to print/stream the contents of the file to stdout. Pipe the streaming contents from our cat command to the next command cut. The cut commands specifies the delimiter by the argument -d and the column by the argument -f and streams the output to stdout. Pipe the streaming content to the sort command which sorts the input and streams only the distinct values to the stdout. It takes the argument -u that specifies that we only need unique values. Pipe the output to the wc -l command which counts the number of lines in the input.  There is a lot going on here and I will try my best to ensure that you will be able to understand most of it by the end of this Blog post.Although I will also try to explain more advanced concepts than the above command in this post.\nNow, I use shell commands extensively at my job. I will try to explain the usage of each of the commands based on use cases that I counter nearly daily at may day job as a data scientist.\nSome Basic Commands in Shell: There are a lot of times when you just need to know a little bit about the data. You just want to see may be a couple of lines to inspect a file. One way of doing this is opening the txt/csv file in the notepad. And that is probably the best way for small files. But you could also do it in the shell using:\n1. cat cat data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  Now the cat command prints the whole file in the terminal window for you.I have not shown the whole file here.\nBut sometimes the files will be so big that you wont be able to open them up in notepad++ or any other software utility and there the cat command will shine.\n2. Head and Tail Now you might ask me why would you print the whole file in the terminal itself? Generally I won\u0026rsquo;t. But I just wanted to tell you about the cat command. For the use case when you want only the top/bottom n lines of your data you will generally use the head / tail commands. You can use them as below.\nhead data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  head -n 3 data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000  tail data.txt 2013|WAS|NL|bernaro01|1212500 2013|WAS|NL|tracych01|1000000 2013|WAS|NL|stammcr01|875000 2013|WAS|NL|dukeza01|700000 2013|WAS|NL|espinda01|526250 2013|WAS|NL|matthry01|504500 2013|WAS|NL|lombast02|501250 2013|WAS|NL|ramoswi01|501250 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  tail -n 2 data.txt 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  Notice the structure of the shell command here.\nCommandName [-arg1name] [arg1value] [-arg2name] [arg2value] filename   3. Piping Now we could have also written the same command as:\ncat data.txt | head yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  This brings me to one of the most important concepts of Shell usage - \u0026lt;strong\u0026gt;piping\u0026lt;/strong\u0026gt; . You won\u0026rsquo;t be able to utilize the full power the shell provides without using this concept. And the concept is actually simple.\nJust read the \u0026ldquo;|\u0026rdquo; in the command as \u0026ldquo;pass the data on to\u0026rdquo;\nSo I would read the above command as:\ncat(print) the whole data to stream, pass the data on to head so that it can just give me the first few lines only.\nSo did you understood what piping did? It is providing us a way to use our basic commands in a consecutive manner. There are a lot of commands that are fairly basic and it lets us use these basic commands in sequence to do some fairly non trivial things.\nNow let me tell you about a couple of more commands before I show you how we can chain them to do fairly advanced tasks.\n4. wc  wc is a fairly useful shell utility/command that lets us count the number of lines(-l), words(-w) or characters(-c) in a given file\nwc -l data.txt 23957 data.txt  5. grep You may want to print all the lines in your file which have a particular word. Or as a Data case you might like to see the salaries for the team BAL in 2000. In this case we have printed all the lines in the file which contain \u0026ldquo;2000|BAL\u0026rdquo;. grep is your friend.\ngrep \u0026#34;2000|BAL\u0026#34; data.txt | head 2000|BAL|AL|belleal01|12868670 2000|BAL|AL|anderbr01|7127199 2000|BAL|AL|mussimi01|6786032 2000|BAL|AL|ericksc01|6620921 2000|BAL|AL|ripkeca01|6300000 2000|BAL|AL|clarkwi02|6000000 2000|BAL|AL|johnsch04|4600000 2000|BAL|AL|timlimi01|4250000 2000|BAL|AL|deshide01|4209324 2000|BAL|AL|surhobj01|4146789  you could also use regular expressions with grep.\n6. sort You may want to sort your dataset on a particular column.Sort is your friend. Say you want to find out the top 10 maximum salaries given to any player in your dataset.\nsort -t \u0026#34;|\u0026#34; -k 5 -r -n data.txt | head -10 2010|NYA|AL|rodrial01|33000000 2009|NYA|AL|rodrial01|33000000 2011|NYA|AL|rodrial01|32000000 2012|NYA|AL|rodrial01|30000000 2013|NYA|AL|rodrial01|29000000 2008|NYA|AL|rodrial01|28000000 2011|LAA|AL|wellsve01|26187500 2005|NYA|AL|rodrial01|26000000 2013|PHI|NL|leecl02|25000000 2013|NYA|AL|wellsve01|24642857  So there are certainly a lot of options in this command. Lets go through them one by one.\n -t: Which delimiter to use? -k: Which column to sort on? -n: If you want Numerical Sorting. Dont use this option if you want Lexographical sorting. -r: I want to sort Descending. Sorts Ascending by Default.  7. cut This command lets you select certain columns from your data. Sometimes you may want to look at just some of the columns in your data. As in you may want to look only at the year, team and salary and not the other columns. cut is the command to use.\ncut -d \u0026#34;|\u0026#34; -f 1,2,5 data.txt | head yearID|teamID|salary 1985|BAL|1472819 1985|BAL|1090000 1985|BAL|800000 1985|BAL|725000 1985|BAL|641667 1985|BAL|625000 1985|BAL|581250 1985|BAL|560000 1985|BAL|558333  The options are:\n -d: Which delimiter to use? -f: Which column/columns to cut?  8. uniq  uniq is a little bit tricky as in you will want to use this command in sequence with sort. This command removes sequential duplicates. So in conjunction with sort it can be used to get the distinct values in the data. For example if I wanted to find out 10 distinct teamIDs in data, I would use:\ncat data.txt | cut -d \u0026#34;|\u0026#34; -f 2 | sort | uniq | head ANA ARI ATL BAL BOS CAL CHA CHN CIN CLE  This command could be used with argument -c to count the occurrence of these distinct values. Something akin to count distinct.\ncat data.txt | cut -d \u0026#34;|\u0026#34; -f 2 | sort | uniq -c | head 247 ANA 458 ARI 838 ATL 855 BAL 852 BOS 368 CAL 812 CHA 821 CHN 46 CIN 867 CLE  Some Other Utility Commands for Other Operations Some Other command line tools that you could use without going in the specifics as the specifics are pretty hard.\n1. Change delimiter in a file Find and Replace Magic.: You may want to replace certain characters in file with something else using the tr command.\ncat data.txt | tr \u0026#39;|\u0026#39; \u0026#39;,\u0026#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  or the \u0026lt;strong\u0026gt;sed\u0026lt;/strong\u0026gt; command\ncat data.txt | sed -e \u0026#39;s/|/,/g\u0026#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  2. Sum of a column in a file Using the awk command you could find the sum of column in file. Divide it by the number of lines and you can get the mean.\ncat data.txt | awk -F \u0026#34;|\u0026#34; \u0026#39;{ sum += $5 } END { printf sum }\u0026#39; 44662539172  awk is a powerful command which is sort of a whole language in itself. Do see the wiki page for awk for a lot of great usecases of awk. I also wrote a post on awk as a second part in this series. Check it HERE 3. Find the files in a directory that satisfy a certain condition You can do this by using the find command. Lets say you want to find all the .txt files in the current working dir that start with lowercase h.\nfind . -name \u0026#34;h*.txt\u0026#34; ./hamlet.txt  To find all .txt files starting with h regarless of case we could use regex.\nfind . -name \u0026#34;[Hh]*.txt\u0026#34; ./hamlet.txt ./Hamlet1.txt  4. Passing file list as Argument.  xargs was suggested by Gaurav in the comments, so I read about it and it is actually a very nice command which you could use in a variety of use cases.\nSo if you just use a pipe, any command/utility receives data on STDIN (the standard input stream) as a raw pile of data that it can sort through one line at a time. However some programs don\u0026rsquo;t accept their commands on standard in. For example the rm command(which is used to remove files), touch command(used to create file with a given name) or a certain python script you wrote(which takes command line arguments). They expect it to be spelled out in the arguments to the command.\nFor example: rm takes a file name as a parameter on the command line like so: rm file1.txt. If I wanted to delete all \u0026lsquo;.txt\u0026rsquo; files starting with \u0026ldquo;h/H\u0026rdquo; from my working directory, the below command won\u0026rsquo;t work because rm expects a file as an input.\nfind . -name \u0026#34;[hH]*.txt\u0026#34; | rm usage: rm [-f | -i] [-dPRrvW] file ... unlink file  To get around it we can use the xargs command which reads the STDIN stream data and converts each line into space separated arguments to the command.\nfind . -name \u0026#34;[hH]*.txt\u0026#34; | xargs ./hamlet.txt ./Hamlet1.txt  Now you could use rm to remove all .txt files that start with h/H. A word of advice: Always see the output of xargs first before using rm.\nfind . -name \u0026#34;[hH]*.txt\u0026#34; | xargs rm Another usage of xargs could be in conjunction with grep to find all files that contain a given string.\nfind . -name \u0026#34;*.txt\u0026#34; | xargs grep \u0026#39;honest soldier\u0026#39; ./Data1.txt:O, farewell, honest soldier; ./Data2.txt:O, farewell, honest soldier; ./Data3.txt:O, farewell, honest soldier;  Hopefully You could come up with varied uses building up on these examples. One other use case could be to use this for passing arguments to a python script.\nOther Cool Tricks Sometimes you want your data that you got by some command line utility(Shell commands/ Python scripts) not to be shown on stdout but stored in a textfile. You can use the \u0026quot;\u0026gt;\u0026quot; operator for that. For Example: You could have stored the file after replacing the delimiters in the previous example into anther file called newdata.txt as follows:\ncat data.txt | tr \u0026#39;|\u0026#39; \u0026#39;,\u0026#39; \u0026gt; newdata.txt I really got confused between \u0026quot;|\u0026quot; (piping) and \u0026quot;\u0026gt;\u0026quot; (to_file) operations a lot in the beginning. One way to remember is that you should only use \u0026quot;\u0026gt;\u0026quot; when you want to write something to a file. \u0026quot;|\u0026quot; cannot be used to write to a file. Another operation you should know about is the \u0026quot;\u0026raquo;\u0026quot; operation. It is analogous to \u0026quot;\u0026gt;\u0026quot; but it appends to an existing file rather that replacing the file and writing over.\nIf you would like to know more about commandline, which I guess you would, here are some books that I would recommend for a beginner:\n \u0026lt;img src=\u0026#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8\u0026amp;amp;ASIN=1593273894\u0026amp;amp;Format=_SL250_\u0026amp;amp;ID=AsinImage\u0026amp;amp;MarketPlace=US\u0026amp;amp;ServiceVersion=20070822\u0026amp;amp;WS=1\u0026amp;amp;tag=mlwhizcon-20\u0026#34; alt=\u0026#34;MLWhiz: Data Science, Machine Learning, Artificial Intelligence\u0026#34;\u0026gt;  \u0026lt;img src=\u0026#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8\u0026amp;amp;ASIN=0596009658\u0026amp;amp;Format=_SL250_\u0026amp;amp;ID=AsinImage\u0026amp;amp;MarketPlace=US\u0026amp;amp;ServiceVersion=20070822\u0026amp;amp;WS=1\u0026amp;amp;tag=mlwhizcon-20\u0026#34; alt=\u0026#34;MLWhiz: Data Science, Machine Learning, Artificial Intelligence\u0026#34;\u0026gt;  The first book is more of a fun read at leisure type of book. THe second book is a little more serious. Whatever suits you.\nSo, this is just the tip of the iceberg. Although I am not an expert in shell usage, these commands reduced my workload to a large extent. If there are some shell commands you use on a regular basis or some shell command that are cool, do tell in the comments. I would love to include it in the blogpost.\nI wrote a blogpost on awk as a second part of this post. Check it Here\n","permalink":"https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/","tags":["Machine Learning","Tools","Awesome Guides"],"title":"Shell Basics every Data Scientist Should know -Part I"},{"categories":["Data Science","Awesome Guides"],"contents":"When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.\nNow as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph. This led me to think that a Blog post to create common Graph types in Python is in order. But being the procrastinator that I am it always got pushed to the back of my head.\nOne thing that helped me in pursuit of my data visualization needs in Python was this awesome course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Highly Recommended.\nBut, yesterday I got introduced to Seaborn and I must say I am quite impressed with it. It makes beautiful graphs that are in my opinion better than R\u0026rsquo;s ggplot2. Gives you enough options to customize and the best part is that it is so easy to learn.\nSo I am finally writing this blog post with a basic purpose of creating a code base that provides me with ready to use codes which could be put into analysis in a fairly straight-forward manner.\nRight. So here Goes.\nWe Start by importing the libraries that we will need to use.\nimport matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes To create a use case for our graphs, we will be working with the Tips data that contains the following information.\ntips = sns.load_dataset(\u0026#34;tips\u0026#34;) tips.head()   Scatterplot With Regression Line Now let us work on visualizing this data. We will use the regplot option in seaborn.\n# We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=\u0026#34;ticks\u0026#34;) # Here we create a matplotlib axes object. The extra parameters we use # \u0026#34;ci\u0026#34; to remove confidence interval # \u0026#34;marker\u0026#34; to have a x as marker. # \u0026#34;scatter_kws\u0026#34; to provide style info for the points.[s for size] # \u0026#34;line_kws\u0026#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=\u0026#34;tip\u0026#34;, y=\u0026#34;total_bill\u0026#34;, data=tips, ci = False, scatter_kws={\u0026#34;color\u0026#34;:\u0026#34;darkred\u0026#34;,\u0026#34;alpha\u0026#34;:0.3,\u0026#34;s\u0026#34;:90}, line_kws={\u0026#34;color\u0026#34;:\u0026#34;g\u0026#34;,\u0026#34;alpha\u0026#34;:0.5,\u0026#34;lw\u0026#34;:4},marker=\u0026#34;x\u0026#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(\u0026#39;Total Bill vs. Tip\u0026#39;, fontsize=34,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(\u0026#34;Tip\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(\u0026#34;Total Bill\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   Now that required a bit of a code but i feel that it looks much better than what either Matplotlib or ggPlot2 could have rendered. We got a lot of customization without too much code.\nBut that is not really what actually made me like Seaborn. The plot type that actually got my attention was lmplot, which lets us use regplot in a faceted mode.\n# So this function creates a faceted plot. The plot is parameterized by the following: # col : divides the data points into days and creates that many plots # palette: deep, muted, pastel, bright, dark, and colorblind. change the colors in graph. Experiment with these # col_wrap: we want 2 graphs in a row? Yes.We do # scatter_kws: attributes for points # hue: Colors on a particular column. # size: controls the size of graph g = sns.lmplot(x=\u0026#34;tip\u0026#34;, y=\u0026#34;total_bill\u0026#34;,ci=None,data=tips, col=\u0026#34;day\u0026#34;, palette=\u0026#34;muted\u0026#34;,col_wrap=2,scatter_kws={\u0026#34;s\u0026#34;: 100,\u0026#34;alpha\u0026#34;:.5}, line_kws={\u0026#34;lw\u0026#34;:4,\u0026#34;alpha\u0026#34;:0.5},hue=\u0026#34;day\u0026#34;,x_jitter=1.0,y_jitter=1.0,size=6) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(\u0026#39;Total Bill vs. Tip\u0026#39;, fontsize=34,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabels(\u0026#34;Tip\u0026#34;,size = 50,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabels(\u0026#34;Total Bill\u0026#34;,size = 50,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here titles = [\u0026#39;Thursday\u0026#39;,\u0026#39;Friday\u0026#39;,\u0026#39;Saturday\u0026#39;,\u0026#39;Sunday\u0026#39;] for ax,title in zip(g.axes.flat,titles): ax.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   A side Note on Palettes:\nYou can build your own color palettes using color_palette() function. color_palette() will accept the name of any seaborn palette or matplotlib colormap(except jet, which you should never use). It can also take a list of colors specified in any valid matplotlib format (RGB tuples, hex color codes, or HTML color names). The return value is always a list of RGB tuples. This allows you to use your own color palettes in graph.   Barplots sns.set(style=\u0026#34;ticks\u0026#34;) flatui = [\u0026#34;#9b59b6\u0026#34;, \u0026#34;#3498db\u0026#34;, \u0026#34;#95a5a6\u0026#34;, \u0026#34;#e74c3c\u0026#34;, \u0026#34;#34495e\u0026#34;, \u0026#34;#2ecc71\u0026#34;] # This Function takes as input a custom palette g = sns.barplot(x=\u0026#34;sex\u0026#34;, y=\u0026#34;tip\u0026#34;, hue=\u0026#34;day\u0026#34;, palette=sns.color_palette(flatui),data=tips,ci=None) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(\u0026#39;Do We tend to \\nTip high on Weekends?\u0026#39;, fontsize=34,color=\u0026#34;b\u0026#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(\u0026#34;Gender\u0026#34;,size = 67,color=\u0026#34;g\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(\u0026#34;Mean Tips\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   Histograms and Distribution Diagrams They form another part of my workflow. Lets plot the normal Histogram using seaborn. For this we will use the distplot function. This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() function. It can also fit scipy.stats distributions and plot the estimated PDF over the data.\n# Create a list of 1000 Normal RVs x = np.random.normal(size=1000) sns.set_context(\u0026#34;poster\u0026#34;) sns.set_style(\u0026#34;ticks\u0026#34;) # This Function creates a normed Histogram by default. # If we use the parameter kde=False and norm_hist=False then # we will be using a count histogram g=sns.distplot(x, kde_kws={\u0026#34;color\u0026#34;:\u0026#34;g\u0026#34;,\u0026#34;lw\u0026#34;:4,\u0026#34;label\u0026#34;:\u0026#34;KDE Estim\u0026#34;,\u0026#34;alpha\u0026#34;:0.5}, hist_kws={\u0026#34;color\u0026#34;:\u0026#34;r\u0026#34;,\u0026#34;alpha\u0026#34;:0.3,\u0026#34;label\u0026#34;:\u0026#34;Freq\u0026#34;}) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(\u0026#39;Normal Simulation\u0026#39;, fontsize=34,color=\u0026#34;b\u0026#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(\u0026#34;X\u0026#34;,size = 67,color=\u0026#34;g\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(\u0026#34;Density\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   import scipy.stats as stats a = 1.5 b = 1.5 x = np.arange(0.01, 1, 0.01) y = stats.beta.rvs(a,b,size=10000) y_act = stats.beta.pdf(x,a,b) g=sns.distplot(y,kde=False,norm_hist=True, kde_kws={\u0026#34;color\u0026#34;:\u0026#34;g\u0026#34;,\u0026#34;lw\u0026#34;:4,\u0026#34;label\u0026#34;:\u0026#34;KDE Estim\u0026#34;,\u0026#34;alpha\u0026#34;:0.5}, hist_kws={\u0026#34;color\u0026#34;:\u0026#34;r\u0026#34;,\u0026#34;alpha\u0026#34;:0.3,\u0026#34;label\u0026#34;:\u0026#34;Freq\u0026#34;}) # Note that we plotted on the graph using plt matlabplot function plt.plot(x,y_act) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title((\u0026#34;Beta Simulation vs. Calculated Beta Density\\nFor a=%s,b=%s\u0026#34;) %(a,b),fontsize=34,color=\u0026#34;b\u0026#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(\u0026#34;X\u0026#34;,size = 67,color=\u0026#34;g\u0026#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(\u0026#34;Density\u0026#34;,size = 67,color=\u0026#34;r\u0026#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=\u0026#34;black\u0026#34;)   PairPlots You need to see how variables vary with one another. What is the distribution of variables in the dataset. This is the graph to use with the pairplot function. Very helpful And Seaborn males it a joy to use. We will use Iris Dataset here for this example.\niris = sns.load_dataset(\u0026#34;iris\u0026#34;) iris.head()   # Create a Pairplot g = sns.pairplot(iris,hue=\u0026#34;species\u0026#34;,palette=\u0026#34;muted\u0026#34;,size=5, vars=[\u0026#34;sepal_width\u0026#34;, \u0026#34;sepal_length\u0026#34;],kind=\u0026#39;reg\u0026#39;,markers=[\u0026#39;o\u0026#39;,\u0026#39;x\u0026#39;,\u0026#39;+\u0026#39;]) # To change the size of the scatterpoints in graph g = g.map_offdiag(plt.scatter, s=35,alpha=0.5) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(\u0026#39;Relation between Sepal Width and Sepal Length\u0026#39;, fontsize=34,color=\u0026#34;b\u0026#34;,alpha=0.3)   Hope you found this post useful and worth your time. You can find the iPython notebook at github\nI tried to make this as simple as possible but You may always ask me or see the documentation for doubts.\nIf you have any more ideas on how to use Seaborn or which graphs should i add here, please suggest in the comments section.\nI will definitely try to add to this post as I start using more visualizations and encounter other libraries as good as seaborn.\nAlso since this is my first visualization post on this blog, I would like to call out a good course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.\n","permalink":"https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/","tags":["Visualization","Python","Data Science","Machine Learning","Best Content"],"title":"Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet"},{"categories":["Big Data","Data Science","Awesome Guides"],"contents":"I generally have a use case for Hadoop in my daily job. It has made my life easier in a sense that I am able to get results which I was not able to see with SQL queries. But still I find it painfully slow. I have to write procedural programs while I work. As in merge these two datasets and then filter and then merge another dataset and then filter using some condition and yada-yada. You get the gist. And in hadoop its painstakingly boring to do this. You have to write more than maybe 3 Mapreduce Jobs. One job will read the data line by line and write to the disk.\nThere is a lot of data movement that happens in between that further affects the speed. Another thing I hate is that there is no straight way to pass files to mappers and reducers and that generally adds up another mapreduce job to the whole sequence.\nAnd that is just procedural tasks. To implement an iterative algorithm even after geting the whole logic of parallelization is again a challenge. There would be a lot of mapreduce tasks, a shell based driver program and a lot of unique thinking to bring everything together. And the running times are like crazy. Though sometimes it has its benefits:\n  That makes me think about the whole way Hadoop is implemented. While at the time Hadoop appeared the RAM was costly. Now that is not the case. We already have 64GB machines in our Hadoop cluster. So is it really a good idea to not use a larger chunk of memory and read line by line. Also can we have something that allows us to keep a particular piece of data in the memory, So that the next time our program needs it it doesnt have to read it again and waste time. Wouldnt it be better if we have some variable that lets us keep the state our iterative algorithm is in.\nThe Solution? And here is where Spark comes to rescue. Now working on Spark is very different from Hadoop but when you start using it you find that it makes things so much easier. You still do have to think in the mapreduce way sort of but the way the map and reduce steps are done are a little bit different.\nSo lets first get Spark on our System (But keep in mind that for running spark in production environments you will need whole clusters set up. A liberty which you may or may not have at present)\nThe best way that I found to install Spark is following the Apache Spark installation guidelines with the Apache Spark eDx course. It lets you get Spark in your system and work with Spark with iPython notebooks. Something I prefer a lot and find the best way to code in Python.\nThe installation instructions can be found HERE. You may have to login in to an edX account to follow these instructions, but it is worth it.\nSo once you have gone through all the steps mentioned there and installed spark using these instructions, you would see something like this in your browser.\n  Ahh! so you have got Spark up and running now. That\u0026rsquo;s actually like half the process. I like to learn by examples so let\u0026rsquo;s get done with the \u0026ldquo;Hello World\u0026rdquo; of Distributed computing: The WordCount Program.\nlines = sc.textFile(\u0026#34;shakespeare.txt\u0026#34;) # Distribute the data - Create a RDD counts = (lines.flatMap(lambda x: x.split(\u0026#39; \u0026#39;)) # Create a list with all words .map(lambda x: (x, 1)) # Create tuple (word,1) .reduceByKey(lambda x,y : x + y)) # reduce by key i.e. the word output = counts.take(10) # get the output on local for (word, count) in output: # print output print(\u0026#34;%s: %i\u0026#34; % (word, count))   So that is a small example. Pretty small code when you compare it with Hadoop. And most of the work gets done in the second command. Don\u0026rsquo;t worry if you are not able to follow this yet as I need to tell you about the things that make Spark work.\nBut before we get into Spark basics, Let us refresh some of our python Basics. Understanding Spark becomes a lot easier if you have used Lambda functions in Python.\nFor those of you who haven\u0026rsquo;t used it, below is a brief intro.\nLambda Functions in Python Map Map is used to map a function to a array or a list. Say you want to apply some function to every element in a list. You can do this by simply using a for loop but python lambda functions let you do this in a single line in Python.\nmy_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(lambda x:x**2,my_list) print squared_list [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nIn the above example you could think of map as a function which takes two arguments - A function and a list. It then applies the function to every element of the list. What lambda allows you to do is write an inline function. In here the part \u0026ldquo;lambda x:x**2\u0026rdquo; defines a function that takes x as input and returns x^2.\nYou could have also provided a proper function in place of lambda. For Example:\ndef squared(x): return x**2 \u0026lt;br\u0026gt;my_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(squared,my_list) print squared_list [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nThe same result, but the lambda expressions make the code compact and a lot more readable.\nFilter The other function that is used extensively is the filter function. This function takes two arguments - A condition and the list to filter. If you want to filter your list using some condition you use filter.\nmy_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want only the even numbers in my list. filtered_list = filter(lambda x:x%2==0,my_list) print filtered_list [2, 4, 6, 8, 10]\nReduce The next function is the reduce function. This function will be the workhorse in Spark. This function takes two arguments - a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.\nmy_list = [1,2,3,4,5] # Lets say I want to sum all elements in my list. sum_list = reduce(lambda x,y:x+y,my_list) print sum_list 15\nHere the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:\nReduce function first sends 1,2 ; the lambda function returns 3 Reduce function then sends 3,3 ; the lambda function returns 6 Reduce function then sends 6,4 ; the lambda function returns 10 Reduce function finally sends 10,5 ; the lambda function returns 15  A condition on the lambda function we use in reduce is that it must be commutative that is a + b = b + a and associative that is (a + b) + c == a + (b + c). In the above case we used sum which is commutative as well as associative. Other functions that we could have used are max, min, multiplication etc.\nMoving Again to Spark As we have now got the fundamentals of Python Functional Programming out of the way, lets again head to Spark.\nBut first let us delve a little bit into how spark works. Spark actually consists of two things a driver and workers. Workers normally do all the work and the driver makes them do that work.\nAn RDD is defined a parallelized data structure that gets distributed across the worker nodes. In our wordcount example, in the first line\nlines = sc.textFile(\u0026quot;data/cs100/lab1/shakespeare.txt\u0026quot;)  We took a text file and distributed it across worker nodes so that they can work on it in parallel. We could also parallelize lists using the function\nsc.parallelize  For example:\ndata = [1,2,3,4,5,6,7,8,9,10] new_rdd = sc.parallelize(data,4) new_rdd ParallelCollectionRDD[15] at parallelize at PythonRDD.scala:392\nIn Spark we classify the operations into two Basic Types: Transformations and Actions.\n  Transformations : Create new datasets from existing RDDs\n  Actions : Mechanism to get results out of Spark\n  Understanding Transformations So lets say you have got your data in the form of an RDD. To requote your data is now accesible b all the worker machines. You want to do some transformations on the data now. You may want to filter, Apply some function etc. In Spark this is done using Transformation functions. Spark provides many transformation functions. You can see a comprehensive list here . Some of the main ones that I use frequently are:\n1. Map: Applies a given function to an RDD. Note that the syntax is a little bit different from python, but it necessarily does the same thing. Don\u0026rsquo;t worry about collet yet. For now just think of it as a function that collects the data in squared_rdd back to a list.\ndata = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) squared_rdd = rdd.map(lambda x:x**2) squared_rdd.collect() [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n2. Filter: Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.\ndata = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) filtered_rdd = rdd.filter(lambda x:x%2==0) filtered_rdd.collect() [2, 4, 6, 8, 10]\n3. Distinct: Returns only distinct elements in an RDD\ndata = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10] rdd = sc.parallelize(data,4) distinct_rdd = rdd.distinct() distinct_rdd.collect() [8, 4, 1, 5, 9, 2, 10, 6, 3, 7]\n4. Flatmap: Similar to map, but each input item can be mapped to 0 or more output items\ndata = [1,2,3,4] rdd = sc.parallelize(data,4) flat_rdd = rdd.flatMap(lambda x:[x,x**3]) flat_rdd.collect() [1, 1, 2, 8, 3, 27, 4, 64]\n5. Reduce By Key: The analogue to the reduce in Hadoop Mapreduce. Now Spark cannot provide the value if it just worked with Lists. In Spark there is a concept of pair RDDs that makes it a lot more flexible. Lets assume we have a data in which we have product, its category and its selling price. We can still parallelize the data.\ndata = [(\u0026#39;Apple\u0026#39;,\u0026#39;Fruit\u0026#39;,200),(\u0026#39;Banana\u0026#39;,\u0026#39;Fruit\u0026#39;,24),(\u0026#39;Tomato\u0026#39;,\u0026#39;Fruit\u0026#39;,56),(\u0026#39;Potato\u0026#39;,\u0026#39;Vegetable\u0026#39;,103),(\u0026#39;Carrot\u0026#39;,\u0026#39;Vegetable\u0026#39;,34)] rdd = sc.parallelize(data,4) Right now our RDD rdd holds tuples. Now we want to find out the total sum of revenue that we got from each category. To do that we have to transform our rdd to a pair rdd so that it only contatins key-value pairs/tuples.\ncategory_price_rdd = rdd.map(lambda x: (x[1],x[2])) category_price_rdd.collect() [(\u0026lsquo;Fruit\u0026rsquo;, 200), (\u0026lsquo;Fruit\u0026rsquo;, 24), (\u0026lsquo;Fruit\u0026rsquo;, 56), (\u0026lsquo;Vegetable\u0026rsquo;, 103), (\u0026lsquo;Vegetable\u0026rsquo;, 34)]\nHere we used the map function to get it in the format we wanted. When working with textfile, the rdd that gets formed has got a lot of strings. We use map to convert it into a format that we want.\nSo now our category_price_rdd contains the product category and the price at which the prouct sold. Now we want to reduce on the key and sum the prices. We can do this by:\ncategory_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y) category_total_price_rdd.collect() [(\u0026lsquo;Vegetable\u0026rsquo;, 137), (\u0026lsquo;Fruit\u0026rsquo;, 280)]\n6. Group By Key: Similar to reduce by key but does not reduce just puts all the elements in an iterator. For example if we wanted to keep as key the category and as the value all the products we would use this function.\ndata = [(\u0026#39;Apple\u0026#39;,\u0026#39;Fruit\u0026#39;,200),(\u0026#39;Banana\u0026#39;,\u0026#39;Fruit\u0026#39;,24),(\u0026#39;Tomato\u0026#39;,\u0026#39;Fruit\u0026#39;,56),(\u0026#39;Potato\u0026#39;,\u0026#39;Vegetable\u0026#39;,103),(\u0026#39;Carrot\u0026#39;,\u0026#39;Vegetable\u0026#39;,34)] rdd = sc.parallelize(data,4) category_product_rdd = rdd.map(lambda x: (x[1],x[0])) category_product_rdd.collect() [(\u0026lsquo;Fruit\u0026rsquo;,\u0026lsquo;Apple\u0026rsquo;),(\u0026lsquo;Fruit\u0026rsquo;,\u0026lsquo;Banana\u0026rsquo;),(\u0026lsquo;Fruit\u0026rsquo;,\u0026lsquo;Tomato\u0026rsquo;),(\u0026lsquo;Vegetable\u0026rsquo;,\u0026lsquo;Potato\u0026rsquo;),(\u0026lsquo;Vegetable\u0026rsquo;,\u0026lsquo;Carrot\u0026rsquo;)]\ngrouped_products_by_category_rdd = category_product_rdd.groupByKey() findata = grouped_products_by_category_rdd.collect() for data in findata: print data[0],list(data[1]) Vegetable [\u0026lsquo;Potato\u0026rsquo;, \u0026lsquo;Carrot\u0026rsquo;]\nFruit [\u0026lsquo;Apple\u0026rsquo;, \u0026lsquo;Banana\u0026rsquo;, \u0026lsquo;Tomato\u0026rsquo;]\nHere the grouped by function worked and it returned the category and the list of products in that category.\nUnderstanding Actions Now you have filtered your data, mapped some functions on it. Done your computation. Now you want to get the data on your local machine or save it to a file. You will have to use actions for that. A comprehensive list of actions is provided HERE\nSome of the most common actions that I tend to use are:\n1. Collect: We have already used this actio many times. It takes the whole rdd and brings it back to the driver program.\n2. Reduce: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\nrdd = sc.parallelize([1,2,3,4,5]) rdd.reduce(lambda x,y : x+y) 15\n3.take: Return an list with the first n elements of the dataset.\nrdd = sc.parallelize([1,2,3,4,5]) rdd.take(3) [1, 2, 3]\n4. takeOrdered: Return the first n elements of the RDD using either their natural order or a custom comparator.\nrdd = sc.parallelize([5,3,12,23]) rdd.takeOrdered(3,lambda s:-1*s) # descending order [23, 12, 5]\nrdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)]) rdd.takeOrdered(3,lambda s:-1*s[1]) # descending order [(12, 344), (3, 34), (23, 29)]\nSo now lets take a look at the Wordcount Again\nUnderstanding The WordCount Example Now we sort of understand the transformations and the actions provided to us by Spark. It should not be difficult to understand the work count program now. Lets go through the program niw line by line.\nThe first lines creates a RDD and distributeds to the workers.\nlines = sc.textFile(\u0026quot;data/cs100/lab1/shakespeare.txt\u0026quot;)  This RDD lines contains a list of strings that are actually the line in file. This RDD is of the form:\n['word1 word2 word3','word4 word3 word2']  This next line is actually the workhorse function in the whole script.\ncounts = (lines.flatMap(lambda x: x.split(' ')) .map(lambda x: (x, 1)) .reduceByKey(lambda x,y : x + y))  It contains a series of transformations that we do to the lines RDD. First of all we do a flatmap transformation. The flatmap transformation takes as input the lines and gives words as output. So after the flatmap transformation the RDD is of the form:\n['word1','word2','word3','word4','word3','word2']  Next we do a map transformation on the flatmap output which converts the rdd to :\n[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]  Finally we do a reduceByKey transformation which counts the number of time each word appeared. After which the rdd approaches the final desirable form.\n[('word1',1),('word2',2),('word3',2),('word4',1)]  This next line is an action that takes the first 10 elements of the resulting RDD locally.\noutput = counts.take(10)  This line just prints the output\nfor (word, count) in output: print(\u0026quot;%s: %i\u0026quot; % (word, count))  Getting Serious So till now we have talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don\u0026rsquo;t do wordcount in real life. We have to work on bigger problems which are much more complex. Worry not! whatever we have learned till now will let us do that and more.\nLets work with a concrete example: I will work on an example in which Greg Rada Worked on Movielens Data with Pandas (BTW a great resource to learn Pandas). This example takes care of every sort of transformation that you may like to do with this data.\nSo lets first talk about the dataset. The movielens dataset contains a lot of files but we are going to be working with 3 files only:\n  Users: This file name is kept as \u0026ldquo;u.user\u0026rdquo;, The columns in this file are:\n[\u0026lsquo;user_id\u0026rsquo;, \u0026lsquo;age\u0026rsquo;, \u0026lsquo;sex\u0026rsquo;, \u0026lsquo;occupation\u0026rsquo;, \u0026lsquo;zip_code\u0026rsquo;]\n  Ratings: This file name is kept as \u0026ldquo;u.data\u0026rdquo;, The columns in this file are:\n[\u0026lsquo;user_id\u0026rsquo;, \u0026lsquo;movie_id\u0026rsquo;, \u0026lsquo;rating\u0026rsquo;, \u0026lsquo;unix_timestamp\u0026rsquo;]\n  Movies: This file name is kept as \u0026ldquo;u.item\u0026rdquo;, The columns in this file are:\n[\u0026lsquo;movie_id\u0026rsquo;, \u0026lsquo;title\u0026rsquo;, \u0026lsquo;release_date\u0026rsquo;, \u0026lsquo;video_release_date\u0026rsquo;, \u0026lsquo;imdb_url\u0026rsquo;, and 18 more columns\u0026hellip;..]\n  ##What are the 25 most rated movies? First of all lets load the data in different rdds. And see what the data contains.\nuserRDD = sc.textFile(\u0026#34;/vagrant/ml-100k/u.user\u0026#34;) ratingRDD = sc.textFile(\u0026#34;/vagrant/ml-100k/u.data\u0026#34;) movieRDD = sc.textFile(\u0026#34;/vagrant/ml-100k/u.item\u0026#34;) print \u0026#34;userRDD:\u0026#34;,userRDD.take(1) print \u0026#34;ratingRDD:\u0026#34;,ratingRDD.take(1) print \u0026#34;movieRDD:\u0026#34;,movieRDD.take(1)   Seeing the data we note that to answer this question we will need to use the ratingRdd. But the ratingRDD does not have movie name. So we would have to merge movieRDD and ratingRDD. So lets see how we would do that in Spark. Lets first do it step by step.Read the comments.\n# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating. RDD_movid_rating = ratingRDD.map(lambda x : (x.split(\u0026#34;\\t\u0026#34;)[1],x.split(\u0026#34;\\t\u0026#34;)[2])) print \u0026#34;RDD_movid_rating:\u0026#34;,RDD_movid_rating.take(4) # Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title. RDD_movid_title = movieRDD.map(lambda x : (x.split(\u0026#34;|\u0026#34;)[0],x.split(\u0026#34;|\u0026#34;)[1])) print \u0026#34;RDD_movid_title:\u0026#34;,RDD_movid_title.take(2) # merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin() rdd_movid_title_rating = RDD_movid_rating.leftOuterJoin(RDD_movid_title) print \u0026#34;rdd_movid_title_rating:\u0026#34;,rdd_movid_title_rating.take(1) # use the RDD in previous step to create (movie,1) tuple pair RDD rdd_title_rating = rdd_movid_title_rating.map(lambda x: (x[1][1],1 )) print \u0026#34;rdd_title_rating:\u0026#34;,rdd_title_rating.take(2) # Use the reduceByKey transformation to reduce on the basis of movie_title rdd_title_ratingcnt = rdd_title_rating.reduceByKey(lambda x,y: x+y) print \u0026#34;rdd_title_ratingcnt:\u0026#34;,rdd_title_ratingcnt.take(2) # Get the final answer by using takeOrdered Transformation print \u0026#34;#####################################\u0026#34; print \u0026#34;25 most rated movies:\u0026#34;,rdd_title_ratingcnt.takeOrdered(25,lambda x:-x[1]) print \u0026#34;#####################################\u0026#34;   We could have done all this in a single command using the below command but the code is a little messy now. I did this to show that you can do things sequentially with Spark and you could bypass the process of variable creation.\nprint (((ratingRDD.map(lambda x : (x.split(\u0026#34;\\t\u0026#34;)[1],x.split(\u0026#34;\\t\u0026#34;)[2]))). leftOuterJoin(movieRDD.map(lambda x : (x.split(\u0026#34;|\u0026#34;)[0],x.split(\u0026#34;|\u0026#34;)[1])))). map(lambda x: (x[1][1],1)). reduceByKey(lambda x,y: x+y). takeOrdered(25,lambda x:-x[1])) \u0026lt;div style=\u0026#34;margin-top: 9px; margin-bottom: 10px;\u0026#34;\u0026gt; \u0026lt;center\u0026gt;\u0026lt;img src=\u0026#34;/images/result_rating_cnt_25_2.png\u0026#34;\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;/div\u0026gt; ##Which movies are most highly rated? Now we want to find the most highly rated 25 movvies using the same dataset. We actually want only those movies which have been rated atleast 100 times. Lets do this using Spark:\n# We already have the RDD rdd_movid_title_rating: [(u\u0026#39;429\u0026#39;, (u\u0026#39;5\u0026#39;, u\u0026#39;Day the Earth Stood Still, The (1951)\u0026#39;))] # We create an RDD that contains sum of all the ratings for a particular movie rdd_title_ratingsum = (rdd_movid_title_rating. map(lambda x: (x[1][1],int(x[1][0]))). reduceByKey(lambda x,y:x+y)) print \u0026#34;rdd_title_ratingsum:\u0026#34;,rdd_title_ratingsum.take(2) # Merge this data with the RDD rdd_title_ratingcnt we created in the last step # And use Map function to divide ratingsum by rating count. rdd_title_ratingmean_rating_count = (rdd_title_ratingsum. leftOuterJoin(rdd_title_ratingcnt). map(lambda x:(x[0],(float(x[1][0])/x[1][1],x[1][1])))) print \u0026#34;rdd_title_ratingmean_rating_count:\u0026#34;,rdd_title_ratingmean_rating_count.take(1) # We could use take ordered here only but we want to only get the movies which have count # of ratings more than or equal to 100 so lets filter the data RDD. rdd_title_rating_rating_count_gt_100 = (rdd_title_ratingmean_rating_count. filter(lambda x: x[1][1]\u0026gt;=100)) print \u0026#34;rdd_title_rating_rating_count_gt_100:\u0026#34;,rdd_title_rating_rating_count_gt_100.take(1) # Get the final answer by using takeOrdered Transformation print \u0026#34;#####################################\u0026#34; print \u0026#34;25 highly rated movies:\u0026#34;, print rdd_title_rating_rating_count_gt_100.takeOrdered(25,lambda x:-x[1][0]) print \u0026#34;#####################################\u0026#34;   Conclusion So Spark has Already provided an interface where we could apply transformations sequentially much easily than Hadoop. And it is fast. While in hadoop things are a pain to do sequentially, the infrastructure that Spark provides seem to fit naturally into the analytics use case.\nHopefully I\u0026rsquo;ve covered the basics well enough to pique your interest and help you get started with Spark. If I\u0026rsquo;ve missed something critical, feel free to let me know on Twitter or in the comments - I\u0026rsquo;d love constructive feedback.\nYou can find the Jupyter notebook HERE One of the newest and best resources that you can keep an eye on is the Introduction to Big Data course in the Big Data Specialization from UCSanDiego\nLook out for these two books to learn more about Spark.\n  The first one of these is a bestseller. It presents 9 case studies of data analysis applications in various domains. The topics are diverse and the authors always use real world datasets. Beside learning Spark and a data science you will also have the opportunity to gain insight about topics like taxi traffic in NYC, deforestation or neuroscience. The second one is more of a reference that takes the reader on a tour of the Spark fundamentals, explaining the RDD data model in detail, after which it dives into the main functionality of Spark: Spark SQL, Spark Streaming, MLLib, SparkML, and GraphX. Later on, it covers the operational aspects of setting up a standalone Spark cluster, as well as running it on YARN and Mesos.\n ","permalink":"https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/","tags":["Big Data","Machine Learning","Python"],"title":"Learning Spark using Python: Basics and Applications"},{"categories":["Data Science","Awesome Guides"],"contents":"  Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don\u0026rsquo;t know the normalizing constant.\nSo MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don\u0026rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.\nSo here goes. This time we will cover some applications of MCMC in various areas of Computer Science using Python. If you feel the problems difficult to follow with, I would advice you to go back and read the previous post , which tries to explain MCMC Methods. We Will try to solve the following two problems:\n  Breaking the Code - This problem has got somewhat of a great pedigree as this method was suggested by Persi Diaconis- The Mathemagician. So Someone comes to you with the below text. This text looks like gibberish but this is a code, Could you decrypyt it?\nXZ STAVRK HXVR MYAZ OAKZM JKSSO SO MYR OKRR XDP JKSJRK XBMASD SO YAZ TWDHZ MYR JXMBYNSKF BSVRKTRM NYABY NXZ BXKRTRZZTQ OTWDH SVRK MYR AKSD ERPZMRXP KWZMTRP MYR JXTR OXBR SO X QSWDH NSIXD NXZ KXAZRP ORRETQ OKSI MYR JATTSN XDP X OXADM VSABR AIJRKORBMTQ XKMABWTXMRP MYR NSKPZ TRM IR ZRR MYR BYATP XDP PAR MYR ZWKHRSD YXP ERRD ZAMMADH NAMY YAZ OXBR MWKDRP MSNXKPZ MYR OAKR HAVADH MYR JXTIZ SO YAZ YXDPZ X NXKI XDP X KWE XTMRKDXMRTQ XZ MYR QSWDH NSIXD ZJSFR YR KSZR XDP XPVXDBADH MS MYR ERP Z YRXP ZXAP NAMY ISKR FADPDRZZ MYXD IAHYM YXVR ERRD RGJRBMRP SO YAI\n  The Knapsack Problem - This problem comes from Introduction to Probability by Joseph Blitzstein. You should check out his courses STAT110 and CS109 as they are awesome. Also as it turns out Diaconis was the advisor of Joseph. So you have Bilbo a Thief who goes to Smaug\u0026rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution? This is known as the Knapsack Problem in Computer Science.\n  Breaking the Code   So we look at the data and form a hypothesis that the data has been scrambled using a Substitution Cipher. We don\u0026rsquo;t know the encryption key, and we would like to know the Decryption Key so that we can decrypt the data and read the code.\nTo create this example, this data has actually been taken from Oliver Twist. We scrambled the data using a random encryption key, which we forgot after encrypting and we would like to decrypt this encrypted text using MCMC Chains. The real decryption key actually is \u0026ldquo;ICZNBKXGMPRQTWFDYEOLJVUAHS\u0026rdquo;\nSo lets think about this problem for a little bit. The decryption key could be any 26 letter string with all alphabets appearing exactly once. How many string permutations are there like that? That number would come out to be $26! \\approx 10^{26}$ permutations. That is a pretty large number. If we go for using a brute force approach we are screwed. So what could we do? MCMC Chains come to rescue.\nWe will devise a Chain whose states theoritically could be any of these permutations. Then we will:\n Start by picking up a random current state. Create a proposal for a new state by swapping two random letters in the current state. Use a Scoring Function which calculates the score of the current state $Score_C$ and the proposed State $Score_P$. If the score of the proposed state is more than current state, Move to Proposed State. Else flip a coin which has a probability of Heads $Score_P/Score_C$. If it comes heads move to proposed State. Repeat from 2nd State.  If we get lucky we may reach a steady state where the chain has the stationary distribution of the needed states and the state that the chain is at could be used as a solution.\nSo the Question is what is the scoring function that we will want to use. We want to use a scoring function for each state(Decryption key) which assigns a positive score to each decryption key. This score intuitively should be more if the encrypted text looks more like actual english if decrypted using this decryption key.\nSo how can we quantify such a function. We will check a long text and calculate some statistics. See how many times one alphabet comes after another in a legitimate long text like War and Peace. For example we want to find out how many times does \u0026lsquo;BA\u0026rsquo; appears in the text or how many times \u0026lsquo;TH\u0026rsquo; occurs in the text.\nFor each pair of characters $\\beta_1$ and $\\beta_2$ (e.g. $\\beta_1$ = T and $\\beta_2$ =H), we let $R(\\beta_1,\\beta_2)$ record the number of times that specific pair(e.g. \u0026ldquo;TH\u0026rdquo;) appears consecutively in the reference text.\nSimilarly, for a putative decryption key x, we let $F_x(\\beta_1,\\beta_2)$ record the number of times that pair appears when the cipher text is decrypted using the decryption key x.\nWe then Score a particular decryption key x using:\n$$Score(x) = \\prod R(\\beta_1,\\beta_2)^{F_x(\\beta_1,\\beta_2)}$$ This function can be thought of as multiplying, for each consecutive pair of letters in the decrypted text, the number of times that pair occurred in the reference text. Intuitively, the score function is higher when the pair frequencies in the decrypted text most closely match those of the reference text, and the decryption key is thus most likely to be correct.\nTo make life easier with calculations we will calculate $log(Score(x))$\nSo lets start working through the problem step by step.\n# AIM: To Decrypt a text using MCMC approach. i.e. find decryption key which we will call cipher from now on. import string import math import random # This function takes as input a decryption key and creates a dict for key where each letter in the decryption key # maps to a alphabet For example if the decryption key is \u0026#34;DGHJKL....\u0026#34; this function will create a dict like {D:A,G:B,H:C....} def create_cipher_dict(cipher): cipher_dict = {} alphabet_list = list(string.ascii_uppercase) for i in range(len(cipher)): cipher_dict[alphabet_list[i]] = cipher[i] return cipher_dict # This function takes a text and applies the cipher/key on the text and returns text. def apply_cipher_on_text(text,cipher): cipher_dict = create_cipher_dict(cipher) text = list(text) newtext = \u0026#34;\u0026#34; for elem in text: if elem.upper() in cipher_dict: newtext+=cipher_dict[elem.upper()] else: newtext+=\u0026#34; \u0026#34; return newtext # This function takes as input a path to a long text and creates scoring_params dict which contains the # number of time each pair of alphabet appears together # Ex. {\u0026#39;AB\u0026#39;:234,\u0026#39;TH\u0026#39;:2343,\u0026#39;CD\u0026#39;:23 ..} def create_scoring_params_dict(longtext_path): scoring_params = {} alphabet_list = list(string.ascii_uppercase) with open(longtext_path) as fp: for line in fp: data = list(line.strip()) for i in range(len(data)-1): alpha_i = data[i].upper() alpha_j = data[i+1].upper() if alpha_i not in alphabet_list and alpha_i != \u0026#34; \u0026#34;: alpha_i = \u0026#34; \u0026#34; if alpha_j not in alphabet_list and alpha_j != \u0026#34; \u0026#34;: alpha_j = \u0026#34; \u0026#34; key = alpha_i+alpha_j if key in scoring_params: scoring_params[key]+=1 else: scoring_params[key]=1 return scoring_params # This function takes as input a text and creates scoring_params dict which contains the # number of time each pair of alphabet appears together # Ex. {\u0026#39;AB\u0026#39;:234,\u0026#39;TH\u0026#39;:2343,\u0026#39;CD\u0026#39;:23 ..} def score_params_on_cipher(text): scoring_params = {} alphabet_list = list(string.ascii_uppercase) data = list(text.strip()) for i in range(len(data)-1): alpha_i =data[i].upper() alpha_j = data[i+1].upper() if alpha_i not in alphabet_list and alpha_i != \u0026#34; \u0026#34;: alpha_i = \u0026#34; \u0026#34; if alpha_j not in alphabet_list and alpha_j != \u0026#34; \u0026#34;: alpha_j = \u0026#34; \u0026#34; key = alpha_i+alpha_j if key in scoring_params: scoring_params[key]+=1 else: scoring_params[key]=1 return scoring_params # This function takes the text to be decrypted and a cipher to score the cipher. # This function returns the log(score) metric def get_cipher_score(text,cipher,scoring_params): cipher_dict = create_cipher_dict(cipher) decrypted_text = apply_cipher_on_text(text,cipher) scored_f = score_params_on_cipher(decrypted_text) cipher_score = 0 for k,v in scored_f.iteritems(): if k in scoring_params: cipher_score += v*math.log(scoring_params[k]) return cipher_score # Generate a proposal cipher by swapping letters at two random location def generate_cipher(cipher): pos1 = random.randint(0, len(list(cipher))-1) pos2 = random.randint(0, len(list(cipher))-1) if pos1 == pos2: return generate_cipher(cipher) else: cipher = list(cipher) pos1_alpha = cipher[pos1] pos2_alpha = cipher[pos2] cipher[pos1] = pos2_alpha cipher[pos2] = pos1_alpha return \u0026#34;\u0026#34;.join(cipher) # Toss a random coin with robability of head p. If coin comes head return true else false. def random_coin(p): unif = random.uniform(0,1) if unif\u0026gt;=p: return False else: return True # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states def MCMC_decrypt(n_iter,cipher_text,scoring_params): current_cipher = string.ascii_uppercase # Generate a random cipher to start state_keeper = set() best_state = \u0026#39;\u0026#39; score = 0 for i in range(n_iter): state_keeper.add(current_cipher) proposed_cipher = generate_cipher(current_cipher) score_current_cipher = get_cipher_score(cipher_text,current_cipher,scoring_params) score_proposed_cipher = get_cipher_score(cipher_text,proposed_cipher,scoring_params) acceptance_probability = min(1,math.exp(score_proposed_cipher-score_current_cipher)) if score_current_cipher\u0026gt;score: best_state = current_cipher if random_coin(acceptance_probability): current_cipher = proposed_cipher if i%500==0: print \u0026#34;iter\u0026#34;,i,\u0026#34;:\u0026#34;,apply_cipher_on_text(cipher_text,current_cipher)[0:99] return state_keeper,best_state ## Run the Main Program: scoring_params = create_scoring_params_dict(\u0026#39;war_and_peace.txt\u0026#39;) plain_text = \u0026#34;As Oliver gave this first proof of the free and proper action of his lungs, \\ the patchwork coverlet which was carelessly flung over the iron bedstead, rustled; \\ the pale face of a young woman was raised feebly from the pillow; and a faint voice imperfectly \\ articulated the words, Let me see the child, and die. \\ The surgeon had been sitting with his face turned towards the fire: giving the palms of his hands a warm \\ and a rub alternately. As the young woman spoke, he rose, and advancing to the bed\u0026#39;s head, said, with more kindness \\ than might have been expected of him: \u0026#34; encryption_key = \u0026#34;XEBPROHYAUFTIDSJLKZMWVNGQC\u0026#34; cipher_text = apply_cipher_on_text(plain_text,encryption_key) decryption_key = \u0026#34;ICZNBKXGMPRQTWFDYEOLJVUAHS\u0026#34; print\u0026#34;Text To Decode:\u0026#34;, cipher_text print \u0026#34;\\n\u0026#34; states,best_state = MCMC_decrypt(10000,cipher_text,scoring_params) print \u0026#34;\\n\u0026#34; print \u0026#34;Decoded Text:\u0026#34;,apply_cipher_on_text(cipher_text,best_state) print \u0026#34;\\n\u0026#34; print \u0026#34;MCMC KEY FOUND:\u0026#34;,best_state print \u0026#34;ACTUAL DECRYPTION KEY:\u0026#34;,decryption_key   This chain converges around the 2000th iteration and we are able to unscramble the code. That\u0026rsquo;s awesome!!! Now as you see the MCMC Key found is not exactly the encryption key. So the solution is not a deterministic one, but we can see that it does not actually decrease any of the value that the MCMC Methods provide. Now Lets Help Bilbo :)\nThe Knapsack Problem Restating, we have Bilbo a Thief who goes to Smaug\u0026rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution?\nSo in this problem we have an $1$x$M$ array of Weight Values W, Gold Values G and a value for the maximum weight $w_{MAX}$ that Bilbo can carry. We want to find out an $1$x$M$ array $X$ of 1\u0026rsquo;s and 0\u0026rsquo;s, which holds weather Bilbo Carries a particular treasure or not. This array needs to follow the constraint $WX^T \u0026lt; w_{MAX}$ and we want to maximize $GX^T$ for a particular state X.(Here the T means transpose)\nSo lets first discuss as to how we will create a proposal from a previous state.\n Pick a random index from the state and toggle the index value. Check if we satisfy our constraint. If yes this state is the proposal state. Else pick up another random index and repeat.  We also need to think about the Scoring Function. We need to give high values to states with high gold value. We will use: $$Score(X)=e^{\\beta GX^T}$$ We give exponentially more value to higher score. The Beta here is a +ve constant. But how to choose it? If $\\beta$ is big we will give very high score to good solutions and the chain will not be able to try new solutions as it can get stuck in local optimas. If we give a small value the chain will not converge to very good solutions. So weuse an Optimization Technique called  Simulated Annealing  i.e. we will start with a small value of $\\beta$ and increase as no of iterations go up. That way the chain will explore in the starting stages and stay at the best solution in the later stages.\nSo now we have everything we need to get started\nimport numpy as np W = [20,40,60,12,34,45,67,33,23,12,34,56,23,56] G = [120,420,610,112,341,435,657,363,273,812,534,356,223,516] W_max = 150 # This function takes a state X , The gold vector G and a Beta Value and return the Log of score def score_state_log(X,G,Beta): return Beta*np.dot(X,G) # This function takes as input a state X and the number of treasures M, The weight vector W and the maximum weight W_max # and returns a proposal state def create_proposal(X,W,W_max): M = len(W) random_index = random.randint(0,M-1) #print random_index proposal = list(X) proposal[random_index] = 1 - proposal[random_index] #Toggle #print proposal if np.dot(proposal,W)\u0026lt;=W_max: return proposal else: return create_proposal(X,W,W_max) # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states def MCMC_Golddigger(n_iter,W,G,W_max, Beta_start = 0.05, Beta_increments=.02): M = len(W) Beta = Beta_start current_X = [0]*M # We start with all 0\u0026#39;s state_keeper = [] best_state = \u0026#39;\u0026#39; score = 0 for i in range(n_iter): state_keeper.append(current_X) proposed_X = create_proposal(current_X,W,W_max) score_current_X = score_state_log(current_X,G,Beta) score_proposed_X = score_state_log(proposed_X,G,Beta) acceptance_probability = min(1,math.exp(score_proposed_X-score_current_X)) if score_current_X\u0026gt;score: best_state = current_X if random_coin(acceptance_probability): current_X = proposed_X if i%500==0: Beta += Beta_increments # You can use these below two lines to tune value of Beta #if i%20==0: # print \u0026#34;iter:\u0026#34;,i,\u0026#34; |Beta=\u0026#34;,Beta,\u0026#34; |Gold Value=\u0026#34;,np.dot(current_X,G) return state_keeper,best_state Running the Main program:\nmax_state_value =0 Solution_MCMC = [0] for i in range(10): state_keeper,best_state = MCMC_Golddigger(50000,W,G,W_max,0.0005, .0005) state_value=np.dot(best_state,G) if state_value\u0026gt;max_state_value: max_state_value = state_value Solution_MCMC = best_state print \u0026#34;MCMC Solution is :\u0026#34; , str(Solution_MCMC) , \u0026#34;with Gold Value:\u0026#34;, str(max_state_value) MCMC Solution is : [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0] with Gold Value: 2435  Now I won\u0026rsquo;t say that this is the best solution. The deterministic solution using DP will be the best for such use case but sometimes when the problems gets large, having such techniques at disposal becomes invaluable.\nSo tell me What do you think about MCMC Methods?\nAlso, If you find any good applications or would like to apply these techniques to some area, I would really be glad to know about them and help if possible.\nThe codes for both examples are sourced at Github References and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia The Markov Chain Monte Carlo Revolution, Persi Diaconis Decrypting Classical Cipher Text Using Markov Chain Monte Carlo, Jian Chen and Jeffrey S. Rosenthal  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)\nApart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn\u0026rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine √áetinkaya-Rundel), I am very hopeful for this course. Let\u0026rsquo;s see.\nAlso look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:\n  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)\n","permalink":"https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/","tags":["Statistics","Data Science","Best Content"],"title":"Behold the power of MCMC"},{"categories":["Data Science","Awesome Guides"],"contents":"The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.\n  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.\nSo what really are MCMC Methods? First of all we have to understand what are Monte Carlo Methods!!!\n Monte Carlo methods derive their name from Monte Carlo Casino in Monaco. There are many card games that need probability of winning against the dealer. Sometimes calculating this probability can be mathematically complex or highly intractable. But we can always run a computer simulation to simulate the whole game many times and see the probability as the number of wins divided by the number of games played.\nSo that is all you need to know about Monte carlo Methods. Yes it is just a simple simulation technique with a Fancy Name.\nSo as we have got the first part of MCMC, we also need to understand what are  Markov Chains . Before Jumping onto Markov Chains let us learn a little bit about Markov Property.\nSuppose you have a system of $M$ possible states, and you are hopping from one state to another. Markov Property says that given a process which is at a state $X_n$ at a particular point of time, the probability of $X_{n+1} = k$, where $k$ is any of the $M$ states the process can hop to, will only be dependent on which state it is at the given moment of time. And not on how it reached the current state.\nMathematically speaking:\n $$P(X_{n+1}=k | X_n=k_n,X_{n-1}=k_{n-1},....,X_1=k_1) = P(X_{n+1}=k|X_n=k_n)$$ If a process exhibits the Markov Property than it is known as a Markov Process.\nNow Why is a Markov Chain important? It is important because of its stationary distribution.\nSo what is a Stationary Distribution?\nAssume you have a markov process like below. You start from any state $X_i$ and want to find out the state Probability distribution at $X_{i+1}$.\n  You have a matrix of transition probability\n  which defines the probability of going from a state $X_i$ to $X_j$. You start calculating the Probability distribution for the next state. If you are at Bull Market State at time $i$ , you have a state Probability distribution as [0,1,0]\nyou want to get the state pdf at $X_{i+1}$. That is given by\n$$s_{i+1} = s_{i}Q$$ $$ s_{i+1}=\\left[ {\\begin{array}{cc} .15 \u0026 .8 \u0026 .05 \\end{array} } \\right]$$ And the next state distribution could be found out by $$s_{i+1} = s_iQ^2$$div and so on. Eventually you will reach a stationary state s where:\n$$sQ=s$$ For this transition matrix Q the Stationary distribution $s$ is $$ s_{i+1}=\\left[ {\\begin{array}{cc} .625 \u0026 .3125 \u0026 .0625 \\end{array} } \\right]$$ The stationary state distribution is important because it lets you define the probability for every state of a system at a random time. That is for this particular example we can say that 62.5% of the times market will be in a bull market state, 31.25% of weeks it will be a bear market and 6.25% of weeks it will be stagnant\nIntuitively you can think of it as an random walk on a chain. You might visit some nodes more often than others based on node probabilities. In the Google Pagerank problem you might think of a node as a page, and the probability of a page in the stationary distribution as its relative importance.\nWoah! That was a lot of information and we have yet not started talking about the MCMC Methods. Well if you are with me till now, we can now get on to the real topic now.\nSo What is MCMC? According to Wikipedia:\n **Markov Chain Monte Carlo** (MCMC) methods are a class of algorithms for **sampling from a probability distribution** based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.  So let\u0026rsquo;s explain this with an example: Assume that we want to sample from a Beta distribution. The PDF is:\n$$f(x) = Cx^{\\alpha -1}(1-x)^{\\beta -1}$$ where $C$ is the normalizing constant *(which we actually don't need to Sample from the distribution as we will see later)*. This is a fairly difficult problem with the Beta Distribution if not intractable. In reality you might need to work with a lot harder Distribution Functions and sometimes you won\u0026rsquo;t actually know the normalizing constants.\nMCMC methods make life easier for us by providing us with algorithms that could create a Markov Chain which has the Beta distribution as its stationary distribution given that we can sample from a uniform distribution(which is fairly easy).\nIf we start from a random state and traverse to the next state based on some algorithm repeatedly, we will end up creating a Markov Chain which has the Beta distribution as its stationary distribution and the states we are at after a long time could be used as sample from the Beta Distribution.\nOne such MCMC Algorithm is the Metropolis Hastings Algorithm\nMetropolis Hastings Algorithm Let $s=(s_1,s_2,\u0026hellip;.,s_M)$ be the desired stationary distribution. We want to create a Markov Chain that has this stationary distribution. We start with an arbitrary Markov Chain $P$ with $M$ states with transition matrix $Q$, so that $Q_{ij}$ represents the probability of going from state $i$ to $j$. Intuitively we know how to wander around this Markov Chain but this Markov Chain does not have the required Stationary Distribution. This chain does have some stationary distribution(which is not of our use)\nOur Goal is to change the way we wander on the this Markov Chain $P$ so that this chain has the desired Stationary distribution.\nTo do this we:\n Start at a random initial State $i$. Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Compute an measure called the Acceptance Probability which is defined as: $a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$ Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  After a long time this chain will converge and will have a stationary distribution $s$. We can then use the states of the chain as the sample from any distribution.\nWhile doing this to sample the Beta Distribution, the only time we are using the PDF is to find the acceptance probability and in that we divide $s_j$ by $s_i$, i.e. the normalizing constant $C$ gets cancelled.\nNow Let\u0026rsquo;s Talk about the intuition. For the Intuition I am quoting an Answer from the site Stack Exchange,as this was the best intuitive explanation that I could find:\n I think there's a nice and simple intuition to be gained from the (independence-chain) Metropolis-Hastings algorithm. First, what's the goal? The goal of MCMC is to **draw samples from some probability distribution** without having to know its exact height at any point(We don't need to know C). The way MCMC achieves this is to **\"wander around\" on that distribution in such a way that the amount of time spent in each location is proportional to the height of the distribution**. If the \"wandering around\" process is set up correctly, you can make sure that this proportionality (between time spent and height of the distribution) is achieved. Intuitively, what we want to do is to to walk around on some (lumpy) surface in such a way that the amount of time we spend (or # samples drawn) in each location is proportional to the height of the surface at that location. So, e.g., we'd like to spend twice as much time on a hilltop that's at an altitude of 100m as we do on a nearby hill that's at an altitude of 50m. The nice thing is that we can do this even if we don't know the absolute heights of points on the surface: all we have to know are the relative heights. e.g., if one hilltop A is twice as high as hilltop B, then we'd like to spend twice as much time at A as we spend at B. The simplest variant of the Metropolis-Hastings algorithm (independence chain sampling) achieves this as follows: assume that in every (discrete) time-step, we pick a random new \"proposed\" location (selected uniformly across the entire surface). If the proposed location is higher than where we're standing now, move to it. If the proposed location is lower, then move to the new location with probability p, where p is the ratio of the height of that point to the height of the current location. (i.e., flip a coin with a probability p of getting heads; if it comes up heads, move to the new location; if it comes up tails, stay where we are). Keep a list of the locations you've been at on every time step, and that list will (asyptotically) have the right proportion of time spent in each part of the surface. (And for the A and B hills described above, you'll end up with twice the probability of moving from B to A as you have of moving from A to B). There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: **(1) pick a new \"proposed\" location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location.**  Sampling from Beta Distribution Now Let\u0026rsquo;s Move on to the problem of Simulating from Beta Distribution. Now Beta Distribution is a continuous Distribution on [0,1] and it can have infinite states on [0,1].\nLets Assume an arbitrary Markov Chain P with infinite states on [0,1] having transition Matrix Q such that $Q_{ij} = Q_{ji} = $ All entries in Matrix. We don\u0026rsquo;t really need the Matrix Q as we will see later, But I want to keep the problem description as close to the algorihm we suggested.\n Start at a random initial State $i$ given by Unif(0,1). Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Lets say we pick up another Unif(0,1) state as a proposal state $j$. Compute an measure called the Acceptance Probability :  $$a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$$ which is, $$a_{ij} = min(s_j/s_i,1)$$ where, $$s_i = Ci^{\\alpha -1}(1-i)^{\\beta -1}$$ and, $$s_j = Cj^{\\alpha -1}(1-j)^{\\beta -1}$$  Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  So enough with theory, Let\u0026rsquo;s Move on to python to create our Beta Simulations Now\u0026hellip;.\nimport random # Lets define our Beta Function to generate s for any particular state. We don\u0026#39;t care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif\u0026gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain Let us check our results of the MCMC Sampled Beta distribution against the actual beta distribution.\nimport numpy as np import pylab as pl import scipy.special as ss %matplotlib inline pl.rcParams[\u0026#39;figure.figsize\u0026#39;] = (17.0, 4.0) # Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label=\u0026#34;Real Distribution: a=\u0026#34;+str(a)+\u0026#34;, b=\u0026#34;+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype=\u0026#39;step\u0026#39;,label=\u0026#34;Simulated_MCMC: a=\u0026#34;+str(a)+\u0026#34;, b=\u0026#34;+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)  As we can see our sampled beta values closely resemble the beta distribution.\nSo MCMC Methods are useful for the following basic problems.\n Simulating from a Random Variable PDF. Example: Simulate from a Beta(0.5,0.5) or from a Normal(0,1). Solve problems with a large state space.For Example: Knapsack Problem, Encrytion Cipher etc. We will work on this in the Next Blog Post as this one has already gotten bigger than what I expected.  Till Then Ciao!!!!!!\nReferences and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia StackExchange  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)\nApart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn\u0026rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine √áetinkaya-Rundel), I am very hopeful for this course. Let\u0026rsquo;s see.\nAlso look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:\n  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)\n","permalink":"https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/","tags":["Statistics","Data Science","Best Content"],"title":"My Tryst With MCMC Algorithms"},{"categories":["Big Data","Data science"],"contents":"I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.\nUsing Shell Scripts to run your Programs I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. You can effectively organize your workflow by using shell scripts. Now Shell scripts are not as formidable as they sound. We wont be doing programming perse using these shell scripts(Though they are pretty good at that too), we will just use them to store commands that we need to use sequentially.\nBelow is a sample of the shell script I use to run my Mapreduce Codes.\n#!/bin/bash #Defining program variables IP=\u0026#34;/data/input\u0026#34; OP=\u0026#34;/data/output\u0026#34; HADOOP_JAR_PATH=\u0026#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar\u0026#34; MAPPER=\u0026#34;test_m.py\u0026#34; REDUCER=\u0026#34;test_r.py\u0026#34; hadoop fs -rmr -skipTrash\u0026amp;nbsp;$OP hadoop jar\u0026amp;nbsp;$HADOOP_JAR_PATH \\ -file\u0026amp;nbsp;$MAPPER -mapper \u0026#34;python test_m.py\u0026#34; \\ -file\u0026amp;nbsp;$REDUCER -reducer \u0026#34;python test_r.py\u0026#34; \\ -input\u0026amp;nbsp;$IP -output\u0026amp;nbsp;$OP I generally save them as test_s.sh and whenever i need to run them i simply type sh test_s.sh. This helps in three ways.\n It helps me to store hadoop commands in a manageable way.   It is easy to run the mapreduce code using the shell script.   If the code fails, I do not have to manually delete the output directory    The simplification of anything is always sensational.  Gilbert K. Chesterton  Using Distributed Cache to provide mapper with a dictionary Often times it happens that you want that your Hadoop Mapreduce program is able to access some static file. This static file could be a dictionary, could be parameters for the program or could be anything. What distributed cache does is that it provides this file to all the mapper nodes so that you can use that file in any way across all your mappers. Now this concept although simple would help you to think about Mapreduce in a whole new light. Lets start with an example. Supppose you have to create a sample Mapreduce program that reads a big file containing the information about all the characters in Game of Thrones stored as \u0026quot;/data/characters/\u0026quot;:\n  Cust_ID User_Name House     1 Daenerys Targaryen Targaryen   2 Tyrion Lannister Lannister   3 Cersei Lannister Lannister  4 Robert Baratheon Baratheon  5 Robb Stark Stark     But you dont want to use the dead characters in the file for the analysis you want to do. You want to count the number of living characters in Game of Thrones grouped by their House. (I know its easy!!!!!) One thing you could do is include an if statement in your Mapper Code which checks if the persons ID is 4 then exclude it from the mapper and such. But the problem is that you would have to do it again and again for the same analysis as characters die like flies when it comes to George RR Martin.(Also where is the fun in that) So you create a file which contains the Ids of all the dead characters at \u0026quot;/data/dead_characters.txt\u0026quot;:\n  Died     4   5     Whenever you have to run the analysis you can just add to this file and you wont have to change anything in the code. Also sometimes this file would be long and you would not want to clutter your code with IDs and such.\nSo How Would we do it. Let\u0026rsquo;s go in a step by step way around this. We will create a shell script, a mapper script and a reducer script for this task.\n1) Shell Script #!/bin/bash #Defining program variables DC=\u0026#34;/data/dead_characters.txt\u0026#34; IP=\u0026#34;/data/characters\u0026#34; OP=\u0026#34;/data/output\u0026#34; HADOOP_JAR_PATH=\u0026#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar\u0026#34; MAPPER=\u0026#34;got_living_m.py\u0026#34; REDUCER=\u0026#34;got_living_r.py\u0026#34; hadoop jar\u0026amp;nbsp;$HADOOP_JAR_PATH \\ -file\u0026amp;nbsp;$MAPPER -mapper \u0026#34;python got_living_m.py\u0026#34; \\ -file\u0026amp;nbsp;$REDUCER -reducer \u0026#34;python got_living_r.py\u0026#34; \\ -cacheFile\u0026amp;nbsp;$DC#ref \\ -input\u0026amp;nbsp;$IP -output\u0026amp;nbsp;$OP Note how we use the \u0026quot;-cacheFile\u0026quot; option here. We have specified that we will refer to the file that has been provided in the Distributed cache as #ref.\nNext is our Mapper Script.\n2) Mapper Script import sys dead_ids = set() def read_cache(): for line in open(\u0026#39;ref\u0026#39;): id = line.strip() dead_ids.add(id) read_cache() for line in sys.stdin: rec = line.strip().split(\u0026#34;|\u0026#34;) # Split using Delimiter \u0026#34;|\u0026#34; id = rec[0] house = rec[2] if id not in dead_ids: print \u0026#34;%s\\t%s\u0026#34; % (house,1) And our Reducer Script.\n3) Reducer Script import sys current_key = None key = None count = 0 for line in sys.stdin: line = line.strip() rec = line.split(\u0026#39;\\t\u0026#39;) key = rec[0] value = int(rec[1]) if current_key == key: count += value else: if current_key: print \u0026#34;%s:%s\u0026#34; %(key,str(count))\tcurrent_key = key count = value if current_key == key: print \u0026#34;%s:%s\u0026#34; %(key,str(count)) This was a simple program and the output will be just what you expected and not very exciting. But the Technique itself solves a variety of common problems. You can use it to pass any big dictionary to your Mapreduce Program. Atleast thats what I use this feature mostly for. Hope You liked it. Will try to expand this post with more tricks.\nThe codes for this post are posted at github here.\nOther Great Learning Resources For Hadoop:\n  Michael Noll's Hadoop Mapreduce Tutorial   Apache's Hadoop Streaming Documentation   Also I like these books a lot. Must have for a Hadooper\u0026hellip;.\n\n The first book is a guide for using Hadoop as well as spark with Python. While the second one contains a detailed overview of all the things in Hadoop. Its the definitive guide.\n ","permalink":"https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/","tags":["Big Data","Machine Learning"],"title":"Hadoop Mapreduce Streaming Tricks and Techniques"},{"categories":["Data Science"],"contents":"In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.\nFor this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.\nData Fields id: ad identifier click: 0/1 for non-click/click hour: format is YYMMDDHH, so 14091123 means 23:00 on Sept. 11, 2014 UTC. C1 -- anonymized categorical variable banner_pos site_id site_domain site_category app_id app_domain app_category device_id device_ip device_model device_type device_conn_type C14-C21 -- anonymized categorical variables  Loading Data ## Loading the data import pandas as pd import numpy as np import string as stri #too large data not keeping it in memory. # will be using line by line scripting. #data = pd.read_csv(\u0026#34;/Users/RahulAgarwal/kaggle_cpr/train\u0026#34;) Since the data is too large around 6 gb , we will proceed by doing line by line analysis of data. We will try to use vowpal wabbit first of all as it is an online model and it also gives us the option of minimizing log loss as a default. It is also very fast to run and will give us quite an intuition as to how good our prediction can be.\nI will use all the variables in the first implementation and we will rediscover things as we move on\nRunning Vowpal Wabbit Creating data in vowpal format (One Time Only) from datetime import datetime def csv_to_vw(loc_csv, loc_output, train=True): start = datetime.now() print(\u0026#34;\\nTurning %sinto %s. Is_train_set? %s\u0026#34;%(loc_csv,loc_output,train)) i = open(loc_csv, \u0026#34;r\u0026#34;) j = open(loc_output, \u0026#39;wb\u0026#39;) counter=0 with i as infile: line_count=0 for line in infile: # to counter the header if line_count==0: line_count=1 continue # The data has all categorical features #numerical_features = \u0026#34;\u0026#34; categorical_features = \u0026#34;\u0026#34; counter = counter+1 #print counter line = line.split(\u0026#34;,\u0026#34;) if train: #working on the date column. We will take day , hour a = line[2] new_date= datetime(int(\u0026#34;20\u0026#34;+a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(\u0026#34;%A\u0026#34;) hour= a[6:8] categorical_features += \u0026#34; |hr %s\u0026#34; % hour categorical_features += \u0026#34; |day %s\u0026#34; % day # 24 columns in data  for i in range(3,24): if line[i] != \u0026#34;\u0026#34;: categorical_features += \u0026#34;|c%s%s\u0026#34; % (str(i),line[i]) else: a = line[1] new_date= datetime(int(\u0026#34;20\u0026#34;+a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(\u0026#34;%A\u0026#34;) hour= a[6:8] categorical_features += \u0026#34; |hr %s\u0026#34; % hour categorical_features += \u0026#34; |day %s\u0026#34; % day for i in range(2,23): if line[i] != \u0026#34;\u0026#34;: categorical_features += \u0026#34; |c%s%s\u0026#34; % (str(i+1),line[i]) #Creating the labels #print \u0026#34;a\u0026#34; if train: #we care about labels if line[1] == \u0026#34;1\u0026#34;: label = 1 else: label = -1 #we set negative label to -1 #print (numerical_features) #print categorical_features j.write( \u0026#34;%s\u0026#39;%s%s\\n\u0026#34; % (label,line[0],categorical_features)) else: #we dont care about labels #print ( \u0026#34;1 \u0026#39;%s |i%s |c%s\\n\u0026#34; % (line[0],numerical_features,categorical_features) ) j.write( \u0026#34;1 \u0026#39;%s%s\\n\u0026#34; % (line[0],categorical_features) ) #Reporting progress #print counter if counter % 1000000 == 0: print(\u0026#34;%s\\t%s\u0026#34;%(counter, str(datetime.now() - start))) print(\u0026#34;\\n%sTask execution time:\\n\\t%s\u0026#34;%(counter, str(datetime.now() - start))) #csv_to_vw(\u0026#34;/Users/RahulAgarwal/kaggle_cpr/train\u0026#34;, \u0026#34;/Users/RahulAgarwal/kaggle_cpr/click.train_original_data.vw\u0026#34;,train=True) #csv_to_vw(\u0026#34;/Users/RahulAgarwal/kaggle_cpr/test\u0026#34;, \u0026#34;/Users/RahulAgarwal/kaggle_cpr/click.test_original_data.vw\u0026#34;,train=False) Running Vowpal Wabbit on the data The Vowpal Wabbit will be run on the command line itself.\nTraining VW:\nvw click.train_original_data.vw -f click.model.vw --loss_function logistic Testing VW:\nvw click.test_original_data.vw -t -i click.model.vw -p click.preds.txt Creating Kaggle Submission File import math def zygmoid(x): return 1 / (1 + math.exp(-x)) with open(\u0026#34;kaggle.click.submission.csv\u0026#34;,\u0026#34;wb\u0026#34;) as outfile: outfile.write(\u0026#34;id,click\\n\u0026#34;) for line in open(\u0026#34;click.preds.txt\u0026#34;): row = line.strip().split(\u0026#34; \u0026#34;) try: outfile.write(\u0026#34;%s,%f\\n\u0026#34;%(row[1],zygmoid(float(row[0])))) except: pass This solution ranked 211/371 submissions at the time and the leaderboard score was 0.4031825 while the best leaderboard score was 0.3901120\nNext Steps   Create a better VW model\n Shuffle the data before making the model as the VW algorithm is an online learner and might have given more preference to the latest data provide high weights for clicks as data is skewed. How Much? tune VW algorithm using vw-hypersearch. What should be tuned? Use categorical features like |C1 \u0026ldquo;C1\u0026rdquo;\u0026amp;\u0026ldquo;1\u0026rdquo;    Create a XGBoost Model.\n  Create a Sofia-ML Model and see how it works on this data.\n   ","permalink":"https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/","tags":["Machine Learning","Data Science","Python"],"title":"Exploring Vowpal Wabbit with the Avazu Clickthrough Prediction Challenge"},{"categories":["Data Science"],"contents":"This is a simple illustration of using Pattern Module to scrape web data using Python. We will be scraping the data from imdb for the top TV Series along with their ratings\nWe will be using this link for this:\nhttp://www.imdb.com/search/title?count=100\u0026num_votes=5000,\u0026ref_=gnr_tv_hr\u0026sort=user_rating,desc\u0026start=1\u0026title_type=tv_series,mini_series  This URL gives a list of top Rated TV Series which have number of votes atleast 5000. The Thing to note in this URL is the \u0026ldquo;\u0026amp;start=\u0026rdquo; parameter where we can specify which review should the list begin with. If we specify 1 we will get reviews starting from 1-100, if we specify 101 we get reviews from 101-200 and so on.\nLets Start by importing some Python Modules that will be needed for Scraping Data:\nimport requests # This is a module that is used for getting html data from a webpage in the text format from pattern import web # We use this module to parse through the dtaa that we loaded using requests Loading the data using requests and pattern So the modules are loaded at this point, next we will try to catch the url using python and put this into a dict in python. We will start with a single URL and then try to parse it using pattern module\nurl= \u0026#34;http://www.imdb.com/search/title?count=100\u0026amp;num_votes=5000,\u0026amp;ref_=gnr_tv_hr\u0026amp;sort=user_rating,desc\u0026amp;start=1\u0026amp;title_type=tv_series,mini_series\u0026#34; html_data = requests.get(url).text dom=web.Element(html_data) Parsing the data This is the data of Interest found out after some nspection of the html code. This is for a single TV Series Band of brothers, but if you are able to parse this you just have to move hrough a loop.\n\u0026lt;html\u0026gt; \u0026lt;td class=\u0026#34;title\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;wlb_wrapper\u0026#34; data-tconst=\u0026#34;tt0185906\u0026#34; data-size=\u0026#34;small\u0026#34; data-caller-name=\u0026#34;search\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;a href=\u0026#34;/title/tt0185906/\u0026#34;\u0026gt;Band of Brothers\u0026lt;/a\u0026gt; \u0026lt;span class=\u0026#34;year_type\u0026#34;\u0026gt;(2001 Mini-Series)\u0026lt;/span\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;div class=\u0026#34;user_rating\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;rating rating-list\u0026#34; data-auth=\u0026#34;BCYm-Mk2Ros7BTxsLNL2XJX_icfZVahNr1bE9-5Ajb2N3381yxcaNN4ZQqyrX7KgEFGqHWmwv10lv7lAnXyC8CCkh9hPqQfzwVTumCeRzjpnndW4_ft97qQkBYLUvFxYnFgR\u0026#34; id=\u0026#34;tt0185906|imdb|9.6|9.6|advsearch\u0026#34; data-ga-identifier=\u0026#34;advsearch\u0026#34; title=\u0026#34;Users rated this 9.6/10 (156,073 votes) - click stars to rate\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;rating-bg\u0026#34;\u0026gt;\u0026amp;nbsp;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;rating-imdb\u0026#34; style=\u0026#34;width: 134px\u0026#34;\u0026gt;\u0026amp;nbsp;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;rating-stars\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;1\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;2\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;3\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;4\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;5\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;6\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;7\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;8\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;9\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/register/login?why=vote\u0026#34; title=\u0026#34;Register or login to rate this title\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;10\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;rating-rating\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;value\u0026#34;\u0026gt;9.6\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;grey\u0026#34;\u0026gt;/\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026#34;grey\u0026#34;\u0026gt;10\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;rating-cancel\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/title/tt0185906/vote?v=X;k=BCYm-Mk2Ros7BTxsLNL2XJX_icfZVahNr1bE9-5Ajb2N3381yxcaNN4ZQqyrX7KgEFGqHWmwv10lv7lAnXyC8CCkh9hPqQfzwVTumCeRzjpnndW4_ft97qQkBYLUvFxYnFgR\u0026#34; title=\u0026#34;Delete\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;span\u0026gt;X\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt; \u0026amp;nbsp;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span class=\u0026#34;outline\u0026#34;\u0026gt;The story of Easy Company of the US Army 101st Airborne division and their mission in WWII Europe from Operation Overlord through V-J Day.\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;credit\u0026#34;\u0026gt; With: \u0026lt;a href=\u0026#34;/name/nm0342241/\u0026#34;\u0026gt;Scott Grimes\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;/name/nm0500614/\u0026#34;\u0026gt;Matthew Leitch\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;/name/nm0507073/\u0026#34;\u0026gt;Damian Lewis\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;genre\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/genre/action\u0026#34;\u0026gt;Action\u0026lt;/a\u0026gt; | \u0026lt;a href=\u0026#34;/genre/drama\u0026#34;\u0026gt;Drama\u0026lt;/a\u0026gt; | \u0026lt;a href=\u0026#34;/genre/history\u0026#34;\u0026gt;History\u0026lt;/a\u0026gt; | \u0026lt;a href=\u0026#34;/genre/war\u0026#34;\u0026gt;War\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;certificate\u0026#34;\u0026gt;\u0026lt;span title=\u0026#34;TV_MA\u0026#34; class=\u0026#34;us_tv_ma titlePageSprite\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;runtime\u0026#34;\u0026gt;705 mins.\u0026lt;/span\u0026gt; \u0026lt;/td\u0026gt; Now we have loaded the data we need to parse it using the functions from pattern module. The main function in pattern module is the by_tag() function which lets you get all the elements with that particular tagname. For us the main interest is this \u0026ldquo;td\u0026rdquo; tag with class as \u0026ldquo;title\u0026rdquo;. This \u0026ldquo;td\u0026rdquo; tag contains:\n Title in the \u0026ldquo;a\u0026rdquo; tag Rating in the \u0026ldquo;span\u0026rdquo; tag with class \u0026ldquo;value\u0026rdquo; Genres in the \u0026ldquo;span\u0026rdquo; tag with class \u0026ldquo;genre\u0026rdquo; and then looping through the \u0026ldquo;a\u0026rdquo; tags Runtime in \u0026ldquo;span\u0026rdquo; tag with class \u0026ldquo;runtime\u0026rdquo; Artists in \u0026ldquo;span\u0026rdquo; tag with class \u0026ldquo;credit\u0026rdquo; loop through \u0026ldquo;a\u0026rdquo; tags  Now lets write some code to parse this data.\nfor tv_series in dom.by_tag(\u0026#39;td.title\u0026#39;): title = tv_series.by_tag(\u0026#39;a\u0026#39;)[0].content genres = tv_series.by_tag(\u0026#39;span.genre\u0026#39;)[0].by_tag(\u0026#39;a\u0026#39;) genres = [g.content for g in genres] try: runtime = tv_series.by_tag(\u0026#39;span.runtime\u0026#39;)[0].content except: runtime = \u0026#34;NA\u0026#34; rating = tv_series.by_tag(\u0026#39;span.value\u0026#39;)[0].content artists = tv_series.by_tag(\u0026#39;span.credit\u0026#39;)[0].by_tag(\u0026#39;a\u0026#39;) artists = [a.content for a in artists] print title, genres, runtime, rating, artists Band of Brothers [u'Action', u'Drama', u'History', u'War'] 705 mins. 9.6 [u'Scott Grimes', u'Matthew Leitch', u'Damian Lewis'] Breaking Bad [u'Crime', u'Drama', u'Thriller'] 45 mins. 9.6 [u'Bryan Cranston', u'Aaron Paul', u'Anna Gunn'] Game of Thrones [u'Adventure', u'Drama', u'Fantasy'] 55 mins. 9.5 [u'Lena Headey', u'Peter Dinklage', u'Maisie Williams'] So finally we are OK with parsing. We have understood the structure of the webpage, the tags and classes we will need to use and how to use pattern module to find data for a single page. Now lets use the power of for loops to get all the data.\nGetting Whole Data Lets Go through it the pythonic way. We will create functions and try to execute small chunks of code rather than doing it all at once. Lets first create a funcion that takes a start_val(for the start parameter) and returns a dom element.\ndef get_dom(start_val): url= \u0026#34;http://www.imdb.com/search/title?count=100\u0026amp;num_votes=5000,\u0026amp;ref_=gnr_tv_hr\u0026amp;sort=user_rating,desc\u0026amp;start=\u0026#34;+str(start_val)+\u0026#34;\u0026amp;title_type=tv_series,mini_series\u0026#34; html_data = requests.get(url).text dom=web.Element(html_data) return dom Now lets create a function parse_dom that takes as input dom an throws out a list containing all the data. The list is like this :\n[ ['Band of Brothers','Action|Drama|History|War','705 mins.','9.6','Scott Grimes|Matthew Leitch|Damian Lewis'], ['Breaking Bad','Crime|Drama|Thriller','45 mins.', '9.6' ,'Bryan Cranston|Aaron Paul|Anna Gunn'],..... ]  def parse_dom(dom): result=[] for tv_series in dom.by_tag(\u0026#39;td.title\u0026#39;): title = tv_series.by_tag(\u0026#39;a\u0026#39;)[0].content genres = tv_series.by_tag(\u0026#39;span.genre\u0026#39;)[0].by_tag(\u0026#39;a\u0026#39;) genres = \u0026#34;|\u0026#34;.join([g.content for g in genres]) try: runtime = tv_series.by_tag(\u0026#39;span.runtime\u0026#39;)[0].content except: runtime = \u0026#34;NA\u0026#34; rating = tv_series.by_tag(\u0026#39;span.value\u0026#39;)[0].content artists = tv_series.by_tag(\u0026#39;span.credit\u0026#39;)[0].by_tag(\u0026#39;a\u0026#39;) artists = \u0026#34;|\u0026#34;.join([a.content for a in artists]) temp_res=[] temp_res.extend([title, genres, runtime, rating, artists]) result.append(temp_res) return result Now Lets Use these functions and a simple while loop to scrap all the pages\ni=1 all_data = [] while True: dom = get_dom(i) datalist=parse_dom(dom) if len(datalist)==0: break all_data = all_data + parse_dom(dom) i += 100 print \u0026#34;Total Elements:\u0026#34; + str(len(all_data)) print \u0026#34;First Five Elements :\u0026#34; + str(all_data[1:5]) Total Elements:898 First Five Elements :[[u'Breaking Bad', u'Crime|Drama|Thriller', u'45 mins.', u'9.6', u'Bryan Cranston|Aaron Paul|Anna Gunn'], [u'Game of Thrones', u'Adventure|Drama|Fantasy', u'55 mins.', u'9.5', u'Lena Headey|Peter Dinklage|Maisie Williams'], [u'Planet Earth', u'Documentary', u'570 mins.', u'9.5', u'David Attenborough|Sigourney Weaver|Huw Cordey'], [u'Cosmos: A SpaceTime Odyssey', u'Documentary', u'60 mins.', u'9.5', u'Neil deGrasse Tyson|Stoney Emshwiller|Piotr Michael']]  Voila!!! The number of elements we had to scrap were 898 and We got all of them. And to tell you, IMDB is one of the worst written HTML\u0026rsquo;s. So that\u0026rsquo;s Great.\nIn the next part of the tutorial we will run exploratory data analysis on this data using pandas and maplotlib.\nTill then keep learning.\n ","permalink":"https://mlwhiz.com/blog/2014/10/02/data_science_101_python_pattern/","tags":["Machine Learning","Data Science","Python"],"title":"Data Science 101 : Playing with Scraping in Python"},{"categories":["Data Science"],"contents":"THE PROBLEM: Recently I was working on the Criteo Advertising Competition on Kaggle. The competition was a classification problem which basically involved predicting the click through rates based on several features provided in the train data. Seeing the size of the data (11 GB Train), I felt that going with Vowpal Wabbit might be a better option.\nBut after getting to an CV error of .47 on the Kaggle LB and being stuck there , I felt the need to go back to Scikit learn. While SciKit learn seemed to have a partial_fit method in SGDClassifier, I still could not find a partial_fit method in the OneHotEncoder or DictVectorizer class which made me look to the internet again. Now while I could find many advices on how to use OneHotEncoding and DictVectorizer on small data, I cannot find something relate to data too big to store in the memory. How do I OneHotEncode such a large data file?\nDICTVECTORIZER How does a DictVectorizer works. There is a lot of stuff around the net for this but I dint get to understand much around it. This blog from Zygmuntz of Fastml came to rescue then. Although still it didn‚Äôt resolve how to apply that to such large amount of data.\nfrom sklearn.feature_extraction import DictVectorizer as DV # Create Vectorizer vectorizer = DV( sparse = False ) # Read the whole Data traindata = pd.read_csv(train_file, header=None, sep=\u0026#39;,\u0026#39;, names = colnames) # Retain the categorical Columns train_df = traindata[cat_col] # Convert Panda Data frame to Dict train_dict = train_df.T.to_dict().values() # Create Fit vectorizer.fit(test_dict) THE DATA The data was basically comprised of 40 Features with: 1. First two Columns as ID, Label 2. Next 13 columns Continuous columns labelled I1-I13 3. Next 26 Columns Categorical labelled C1-C26 Further the categorical columns were very sparse and some of the categorical variables could take more than a million different values.\nTHE WORKAROUNDS The main problem that I faced was that I could not fit that much data in a DataFrame, even when I have a machine of 16GB, and that lead me to think that do I have a need for such a large data frame. And that lead me to the first part of the solution. I don‚Äôt need to load the whole data at once. I just needed to create another dictionary with all the possible combinations and then fit my dictvectorizer on it.\nI know that it is a lot to take in, so let‚Äôs take an example to understand it: Let‚Äôs say we have a data of infinite size, which has 3 categorical variables: C1 could take values 1-100 C2 could take values 1-3 C3 could take values 1-1000 Then we just have to find which category could take the maximum number of values (i.e. C3 in the above case) and make a dict which contains other categories replicated to contain as many values In other words, we need to make a dict like: {C1 : [1,2,3,‚Ä¶‚Ä¶,97,98,99,100]*10 , C2 : [1,2,3]*333+[1] , C3: [1‚Ä¶.1000]} Notice the star sign at the last of the list. That means that for every key in the dict the number of values is now 1000(i.e. the maximum number of features).\nAnd so that is what I did. After we have the Vectorizer Fit, the next task was to transform the data. I took the data transformed it and sent it to my model line by line. P.S. Don‚Äôt store the transformed data as around a 100000 records takes ~ 10GB of Hard Disk Space due to the high number of features.\nHope you find it Informative and happy learning.\n ","permalink":"https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/","tags":["Machine Learning","Data Science"],"title":"Dictvectorizer for One Hot Encoding of Categorical Data"},{"categories":["Data Science"],"contents":"This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.\nThe installation is pretty simple. These steps were done on Mac OS Mavericks but should work for Linux too. Here are the steps for the installation:\n1. Download the Binaries: Spark : http://spark.apache.org/downloads.html Scala : http://www.scala-lang.org/download/ Dont use Latest Version of Scala, Use Scala 2.10.x 2. Add these lines to your .bash_profile: export SCALA_HOME=your_path_to_scala export SPARK_HOME=your_path_to_spark 3. Build Spark(This will take time): brew install sbt cd $SPARK_HOME sbt/sbt assembly 4. Start the Pyspark Shell: $SPARK_HOME/bin/pyspark And Voila. You are running pyspark on your Machine\nTo check that everything is properly installed, Lets run a simple program:\ntest = sc.parallelize([1,2,3]) test.count() This should return 3. So Now Just Run Hadoop On your Machine and then run pyspark Using:\ncd /usr/local/hadoop/ bin/start-all.sh jps $SPARK_HOME/bin/pyspark  ","permalink":"https://mlwhiz.com/blog/2014/09/28/learning_pyspark/","tags":["Big Data","Machine Learning"],"title":"Learning pyspark ‚Äì Installation ‚Äì Part 1"},{"categories":["Data Science","Big Data"],"contents":"It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all. What I understood finally is that Hadoop is basically comprised of 3 elements:\n A File System Map ‚Äì Reduce Its many individual Components.  Let‚Äôs go through each of them one by one.\n1. Hadoop as a File System: One of the main things that Hadoop provides is cheap data storage. What happens intrinsically is that the Hadoop system takes a file, cuts it into chunks and keeps those chunks at different places in a cluster. Suppose you have a big big file in your local system and you want that file to be:\n On the cloud for easy access Processable in human time  The one thing you can look forward to is Hadoop.\nAssuming that you have got hadoop installed on the amazon cluster you are working on.\nStart the Hadoop Cluster: You need to run the following commands to start the hadoop cluster(Based on location of hadoop installation directory):\ncd /usr/local/hadoop/ bin/start-all.sh jps Adding File to HDFS: Every command in Hadoop starts with hadoop fs and the rest of it works like the UNIX syntax. To add a file ‚Äúpurchases.txt‚Äù to the hdfs system:\nhadoop fs -put purchases.txt /usr/purchases.txt 2. Hadoop for Map-Reduce: MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.\nWhile Hadoop is implemented in Java, you can use almost any language to do map-reduce in hadoop using hadoop streaming. Suppose you have a big file containing the Name of store and sales of store each hour. And you want to find out the sales per store using map-reduce. Lets Write a sample code for that:\nInputFile\nA,300,12:00 B,234,1:00 C,234,2:00 D,123,3:00 A,123,1:00 B,346,2:00  Mapper.py\nimport sys def mapper(): # The Mapper takes inputs from stdin and prints out store name and value for line in sys.stdin: data = line.strip().split(\u0026#34;,\u0026#34;) storeName,Value,time=data print \u0026#34;{0},{1}\u0026#34;.format(storeName,Value) Reducer.py\nimport sys def reducer(): # The reducer takes inputs from mapper and prints out aggregated store name and value salesTotal = 0 oldKey = None for line in sys.stdin: data = line.strip().split(\u0026#34;,\u0026#34;) #Adding a little bit of Defensive programming if len(data) != 2: continue curKey,curVal = data if oldKey adn oldKey != curKey: print \u0026#34;{0},{1}\u0026#34;.format(oldKey,salesTotal) salesTotal=0 oldKey=curKey salesTotal += curVal if oldkey!=None: print \u0026#34;{0},{1}\u0026#34;.format(oldKey,salesTotal) Running the program on shell using pipes\ntextfile.txt | ./mapper.py | sort | ./reducer.py Running the program on mapreduce using Hadoop Streaming\nhadoop jar contrib/streaming/hadoop-*streaming*.jar / -file mapper.py -mapper mapper.py / -file reducer.py -reducer reducer.py / -input /inputfile -output /outputfile 3. Hadoop Components: Now if you have been following Hadoop you might have heard about Apache, Cloudera, HortonWorks etc. All of these are Hadoop vendors who provide Hadoop Along with its components. I will talk about the main component of Hadoop here ‚Äì Hive. So what exactly is Hive: Hive is a SQL like interface to map-reduce queries. So if you don‚Äôt understand all the hocus-pocus of map-reduce but know SQL, you can do map-reduce via Hive. Seems Promising? It is. While the syntax is mainly SQL, it is still a little different and there are some quirks that we need to understand to work with Hive. First of all lets open hive command prompt: For that you just have to type ‚Äúhive‚Äù, and voila you are in. Here are some general commands\nshow databases # -- See all Databases use database # -- Use a particular Database show tables # -- See all tables in a particular Database describe table Creating an external table:\nCREATE EXTERNAL TABLE IF NOT EXISTS BXDataSet (ISBN STRING,BookTitle STRING, ImageURLL STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‚Äò;‚Äô STORED AS TEXTFILE; LOAD DATA INPATH ‚Äò/user/book.csv‚Äô OVERWRITE INTO TABLE BXDataSet; The query commands work the same way as in SQL. You can do all the group by and hive will automatically convert it in map-reduce:\nselect * from tablename; Stay Tuned for Part 2 ‚Äì Where we will talk about another components of Hadoop ‚Äì PIG To learn more about hadoop in the meantime these are the books I recommend:\n  ","permalink":"https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/","tags":["Big Data","Machine Learning"],"title":"Hadoop, Mapreduce and More ‚Äì Part 1"}]