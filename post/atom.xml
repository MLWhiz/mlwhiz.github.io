<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Posts on MLWhiz</title>
    <link>https://mlwhiz.com/post/</link>
    <description>Recent content in Posts on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/post/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>3 Awesome Visualization Techniques for every dataset</title>
      <link>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/visualizations/kelly_Sikemma.jpeg"></media:content>
      

      
      <description>Visualizations are awesome. However, a good visualization is annoyingly hard to make.
Moreover, it takes time and effort when it comes to present these visualizations to a bigger audience.
We all know how to make Bar-Plots, Scatter Plots, and Histograms, yet we don&amp;rsquo;t pay much attention to beautify them.
This hurts us - our credibility with peers and managers. You won&amp;rsquo;t feel it now, but it happens.</description>
      
      
    </item>
    

    <item>
      <title>Chatbots  aren&#39;t as difficult to make as You Think</title>
      <link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/chatbot/dvader.jpeg"></media:content>
      

      
      <description>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them. Are they hard to create? What technologies should I know before attempting to work on them?</description>
      
      
    </item>
    

    <item>
      <title>Why Sublime Text for Data Science is Hotter than Jennifer Lawrence?</title>
      <link>https://mlwhiz.com/blog/2019/03/31/sublime_ds_post/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/31/sublime_ds_post/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/sublime_ds/sublime_tool.jpeg"></media:content>
      

      
      <description>Just Kidding, Nothing is hotter than Jennifer Lawrence. But as you are here, let&amp;rsquo;s proceed.
For a practitioner in any field, they turn out as good as the tools they use. Data Scientists are no different. But sometimes we don&amp;rsquo;t even know which tools we need and also if we need them. We are not able to fathom if there could be a more natural way to solve the problem we face.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/nlp_tl/spiderman.jpeg"></media:content>
      

      
      <description>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/birnn.png"></media:content>
      

      
      <description>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc.</description>
      
      
    </item>
    

    <item>
      <title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title>
      <link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/silver/CV_vs_LB.png"></media:content>
      

      
      <description>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 2 - Conventional Methods for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/tfidf.png"></media:content>
      

      
      <description>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the various preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title>
      <link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/text_processing_flow_1.png"></media:content>
      

      
      <description>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.
Since we have a large amount of material to cover, I am splitting this post into a series of posts.</description>
      
      
    </item>
    

    <item>
      <title>A Layman guide to moving from Keras to Pytorch</title>
      <link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comimages/artificial-neural-network.png"></media:content>
      

      
      <description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds.</description>
      
      
    </item>
    

    <item>
      <title>What Kagglers are using for Text Classification</title>
      <link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comimages/text_convolution.png"></media:content>
      

      
      <description>With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. How could you use that?
 To find sentiment of a review. Find toxic comments in a platform like Facebook Find Insincere questions on Quora.</description>
      
      
    </item>
    

    <item>
      <title>To all Data Scientists - The one Graph Algorithm you need to know</title>
      <link>https://mlwhiz.com/blog/2018/12/07/connected_components/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/12/07/connected_components/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comhttps://upload.wikimedia.org/wikipedia/commons/8/85/Pseudoforest.svg"></media:content>
      

      
      <description>Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that.
Here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.</description>
      
      
    </item>
    

    <item>
      <title>Object Detection: An End to End Theoretical Perspective</title>
      <link>https://mlwhiz.com/blog/2018/09/22/object_detection/</link>
      <pubDate>Sat, 22 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/09/22/object_detection/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comimages/id1.png"></media:content>
      

      
      <description>We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets. ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer&amp;rsquo;s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset.</description>
      
      
    </item>
    

    <item>
      <title>Hyperopt - A bayesian Parameter Tuning Framework</title>
      <link>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comhttps://1.gravatar.com/avatar/14e38645b7816711ca19e971e879c63b?s=180&amp;d=identicon&amp;r=G"></media:content>
      

      
      <description>Recently I was working on a in-class competition from the &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using XGBoost for time series prediction tasks.
The one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed.</description>
      
      
    </item>
    

    <item>
      <title>Using XGBoost for time series prediction tasks</title>
      <link>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comimages/lboard.png"></media:content>
      

      
      <description>Recently Kaggle master Kazanova along with some of his friends released a &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.
  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.</description>
      
      
    </item>
    

    <item>
      <title>Good Feature Building Techniques - Tricks for Kaggle -  My Kaggle Code Repository</title>
      <link>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comhttps://storage.googleapis.com/kaggle-organizations/4/thumbnail.png"></media:content>
      

      
      <description>Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing).</description>
      
      
    </item>
    

    <item>
      <title>The story of every distribution - Discrete Distributions</title>
      <link>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comimages/output_14_0.png"></media:content>
      

      
      <description>Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track. One of the most helpful way to learn more about these is the STAT110 course by Joe Blitzstein and his book.</description>
      
      
    </item>
    

    <item>
      <title>Today I Learned This Part 2: Pretrained Neural Networks What are they?</title>
      <link>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</link>
      <pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comhttps://image.slidesharecdn.com/practicaldeeplearning-160329181459/95/practical-deep-learning-16-638.jpg"></media:content>
      

      
      <description>Deeplearning is the buzz word right now. I was working on the course for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the Dogs vs Cats competition on Kaggle to achieve very good results.
What are pretrained Neural Networks? So let me tell you about the background a little bit. There is a challenge that happens every year in the visual recognition community - The Imagenet Challenge.</description>
      
      
    </item>
    

    <item>
      <title>Maths Beats Intuition probably every damn time</title>
      <link>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</guid>
      
      

      
      <description>Newton once said that &amp;ldquo;God does not play dice with the universe&amp;rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.
One of such ways intuition plays with us is with the Birthday problem.
Problem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?</description>
      
      
    </item>
    

    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      
      

      
      <description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;rsquo;t know bag of words: If we have a series of sentences(documents)</description>
      
      
    </item>
    

    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      
      

      
      <description>I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me.</description>
      
      
    </item>
    

    <item>
      <title>Basics Of Linear Regression</title>
      <link>https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/23/basics_of_linear_regression/</guid>
      
      

      
      <description>Today we will look into the basics of linear regression. Here we go :
Contents  Simple Linear Regression (SLR) Multiple Linear Regression (MLR) Assumptions  1. Simple Linear Regression Regression is the process of building a relationship between a dependent variable and set of independent variables. Linear Regression restricts this relationship to be linear in terms of coefficients. In SLR, we consider only one independent variable.
Example: The Waist Circumference – Adipose Tissue data  Studies have shown that individuals with excess Adipose tissue (AT) in the abdominal region have a higher risk of cardio-vascular diseases</description>
      
      
    </item>
    

    <item>
      <title>Top advice for a Data Scientist</title>
      <link>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</guid>
      
      

      
      <description>A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:
1. Beware of the Clean Data Syndrome You need to ask yourself questions even before you start working on the data. Does this data make sense? Falsely assuming that the data is clean could lead you towards wrong Hypotheses.</description>
      
      
    </item>
    

    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</guid>
      
      

      
      <description>As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.</description>
      
      
    </item>
    

    <item>
      <title>Things to see while buying a Mutual Fund</title>
      <link>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</guid>
      
      

      
      <description>This is a post which deviates from my pattern fo blogs that I have wrote till now but I found that Finance also uses up a lot of Statistics. So it won&amp;rsquo;t be a far cry to put this on my blog here. I recently started investing in Mutual funds so thought of rersearching the area before going all in. Here is the result of some of my research.</description>
      
      
    </item>
    

    <item>
      <title>Pandas For All - Some Basic Pandas Functions</title>
      <link>https://mlwhiz.com/blog/2016/10/27/baby_panda/</link>
      <pubDate>Thu, 27 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/10/27/baby_panda/</guid>
      
      

      
      <description>It has been quite a few days I have been working with Pandas and apparently I feel I have gotten quite good at it. (Quite a Braggard I know) So thought about adding a post about Pandas usage here. I intend to make this post quite practical and since I find the pandas syntax quite self explanatory, I won&amp;rsquo;t be explaining much of the codes. Just the use cases and the code to achieve them.</description>
      
      
    </item>
    

    <item>
      <title>Deploying ML Apps using Python and Flask- Learning about Flask</title>
      <link>https://mlwhiz.com/blog/2016/01/10/deploying_ml_apps_using_python_flask/</link>
      <pubDate>Sun, 10 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/01/10/deploying_ml_apps_using_python_flask/</guid>
      
      

      
      <description>It has been a long time since I wrote anything on my blog. So thought about giving everyone a treat this time. Or so I think it is.
Recently I was thinking about a way to deploy all these machine learning models I create in python. I searched through the web but couldn&amp;rsquo;t find anything nice and easy. Then I fell upon this book by Sebastian Rashcka and I knew that it was what I was looking for.</description>
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know - Part II(AWK)</title>
      <link>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</link>
      <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</guid>
      
      

      
      <description>Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.
And it is easy to learn.</description>
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know -Part I</title>
      <link>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</guid>
      
      

      
      <description>Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).
Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.</description>
      
      
    </item>
    

    <item>
      <title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title>
      <link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid>
      
      

      
      <description>When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph.</description>
      
      
    </item>
    

    <item>
      <title>Learning Spark using Python: Basics and Applications</title>
      <link>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</link>
      <pubDate>Mon, 07 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</guid>
      
      

      
      <description>I generally have a use case for Hadoop in my daily job. It has made my life easier in a sense that I am able to get results which I was not able to see with SQL queries. But still I find it painfully slow. I have to write procedural programs while I work. As in merge these two datasets and then filter and then merge another dataset and then filter using some condition and yada-yada.</description>
      
      
    </item>
    

    <item>
      <title>Behold the power of MCMC</title>
      <link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link>
      <pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid>
      
      

      
      <description>Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.
So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.</description>
      
      
    </item>
    

    <item>
      <title>My Tryst With MCMC Algorithms</title>
      <link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link>
      <pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid>
      
      

      
      <description>The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.
  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.</description>
      
      
    </item>
    

    <item>
      <title>Hadoop Mapreduce Streaming Tricks and Techniques</title>
      <link>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</link>
      <pubDate>Sat, 09 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</guid>
      
      

      
      <description>I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.
Using Shell Scripts to run your Programs I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. You can effectively organize your workflow by using shell scripts.</description>
      
      
    </item>
    

    <item>
      <title>Exploring Vowpal Wabbit with the Avazu Clickthrough Prediction Challenge</title>
      <link>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</guid>
      
      

      
      <description>In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.</description>
      
      
    </item>
    

    <item>
      <title>Data Science 101 : Playing with Scraping in Python</title>
      <link>https://mlwhiz.com/blog/2014/10/02/data_science_101_python_pattern/</link>
      <pubDate>Thu, 02 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/10/02/data_science_101_python_pattern/</guid>
      
      

      
      <description>This is a simple illustration of using Pattern Module to scrape web data using Python. We will be scraping the data from imdb for the top TV Series along with their ratings
We will be using this link for this:
http://www.imdb.com/search/title?count=100&amp;num_votes=5000,&amp;ref_=gnr_tv_hr&amp;sort=user_rating,desc&amp;start=1&amp;title_type=tv_series,mini_series  This URL gives a list of top Rated TV Series which have number of votes atleast 5000. The Thing to note in this URL is the &amp;ldquo;&amp;amp;start=&amp;rdquo; parameter where we can specify which review should the list begin with.</description>
      
      
    </item>
    

    <item>
      <title>Dictvectorizer for One Hot Encoding of Categorical Data</title>
      <link>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</link>
      <pubDate>Tue, 30 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</guid>
      
      

      
      <description>THE PROBLEM: Recently I was working on the Criteo Advertising Competition on Kaggle. The competition was a classification problem which basically involved predicting the click through rates based on several features provided in the train data. Seeing the size of the data (11 GB Train), I felt that going with Vowpal Wabbit might be a better option.
But after getting to an CV error of .47 on the Kaggle LB and being stuck there , I felt the need to go back to Scikit learn.</description>
      
      
    </item>
    

    <item>
      <title>Learning pyspark – Installation – Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</guid>
      
      

      
      <description>This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.
The installation is pretty simple. These steps were done on Mac OS Mavericks but should work for Linux too. Here are the steps for the installation:
1. Download the Binaries: Spark : http://spark.apache.org/downloads.html Scala : http://www.scala-lang.org/download/ Dont use Latest Version of Scala, Use Scala 2.10.x 2. Add these lines to your .</description>
      
      
    </item>
    

    <item>
      <title>Hadoop, Mapreduce and More – Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</link>
      <pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</guid>
      
      

      
      <description>It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all. What I understood finally is that Hadoop is basically comprised of 3 elements:
 A File System Map – Reduce Its many individual Components.  Let’s go through each of them one by one.
1. Hadoop as a File System: One of the main things that Hadoop provides is cheap data storage.</description>
      
      
    </item>
    
  </channel>
</rss>