<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.github.io/" rel="alternate"></link><link href="http://mlwhiz.github.io/feeds/statisticsprobability.atom.xml" rel="self"></link><id>http://mlwhiz.github.io/</id><updated>2018-12-17T04:43:00-02:00</updated><entry><title>Text Classification basics for Deep Learning</title><link href="http://mlwhiz.github.io/blog/2018/12/17/text_classification/" rel="alternate"></link><updated>2018-12-17T04:43:00-02:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2018-12-17:blog/2018/12/17/text_classification/</id><summary type="html">&lt;p&gt;With the problem of Image Classification is more or less solved by Deep learning, &lt;em&gt;Text Classification is the next new developing theme in deep learning&lt;/em&gt;. For those who don't know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that? &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To find sentiment of a review. &lt;/li&gt;
&lt;li&gt;Find toxic comments in a platform like Facebook&lt;/li&gt;
&lt;li&gt;Find Insincere questions on Quora. A current ongoing competition on kaggle&lt;/li&gt;
&lt;li&gt;Find fake reviews on websites&lt;/li&gt;
&lt;li&gt;Will a text advert get clicked or not&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And much more. The whole internet is filled with text and to categorise that information algorithmically will only give us incremental benefits to say the least in the field of AI. &lt;/p&gt;
&lt;p&gt;Here I am going to use the data from Quora's Insincere questions to talk about the different models that people are building and sharing to perform this task. Obviously these standalone models are not going to put you on the top of the leaderboard, yet I hope that this ensuing discussion would be helpful for people who want to learn more about text classification. This is going to be a long post in that regard.&lt;/p&gt;
&lt;p&gt;As a side note: if you want to know more about NLP, I would like to recommend this awesome course on &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Advanced machine learning specialization&lt;/a&gt;. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. &lt;/p&gt;
&lt;p&gt;So let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them.&lt;/p&gt;
&lt;h3&gt;1. TextCNN:&lt;/h3&gt;
&lt;p&gt;The idea of using a CNN to classify text was first presented in the paper &lt;a href="https://www.aclweb.org/anthology/D14-1181"&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt; by Yoon Kim. Instead of image pixels, the input to the tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one word vector. That is, each row is word-vector that represents a word. Thus a sequence of max length 70 gives us a image of 70(max sequence length)x300(embedding size)&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/text_convolution.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Now for some intuition. While for a image we move our conv filter horizontally also since here we have fixed our kernel size to filter_size x embed_size i.e. (3,300) we are just going to move down for the convolution taking look at three words at once since our filter size is 3 in this case.Also one can think of filter sizes as unigrams, bigrams, trigrams etc. Since we are looking at a context window of 1,2,3, and 5 words respectively. Here is the text classification network coded in Keras: &lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;# https://www.kaggle.com/yekenot/2dcnn-textclassifier
def model_cnn(embedding_matrix):
    filter_sizes = [1,2,3,5]
    num_filters = 36

    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
    x = Reshape((maxlen, embed_size, 1))(x)

    maxpool_pool = []
    for i in range(len(filter_sizes)):
        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),
                                     kernel_initializer='he_normal', activation='elu')(x)
        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))

    z = Concatenate(axis=1)(maxpool_pool)   
    z = Flatten()(z)
    z = Dropout(0.1)(z)

    outp = Dense(1, activation="sigmoid")(z)

    model = Model(inputs=inp, outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a &lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-textcnn"&gt;kaggle kernel&lt;/a&gt; for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.661 on the public leaderboard. &lt;/p&gt;
&lt;h3&gt;2. BiDirectional RNN(LSTM/GRU):&lt;/h3&gt;
&lt;p&gt;TextCNN takes care of a lot of things. For example it takes care of words in close range. It is able to see "new york" together. But it still can't take care of all the context provided in a particular text sequence. It still does not learn the seem to learn the sequential structure of the data, where every word is dependednt on the previous word. Or a word in the previous sentence. &lt;/p&gt;
&lt;p&gt;RNN help us with that. &lt;em&gt;They are able to remember previous information using hidden states and connect it to the current task.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time. More over the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won't work for a time sweries prediction task).&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/birnn.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;For a most simplistic explanation of Bidirectional RNN, think of RNN cell as taking as input a hidden state(a vector) and the word vector and giving out an output vector and the next hidden state. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;        &lt;span class="n"&gt;Hidden&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Word&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RNN&lt;/span&gt; &lt;span class="n"&gt;Cell&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Output&lt;/span&gt; &lt;span class="n"&gt;Vector&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Next&lt;/span&gt; &lt;span class="n"&gt;Hidden&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For a sequence of length 4 like 'you will never believe', The RNN cell will give 4 output vectors. Which can be concatenated and then used as part of a dense feedforward architecture. &lt;/p&gt;
&lt;p&gt;In the Bidirectional RNN the only change is that we read the text in the normal fashion as well in reverse. So we stack two RNNs in parallel and hence we get 8 output vectors to append. &lt;/p&gt;
&lt;p&gt;Once we get the output vectors we send them through a series of dense layers and finally a softmax layer to build a text classifier.&lt;/p&gt;
&lt;p&gt;Due to the limitations of RNNs like not remembering long term dependencies, in practice we almost always use LSTM/GRU to model long term dependencies. In such a case you can just think of the RNN cell being replaced by a LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangably with CuDNNLSTM, when you build models. &lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;# BiDirectional LSTM
def model_lstm_du(embedding_matrix):
    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
    '''
    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?
        64*70(maxlen)*2(bidirection concat)
    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU
    '''
    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)
    avg_pool = GlobalAveragePooling1D()(x)
    max_pool = GlobalMaxPooling1D()(x)
    conc = concatenate([avg_pool, max_pool])
    conc = Dense(64, activation="relu")(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="sigmoid")(conc)
    model = Model(inputs=inp, outputs=outp)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a &lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-bidirectionalrnn"&gt;kaggle kernel&lt;/a&gt; for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.671 on the public leaderboard. &lt;/p&gt;
&lt;h3&gt;3. Attention Models&lt;/h3&gt;
&lt;p&gt;The concept of Attention is relatively new as it comes from &lt;a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf"&gt;Hierarchical Attention Networks for Document Classification&lt;/a&gt; paper written jointly by CMU and Microsoft guys in 2016. &lt;/p&gt;
&lt;p&gt;So in the past we used to find features from text by doing a keyword extraction. Some word are more helpful in determining the category of a text than others. But in this method we sort of lost the sequential structure of text. With LSTM and deep learning methods while we are able to take case of the sequence structure we lose the ability to give higher weightage to more important words. 
Can we have the best of both worlds?&lt;/p&gt;
&lt;p&gt;And that is attention for you. In the author's words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract
such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/birnn attention.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In essense we want to create scores for every word in the text, which are the attention similarity score for a word. &lt;/p&gt;
&lt;p&gt;To do this we start with a weight matrix(W), a bias vector(b) and a context vector u. All of them will be learned by the optimmization algorithm.&lt;/p&gt;
&lt;p&gt;Then there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as non linearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to an exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v’s to get the Final Scores,s  &lt;/p&gt;
&lt;p&gt;These final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;def dot_product(x, kernel):
    """
    Wrapper for dot product operation, in order to be compatible with both
    Theano and Tensorflow
    Args:
        x (): input
        kernel (): weights
    Returns:
    """
    if K.backend() == 'tensorflow':
        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)
    else:
        return K.dot(x, kernel)

class AttentionWithContext(Layer):
    """
    Attention operation, with a context/query vector, for temporal data.
    Supports Masking.
    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]
    "Hierarchical Attention Networks for Document Classification"
    by using a context vector to assist the attention
    # Input shape
        3D tensor with shape: `(samples, steps, features)`.
    # Output shape
        2D tensor with shape: `(samples, features)`.
    How to use:
    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
    The dimensions are inferred based on the output shape of the RNN.
    Note: The layer has been tested with Keras 2.0.6
    Example:
        model.add(LSTM(64, return_sequences=True))
        model.add(AttentionWithContext())
        # next add a Dense layer (for classification/regression) or whatever...
    """

    def __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        super(AttentionWithContext, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight((input_shape[-1], input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight((input_shape[-1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight((input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_u'.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        super(AttentionWithContext, self).build(input_shape)

    def compute_mask(self, input, input_mask=None):
        # do not pass the mask to the next layers
        return None

    def call(self, x, mask=None):
        uit = dot_product(x, self.W)

        if self.bias:
            uit += self.b

        uit = K.tanh(uit)
        ait = dot_product(uit, self.u)

        a = K.exp(ait)

        # apply mask after the exp. will be re-normalized next
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in theano
            a *= K.cast(mask, K.floatx())

        # in some cases especially in the early stages of training the sum may be almost zero
        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.
        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())
        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]


def model_lstm_atten(embedding_matrix):
    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)
    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)
    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)
    x = AttentionWithContext()(x)
    x = Dense(64, activation="relu")(x)
    x = Dense(1, activation="sigmoid")(x)
    model = Model(inputs=inp, outputs=x)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a &lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-attention"&gt;kaggle kernel&lt;/a&gt; for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.682 on the public leaderboard. &lt;/p&gt;
&lt;p&gt;Hope that Helps! Do checkout the kernels for all the networks and see the comments too. I will try to write a part 2 of this post where I would like to talk about capsule networks and more techniques as they get used in this competition. &lt;/p&gt;
&lt;p&gt;Here are the kernel links again: &lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-textcnn"&gt;TextCNN&lt;/a&gt;,&lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-bidirectionalrnn"&gt;BiLSTM/GRU&lt;/a&gt;,&lt;a href="https://www.kaggle.com/mlwhiz/learning-text-classification-attention"&gt;Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Do upvote the kenels if you find them helpful.&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"&gt;CNN for NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://en.diveintodeeplearning.org/d2l-en.pdf&lt;/li&gt;
&lt;li&gt;https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2&lt;/li&gt;
&lt;li&gt;http://univagora.ro/jour/index.php/ijccc/article/view/3142&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/shujian/fork-of-mix-of-nn-models"&gt;Shujian's kernel on Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="text classification"></category><category term="birnn"></category><category term="bidirectional RNN"></category><category term="bidirectional LSTM for text"></category><category term="bidirectional GRU for text"></category><category term="Attention models for text"></category></entry><entry><title>The story of every distribution - Discrete Distributions</title><link href="http://mlwhiz.github.io/blog/2017/09/14/discrete_distributions/" rel="alternate"></link><updated>2017-09-14T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-09-14:blog/2017/09/14/discrete_distributions/</id><summary type="html">&lt;p&gt;Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track.
One of the most helpful way to learn more about these is the &lt;a href="https://projects.iq.harvard.edu/stat110/home"&gt;STAT110&lt;/a&gt; course by Joe Blitzstein and his &lt;a href="http://amzn.to/2xAsYzE"&gt;book&lt;/a&gt;. You can check out this &lt;a href="https://www.coursera.org/specializations/statistics?siteID=lVarvwc5BD0-1nQtJg8.ENATqSUIufAaaw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Coursera&lt;/a&gt; course too. Hope it could be useful to someone else too. So here goes:&lt;/p&gt;
&lt;h2&gt;1. Bernoulli Distribution:&lt;/h2&gt;
&lt;p&gt;Perhaps the most simple discrete distribution of all.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; A Coin is tossed with probability p of heads.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = \begin{cases}1-p &amp;amp; k = 0\\p &amp;amp; k = 1\end{cases}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X&amp;lt;=k) = \begin{cases}0 &amp;amp; k &amp;lt; 0\\1-p &amp;amp; 0=&amp;lt;k&amp;lt;1 \\1 &amp;amp; k &amp;gt;= 1\end{cases}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = 0*P(X=0)+1*P(X=1) = p$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = p^2$$&lt;/div&gt;
and
&lt;div class="math"&gt;$$E[X^2] = \sum k^2P(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X^2] =  0^2P(X=0) + 1^2P(X=1) = p $$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = p(1-p)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;2. Binomial Distribution:&lt;/h2&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/maxresdefault.jpg"  height="400" width="500" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;One of the most basic distribution in the Statistician toolkit. The parameters of this distribution is n(number of trials) and p(probability of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
Probability of getting exactly k successes in n trials&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n\\ k\end{array}\right) p^{k}(1-p)^{n-k}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = \sum_{i=0}^k  \left(\begin{array}{c}n\\ i\end{array}\right)  p^i(1-p)^{n-i} $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \sum_{k=0}^n k \left(\begin{array}{c}n\\ k\end{array}\right) * p^{k}(1-p)^{n-k} = np $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;A better way to solve this:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;X is the sum on n Indicator Bernoulli random variables.&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = E[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = E[I_{1}] + E[I_{2}] + ....+ E[I_{n-1}]+ E[I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \underbrace{p + p + ....+ p + p}_{n} = np$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;
X is the sum on n Indicator Bernoulli random variables.
&lt;div class="math"&gt;$$Var[X] = Var[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$Var[X] = Var[I_{1}] + Var[I_{2}] + ....+ Var[I_{n-1}]+ Var[I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$Var[X] = \underbrace{p(1-p) + p(1-p) + ....+ p(1-p) + p(1-p)}_{n} = np(1-p)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;3. Geometric Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is p(probability of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The number of failures before the first success(Heads) when a coin with probability p is tossed&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = (1-p)^kp$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = \sum_{i=0}^k (1-p)^{i}p$$&lt;/div&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = p(1+q+q^2...+q^k)= p(1-q^k)/(1-q) = 1-(1-p)^k $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \sum_{k=0}^{inf} k (1-p)^kp$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp +2q^2p +3q^3p +4q^4p .... $$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp(1+2q+3q^2+4q^3+....)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp/(1-q)^2 = q/p $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = q^2/p^2$$&lt;/div&gt;
and
&lt;div class="math"&gt;$$E[X^2] = \sum_0^k k^2q^kp= qp + 4q^2p + 9q^3p +16q^4p ... = qp(1+4q+9q^2+16q^3....)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X^2] = qp^{-2}(1+q)$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Thus,
&lt;div class="math"&gt;$$Var[X] =q/p^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Check Math appendix at bottom of this post for Geometric Series Proofs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. A doctor is seeking an anti-depressant for a newly diagnosed patient. Suppose that, of the available anti-depressant drugs, the probability that any particular drug will be effective for a particular patient is p=0.6. What is the probability that the first drug found to be effective for this patient is the first drug tried, the second drug tried, and so on? What is the expected number of drugs that will be tried to find one that is effective?&lt;/p&gt;
&lt;p&gt;A. Expected number of drugs that will be tried to find one that is effective = q/p = .4/.6 =.67&lt;/p&gt;
&lt;h2&gt;4. Negative Binomial Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is p(probability of success) and r(number of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The &lt;strong&gt;number of failures&lt;/strong&gt; of independent Bernoulli(p) trials before the rth success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Negative Binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;r successes , k failures , last attempt needs to be a success:
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The negative binomial RV could be stated as the sum of r Geometric RVs
&lt;div class="math"&gt;$$X = X^1+X^2.... X^{r-1} +X^r$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$E[X] = E[X^1]+E[X^2].... E[X^{r-1}] +E[X^r]$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = rq/p$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The negative binomial RV could be stated as the sum of r independent Geometric RVs
&lt;div class="math"&gt;$$X = X^1+X^2.... X^{r-1} +X^r$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = Var[X^1]+Var[X^2].... Var[X^{r-1}] +Var[X^r]$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = rq/p^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. Pat is required to sell candy bars to raise money for the 6th grade field trip. There are thirty houses in the neighborhood, and Pat is not supposed to return home until five candy bars have been sold. So the child goes door to door, selling candy bars. At each house, there is a 0.4 probability of selling one candy bar and a 0.6 probability of selling nothing.
What's the probability of selling the last candy bar at the nth house?&lt;/p&gt;
&lt;p&gt;A. r = 5 ; k = n - r&lt;/p&gt;
&lt;p&gt;Probability of selling the last candy bar at the nth house =
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n-1\\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;5. Poisson Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; the rate parameter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt;
There is as such no story to this distribution but only motivation for using this distribution. The Poisson distribution is often used for applications where we count the successes of a large number of trials where the per-trial success rate is small. For example, the Poisson distribution is a good starting point for counting the number of people who email you over the course of an hour.The number of chocolate chips in a chocolate chip cookie is another good candidate for a Poisson distribution, or the number of earthquakes in a year in some particular region&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Poisson Distribution is given by:&lt;/strong&gt;
&lt;div class="math"&gt;$$ P(X=k) = \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \sum_{k=0}^{inf} k \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \lambda e^{-\lambda}\sum_{k=0}^{inf}  \frac{\lambda^{k-1}} {(k-1)!}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \lambda e^{-\lambda} e^{\lambda} = \lambda $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = \lambda + \lambda^2$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = \lambda$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. If electricity power failures occur according to a Poisson distribution with an average of 3 failures every twenty weeks, calculate the probability that there will not be more than one failure during a particular week?&lt;/p&gt;
&lt;p&gt;A. Probability = P(X=0)+P(X=1) = $e^{-3/20} + e^{-3/20}3/20 = 23/20*e^{-3/20} $&lt;/p&gt;
&lt;p&gt;Probability of selling the last candy bar at the nth house =
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n-1\\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;Math Appendix:&lt;/h2&gt;
&lt;p&gt;Some Math (For Geometric Distribution) :&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$a+ar+ar^2+ar^3+⋯=a/(1−r)=a(1−r)^{−1}$$&lt;/div&gt;
Taking the derivatives of both sides, the first derivative with respect to r must be:
&lt;div class="math"&gt;$$a+2ar+3ar^2+4ar^3⋯=a(1−r)^{−2}$$&lt;/div&gt;
Multiplying above with r:
&lt;div class="math"&gt;$$ar+2ar^2+3ar^3+4ar^4⋯=ar(1−r)^{−2}$$&lt;/div&gt;
Taking the derivatives of both sides, the first derivative with respect to r must be:
&lt;div class="math"&gt;$$a+4ar+9ar^2+16ar^3⋯=a(1−r)^{-3}(1+r)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;Bonus - Python Graphs and Functions:&lt;/h2&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Useful Function to create graph
def chart_creator(x,y,title):
    import matplotlib.pyplot as plt  #sets up plotting under plt
    import seaborn as sns           #sets up styles and gives us more plotting options
    import pandas as pd             #lets us handle data as dataframes
    %matplotlib inline
    # Create a list of 100 Normal RVs
    data = pd.DataFrame(zip(x,y))
    data.columns = ['x','y']
    # We dont Probably need the Gridlines. Do we? If yes comment this line
    sns.set(style="ticks")

    # Here we create a matplotlib axes object. The extra parameters we use
    # "ci" to remove confidence interval
    # "marker" to have a x as marker.
    # "scatter_kws" to provide style info for the points.[s for size]
    # "line_kws" to provide style info for the line.[lw for line width]

    g = sns.regplot(x='x', y='y', data=data, ci = False,
        scatter_kws={"color":"darkred","alpha":0.3,"s":90},
        line_kws={"color":"g","alpha":0.5,"lw":0},marker="x")

    # remove the top and right line in graph
    sns.despine()

    # Set the size of the graph from here
    g.figure.set_size_inches(12,8)
    # Set the Title of the graph from here
    g.axes.set_title(title, fontsize=34,color="r",alpha=0.5)
    # Set the xlabel of the graph from here
    g.set_xlabel("k",size = 67,color="r",alpha=0.5)
    # Set the ylabel of the graph from here
    g.set_ylabel("pmf",size = 67,color="r",alpha=0.5)
    # Set the ticklabel size and color of the graph from here
    g.tick_params(labelsize=14,labelcolor="black")
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here I will generate the PMFs of the discrete distributions we just discussed above using Pythons built in functions. For more details on the upper function, please see my previous post - &lt;a href="http://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/"&gt;Create basic graph visualizations with SeaBorn&lt;/a&gt;. Also take a look at the &lt;a href="https://docs.scipy.org/doc/scipy/reference/stats.html"&gt;documentation&lt;/a&gt; guide for the below functions&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Binomial :
from scipy.stats import binom
n=30
p=0.5
k = range(0,n)
pmf = binom.pmf(k, n, p)
chart_creator(k,pmf,"Binomial PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_12_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Geometric :
from scipy.stats import geom
n=30
p=0.5
k = range(0,n)
# -1 here is the location parameter for generating the PMF we want.
pmf = geom.pmf(k, p,-1)
chart_creator(k,pmf,"Geometric PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_13_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Negative Binomial :
from scipy.stats import nbinom
r=5 # number of successes
p=0.5 # probability of Success
k = range(0,25) # number of failures
# -1 here is the location parameter for generating the PMF we want.
pmf = nbinom.pmf(k, r, p)
chart_creator(k,pmf,"Nbinom PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_14_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;#Poisson
from scipy.stats import poisson
lamb = .3 # Rate
k = range(0,5)
pmf = poisson.pmf(k, lamb)
chart_creator(k,pmf,"Poisson PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_15_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://amzn.to/2xAsYzE"&gt;Introduction to Probability by Joe Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next thing I want to come up with is a same sort of post for continuous distributions too. Keep checking for the same. Till then Ciao.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processClass: 'mathjax', " +
        "        ignoreClass: 'no-mathjax', " +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="distributions"></category><category term="pdf"></category><category term="cdf"></category><category term="expected value"></category><category term="variance"></category><category term="binomial"></category><category term="poisson"></category><category term="geometric"></category></entry></feed>