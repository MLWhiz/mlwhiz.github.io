<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.com/" rel="alternate"></link><link href="http://mlwhiz.com/feeds/hadoop.atom.xml" rel="self"></link><id>http://mlwhiz.com/</id><updated>2015-05-09T13:43:00-03:00</updated><entry><title>Hadoop Mapreduce Streaming Tricks and Techniques</title><link href="http://mlwhiz.com/blog/2015/05/09/Hadoop_Mapreduce_Streaming_Tricks_and_Techniques/" rel="alternate"></link><updated>2015-05-09T13:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.com,2015-05-09:blog/2015/05/09/Hadoop_Mapreduce_Streaming_Tricks_and_Techniques/</id><summary type="html">&lt;p&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.&lt;/p&gt;
&lt;h3 id="using-shell-scripts-to-run-your-programs"&gt;Using Shell Scripts to run your Programs&lt;/h3&gt;
&lt;div style="margin-top: -9px; margin-bottom: -30px;"&gt;
&lt;p&gt;&lt;img src="/images/I-love-bash-1024x220.png" &gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt; I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. &lt;em&gt;You can effectively organize your workflow by using shell scripts.&lt;/em&gt; Now Shell scripts are not as formidable as they sound. We wont be doing programming perse using these shell scripts(Though they are pretty good at that too), we will just use them to store commands that we need to use sequentially.&lt;/p&gt;
&lt;p&gt;Below is a sample of the shell script I use to run my Mapreduce Codes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;link rel="stylesheet" href="/theme/highlight/styles/default.css"&gt;
&lt;script src="/theme/highlight/highlight.pack.js"&gt;&lt;/script&gt;
&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em; background-color:#000000;"&gt;
&lt;code class="bash" style="background-color:#000000; color:#FFFFFF"&gt;#!/bin/bash
#Defining program variables
IP="/data/input"
OP="/data/output"
HADOOP_JAR_PATH="/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar"
MAPPER="test_m.py"
REDUCER="test_r.py"

hadoop fs -rmr -skipTrash&amp;nbsp;$OP
hadoop jar&amp;nbsp;$HADOOP_JAR_PATH \
-file&amp;nbsp;$MAPPER -mapper "python test_m.py" \
-file&amp;nbsp;$REDUCER -reducer "python test_r.py" \
-input&amp;nbsp;$IP -output&amp;nbsp;$OP
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
I generally save them as test_s.sh and whenever i need to run them i simply type &lt;code&gt;sh test_s.sh&lt;/code&gt;. This helps in three ways.
&lt;ul&gt;
&lt;li&gt;
It helps me to store hadoop commands in a manageable way.
&lt;/li&gt;
&lt;li&gt;
It is easy to run the mapreduce code using the shell script.
&lt;/li&gt;
&lt;li&gt;
&lt;em&gt;&lt;strong&gt;If the code fails, I do not have to manually delete the output directory&lt;/strong&gt;&lt;/em&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;em&gt; The simplification of anything is always sensational. &lt;br&gt;&lt;/em&gt; &lt;small&gt;Gilbert K. Chesterton&lt;/small&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h3 id="using-distributed-cache-to-provide-mapper-with-a-dictionary"&gt;Using Distributed Cache to provide mapper with a dictionary&lt;/h3&gt;
&lt;div style="margin-top: -9px; margin-bottom: -30px;"&gt;
&lt;p&gt;&lt;img src="/images/Game-Of-Thrones-Wallpaper-House-Sigils-1.png"&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt; Often times it happens that you want that your Hadoop Mapreduce program is able to access some static file. This static file could be a dictionary, could be parameters for the program or could be anything. What distributed cache does is that it provides this file to all the mapper nodes so that you can use that file in any way across all your mappers. Now this concept although simple would help you to think about Mapreduce in a whole new light. Lets start with an example. Supppose you have to create a sample Mapreduce program that reads a big file containing the information about all the characters in &lt;a href="http://www.hbo.com/game-of-thrones"&gt;Game of Thrones&lt;/a&gt; stored as &lt;strong&gt;&lt;code&gt;&amp;quot;/data/characters/&amp;quot;&lt;/code&gt;&lt;/strong&gt;:
&lt;div style="width: 50%; margin: 0 auto;"&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;
Cust_ID
&lt;/th&gt;
&lt;th&gt;
User_Name
&lt;/th&gt;
&lt;th&gt;
House
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
1
&lt;/td&gt;
&lt;td&gt;
Daenerys Targaryen
&lt;/td&gt;
&lt;td&gt;
Targaryen
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
2
&lt;/td&gt;
&lt;td&gt;
Tyrion Lannister
&lt;/td&gt;
&lt;td&gt;
Lannister
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
3
&lt;/td&gt;
&lt;td&gt;
Cersei Lannister
&lt;/td&gt;
&lt;td&gt;
Lannister
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="warning"&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;td&gt;
Robert Baratheon
&lt;/td&gt;
&lt;td&gt;
Baratheon
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="warning"&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;td&gt;
Robb Stark
&lt;/td&gt;
&lt;td&gt;
Stark
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;But you dont want to use the dead characters in the file for the analysis you want to do. &lt;em&gt;You want to count the number of living characters in Game of Thrones grouped by their House&lt;/em&gt;. (I know its easy!!!!!) One thing you could do is include an if statement in your Mapper Code which checks if the persons ID is 4 then exclude it from the mapper and such. But the problem is that you would have to do it again and again for the same analysis as characters die like flies when it comes to George RR Martin.(Also where is the fun in that) So you create a file which contains the Ids of all the dead characters at &lt;strong&gt;&lt;code&gt;&amp;quot;/data/dead_characters.txt&amp;quot;&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;div style="width: 50%; margin: 0 auto;"&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;
Died
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Whenever you have to run the analysis you can just add to this file and you wont have to change anything in the code. Also sometimes this file would be long and you would not want to clutter your code with IDs and such.&lt;/p&gt;
&lt;p&gt;So How Would we do it. Let's go in a step by step way around this. We will create a shell script, a mapper script and a reducer script for this task.&lt;/p&gt;
&lt;h5 id="shell-script"&gt;1) Shell Script&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;link rel="stylesheet" href="/theme/highlight/styles/default.css"&gt;
&lt;script src="/theme/highlight/highlight.pack.js"&gt;&lt;/script&gt;
&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em; background-color:#000000;"&gt;
&lt;code class="bash" style="background-color:#000000; color:#FFFFFF"&gt;#!/bin/bash
#Defining program variables
DC="/data/dead_characters.txt"
IP="/data/characters"
OP="/data/output"
HADOOP_JAR_PATH="/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar"
MAPPER="got_living_m.py"
REDUCER="got_living_r.py"

hadoop jar&amp;nbsp;$HADOOP_JAR_PATH \
-file&amp;nbsp;$MAPPER -mapper "python got_living_m.py" \
-file&amp;nbsp;$REDUCER -reducer "python got_living_r.py" \
-cacheFile&amp;nbsp;$DC#ref \
-input&amp;nbsp;$IP -output&amp;nbsp;$OP
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Note how we use the &lt;code&gt;&amp;quot;-cacheFile&amp;quot;&lt;/code&gt; option here. We have specified that we will refer to the file that has been provided in the Distributed cache as &lt;code&gt;#ref&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next is our Mapper Script.&lt;/p&gt;
&lt;h5 id="mapper-script"&gt;2) Mapper Script&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;link rel="stylesheet" href="/theme/highlight/styles/default.css"&gt;
&lt;script src="/theme/highlight/highlight.pack.js"&gt;&lt;/script&gt;
&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em; background-color:#000000;"&gt;
&lt;code class="python" style="background-color:#000000; color:#FFFFFF"&gt;import sys
dead_ids = set()

def read_cache():
    for line in open('ref'):
        id = line.strip()
        dead_ids.add(id)

read_cache()

for line in sys.stdin:
    rec = line.strip().split("|") # Split using Delimiter "|"
    id = rec[0]
    house = rec[2]
    if id not in dead_ids:
        print "%s\t%s" % (house,1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And our Reducer Script.&lt;/p&gt;
&lt;h5 id="reducer-script"&gt;3) Reducer Script&lt;/h5&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;link rel="stylesheet" href="/theme/highlight/styles/darkula.css"&gt;
&lt;script src="/theme/highlight/highlight.pack.js"&gt;&lt;/script&gt;
&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em; background-color:#000000;"&gt;
&lt;code class="python" style="background-color:#000000; color:#FFFFFF"&gt;import sys
current_key = None
key = None
count = 0

for line in sys.stdin:
    line = line.strip()
    rec = line.split('\t')
    key = rec[0]    
    value = int(rec[1])
    
    if current_key == key:
        count += value
    else:
        if current_key:
            print "%s:%s" %(key,str(count))     
        current_key = key
        count = value

if current_key == key:
    print "%s:%s" %(key,str(count)) 
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;This was a simple program and the output will be just what you expected and not very exciting. &lt;em&gt;&lt;strong&gt;But the Technique itself solves a variety of common problems. You can use it to pass any big dictionary to your Mapreduce Program&lt;/strong&gt;&lt;/em&gt;. Atleast thats what I use this feature mostly for. Hope You liked it. Will try to expand this post with more tricks.&lt;/p&gt;
&lt;p&gt;The codes for this post are posted at github &lt;a href="https://github.com/MLWhiz/Hadoop-Mapreduce-Tricks"&gt;here&lt;/a&gt;.&lt;/p&gt;
Other Great Learning Resources For Hadoop:
&lt;ul&gt;
&lt;li&gt;
&amp;lt;a href=&amp;quot;http://www.google.co.in/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB0QFjAA&amp;amp;url=http%3A%2F%2Fwww.michael-noll.com%2Ftutorials%2Fwriting-an-hadoop-mapreduce-program-in-python%2F&amp;amp;ei=8RRVVdP2IMe0uQShsYDYBg&amp;amp;usg=AFQjCNH3DqrlSIG8D-K8jgQWTALic1no5A&amp;amp;sig2=BivwTW6mdJs5c9w9VaSK2Q&amp;amp;bvm=bv.93112503,d.c2E&amp;quot;&amp;gt;Michael Noll's Hadoop Mapreduce Tutorial&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&amp;lt;a href=&amp;quot;http://www.google.co.in/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CCMQFjAB&amp;amp;url=http%3A%2F%2Fhadoop.apache.org%2Fdocs%2Fr1.2.1%2Fstreaming.html&amp;amp;ei=8RRVVdP2IMe0uQShsYDYBg&amp;amp;usg=AFQjCNEIB4jmqcBs-GepHdn7DRxqTI9zXA&amp;amp;sig2=nYkAnDjjjaum5YVlYuMUJQ&amp;amp;bvm=bv.93112503,d.c2E&amp;quot;&amp;gt;Apache's Hadoop Streaming Documentation&lt;/a&gt;
&lt;/li&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Also I like these books a lot. Must have for a Hadooper....&lt;/p&gt;
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a target="_blank"  href="https://www.amazon.com/gp/product/1785887211/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1785887211&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=a0e7b4f0b2ea4a5146042890e1c04f7e"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1785887211&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1785887211" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a target="_blank"  href="https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491901632&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=4122280e94f7bbd0ceebc9d13e60d103"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491901632&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491901632" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;

&lt;p&gt;The first book is a guide for using Hadoop as well as spark with Python. While the second one contains a detailed overview of all the things in Hadoop. Its the definitive guide.&lt;/p&gt;

&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;</summary><category term="hadoop"></category><category term="python"></category></entry><entry><title>Hadoop, Mapreduce and More – Part 1</title><link href="http://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/" rel="alternate"></link><updated>2014-09-27T13:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.com,2014-09-27:blog/2014/09/27/hadoop_mapreduce/</id><summary type="html">&lt;p&gt;It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all.
What I understood finally is that Hadoop is basically comprised of 3 elements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A File System&lt;/li&gt;
&lt;li&gt;Map – Reduce&lt;/li&gt;
&lt;li&gt;Its many individual Components.
Let’s go through each of them one by one.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;1. Hadoop as a File System:&lt;/h2&gt;
&lt;p&gt;One of the main things that Hadoop provides is cheap data storage. What happens intrinsically is that the Hadoop system takes a file, cuts it into chunks and keeps those chunks at different places in a cluster. Suppose you have a big big file in your local system and you want that file to be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;On the cloud for easy access&lt;/li&gt;
&lt;li&gt;Processable in human time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The one thing you can look forward to is Hadoop.&lt;/p&gt;
&lt;p&gt;Assuming that you have got hadoop instaled on the amazon cluster you are working on.(If you don’t know how to setup a hadoop cluster, go to this guy)&lt;/p&gt;
&lt;h3&gt;Start the Hadoop Cluster:&lt;/h3&gt;
&lt;p&gt;You need to run the following commands to start the hadoop cluster(Based on location of hadoop installation directory):&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;cd /usr/local/hadoop/
bin/start-all.sh
jps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Adding File to HDFS: Every command in Hadoop starts with hadoop fs and the rest of it works like the UNIX syntax. To add a file “purchases.txt” to the hdfs system:&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;hadoop fs -put purchases.txt /usr/purchases.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. Hadoop for Map-Reduce:&lt;/h2&gt;
&lt;p&gt;MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.&lt;/p&gt;
&lt;p&gt;While Hadoop is implemented in Java, you can use almost any language to do map-reduce in hadoop using hadoop streaming. Suppose you have a big file containing the Name of store and sales of store each hour. And you want to find out the sales per store using map-reduce. Lets Write a sample code for that: &lt;/p&gt;
&lt;p&gt;InputFile&lt;/p&gt;
&lt;pre style="font-family:courier new,monospace; background-color:#f6c6529c; color:#000000"&gt;A,300,12:00
B,234,1:00
C,234,2:00
D,123,3:00
A,123,1:00
B,346,2:00
&lt;/pre&gt;

&lt;p&gt;Mapper.py&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;import sys
def mapper():
    # The Mapper takes inputs from stdin and prints out store name and value
    for line in sys.stdin:
        data = line.strip().split(",")
        storeName,Value,time=data
        print "{0},{1}".format(storeName,Value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reducer.py&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;import sys
def reducer():
    # The reducer takes inputs from mapper and prints out aggregated store name and value
    salesTotal = 0
    oldKey = None
    for line in sys.stdin:
        data = line.strip().split(",")
        #Adding a little bit of Defensive programming
        if len(data) != 2:
            continue
        curKey,curVal = data
        if oldKey adn oldKey != curKey:
            print "{0},{1}".format(oldKey,salesTotal)
            salesTotal=0
        oldKey=curKey
        salesTotal += curVal
    if oldkey!=None:
        print "{0},{1}".format(oldKey,salesTotal)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the program on shell using pipes&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="bash"&gt;textfile.txt | ./mapper.py | sort | ./reducer.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the program on mapreduce using Hadoop Streaming &lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="bash"&gt;hadoop jar contrib/streaming/hadoop-*streaming*.jar /
-file mapper.py -mapper mapper.py /
-file reducer.py -reducer reducer.py /
-input /inputfile -output /outputfile
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. Hadoop Components:&lt;/h2&gt;
&lt;p&gt;Now if you have been following Hadoop you might have heard about Apache, Cloudera, HortonWorks etc. All of these are Hadoop vendors who provide Hadoop Along with its components. I will talk about the main component of Hadoop here – Hive.
So what exactly is Hive: Hive is a SQL like interface to map-reduce queries. So if you don’t understand all the hocus-pocus of map-reduce but know SQL, you can do map-reduce via Hive.
Seems Promising? It is.
While the syntax is mainly SQL, it is still a little different and there are some quirks that we need to understand to work with Hive.
First of all lets open hive command prompt: For that you just have to type “hive”, and voila you are in.
Here are some general commands&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;show databases  #   -- See all Databases
use database     #     -- Use a particular Database
show tables       #     -- See all tables in a particular Database
describe table    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating an external table:&lt;/p&gt;
&lt;pre style="font-size:60%"&gt;
&lt;code class="python"&gt;CREATE EXTERNAL TABLE IF NOT EXISTS BXDataSet
(ISBN STRING,BookTitle STRING, ImageURLL STRING)
ROW FORMAT DELIMITED  FIELDS TERMINATED BY ‘;’ STORED AS TEXTFILE;
LOAD DATA INPATH ‘/user/book.csv’ OVERWRITE INTO TABLE BXDataSet;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The query commands work the same way as in SQL. You can do all the group by and hive will automatically convert it in map-reduce:&lt;/p&gt;
&lt;p&gt;select * from tablename;&lt;/p&gt;
&lt;p&gt;Stay Tuned for Part 2 – Where we will talk about another components of Hadoop – PIG
To learn more about hadoop in the meantime these are the books I recommend:&lt;/p&gt;
&lt;div style="text-align: center;"&gt;
&lt;a target="_blank"  href="https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491901632&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=4122280e94f7bbd0ceebc9d13e60d103"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491901632&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491901632" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;

&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;</summary><category term="hadoop"></category><category term="mapreduce"></category></entry></feed>