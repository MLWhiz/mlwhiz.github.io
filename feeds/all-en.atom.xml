<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.github.io/" rel="alternate"></link><link href="http://mlwhiz.github.io/feeds/all-en.atom.xml" rel="self"></link><id>http://mlwhiz.github.io/</id><updated>2018-09-22T04:43:00-03:00</updated><entry><title>Object Detection: An End to End Theoretical Perspective</title><link href="http://mlwhiz.github.io/blog/2018/09/22/object_detection/" rel="alternate"></link><updated>2018-09-22T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2018-09-22:blog/2018/09/22/object_detection/</id><summary type="html">&lt;p&gt;We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and &lt;a href="https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8"&gt;Transfer Learning&lt;/a&gt; using pre-trained nets.
&lt;br&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer's outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.
&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;As a side note: if you want to know more about convnets and Transfer Learning I would like to recommend this awesome course on &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Deep Learning in Computer Vision&lt;/a&gt; in the &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Advanced machine learning specialization&lt;/a&gt;. This course talks about various CNN architetures and covers a wide variety of problems in the image domain including detection and segmentation.&lt;/p&gt;
&lt;p&gt;But there are a lot many interesting problems in the Image domain. The one which we are going to focus on today is the Segmentation, Localization and Detection problem.
So what are these problems?&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id1.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;So these problems are divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class?&lt;/li&gt;
&lt;li&gt;Classification+Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number(commonly 1) in the image.&lt;/li&gt;
&lt;li&gt;Object Detection: A More general case of the Classification+Localization problem. In a real-world setting, we don't know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them?&lt;/li&gt;
&lt;li&gt;Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won't be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post, we will focus mainly on Object Detection.&lt;/p&gt;
&lt;h2&gt;Classification+Localization&lt;/h2&gt;
&lt;p&gt;So lets first try to understand how we can solve the problem when we have a single object in the image. The Classification+Localization case. Pretty neatly said in the CS231n notes:&lt;/p&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
Treat localization as a regression problem!
&lt;/div&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id2.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Input Data:&lt;/strong&gt; Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting we used to have data in the form (X,y) where X is the image and y used to be the class labels.
In the Classification+Localization setting we will have data normally in the form (X,y), where X is still the image and y is a array containing (class_label, x,y,w,h) where,&lt;/p&gt;
&lt;p&gt;x = bounding box top left corner x-coordinate&lt;/p&gt;
&lt;p&gt;y = bounding box top left corner y-coordinate&lt;/p&gt;
&lt;p&gt;w = width of bounding box in pixel&lt;/p&gt;
&lt;p&gt;h = height of bounding box in pixel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; So in this setting we create a multi-output model which takes an image as the input and has (n_labels + 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss:&lt;/strong&gt; In such a setting setting up the loss is pretty important. Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Loss = alpha*SoftmaxLoss + (1-alpha)*L2Loss$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Since these two losses would be on a different scale, the alpha hyper-parameter needs to be tuned.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function. And here in lies the essence of the whole Deep Learning framework - Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem. And that is what we are going to see a lot going forward.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;So how does this idea of localization using regression get mapped to Object Detection?&lt;/em&gt; It doesn't. We don't have a fixed number of objects. So we can't have 4 outputs denoting, the bounding box coordinates.&lt;/p&gt;
&lt;p&gt;One naive idea could be to apply a CNN to many different crops of the image, CNN classifies each crop as object class or background class. This is intractable. There could be a lot of such crops that you can create.&lt;/p&gt;
&lt;h3&gt;Region Proposals:&lt;/h3&gt;
&lt;p&gt;If just there was a method(Normally called Region Proposal Network)which could find some cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is what selective search (Uijlings et al, "&lt;a href="https://medium.com/r/?url=http%3A%2F%2Fwww.huppelen.nl%2Fpublications%2FselectiveSearchDraft.pdf"&gt;Selective Search for Object Recognition&lt;/a&gt;", IJCV 2013) provided for RCNN.&lt;/p&gt;
&lt;p&gt;So what are Region Proposals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find &lt;em&gt;"blobby"&lt;/em&gt; image regions that are likely to contain objects&lt;/li&gt;
&lt;li&gt;Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How the region proposals are being made?&lt;/p&gt;
&lt;h3&gt;Selective Search for Object Recognition:&lt;/h3&gt;
&lt;p&gt;So this paper starts with a set of some initial regions using [13] (P. F. Felzenszwalb and D. P. Huttenlocher. &lt;a href="https://medium.com/r/?url=http%3A%2F%2Fpeople.cs.uchicago.edu%2F~pff%2Fpapers%2Fseg-ijcv.pdf"&gt;Efficient GraphBased Image Segmentation&lt;/a&gt;. IJCV, 59:167–181, 2004. 1, 3, 4, 5, 7)
&lt;br&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels. A weight is associated with each edge based on some property of the pixels that it connects, such as their image intensities. Depending on the method, there may or may not be an edge connecting each pair of vertices.
&lt;/div&gt;
&lt;br&gt;
In this paper they take an approach:
&lt;br&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
Each edge (vi , vj )∈ E has a corresponding weight w((vi , vj )), which is a non-negative measure of the dissimilarity between neighboring elements vi and vj . In the case of image segmentation, the elements in V are pixels and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute). In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ∈ S corresponds to a connected component in a graph.
&lt;/div&gt;
&lt;br&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id3.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions.
For that the authors of &lt;a href="https://medium.com/r/?url=http%3A%2F%2Fwww.huppelen.nl%2Fpublications%2FselectiveSearchDraft.pdf"&gt;Selective Search for Object Recognition&lt;/a&gt; apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id5.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id6.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;RCNN&lt;/h2&gt;
&lt;p&gt;The above selective search is the region proposal they used in RCNN paper. But what is RCNN and how does it use region proposals?&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id7.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;div style="color:black; background-color: #E9DAEE;"&gt;
Object detection system overview. Our system
(1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
Along with this, the authors have also used a class specific bounding box regressor, that takes:
Input : (Px,Py,Ph,Pw) - the location of the proposed region.
Target: (Gx,Gy,Gh,Gw) - Ground truth labels for the region.
The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G)&lt;/p&gt;
&lt;h3&gt;Training RCNN&lt;/h3&gt;
&lt;p&gt;What is the input to an RCNN?
So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next we treat all region proposals with ≥ 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box's class and the rest as negative. We train class specific SVM's&lt;/p&gt;
&lt;p&gt;So every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class specific data.&lt;/p&gt;
&lt;h3&gt;Test Time RCNN&lt;/h3&gt;
&lt;p&gt;At test time we predict detection boxes using class specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id8.jpeg"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3&gt;Problems with RCNN:&lt;/h3&gt;
&lt;p&gt;Training is slow.
Inference (detection) is slow. 47s / image with VGG16 - Since the Convnet needs to be run many times.&lt;/p&gt;
&lt;p&gt;Need for speed. Hence comes in picture by the same authors:&lt;/p&gt;
&lt;h2&gt;Fast RCNN&lt;/h2&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM to classify those crops.
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
Something like:&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id9.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;div style="color:black; background-color: #E9DAEE;"&gt;
From Paper: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all "background" class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
This idea depends a little upon the architecture of the model that get used too. Do we take the 4096 bottleneck layer from VGG16?
So the architecture that the authors have proposed is:&lt;/p&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net's first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network's last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K + 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
This obviously is a little confusing and "hairy", let us break this down. But for that, we need to see the VGG16 architecture.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id10.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id11.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Now the location of ROI coordinates are in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512. As we are using VGG we will transform image (224 x 224 x 3) into (14 x 14 x 512) - height and width is divided by 16. we can map ROIs coordinates onto the feature map just by dividing them by 16.&lt;/p&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the "things" it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
How the ROI pooling is done?&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id12.gif"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In the above image our region proposal is (0,3,5,7) and we divide that area into 4 regions since we want to have a ROI pooling layer of 2x2.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/r/?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F48163961%2Fhow-do-you-do-roi-pooling-on-areas-smaller-than-the-target-size"&gt;How do you do ROI-Pooling on Areas smaller than the target size?&lt;/a&gt; if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.&lt;/p&gt;
&lt;p&gt;After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K + 1 categories(+1 for Background) and category-specific bounding-box regressors.&lt;/p&gt;
&lt;h3&gt;Training Fast-RCNN&lt;/h3&gt;
&lt;p&gt;What is the input to an Fast- RCNN?&lt;/p&gt;
&lt;p&gt;Pretty much similar: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)&lt;/p&gt;
&lt;p&gt;Next we treat all region proposals with ≥ 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box's class and the rest as negative. This time we have a dense layer on top, and we use multi task loss.&lt;/p&gt;
&lt;p&gt;So every ROI becomes a training example. The main difference is that there is concept of multi-task loss:&lt;/p&gt;
&lt;p&gt;A Fast R-CNN network has two sibling output layers. The first outputs a discrete probability distribution (per RoI), p = (p0, . . . , pK), over K + 1 categories. As usual, p is computed by a softmax over the K+1 outputs of a fully connected layer. The second sibling layer outputs bounding-box regression offsets, t= (tx , ty , tw, th), for each of the K object classes. Each training RoI is labeled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labeled RoI to jointly train for classification and bounding-box regression&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id13.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class. Further:&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id14.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3&gt;Problem:&lt;/h3&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id15.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;Faster-RCNN&lt;/h2&gt;
&lt;p&gt;The next question that got asked was : Can the network itself do region proposals?&lt;/p&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
The intuition is that: With FastRCNN we're already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes + bbox coordinates?
&lt;/div&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id16.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3&gt;How does the Region Proposal Network work?&lt;/h3&gt;
&lt;p&gt;One of the main idea in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.&lt;/p&gt;
&lt;p&gt;So first of all we define anchor centers on the image.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id17.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The anchor centers are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224/14).
This is how anchors look like:&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id18.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;So we start with some predefined regions we think our objects could be with Anchors.&lt;/li&gt;
&lt;li&gt;Our RPN Classifies which regions have the object and the offset of the object bounding box. 1 if IOU for anchor with bounding box&amp;gt;0.5 0 otherwise.&lt;/li&gt;
&lt;li&gt;Non-Maximum suppression to reduce region proposals&lt;/li&gt;
&lt;li&gt;Fast RCNN detection network on top of proposals&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Faster-RCNN Loss:&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;The whole network is then jointly trained with 4 losses:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RPN classify object / not object&lt;/li&gt;
&lt;li&gt;RPN regress box coordinates offset&lt;/li&gt;
&lt;li&gt;Final classification score (object classes)&lt;/li&gt;
&lt;li&gt;Final box coordinates offset&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Results:&lt;/h2&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/id19.jpeg"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; &lt;em&gt;This is my own understanding of these papers with inputs from many blogs and slides on the internet. Let me know if you find something wrong with my understanding. I will be sure to correct myself and post.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://cs231n.github.io/transfer-learning/#tf"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf"&gt;CS231 Object detection Lecture Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf"&gt;Efficient Graph-Based Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1311.2524.pdf"&gt;Rich feature hierarchies for accurate object detection and semantic segmentation(RCNN Paper)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/r/?url=http%3A%2F%2Fwww.huppelen.nl%2Fpublications%2FselectiveSearchDraft.pdf"&gt;Selective Search for Object Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.ai/region-of-interest-pooling-explained/"&gt;ROI Pooling Explanation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff"&gt;Faster RCNN Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/48163961/how-do-you-do-roi-pooling-on-areas-smaller-than-the-target-size"&gt;StackOverflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8"&gt;Faster RCNN Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/"&gt;Faster RCNN Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1506.01497.pdf"&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection"&gt;https://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processClass: 'mathjax', " +
        "        ignoreClass: 'no-mathjax', " +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="object detection"></category><category term="rcnn"></category><category term="faster rcnn"></category></entry><entry><title>Hyperopt - A bayesian Parameter Tuning Framework</title><link href="http://mlwhiz.github.io/blog/2017/12/28/hyperopt_tuning_ml_model/" rel="alternate"></link><updated>2017-12-28T04:43:00-02:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-12-28:blog/2017/12/28/hyperopt_tuning_ml_model/</id><summary type="html">&lt;p&gt;Recently I was working on a in-class competition from the &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;"How to win a data science competition"&lt;/a&gt; Coursera course. Learned a lot of new things from that about using &lt;a href="http://mlwhiz.com/blog/2017/12/26/How_to_win_a_data_science_competition/"&gt;XGBoost for time series prediction&lt;/a&gt; tasks.&lt;/p&gt;
&lt;p&gt;The one thing that I tired out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed. Left the machine with hyperopt in the night. And in the morning I had my results. It was really awesome and I did avoid a lot of hit and trial.&lt;/p&gt;
&lt;h2&gt;What really is Hyperopt?&lt;/h2&gt;
&lt;p&gt;From the site:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What the above means is that it is a optimizer that could minimize/maximize the loss function/accuracy(or whatever metric) for you.&lt;/p&gt;
&lt;p&gt;All of us are fairly known to cross-grid search or random-grid search. Hyperopt takes as an input a space of hyperparams in which it will search, and moves according to the result of past trials.&lt;/p&gt;
&lt;p&gt;To know more about how it does this, take a look at this &lt;a href="https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf"&gt;paper&lt;/a&gt; by J Bergstra.
Here is the &lt;a href="https://github.com/hyperopt/hyperopt/wiki/FMin"&gt;documentation&lt;/a&gt; from github.&lt;/p&gt;
&lt;h2&gt;How?&lt;/h2&gt;
&lt;p&gt;Let me just put the code first. This is how I define the objective function. The objective function takes space(the hyperparam space) as the input and returns the loss(The thing you want to minimize.Or negative of the thing you want to maximize)&lt;/p&gt;
&lt;p&gt;(X,y) and (Xcv,ycv) are the train and cross validation dataframes respectively.&lt;/p&gt;
&lt;p&gt;We have defined a hyperparam space by using the variable &lt;code&gt;space&lt;/code&gt; which is actually just a dictionary. We could choose different distributions for different parameter values.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;fmin&lt;/code&gt; function from the hyperopt package to minimize our &lt;code&gt;fn&lt;/code&gt; through the &lt;code&gt;space&lt;/code&gt;.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from sklearn.metrics import mean_squared_error
import xgboost as xgb
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
import numpy as np

def objective(space):
    print(space)
    clf = xgb.XGBRegressor(n_estimators =1000,colsample_bytree=space['colsample_bytree'],
                           learning_rate = .3,
                            max_depth = int(space['max_depth']),
                            min_child_weight = space['min_child_weight'],
                            subsample = space['subsample'],
                           gamma = space['gamma'],
                           reg_lambda = space['reg_lambda'],)

    eval_set  = [( X, y), ( Xcv, ycv)]

    clf.fit(X, y,
            eval_set=eval_set, eval_metric="rmse",
            early_stopping_rounds=10,verbose=False)

    pred = clf.predict(Xcv)
    mse_scr = mean_squared_error(ycv, pred)
    print "SCORE:", np.sqrt(mse_scr)
    #change the metric if you like
    return {'loss':mse_scr, 'status': STATUS_OK }


space ={'max_depth': hp.quniform("x_max_depth", 4, 16, 1),
        'min_child_weight': hp.quniform ('x_min_child', 1, 10, 1),
        'subsample': hp.uniform ('x_subsample', 0.7, 1),
        'gamma' : hp.uniform ('x_gamma', 0.1,0.5),
        'colsample_bytree' : hp.uniform ('x_colsample_bytree', 0.7,1),
        'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1)
    }


trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)

print best
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Finally:&lt;/h2&gt;
&lt;p&gt;Running the above gives us pretty good hyperparams for our learning algorithm.&lt;/p&gt;
&lt;p&gt;In fact I bagged up the results from multiple hyperparam settings and it gave me the best score on the LB.&lt;/p&gt;
&lt;p&gt;If you like this and would like to get more information about such things, subscribe to the mailing list on the right hand side.&lt;/p&gt;
&lt;p&gt;Also I would definitely recommend this &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;course&lt;/a&gt; about winning Kaggle competitions by Kazanova, Kaggle rank 3 . Do take a look.&lt;/p&gt;</summary><category term="machine learning"></category><category term="hyperparameter tuning"></category><category term="bayesian optimization"></category></entry><entry><title>Using XGBoost for time series prediction tasks</title><link href="http://mlwhiz.github.io/blog/2017/12/26/How_to_win_a_data_science_competition/" rel="alternate"></link><updated>2017-12-26T04:43:00-02:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-12-26:blog/2017/12/26/How_to_win_a_data_science_competition/</id><summary type="html">&lt;p&gt;Recently Kaggle master Kazanova along with some of his friends released a &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;"How to win a data science competition"&lt;/a&gt; Coursera course. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/lboard.png"  height="800" width="600" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;Description of the Problem:&lt;/h2&gt;
&lt;p&gt;In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.&lt;/p&gt;
&lt;p&gt;We were asked you to predict total sales for every product and store in the next month.&lt;/p&gt;
&lt;p&gt;The evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.&lt;/p&gt;
&lt;p&gt;The main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.&lt;/p&gt;
&lt;h2&gt;1. Created a dataframe of all Date_block_num, Store and  Item combinations:&lt;/h2&gt;
&lt;p&gt;This is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from itertools import product
# Create "grid" with columns
index_cols = ['shop_id', 'item_id', 'date_block_num']

# For every month we create a grid from all shops/items combinations from that month
grid = []
for block_num in sales['date_block_num'].unique():
    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()
    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()
    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))
grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;2. Cleaned up a little of sales data after some basic EDA:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales = sales[sales.item_price&lt;100000]
sales = sales[sales.item_cnt_day&lt;=1000]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;3. Created Mean Encodings:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_m = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()
sales_m = pd.merge(grid,sales_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)
# adding the category id too
sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')

for type_id in ['item_id','shop_id','item_category_id']:
    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:

        mean_df = sales.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]
        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']

        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num',type_id],how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
These above lines add the following 9 features :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;4. Create Lag Features:&lt;/h2&gt;
&lt;p&gt;Next we create lag features with diferent lag periods on the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;lag_variables  = list(sales_m.columns[7:])+['item_cnt_day']
lags = [1 ,2 ,3 ,4, 5, 12]
for lag in lags:
    sales_new_df = sales_m.copy()
    sales_new_df.date_block_num+=lag
    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]
    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]
    sales_means = pd.merge(sales_means, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;5. Fill NA with zeros:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;for feat in sales_means.columns:
    if 'item_cnt' in feat:
        sales_means[feat]=sales_means[feat].fillna(0)
    elif 'item_price' in feat:
        sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;6. Drop the columns that we are not going to use in training:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;cols_to_drop = lag_variables[:-1] + ['item_name','item_price']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;7. Take a recent bit of data only:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_means = sales_means[sales_means['date_block_num']&gt;12]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;8. Split in train and CV :&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;X_train = sales_means[sales_means['date_block_num']&lt;33].drop(cols_to_drop, axis=1)
X_cv =  sales_means[sales_means['date_block_num']==33].drop(cols_to_drop, axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;9. THE MAGIC SAUCE:&lt;/h2&gt;
&lt;p&gt;In the start I told that the clipping aspect of [0,20] will be important.
In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def clip(x):
    if x&gt;40:
        return 40
    elif x&lt;0:
        return 0
    else:
        return x
train['item_cnt_day'] = train.apply(lambda x: clip(x['item_cnt_day']),axis=1)
cv['item_cnt_day'] = cv.apply(lambda x: clip(x['item_cnt_day']),axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;10: Modelling:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Created a XGBoost model to get the most important features(Top 42 features)&lt;/li&gt;
&lt;li&gt;Use hyperopt to tune xgboost&lt;/li&gt;
&lt;li&gt;Used top 10 models from tuned XGBoosts to generate predictions.&lt;/li&gt;
&lt;li&gt;clipped the predictions to [0,20] range&lt;/li&gt;
&lt;li&gt;Final solution was the average of these 10 predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learned a lot of new things from this &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;awesome course&lt;/a&gt;. Most recommended.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry><entry><title>The story of every distribution - Discrete Distributions</title><link href="http://mlwhiz.github.io/blog/2017/09/14/discrete_distributions/" rel="alternate"></link><updated>2017-09-14T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-09-14:blog/2017/09/14/discrete_distributions/</id><summary type="html">&lt;p&gt;Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track.
One of the most helpful way to learn more about these is the &lt;a href="https://projects.iq.harvard.edu/stat110/home"&gt;STAT110&lt;/a&gt; course by Joe Blitzstein and his &lt;a href="http://amzn.to/2xAsYzE"&gt;book&lt;/a&gt;. You can check out this &lt;a href="https://www.coursera.org/specializations/statistics?siteID=lVarvwc5BD0-1nQtJg8.ENATqSUIufAaaw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Coursera&lt;/a&gt; course too. Hope it could be useful to someone else too. So here goes:&lt;/p&gt;
&lt;h2&gt;1. Bernoulli Distribution:&lt;/h2&gt;
&lt;p&gt;Perhaps the most simple discrete distribution of all.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; A Coin is tossed with probability p of heads.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = \begin{cases}1-p &amp;amp; k = 0\\p &amp;amp; k = 1\end{cases}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X&amp;lt;=k) = \begin{cases}0 &amp;amp; k &amp;lt; 0\\1-p &amp;amp; 0=&amp;lt;k&amp;lt;1 \\1 &amp;amp; k &amp;gt;= 1\end{cases}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = 0*P(X=0)+1*P(X=1) = p$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = p^2$$&lt;/div&gt;
and
&lt;div class="math"&gt;$$E[X^2] = \sum k^2P(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X^2] =  0^2P(X=0) + 1^2P(X=1) = p $$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = p(1-p)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;2. Binomial Distribution:&lt;/h2&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/maxresdefault.jpg"  height="400" width="500" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;One of the most basic distribution in the Statistician toolkit. The parameters of this distribution is n(number of trials) and p(probability of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
Probability of getting exactly k successes in n trials&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n\\ k\end{array}\right) p^{k}(1-p)^{n-k}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = \sum_{i=0}^k  \left(\begin{array}{c}n\\ i\end{array}\right)  p^i(1-p)^{n-i} $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \sum_{k=0}^n k \left(\begin{array}{c}n\\ k\end{array}\right) * p^{k}(1-p)^{n-k} = np $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;A better way to solve this:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;X is the sum on n Indicator Bernoulli random variables.&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = E[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = E[I_{1}] + E[I_{2}] + ....+ E[I_{n-1}]+ E[I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \underbrace{p + p + ....+ p + p}_{n} = np$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;
X is the sum on n Indicator Bernoulli random variables.
&lt;div class="math"&gt;$$Var[X] = Var[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$Var[X] = Var[I_{1}] + Var[I_{2}] + ....+ Var[I_{n-1}]+ Var[I_{n}]$$&lt;/div&gt;
&lt;div class="math"&gt;$$Var[X] = \underbrace{p(1-p) + p(1-p) + ....+ p(1-p) + p(1-p)}_{n} = np(1-p)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;3. Geometric Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is p(probability of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The number of failures before the first success(Heads) when a coin with probability p is tossed&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$P(X=k) = (1-p)^kp$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = \sum_{i=0}^k (1-p)^{i}p$$&lt;/div&gt;
&lt;div class="math"&gt;$$ P(X\leq k) = p(1+q+q^2...+q^k)= p(1-q^k)/(1-q) = 1-(1-p)^k $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = \sum_{k=0}^{inf} k (1-p)^kp$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp +2q^2p +3q^3p +4q^4p .... $$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp(1+2q+3q^2+4q^3+....)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X] = qp/(1-q)^2 = q/p $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = q^2/p^2$$&lt;/div&gt;
and
&lt;div class="math"&gt;$$E[X^2] = \sum_0^k k^2q^kp= qp + 4q^2p + 9q^3p +16q^4p ... = qp(1+4q+9q^2+16q^3....)$$&lt;/div&gt;
&lt;div class="math"&gt;$$E[X^2] = qp^{-2}(1+q)$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Thus,
&lt;div class="math"&gt;$$Var[X] =q/p^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Check Math appendix at bottom of this post for Geometric Series Proofs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. A doctor is seeking an anti-depressant for a newly diagnosed patient. Suppose that, of the available anti-depressant drugs, the probability that any particular drug will be effective for a particular patient is p=0.6. What is the probability that the first drug found to be effective for this patient is the first drug tried, the second drug tried, and so on? What is the expected number of drugs that will be tried to find one that is effective?&lt;/p&gt;
&lt;p&gt;A. Expected number of drugs that will be tried to find one that is effective = q/p = .4/.6 =.67&lt;/p&gt;
&lt;h2&gt;4. Negative Binomial Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is p(probability of success) and r(number of success).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The &lt;strong&gt;number of failures&lt;/strong&gt; of independent Bernoulli(p) trials before the rth success.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Negative Binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;r successes , k failures , last attempt needs to be a success:
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The negative binomial RV could be stated as the sum of r Geometric RVs
&lt;div class="math"&gt;$$X = X^1+X^2.... X^{r-1} +X^r$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$E[X] = E[X^1]+E[X^2].... E[X^{r-1}] +E[X^r]$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = rq/p$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The negative binomial RV could be stated as the sum of r independent Geometric RVs
&lt;div class="math"&gt;$$X = X^1+X^2.... X^{r-1} +X^r$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = Var[X^1]+Var[X^2].... Var[X^{r-1}] +Var[X^r]$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$E[X] = rq/p^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. Pat is required to sell candy bars to raise money for the 6th grade field trip. There are thirty houses in the neighborhood, and Pat is not supposed to return home until five candy bars have been sold. So the child goes door to door, selling candy bars. At each house, there is a 0.4 probability of selling one candy bar and a 0.6 probability of selling nothing.
What's the probability of selling the last candy bar at the nth house?&lt;/p&gt;
&lt;p&gt;A. r = 5 ; k = n - r&lt;/p&gt;
&lt;p&gt;Probability of selling the last candy bar at the nth house =
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n-1\\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;5. Poisson Distribution:&lt;/h2&gt;
&lt;p&gt;The parameters of this distribution is &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; the rate parameter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt;
There is as such no story to this distribution but only motivation for using this distribution. The Poisson distribution is often used for applications where we count the successes of a large number of trials where the per-trial success rate is small. For example, the Poisson distribution is a good starting point for counting the number of people who email you over the course of an hour.The number of chocolate chips in a chocolate chip cookie is another good candidate for a Poisson distribution, or the number of earthquakes in a year in some particular region&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PMF of Poisson Distribution is given by:&lt;/strong&gt;
&lt;div class="math"&gt;$$ P(X=k) = \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Expected Value:&lt;/strong&gt;
&lt;div class="math"&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \sum_{k=0}^{inf} k \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \lambda e^{-\lambda}\sum_{k=0}^{inf}  \frac{\lambda^{k-1}} {(k-1)!}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ E[X] = \lambda e^{-\lambda} e^{\lambda} = \lambda $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Variance:&lt;/strong&gt;
&lt;div class="math"&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;
Now we find,
&lt;div class="math"&gt;$$E[X]^2 = \lambda + \lambda^2$$&lt;/div&gt;
Thus,
&lt;div class="math"&gt;$$Var[X] = \lambda$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Q. If electricity power failures occur according to a Poisson distribution with an average of 3 failures every twenty weeks, calculate the probability that there will not be more than one failure during a particular week?&lt;/p&gt;
&lt;p&gt;A. Probability = P(X=0)+P(X=1) = $e^{-3/20} + e^{-3/20}3/20 = 23/20*e^{-3/20} $&lt;/p&gt;
&lt;p&gt;Probability of selling the last candy bar at the nth house =
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}k+r-1\\ k\end{array}\right) p^r(1-p)^k$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X=k) = \left(\begin{array}{c}n-1\\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;Math Appendix:&lt;/h2&gt;
&lt;p&gt;Some Math (For Geometric Distribution) :&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$a+ar+ar^2+ar^3+⋯=a/(1−r)=a(1−r)^{−1}$$&lt;/div&gt;
Taking the derivatives of both sides, the first derivative with respect to r must be:
&lt;div class="math"&gt;$$a+2ar+3ar^2+4ar^3⋯=a(1−r)^{−2}$$&lt;/div&gt;
Multiplying above with r:
&lt;div class="math"&gt;$$ar+2ar^2+3ar^3+4ar^4⋯=ar(1−r)^{−2}$$&lt;/div&gt;
Taking the derivatives of both sides, the first derivative with respect to r must be:
&lt;div class="math"&gt;$$a+4ar+9ar^2+16ar^3⋯=a(1−r)^{-3}(1+r)$$&lt;/div&gt;
&lt;/p&gt;
&lt;h2&gt;Bonus - Python Graphs and Functions:&lt;/h2&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Useful Function to create graph
def chart_creator(x,y,title):
    import matplotlib.pyplot as plt  #sets up plotting under plt
    import seaborn as sns           #sets up styles and gives us more plotting options
    import pandas as pd             #lets us handle data as dataframes
    %matplotlib inline
    # Create a list of 100 Normal RVs
    data = pd.DataFrame(zip(x,y))
    data.columns = ['x','y']
    # We dont Probably need the Gridlines. Do we? If yes comment this line
    sns.set(style="ticks")

    # Here we create a matplotlib axes object. The extra parameters we use
    # "ci" to remove confidence interval
    # "marker" to have a x as marker.
    # "scatter_kws" to provide style info for the points.[s for size]
    # "line_kws" to provide style info for the line.[lw for line width]

    g = sns.regplot(x='x', y='y', data=data, ci = False,
        scatter_kws={"color":"darkred","alpha":0.3,"s":90},
        line_kws={"color":"g","alpha":0.5,"lw":0},marker="x")

    # remove the top and right line in graph
    sns.despine()

    # Set the size of the graph from here
    g.figure.set_size_inches(12,8)
    # Set the Title of the graph from here
    g.axes.set_title(title, fontsize=34,color="r",alpha=0.5)
    # Set the xlabel of the graph from here
    g.set_xlabel("k",size = 67,color="r",alpha=0.5)
    # Set the ylabel of the graph from here
    g.set_ylabel("pmf",size = 67,color="r",alpha=0.5)
    # Set the ticklabel size and color of the graph from here
    g.tick_params(labelsize=14,labelcolor="black")
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here I will generate the PMFs of the discrete distributions we just discussed above using Pythons built in functions. For more details on the upper function, please see my previous post - &lt;a href="http://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/"&gt;Create basic graph visualizations with SeaBorn&lt;/a&gt;. Also take a look at the &lt;a href="https://docs.scipy.org/doc/scipy/reference/stats.html"&gt;documentation&lt;/a&gt; guide for the below functions&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Binomial :
from scipy.stats import binom
n=30
p=0.5
k = range(0,n)
pmf = binom.pmf(k, n, p)
chart_creator(k,pmf,"Binomial PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_12_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Geometric :
from scipy.stats import geom
n=30
p=0.5
k = range(0,n)
# -1 here is the location parameter for generating the PMF we want.
pmf = geom.pmf(k, p,-1)
chart_creator(k,pmf,"Geometric PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_13_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# Negative Binomial :
from scipy.stats import nbinom
r=5 # number of successes
p=0.5 # probability of Success
k = range(0,25) # number of failures
# -1 here is the location parameter for generating the PMF we want.
pmf = nbinom.pmf(k, r, p)
chart_creator(k,pmf,"Nbinom PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_14_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;#Poisson
from scipy.stats import poisson
lamb = .3 # Rate
k = range(0,5)
pmf = poisson.pmf(k, lamb)
chart_creator(k,pmf,"Poisson PMF")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/output_15_0.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://amzn.to/2xAsYzE"&gt;Introduction to Probability by Joe Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next thing I want to come up with is a same sort of post for continuous distributions too. Keep checking for the same. Till then Ciao.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processClass: 'mathjax', " +
        "        ignoreClass: 'no-mathjax', " +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="distributions"></category><category term="pdf"></category><category term="cdf"></category><category term="expected value"></category><category term="variance"></category><category term="binomial"></category><category term="poisson"></category><category term="geometric"></category></entry><entry><title>Good Feature Building Techniques - Tricks for Kaggle - My Kaggle Code Repository</title><link href="http://mlwhiz.github.io/blog/2017/09/14/kaggle_tricks/" rel="alternate"></link><updated>2017-09-14T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-09-14:blog/2017/09/14/kaggle_tricks/</id><summary type="html">&lt;p&gt;Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.&lt;/p&gt;
&lt;p&gt;Take a look at the &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;How to Win a Data Science Competition: Learn from Top Kagglers&lt;/a&gt; course in the &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;Advanced machine learning specialization&lt;/a&gt; by Kazanova(Number 3 Kaggler at the time of writing)&lt;/p&gt;
&lt;p&gt;This post is inspired by a &lt;a href="https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-368"&gt;Kernel&lt;/a&gt; on Kaggle written by Beluga, one of the top Kagglers, for a knowledge based &lt;a href="https://www.kaggle.com/c/nyc-taxi-trip-duration"&gt;competition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some of the techniques/tricks I am sharing have been taken directly from that kernel so you could take a look yourself.
Otherwise stay here and read on.&lt;/p&gt;
&lt;h2&gt;1. Don't try predicting the future when you don't have to:&lt;/h2&gt;
&lt;p&gt;If both training/test comes from the same timeline, we can get really crafty with features. Although this is a case with Kaggle only, we can use this to our advantage. For example: In the Taxi Trip duration challenge the test data is randomly sampled from the train data. In this case we can use the target variable averaged over different categorical variable as a feature. Like in this case Beluga actually used the averaged the target variable over different weekdays. He then mapped the same averaged value as a variable by mapping it to test data too.&lt;/p&gt;
&lt;h2&gt;2. logloss clipping Technique:&lt;/h2&gt;
&lt;p&gt;Something that I learned in the Neural Network course by Jeremy Howard. Its based on a very simple Idea. Logloss penalises a lot if we are very confident and wrong. So in case of Classification problems where we have to predict probabilities, it would be much better to clip our probabilities between 0.05-0.95 so that we are never very sure about our prediction.&lt;/p&gt;
&lt;h2&gt;3. kaggle submission in gzip format:&lt;/h2&gt;
&lt;p&gt;A small piece of code that will help you save countless hours of uploading. Enjoy.
df.to_csv('submission.csv.gz', index=False, compression='gzip')&lt;/p&gt;
&lt;h2&gt;4. How best to use Latitude and Longitude features - Part 1:&lt;/h2&gt;
&lt;p&gt;One of the best things that I liked about the Beluga Kernel is how he used the Lat/Lon Data. So in the example we had pickup Lat/Lon and Dropoff Lat/Lon. We created features like:&lt;/p&gt;
&lt;h4&gt;A. Haversine Distance Between the Two Lat/Lons:&lt;/h4&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def haversine_array(lat1, lng1, lat2, lng2):
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    AVG_EARTH_RADIUS = 6371  # in km
    lat = lat2 - lat1
    lng = lng2 - lng1
    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2
    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))
    return h
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;B. Manhattan Distance Between the two Lat/Lons:&lt;/h4&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def dummy_manhattan_distance(lat1, lng1, lat2, lng2):
    a = haversine_array(lat1, lng1, lat1, lng2)
    b = haversine_array(lat1, lng1, lat2, lng1)
    return a + b
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;C. Bearing Between the two Lat/Lons:&lt;/h4&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def bearing_array(lat1, lng1, lat2, lng2):
    AVG_EARTH_RADIUS = 6371  # in km
    lng_delta_rad = np.radians(lng2 - lng1)
    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
    y = np.sin(lng_delta_rad) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)
    return np.degrees(np.arctan2(y, x))
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;D. Center Latitude and Longitude between Pickup and Dropoff:&lt;/h4&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2
train.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;5. How best to use Latitude and Longitude features - Part 2:&lt;/h2&gt;
&lt;p&gt;The Second way he used the Lat/Lon Feats was to create clusters for Pickup and Dropoff Lat/Lons. The way it worked was it created sort of Boroughs in the data by design.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from sklearn.cluster import MiniBatchKMeans
coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,
                    train[['dropoff_latitude', 'dropoff_longitude']].values,
                    test[['pickup_latitude', 'pickup_longitude']].values,
                    test[['dropoff_latitude', 'dropoff_longitude']].values))

sample_ind = np.random.permutation(len(coords))[:500000]
kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])

train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])
train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])
test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])
test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;He then used these Clusters to create features like counting no of trips going out and coming in on a particular day.&lt;/p&gt;
&lt;h2&gt;6. How best to use Latitude and Longitude features - Part 3&lt;/h2&gt;
&lt;p&gt;He used PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since he transformed 2D-&amp;gt; 2D. The rotation could help for decision tree splits, and it did actually.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;pca = PCA().fit(coords)
train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]
train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]
train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]
train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]
test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]
test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]
test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]
test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;7. Lets not forget the Normal Things you can do with your features:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Scaling by Max-Min&lt;/li&gt;
&lt;li&gt;Normalization using Standard Deviation&lt;/li&gt;
&lt;li&gt;Log based feature/Target: use log based features or log based target function.&lt;/li&gt;
&lt;li&gt;One Hot Encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;8. Creating Intuitive Additional Features:&lt;/h2&gt;
&lt;p&gt;A) Date time Features: Time based Features like "Evening", "Noon", "Night", "Purchases_last_month", "Purchases_last_week" etc.&lt;/p&gt;
&lt;p&gt;B) Thought Features: Suppose you have shopping cart data and you want to categorize TripType (See Walmart Recruiting: Trip Type Classification on &lt;a href="https://www.kaggle.com/c/walmart-recruiting-trip-type-classification/"&gt;Kaggle&lt;/a&gt; for some background).&lt;/p&gt;
&lt;p&gt;You could think of creating a feature like "Stylish" where you create this variable by adding together number of items that belong to category Men's Fashion, Women's Fashion, Teens Fashion.&lt;/p&gt;
&lt;p&gt;You could create a feature like "Rare" which is created by tagging some items as rare, based on the data we have and then counting the number of those rare items in the shopping cart. Such features might work or might not work. From what I have observed they normally provide a lot of value.&lt;/p&gt;
&lt;p&gt;I feel this is the way that Target's "Pregnant Teen model" was made. They would have had a variable in which they kept all the items that a pregnant teen could buy and put it into a classification algorithm.&lt;/p&gt;
&lt;h2&gt;9 . The not so Normal Things which people do:&lt;/h2&gt;
&lt;p&gt;These features are highly unintuitive and should not be created where the machine learning model needs to be interpretable.&lt;/p&gt;
&lt;p&gt;A) Interaction Features: If you have features A and B create features A*B, A+B, A/B, A-B. This explodes the feature space. If you have 10 features and you are creating two variable interactions you will be adding 10C2 * 4  features = 180 features to your model. And most of us have a lot more than 10 features.&lt;/p&gt;
&lt;p&gt;B) Bucket Feature Using Hashing: Suppose you have a lot of features. In the order of Thousands but you don't want to use all the thousand features because of the training times of algorithms involved. People bucket their features using some hashing algorithm to achieve this.Mostly done for text classification tasks.
For example:
If we have 6 features A,B,C,D,E,F.
And the row of data is:
A:1,B:1,C:1,D:0,E:1,F:0
I may decide to use a hashing function so that these 6 features correspond to 3 buckets and create the data using this feature hashing vector.
After processing my data might look like:
Bucket1:2,Bucket2:2,Bucket3:0
Which happened because A and B fell in bucket1, C and E fell in bucket2 and D and F fell in bucket 3. I summed up the observations here, but you could substitute addition with any math function you like.
Now i would use Bucket1,Bucket2,Bucket3 as my variables for machine learning.&lt;/p&gt;
&lt;p&gt;Will try to keep on expanding. Wait for more....&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry><entry><title>Today I Learned This Part 2: Pretrained Neural Networks What are they?</title><link href="http://mlwhiz.github.io/blog/2017/04/17/deep_learning_pretrained_models/" rel="alternate"></link><updated>2017-04-17T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-04-17:blog/2017/04/17/deep_learning_pretrained_models/</id><summary type="html">&lt;p&gt;Deeplearning is the buzz word right now. I was working on the &lt;a href="http://www.fast.ai/"&gt;course&lt;/a&gt; for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the &lt;a href="https://www.kaggle.com/c/dogs-vs-cats"&gt;Dogs vs Cats&lt;/a&gt; competition on Kaggle to achieve very good results.&lt;/p&gt;
&lt;h2&gt;What are pretrained Neural Networks?&lt;/h2&gt;
&lt;p&gt;So let me tell you about the background a little bit. There is a challenge that happens every year in the visual recognition community - The Imagenet Challenge. The task there is to classify the images in 1000 categories using Image training data. People train big convolutional deep learning models for this challenge.&lt;/p&gt;
&lt;p&gt;Now what does training a neural model actually mean? It just means that they learn the weights for a NN. What if we can get the weights they learn? We can use those weights to load them into our own NN model and predict on the test dataset. Right?&lt;/p&gt;
&lt;p&gt;But actually we can go further than that. We can add an extra layer on top of the NN they have prepared to classify our own dataset.&lt;/p&gt;
&lt;p&gt;In a way you can think of the intermediate features created by the Pretrained neural networks to be the features for the next layer.&lt;/p&gt;
&lt;h2&gt;Why it works?&lt;/h2&gt;
&lt;p&gt;We are essentially doing the image classification task only. We need to find out edges, shapes, intensities and other features from the images that are given to us. The pretrained model is already pretty good at finding these sort of features. Forget neural nets, if we plug these features into a machine learning algorithm we should be good.&lt;/p&gt;
&lt;p&gt;What we actually do here is replace the last layer of the neural network with a new prediction/output layer and train while keeping the weights for all the layers before the second last layer constant.&lt;/p&gt;
&lt;h2&gt;Code:&lt;/h2&gt;
&lt;p&gt;I assume that you understand Keras a little. If not you can look at the docs.
Let us get into coding now. First of all we will create the architecture of the neural network the VGG Team created in 2014. Then we will load the weights.&lt;/p&gt;
&lt;p&gt;Import some stuff&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;import numpy as np
from numpy.random import random, permutation
from scipy import misc, ndimage
from scipy.ndimage.interpolation import zoom
import keras
from keras import backend as K
from keras.utils.data_utils import get_file
from keras.models import Sequential, Model
from keras.layers.core import Flatten, Dense, Dropout, Lambda
from keras.layers import Input
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD, RMSprop, Adam
from keras.preprocessing import image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;VGG has just one type of convolutional block, and one type of fully connected ('dense') block. We start by defining the building blocks of our Deep learning model.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def ConvBlock(layers, model, filters):
    for i in range(layers):
        model.add(ZeroPadding2D((1,1)))
        model.add(Convolution2D(filters, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

def FCBlock(model):
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
&lt;/code&gt;&lt;/pre&gt;

&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;

&lt;p&gt;Now the input of the VGG Model was images. When the VGG model was trained in 2014, the creators subtracted the average of each of the three (R,G,B) channels first, so that the data for each channel had a mean of zero. Furthermore, their software that expected the channels to be in B,G,R order, whereas Python by default uses R,G,B. We need to preprocess our data to make these two changes, so that it is compatible with the VGG model. We also add some helper functions.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;#Mean of each channel as provided by VGG researchers
vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1))

def vgg_preprocess(x):
    x = x - vgg_mean     # subtract mean
    return x[:, ::-1]    # reverse axis bgr-&gt;rgb

def VGG_16():
    model = Sequential()
    model.add(Lambda(vgg_preprocess, input_shape=(3,224,224)))
    ConvBlock(2, model, 64)
    ConvBlock(2, model, 128)
    ConvBlock(3, model, 256)
    ConvBlock(3, model, 512)
    ConvBlock(3, model, 512)
    model.add(Flatten())
    FCBlock(model)
    FCBlock(model)
    model.add(Dense(1000, activation='softmax'))
    return model


def finetune(model, num_classes):
    # Drop last layer
    model.pop()
    # Make all layers untrainable. i.e fix all weights
    for layer in model.layers: layer.trainable=False
    # Add a new layer which is the new output layer
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer=Adam(lr=0.001),
                loss='categorical_crossentropy', metrics=['accuracy'])
    return model


# A way to generate batches of images
def get_batches(path, dirname, gen=image.ImageDataGenerator(), shuffle=True,
                batch_size=64, class_mode='categorical'):
    return gen.flow_from_directory(path+dirname, target_size=(224,224),
                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The hard part is done now. Just create a VGG object and load the weights.We will need to load pretrained weights into the model too. You can download the "VGG16_weights.h5" file &lt;a href="https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model = VGG_16()
model.load_weights('VGG16_weights.h5')
# Since our dogs vs cat dataset is binary classification model
ftmodel = finetune(model,2)
print ftmodel.summary()
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/keras_net.png"  height="400" width="500" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Showing a little bit of output here. This is how the last layers of our Neural net look after training. Now we have got a architecture which we got to train. Here we are only training to get the last layer weights. As you can see from the trainable params.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;path = "dogscats/"
batch_size=64

# Iterators to get our images from our datasets. The datasets are folders named train and valid. Both folder contain two directories 'dogs' and 'cats'. In each directory the corresponding images are kept.

batches = get_batches(path,'train', batch_size=batch_size)
val_batches = get_batches(path,'valid', batch_size=batch_size)

# Now run for some epochs till the validation loss stops decreasing.
no_of_epochs=1

for epoch in range(no_of_epochs):
    print "Running epoch: %d" % epoch
    ftmodel.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=1,
                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)
    latest_weights_filename = 'ft%d.h5' % epoch
    ftmodel.save_weights(latest_weights_filename)

#Create Predictions on test set. The test images should be in the folder dogscats/test/test_images/ , which is a single directory containing all images.

test_batches = get_batches(path, 'test', batch_size=2*batch_size, class_mode=None)

preds = ftmodel.predict_generator(test_batches, test_batches.nb_sample)

isdog = preds[:,1]
image_id = batches.filenames
final_submission = np.stack([ids,isdog], axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we are done!&lt;/p&gt;</summary><category term="deeplearning"></category><category term="pretrained models"></category></entry><entry><title>Maths Beats Intuition probably every damn time</title><link href="http://mlwhiz.github.io/blog/2017/04/16/maths_beats_intuition/" rel="alternate"></link><updated>2017-04-16T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-04-16:blog/2017/04/16/maths_beats_intuition/</id><summary type="html">&lt;p&gt;Newton once said that &lt;strong&gt;"God does not play dice with the universe"&lt;/strong&gt;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.&lt;/p&gt;
&lt;p&gt;One of such ways intuition plays with us is with the Birthday problem.&lt;/p&gt;
&lt;h2&gt;Problem Statement:&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By the &lt;a href="https://en.wikipedia.org/wiki/Pigeonhole_principle"&gt;pigeonhole principle&lt;/a&gt;, the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Mathematical Proof:&lt;/h2&gt;
&lt;p&gt;Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event.
We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement)
Since there are k ways in which birthdays can be chosen with replacement.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(birthday Match) = 1 - \dfrac{(365).364...(365−k+1)}{365^k}\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Simulation:&lt;/h2&gt;
&lt;p&gt;Lets try to build around this result some more by trying to simulate this result:&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;%matplotlib inline
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt  #sets up plotting under plt
import seaborn as sns           #sets up styles and gives us more plotting options
import pandas as pd             #lets us handle data as dataframes
import random

def sim_bithday_problem(num_people_room, trials =1000):
    '''This function takes as input the number of people in the room.
    Runs 1000 trials by default and returns
    (number of times same brthday found)/(no of trials)
    '''
    same_birthdays_found = 0
    for i in range(trials):
        # randomly sample from the birthday space which could be any of a number from 1 to 365
        birthdays = [random.randint(1,365) for x in range(num_people_room)]
        if len(birthdays) - len(set(birthdays))&gt;0:
            same_birthdays_found+=1
    return same_birthdays_found/float(trials)

num_people = range(2,100)
probs = [sim_bithday_problem(i) for i in num_people]
data = pd.DataFrame()
data['num_peeps'] = num_people
data['probs'] = probs
sns.set(style="ticks")

g = sns.regplot(x="num_peeps", y="probs", data=data, ci = False,
    scatter_kws={"color":"darkred","alpha":0.3,"s":90},
    marker="x",fit_reg=False)

sns.despine()
g.figure.set_size_inches(10,6)
g.axes.set_title('As the Number of people in room reaches 23 the probability reaches ~0.5\nAt more than 50 people the probability is reaching 1', fontsize=15,color="g",alpha=0.5)
g.set_xlabel("# of people in room",size = 30,color="r",alpha=0.5)
g.set_ylabel("Probability",size = 30,color="r",alpha=0.5)
g.tick_params(labelsize=14,labelcolor="black")
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/bithdayproblem.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can see from the &lt;a href="http://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/"&gt;graph&lt;/a&gt; that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.&lt;/p&gt;
&lt;h2&gt;Intuition:&lt;/h2&gt;
&lt;p&gt;To understand it we need to think of this problem in terms of pairs. There are &lt;span class="math"&gt;\({{23}\choose{2}} = 253\)&lt;/span&gt; pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at &lt;span class="math"&gt;\({{70}\choose{2}} = 2450\)&lt;/span&gt; pairs.&lt;/p&gt;
&lt;p&gt;So thats it for now. To learn more about this go to &lt;a href="https://en.wikipedia.org/wiki/Birthday_problem"&gt;Wikipedia&lt;/a&gt; which has an awesome page on this topic.&lt;/p&gt;
&lt;h2&gt;References:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://amzn.to/2nIUkxq"&gt;Introduction to Probability by Joseph K. Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Birthday_problem"&gt;Birthday Problem on Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processClass: 'mathjax', " +
        "        ignoreClass: 'no-mathjax', " +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Python"></category><category term="machine learning"></category><category term="probability"></category></entry><entry><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link href="http://mlwhiz.github.io/blog/2017/04/09/word_vec_embeddings_examples_understanding/" rel="alternate"></link><updated>2017-04-09T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-04-09:blog/2017/04/09/word_vec_embeddings_examples_understanding/</id><summary type="html">&lt;p&gt;Recently Quora put out a &lt;a href="https://www.kaggle.com/c/quora-question-pairs"&gt;Question similarity&lt;/a&gt; competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p&gt;
&lt;p&gt;Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.&lt;/p&gt;
&lt;p&gt;For those who don't know &lt;em&gt;bag of words&lt;/em&gt;:
If we have a series of sentences(documents)&lt;/p&gt;
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. This is good       - [1,1,1,0,0]&lt;br&gt;
2. This is bad        - [1,1,0,1,0]&lt;br&gt;
3. This is awesome    - [1,1,0,0,1]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;Bag of words would encode it using &lt;em&gt;0:This 1:is 2:good 3:bad 4:awesome&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But it is much more powerful than that.&lt;/p&gt;
&lt;p&gt;What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.&lt;/p&gt;
&lt;h2&gt;How does that help?&lt;/h2&gt;
&lt;p&gt;We can use this for multiple scenarios but the most common are:&lt;/p&gt;
&lt;p&gt;A. &lt;em&gt;Using word2vec embeddings we can find out similarity between words&lt;/em&gt;.
Assume you have to answer if these two statements signify the same thing:
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. President greets press in Chicago&lt;br&gt;
2. Obama speaks to media in Illinois.
&lt;/div&gt;
If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/word2vecembed.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;But with a word encoding we can say that&lt;/p&gt;
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. President is similar to Obama&lt;br&gt;
2. greets is similar to speaks&lt;br&gt;
3. press is similar to media&lt;br&gt;
4. Chicago is similar to Illinois&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;B. &lt;em&gt;Encode Sentences&lt;/em&gt;: I read a &lt;a href="https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur"&gt;post&lt;/a&gt; from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.&lt;/p&gt;
&lt;p&gt;His Approach: Lets say the sentence is "What is this"
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. what : [.25 ,.25 ,.25 ,.25]&lt;br&gt;
2. is   : [  1 ,  0 ,  0 ,  0]&lt;br&gt;
3. this : [ .5 ,  0 ,  0 , .5]&lt;br&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.&lt;/p&gt;
&lt;div style="margin-bottom:9px;margin-left: 10px"&gt;
Elementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
&lt;br&gt;
divided by
&lt;br&gt;
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
&lt;br&gt;
gives:[1.16, .17, .17, 0.5]
&lt;/div&gt;

&lt;p&gt;Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.&lt;/p&gt;
&lt;p&gt;C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?&lt;br&gt;
Guess what it comes out to be : &lt;em&gt;Queen&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Application/Coding:&lt;/h2&gt;
&lt;p&gt;Now lets get down to the coding part as we know a little bit of fundamentals.&lt;/p&gt;
&lt;p&gt;First of all we download a custom word embedding from Google. There are many other embeddings too.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above file is pretty big. Might take some time. Then moving on to coding.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from gensim.models import word2vec
model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;1. Starting simple, lets find out similar words. Want to find similar words to python?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar('python')
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'pythons', 0.6688377261161804),&lt;br&gt;
 (u'Burmese_python', 0.6680364608764648),&lt;br&gt;
 (u'snake', 0.6606293320655823),&lt;br&gt;
 (u'crocodile', 0.6591362953186035),&lt;br&gt;
 (u'boa_constrictor', 0.6443519592285156),&lt;br&gt;
 (u'alligator', 0.6421656608581543),&lt;br&gt;
 (u'reptile', 0.6387745141983032),&lt;br&gt;
 (u'albino_python', 0.6158879995346069),&lt;br&gt;
 (u'croc', 0.6083582639694214),&lt;br&gt;
 (u'lizard', 0.601341724395752)]&lt;br&gt;
 &lt;/div&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;2. Now we can use this model to find the solution to the equation:&lt;/h3&gt;
&lt;p&gt;What is king - man + woman?&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['king','woman'],negative = ['man'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'queen', 0.7118192315101624),&lt;br&gt;
 (u'monarch', 0.6189674139022827),&lt;br&gt;
 (u'princess', 0.5902431011199951),&lt;br&gt;
 (u'crown_prince', 0.5499460697174072),&lt;br&gt;
 (u'prince', 0.5377321839332581),&lt;br&gt;
 (u'kings', 0.5236844420433044),&lt;br&gt;
 (u'Queen_Consort', 0.5235946178436279),&lt;br&gt;
 (u'queens', 0.5181134343147278),&lt;br&gt;
 (u'sultan', 0.5098593235015869),&lt;br&gt;
 (u'monarchy', 0.5087412595748901)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;You can do plenty of freaky/cool things using this:&lt;/p&gt;
&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;

&lt;h3&gt;3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['emma','he','male','mr'],negative = ['she','mrs','female'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'sanchez', 0.4920658469200134),&lt;br&gt;
 (u'kenny', 0.48300960659980774),&lt;br&gt;
 (u'alves', 0.4684845209121704),&lt;br&gt;
 (u'gareth', 0.4530612826347351),&lt;br&gt;
 (u'bellamy', 0.44884198904037476),&lt;br&gt;
 (u'gibbs', 0.445194810628891),&lt;br&gt;
 (u'dos_santos', 0.44508373737335205),&lt;br&gt;
 (u'gasol', 0.44387346506118774),&lt;br&gt;
 (u'silva', 0.4424275755882263),&lt;br&gt;
 (u'shaun', 0.44144102931022644)]&lt;br&gt;&lt;br&gt;
&lt;/div&gt;

&lt;h3&gt;4. Find which word doesn't belong to a &lt;a href="https://github.com/dhammack/Word2VecExample/blob/master/main.py"&gt;list&lt;/a&gt;?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.doesnt_match("math shopping reading science".split(" "))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think staple doesnt belong in this list!&lt;/p&gt;
&lt;h2&gt;Other Cool Things&lt;/h2&gt;
&lt;h3&gt;1. Recommendations:&lt;/h3&gt;
&lt;div style="margin-top: 4px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/recommendationpaper.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href="https://arxiv.org/abs/1603.04259"&gt;paper&lt;/a&gt;, the authors have shown that itembased CF can be cast in the same framework of word embedding.&lt;/p&gt;
&lt;h3&gt;2. Some other &lt;a href="http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html"&gt;examples&lt;/a&gt; that people have seen after using their own embeddings:&lt;/h3&gt;
&lt;p&gt;Library - Books = Hall&lt;br&gt;
Obama + Russia - USA = Putin&lt;br&gt;
Iraq - Violence = Jordan&lt;br&gt;
President - Power = Prime Minister (Not in India Though)&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;3.Seeing the above I started playing with it a little.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Is this model sexist?&lt;/strong&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ["donald_trump"],negative = ['brain'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'novak', 0.40405112504959106),&lt;br&gt;
 (u'ozzie', 0.39440611004829407),&lt;br&gt;
 (u'democrate', 0.39187556505203247),&lt;br&gt;
 (u'clinton', 0.390536367893219),&lt;br&gt;
 (u'hillary_clinton', 0.3862358033657074),&lt;br&gt;
 (u'bnp', 0.38295692205429077),&lt;br&gt;
 (u'klaar', 0.38228923082351685),&lt;br&gt;
 (u'geithner', 0.380607008934021),&lt;br&gt;
 (u'bafana_bafana', 0.3801495432853699),&lt;br&gt;
 (u'whitman', 0.3790769875049591)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry><entry><title>Top Data Science Resources on the Internet right now</title><link href="http://mlwhiz.github.io/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/" rel="alternate"></link><updated>2017-03-26T13:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-03-26:blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</id><summary type="html">&lt;p&gt;I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.&lt;/p&gt;
&lt;p&gt;To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn't cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?&lt;/p&gt;
&lt;p&gt;But it is not magic. And this is a way an aspiring guy could take to become a &lt;b&gt;&lt;u&gt;self-trained data scientist&lt;/u&gt;&lt;/b&gt;. Follow in order. I have tried to include everything that comes to my mind. So here goes:&lt;/p&gt;
&lt;h2&gt;1. &lt;a href="http://projects.iq.harvard.edu/stat110/youtube"&gt;Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The one stat course you gotta take&lt;/em&gt;. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.&lt;/p&gt;
&lt;p&gt;This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.&lt;/p&gt;
&lt;p&gt;One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of &lt;strong&gt; Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox &lt;/strong&gt; etc. made this course much much more exciting than a normal Statistics Course.&lt;/p&gt;
&lt;p&gt;It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.&lt;/p&gt;
&lt;p&gt;He got a textbook out based on this course which is clearly a great text:&lt;/p&gt;
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li3&amp;tag=mlwhizcon-20&amp;linkId=7254baef925507e0d8dfd07cca2f519d" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=B00MMOJ19I&amp;Format=_SL250_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li3&amp;o=1&amp;a=B00MMOJ19I" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;

&lt;h2&gt;2. &lt;a href="http://cs109.github.io/2015/"&gt;Data Science CS109&lt;/a&gt;: -&lt;/h2&gt;
&lt;p&gt;Again by Professor Blitzstein. Again an awesome course. &lt;em&gt;Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts&lt;/em&gt;. You will learn about &lt;em&gt;Python Libraries&lt;/em&gt; like &lt;strong&gt;Numpy,Pandas&lt;/strong&gt; for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:&lt;/p&gt;
&lt;div style="color:black; background-color: #E9DAEE;"&gt;
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.
&lt;/div&gt;

&lt;h2&gt;3. &lt;a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=495576.248&amp;amp;type=3&amp;amp;subid=0"&gt;CS229: Andrew Ng&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;After doing these two above courses you will gain the status of what I would like to call a &lt;strong&gt;"Beginner"&lt;/strong&gt;. Congrats!!!. &lt;em&gt;You know stuff, you know how to implement stuff&lt;/em&gt;. Yet you do not fully understand all the math and grind that goes behind all this.&lt;/p&gt;
&lt;p&gt;Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms.  I will put this course as the &lt;em&gt;one course you gotta take&lt;/em&gt; as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.&lt;/p&gt;
&lt;p&gt;Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website &lt;a href="http://www.mlyearning.org/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D'artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.&lt;/p&gt;
&lt;h2&gt;4. &lt;a href="https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617"&gt;Intro to Hadoop &amp;amp; Mapreduce - Udacity&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Let us first focus on the literal elephant in the room - Hadoop.&lt;/em&gt; Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.&lt;/p&gt;
&lt;p&gt;Once  you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem.
Read this amazing Blog Post from Michael Noll: &lt;a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/"&gt;Writing An Hadoop MapReduce Program In Python - Michael G. Noll&lt;/a&gt;.  Just read the basic mapreduce codes. Don't use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now try to solve these two problems from the CS109 Harvard course from 2013:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; First, grab the file word_list.txt from &lt;a href="https://raw.github.com/cs109/content/master/labs/lab8/word_list.txt"&gt;here&lt;/a&gt;.  This contains a list of six-letter words. To keep things simple, all of  the words consist of lower-case letters only.Write a mapreduce job that  finds all anagrams in word_list.txt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;B.&lt;/strong&gt; For the next problem, download the file &lt;a href="https://raw.github.com/cs109/content/master/labs/lab8/baseball_friends.csv"&gt;baseball_friends.csv&lt;/a&gt;. Each row of this csv file contains the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A person's name&lt;/li&gt;
&lt;li&gt;The team that person is rooting for -- either "Cardinals" or "Red Sox"&lt;/li&gt;
&lt;li&gt;A list of that person's friends, which could have arbitrary length&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;For  example:&lt;/em&gt; The first line tells us that Aaden is a Red Sox friend and he  has 65  friends, who are all listed here. For this problem, it's safe to  assume  that all of the names are unique and that the friendship  structure is  symmetric (i.e. if Alannah shows up in Aaden's friends list, then Aaden will show up in Alannah's friends list).
Write  an mr job that lists each person's name, their favorite  team, the  number of Red Sox fans they are friends with, and the number  of  Cardinals fans they are friends with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try to do this yourself.&lt;/strong&gt; Don't use the mrjob (pronounced Mr. Job) way that  they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.&lt;/p&gt;
&lt;p&gt;If you are done with these, you can safely call yourself as someone who could &lt;strong&gt;"think in Mapreduce"&lt;/strong&gt; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:&lt;br&gt;
&lt;a href="http://mlwhiz.com/blog/2015/05/09/Hadoop_Mapreduce_Streaming_Tricks_and_Techniques/"&gt;Hadoop Mapreduce Streaming Tricks and Techniques&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are someone who likes learning from a book you can get:
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/Hadoop-Definitive-Storage-Analysis-Internet/dp/1491901632/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543345&amp;sr=1-1&amp;keywords=hadoop+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=e0a6c64497866b874326afa08a069654" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491901632&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491901632" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;h2&gt;5. Spark - In memory Big Data tool.&lt;/h2&gt;
&lt;p&gt;Now  comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn't.&lt;/p&gt;
&lt;p&gt;Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.edx.org/xseries/data-science-engineering-apacher-sparktm"&gt;Data Science and Engineering with Apache® Spark™&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have written a little bit about Basic data processing with Spark here. Take a look:
&lt;a href="http://mlwhiz.com/blog/2015/09/07/Spark_Basics_Explained/"&gt;Learning Spark using Python: Basics and Applications&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also take a look at some of the projects I did as part of course at &lt;a href="http://www.github.com/MLWhiz/Spark_Projects/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you would like a book to read:
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491912766/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543902&amp;sr=1-1&amp;keywords=spark+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=85591cf408de278e23e8570b7e9c284b" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491912766&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491912766" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;If you don't go through the courses, &lt;strong&gt;try solving the same two problems above that you solved by Hadoop using Spark too&lt;/strong&gt;. Otherwise the problem sets in the courses are more than enough.&lt;/p&gt;
&lt;h2&gt;6. Understand Linux Shell:&lt;/h2&gt;
&lt;p&gt;Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn't emphasize how much time shell saves for me everyday.&lt;/p&gt;
&lt;p&gt;Read these tutorials by me for doing that:&lt;br&gt;
&lt;a href="http://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/"&gt;Shell Basics every Data Scientist Should know -Part I&lt;/a&gt;
&lt;a href="http://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/"&gt;Shell Basics every Data Scientist Should know - Part II(AWK)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you would like a course you can go for &lt;a href="https://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1#!"&gt;this course on edX&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want a book, go for:&lt;/p&gt;
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/Linux-Command-Line-Complete-Introduction/dp/1593273894/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490544715&amp;sr=1-1&amp;keywords=the+linux+command+line&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=9f155a16f16c7ae34e682e0e0312ee8f" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1593273894&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1593273894" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Congrats you are an "Hacker" now.&lt;/strong&gt; You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &lt;strong&gt;"All Roads lead to Rome"&lt;/strong&gt; as long as you are running.&lt;/p&gt;
&lt;h2&gt;7. &lt;a href="https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=467035.204&amp;amp;type=3&amp;amp;subid=0"&gt;Learn Statistical Inference and Bayesian Statistics&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I took the previous version of the specialization which was a single course taught by Mine Çetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
You can also use these books:&lt;/p&gt;
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/dp/1943450056/ref=as_li_ss_il?m=A3EEBE82C3HYRD&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=cfd246ebddfde379bc01dcb2c467c199" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1943450056&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1943450056" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a href="https://www.amazon.com/Probability-Statistics-4th-Morris-DeGroot/dp/0321500466/ref=as_li_ss_il?ie=UTF8&amp;qid=1490547535&amp;sr=8-1&amp;keywords=degroot+statistics&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=fc106a3b8c56be8baf34793816762ec8" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=0321500466&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=0321500466" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;

&lt;h2&gt;8. Deep Learning&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.fast.ai/"&gt;Intro&lt;/a&gt; - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cs231n.github.io/"&gt;Advanced&lt;/a&gt; - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Bonus&lt;/a&gt; - A free online book by Michael Nielsen.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://amzn.to/2npItnM"&gt;Advanced Math Book&lt;/a&gt; - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow&lt;/p&gt;
&lt;h2&gt;9. &lt;a href="https://www.youtube.com/watch?v=xoA5v9AO7S0&amp;amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV"&gt;Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This course used to be there on Coursera but now only video links on youtube available.
You can learn from this book too:
&lt;div style="margin-left:1em ; text-align: center;"&gt;
&lt;a href="https://www.amazon.com/Mining-Massive-Datasets-Jure-Leskovec/dp/1107077230/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=ba893a022640a279d427fd0c5ea44c1a" target="_blank"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1107077230&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20" &gt;&lt;/a&gt;&lt;img src="https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1107077230" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Apart from that if you want to learn about Python and the basic intricacies of the language you can take the &lt;strong&gt;&lt;a href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=495576.1921197134&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fcomputer-fundamentals"&gt;Computer Science Mini Specialization from RICE university&lt;/a&gt;&lt;/strong&gt; too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.&lt;/p&gt;
&lt;h2&gt;10. Advanced Maths:&lt;/h2&gt;
&lt;p&gt;Couldn't write enough of the importance of Math. But here are a few awesome resources that you can go for.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/"&gt;Linear Algebra By Gilbert Strang&lt;/a&gt; - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/"&gt;Multivariate Calculus - MIT OCW&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about"&gt;Convex Optimization&lt;/a&gt; - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.&lt;/p&gt;
&lt;p&gt;The Machine learning field is evolving and new advancements are made every day. That's why I didn't put a third tier. The maximum I can call myself is a "Hacker" and my learning continues. Hope you do the same.&lt;/p&gt;
&lt;p&gt;Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.&lt;/p&gt;
&lt;p&gt;Till then. Ciao!!!&lt;/p&gt;</summary><category term="Data Science"></category><category term="Statistics"></category><category term="Resources"></category><category term="Learning"></category><category term="Books"></category><category term="Python"></category><category term="Distributions"></category><category term="Statistical Inference"></category><category term="hadoop"></category><category term="spark"></category><category term="deep learning"></category></entry><entry><title>Basics Of Linear Regression</title><link href="http://mlwhiz.github.io/blog/2017/03/23/basics_of_linear_regression/" rel="alternate"></link><updated>2017-03-23T04:43:00-03:00</updated><author><name>Naveen Kumar Kaveti</name></author><id>tag:mlwhiz.github.io,2017-03-23:blog/2017/03/23/basics_of_linear_regression/</id><summary type="html">&lt;p&gt;Today we will look into the basics of linear regression. Here we go :&lt;/p&gt;
&lt;h2&gt;Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Simple Linear Regression (SLR)&lt;/li&gt;
&lt;li&gt;Multiple Linear Regression (MLR)&lt;/li&gt;
&lt;li&gt;Assumptions&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;1. Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;Regression is the process of building a relationship between a dependent variable and set of independent variables. Linear Regression restricts this relationship to be linear in terms of coefficients. In SLR, we consider only one independent variable.&lt;/p&gt;
&lt;h3&gt;Example: The Waist Circumference – Adipose Tissue data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Studies have shown that individuals with excess Adipose tissue (AT) in the abdominal region have a higher risk of cardio-vascular diseases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computed Tomography, commonly called the CT Scan is the only technique that allows for the precise and reliable measurement of the AT (at any site in the body)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The problems with using the CT scan are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Many physicians do not have access to this technology&lt;/li&gt;
&lt;li&gt;Irradiation of the patient (suppresses the immune system)&lt;/li&gt;
&lt;li&gt;Expensive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is there a simpler yet reasonably accurate way to predict the AT area? i.e.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easily available&lt;/li&gt;
&lt;li&gt;Risk free&lt;/li&gt;
&lt;li&gt;Inexpensive&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A group of researchers  conducted a study with the aim of predicting abdominal AT area using simple anthropometric measurements i.e. measurements on the human body&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Waist Circumference – Adipose Tissue data is a part of this study wherein the aim is to study how well waist circumference(WC) predicts the AT area&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;# Setting working directory&lt;/span&gt;
filepath &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/Users/nkaveti/Documents/Work_Material/Statistics Learning/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
setwd&lt;span class="p"&gt;(&lt;/span&gt;filepath&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Reading data&lt;/span&gt;
Waist_AT &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;adipose_tissue.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
cat&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of rows: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; nrow&lt;span class="p"&gt;(&lt;/span&gt;Waist_AT&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
head&lt;span class="p"&gt;(&lt;/span&gt;Waist_AT&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;109&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;&lt;th scope=col&gt;Waist&lt;/th&gt;&lt;th scope=col&gt;AT&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;&lt;td&gt;74.75&lt;/td&gt;&lt;td&gt;25.72&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;72.60&lt;/td&gt;&lt;td&gt;25.89&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;81.80&lt;/td&gt;&lt;td&gt;42.60&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;83.95&lt;/td&gt;&lt;td&gt;42.80&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;74.65&lt;/td&gt;&lt;td&gt;29.84&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;71.85&lt;/td&gt;&lt;td&gt;21.68&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let's start with a scatter plot of &lt;strong&gt;Waist&lt;/strong&gt; Vs &lt;strong&gt;AT&lt;/strong&gt;, to understand the relationship between these two variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="output_6_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;Any observations from above plot?&lt;/p&gt;
&lt;p&gt;Now the objective is to find a linear relation between &lt;code&gt;Waist&lt;/code&gt; and &lt;code&gt;AT&lt;/code&gt;. In otherwords, finding the amount of change in &lt;code&gt;AT&lt;/code&gt; per one unit change (increment/decrement) in &lt;code&gt;Waist&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In SLR, it is equivalent to finding an optimal straight line equation such that the sum of squares of differences between straight line and the points will be minimum. This method of estimation is called as &lt;a href="https://en.wikipedia.org/wiki/Ordinary_least_squares"&gt;Ordiany Least Squares (OLS)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$AT  = \beta_0 + \beta_1 \ Waist + \epsilon$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Min_{\beta_0 , \beta_1} \ \ \epsilon^\intercal \epsilon \implies Min_{\beta_0 , \beta_1} \ \ (AT  - \beta_0 - \beta_1 \ Waist)^\intercal (AT  - \beta_0 - \beta_1 \ Waist)$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; represents the amount of change in &lt;code&gt;AT&lt;/code&gt; per one unit change in &lt;code&gt;Waist&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now our problem becomes an unconstrained optimization problem. We can find optimal values for &lt;span class="math"&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; using basic calculus.&lt;/p&gt;
&lt;p&gt;Lets re-write above regression equation in matrix form&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ AT = X \beta + \epsilon$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\( X = [1 \ \ Waist]\)&lt;/span&gt; 1 is a vector of ones and &lt;span class="math"&gt;\(\beta = (\beta_0, \ \beta_1)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
\begin{split}
\epsilon^\intercal \epsilon &amp;amp; = {(AT - X \beta)}^\intercal {(AT - X \beta)} \\
&amp;amp; = AT^\intercal AT - AT^\intercal X \beta - {(X \beta)}^\intercal AT + {(X \beta)}^\intercal (X \beta)
\end{split}
\end{equation}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Now differentiate this w.r.t to &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; and equate it to zero. Then we have,
&lt;div class="math"&gt;$$\hat{\beta} = (X^\intercal X)^{-1} X^\intercal AT $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Now we can find the fitted values of model by substituting &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt; in above regression equation
&lt;div class="math"&gt;$$\hat{AT} = X \hat{\beta}=X(X^\intercal X)^{-1} X^\intercal AT$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We are arriving to above equation through an assumption&lt;a href="#Assumptions"&gt;&lt;span class="math"&gt;\(^1\)&lt;/span&gt;&lt;/a&gt; of &lt;span class="math"&gt;\(E(\epsilon)=0\)&lt;/span&gt;. What happens if this assumption violates?&lt;/p&gt;
&lt;p&gt;Let, &lt;span class="math"&gt;\(X(X^\intercal X)^{-1} X^\intercal = H\)&lt;/span&gt;
&lt;div class="math"&gt;$$\hat{AT} = H \ AT$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;We call H as an hat matrix, because it transforms &lt;span class="math"&gt;\(AT\)&lt;/span&gt; into &lt;span class="math"&gt;\(\hat{AT}\)&lt;/span&gt; :D&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Lets compute the hat matrix&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%*%&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%*%&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;betahat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;%*%&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Estimated&lt;/span&gt; &lt;span class="n"&gt;coefficients&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Let&amp;#39;s compare the computed values with lm() output: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Computed Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;#cat(&amp;quot;Optimal value for beta_0 is: &amp;quot;, betahat[1], &amp;quot;and for beta_1 is: &amp;quot;, betahat[2], &amp;quot;\n \n&amp;quot;)&lt;/span&gt;
&lt;span class="n"&gt;fit_lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;#cat(&amp;quot;Compare our computed estimates with lm() estimates&amp;quot;, &amp;quot;\n&amp;quot;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;%*%&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Computing&lt;/span&gt; &lt;span class="n"&gt;hat&lt;/span&gt; &lt;span class="n"&gt;matrix&lt;/span&gt;
&lt;span class="n"&gt;AThat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;%*%&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Computing&lt;/span&gt; &lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Therefore, there is a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;increment in AT per one unit change in Waist &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Let&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;compare&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;computed&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;Computed&lt;/span&gt; &lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

  &lt;span class="n"&gt;Intercept&lt;/span&gt;    &lt;span class="n"&gt;Waist&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;215.9815&lt;/span&gt; &lt;span class="mf"&gt;3.458859&lt;/span&gt;
&lt;span class="o"&gt;=======================================================================&lt;/span&gt;

&lt;span class="nl"&gt;Call:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nl"&gt;Coefficients:&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="n"&gt;Waist&lt;/span&gt;
   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;215.981&lt;/span&gt;        &lt;span class="mf"&gt;3.459&lt;/span&gt;

&lt;span class="o"&gt;=======================================================================&lt;/span&gt;

&lt;span class="n"&gt;Therefore&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="mf"&gt;3.458859&lt;/span&gt; &lt;span class="n"&gt;increment&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="n"&gt;unit&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;What's next?&lt;/h2&gt;
&lt;p&gt;We succesfully computed estimates for regression coefficients and fitted values.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We are working on only one sample, how can we generalise these results to population?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to measure model's performance quantitatively?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;  We are working on only one sample, how can we generalise these results to population? &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's focus on question 1. Our regression coefficients are computed using only one sample and these values will change, if we change the sample. But how much they vary? We need to estimate the variation for each beta coefficient to check whether the corresponding regressor is consistently explaining the same behaviour even if we change the sample.&lt;/p&gt;
&lt;p&gt;Now the big problem is collecting multiple samples to check the above hypothesis. Hence, we use distributions to check statistical significance of regressors.&lt;/p&gt;
&lt;p&gt;For our example, we need to test below two hypotheses.&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ Null \ Hypothesis: \beta_{0} = 0 $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ Alternative \ Hypothesis: \beta_{0} \neq 0$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ Null \ Hypothesis: \beta_{1} = 0 $$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ Alternative \ Hypothesis: \beta_{1} \neq 0$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Test Statistic for these hypotheses is,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$t = \frac{\hat{\beta_{i}}}{\sqrt{Var(\hat{\beta_{i}})}}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Test statistic &lt;code&gt;t&lt;/code&gt; follows &lt;code&gt;t-distribution&lt;/code&gt;, assuming&lt;a href="#Assumptions"&gt;&lt;span class="math"&gt;\(^2\)&lt;/span&gt;&lt;/a&gt; dependent variable follows &lt;code&gt;normal distribution&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Suggestion:&lt;/strong&gt; If your not aware of &lt;a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"&gt;testing of hypothesis&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Probability_distribution"&gt;probability distributions&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/P-value"&gt;p-values&lt;/a&gt; please browse through the Google.&lt;/p&gt;
&lt;p&gt;Let's recall that,  &lt;span class="math"&gt;\(\hat{\beta} = (X^\intercal X)^{-1} X^\intercal AT\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\begin{equation}
\begin{split}
Var(\hat{\beta}) &amp;amp; = Var((X^\intercal X)^{-1} X^\intercal AT) \\
 &amp;amp; = (X^\intercal X)^{-1} X^\intercal \ Var(AT) \ X(X^\intercal X)^{-1} \\
 &amp;amp; = (X^\intercal X)^{-1} X^\intercal \ X(X^\intercal X)^{-1} \ \sigma^2 \\
 &amp;amp; = (X^\intercal X)^{-1} \sigma^2
\end{split}
\end{equation}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In the above calculations we assumed&lt;a href="#Assumptions"&gt;&lt;span class="math"&gt;\(^3\)&lt;/span&gt;&lt;/a&gt; &lt;span class="math"&gt;\(Var(AT) = \sigma^2\)&lt;/span&gt; (Constant). Where, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; is variation in population AT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Suggestion:&lt;/strong&gt; Try solving &lt;span class="math"&gt;\((X^\intercal X)^{-1}\)&lt;/span&gt; with &lt;span class="math"&gt;\(X = [1, \  x]\)&lt;/span&gt; where &lt;span class="math"&gt;\(x = (x_1, x_2, x_3 ... x_n)\)&lt;/span&gt;. You will get the following expression.&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\
Var(\hat{\beta}) =
\frac{1}{n \sum x_i^2 - (\sum x_i)^2}
\begin{bmatrix}
    \sum_{i=1}^n x_i^2 &amp;amp; -\sum x_i \\
    -\sum x_i &amp;amp; n
\end{bmatrix}
\sigma^2
\
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Diagonal elements of above matrix are varinaces of &lt;span class="math"&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; respectively. Off-diagonal element is covariance between &lt;span class="math"&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Hence,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var(\hat{\beta_0}) = \frac{\sigma^2 \sum_{i = 1}^n x_i^2}{n \sum_{i = 1}^n (x_i - \bar{x})^2}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var(\hat{\beta_1}) = \frac{\sigma^2}{\sum_{i = 1}^n (x_i - \bar{x})^2}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;One important observation from &lt;span class="math"&gt;\(Var(\hat{\beta})\)&lt;/span&gt; expressions is, &lt;span class="math"&gt;\(Var(x)\)&lt;/span&gt; is inversely proportional to &lt;span class="math"&gt;\(Var(\hat{\beta})\)&lt;/span&gt;. That is, we will get more consistent estimators if there is high variation in corresponding predictors.&lt;/p&gt;
&lt;p&gt;Recall that, &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; in above expression is the population variance, not the sample. Hence, we need to estimate this using the sample that we have.&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\hat{\sigma^2} = \frac{1}{n-2} \sum_{i = 1}^n e_i^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\(e_i = AT_i - \hat{AT}_i\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Let&amp;#39;s compute variances of beta hat and test statistic &amp;#39;t&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;sigmasq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AThat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]))&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;AThat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;VarBeta0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigmasq&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AThat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;VarBeta1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmasq&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Let&amp;#39;s compare the computed values with lm() output: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Computed Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarBeta0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarBeta1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;t_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarBeta0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;betahat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarBeta1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;(Intercept)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Waist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;p_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;t_value&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;=======================================================================&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;=======================================================================&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Let&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;compare&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;computed&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;span class="n"&gt;Computed&lt;/span&gt; &lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

               &lt;span class="n"&gt;Estimate&lt;/span&gt;  &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Error&lt;/span&gt;   &lt;span class="n"&gt;t_value&lt;/span&gt;      &lt;span class="n"&gt;p_value&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;215.981488&lt;/span&gt; &lt;span class="mf"&gt;21.7962708&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;9.909103&lt;/span&gt; &lt;span class="mf"&gt;7.507198e-17&lt;/span&gt;
&lt;span class="n"&gt;Waist&lt;/span&gt;          &lt;span class="mf"&gt;3.458859&lt;/span&gt;  &lt;span class="mf"&gt;0.2346521&lt;/span&gt; &lt;span class="mf"&gt;14.740376&lt;/span&gt; &lt;span class="mf"&gt;1.297124e-27&lt;/span&gt;
&lt;span class="o"&gt;=======================================================================&lt;/span&gt;



&lt;span class="nl"&gt;Call:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nl"&gt;Residuals:&lt;/span&gt;
     &lt;span class="n"&gt;Min&lt;/span&gt;       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;   &lt;span class="n"&gt;Median&lt;/span&gt;       &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;      &lt;span class="n"&gt;Max&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;107.288&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;19.143&lt;/span&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.939&lt;/span&gt;   &lt;span class="mf"&gt;16.376&lt;/span&gt;   &lt;span class="mf"&gt;90.342&lt;/span&gt;

&lt;span class="nl"&gt;Coefficients:&lt;/span&gt;
             &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;215.9815&lt;/span&gt;    &lt;span class="mf"&gt;21.7963&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;9.909&lt;/span&gt;   &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mf"&gt;2e-16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;Waist&lt;/span&gt;          &lt;span class="mf"&gt;3.4589&lt;/span&gt;     &lt;span class="mf"&gt;0.2347&lt;/span&gt;  &lt;span class="mf"&gt;14.740&lt;/span&gt;   &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mf"&gt;2e-16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;33.06&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;107&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;   &lt;span class="mf"&gt;0.67&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.667&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;217.3&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;107&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2e-16&lt;/span&gt;



&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Residual standard error = &lt;span class="math"&gt;\(\sqrt{sigmasq}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to measure model's performance quantitatively?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let's focus on question 2 (How to measure model's performance quantitatively?). Recall that, our objective of building model is to explain the variation in &lt;code&gt;AT&lt;/code&gt; using the variation in &lt;code&gt;Waist&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Total variation in AT is, &lt;span class="math"&gt;\(\sum_{i=1}^n (AT - mean(AT))^2\)&lt;/span&gt; this can be splitted into two parts as follows:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$
\begin{equation}
\begin{split}
\sum_{i=1}^n (AT_i - \bar{AT})^2 &amp;amp; = \sum_{i=1}^n (AT  - \hat{AT_i} + \hat{AT_i} - \bar{AT})^2 \\
&amp;amp; = \sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2 + \sum_{i=1}^n (AT_i - \hat{AT_i})^2
\end{split}
\end{equation}
$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\(\sum_{i=1}^n (AT_i - \bar{AT})^2\)&lt;/span&gt; is the total variation in AT, &lt;span class="math"&gt;\(\sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2\)&lt;/span&gt; is the explained variation in AT, this is also called as &lt;strong&gt;Regression Sum of Squares&lt;/strong&gt; and &lt;span class="math"&gt;\(\sum_{i=1}^n (AT_i - \hat{AT_i})^2\)&lt;/span&gt; is the unexplained variation in AT, this is also called as &lt;strong&gt;Error Sum of Squares&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can measure our model using the proportion of total variation explained by independent variable(s). That is, &lt;span class="math"&gt;\(\frac{Regression \  Sum \ of \ Squares}{Total \ Sum \ of \ Squares}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above measure is called as Multiple R-squared:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Multiple \ R-squared = \frac{\sum_{i = 1}^n (\hat{AT_i} - \bar{AT})^2}{\sum_{i=1}^n (AT_i - \bar{AT})^2}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interesting facts:&lt;/strong&gt; Multiple R-squared value in SLR is equals to &lt;span class="math"&gt;\(r^2\)&lt;/span&gt; and (1 - Multiple R-squared) is equals to the variance in residuals.&lt;/p&gt;
&lt;p&gt;Where, r is &lt;a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"&gt;pearson's correlation coefficient&lt;/a&gt; between dependent and independent variable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Let&amp;#39;s compute Multiple R-squared measure for our example&lt;/span&gt;
&lt;span class="n"&gt;SSR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;AThat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;SST&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;MulRSq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SSR&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;SST&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Compute Multiple R-squared: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;MulRSq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Note that computed R squared value is matching with lm() Multiple R-squared value in above output &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.6700369&lt;/span&gt;

&lt;span class="n"&gt;Note&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;computed&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;squared&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;matching&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;above&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;

&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;What happens to the Multiple R-squared value when you add an irrelevant variable to the model?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the below model, I am generating a random sample of uniform numbers between 1 to 100 and considering this as one of indepedent variable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;fit_lm2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;runif&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_lm2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Call&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Waist&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;runif&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;Min&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;  &lt;span class="n"&gt;Median&lt;/span&gt;      &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;     &lt;span class="n"&gt;Max&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;106.06&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;17.53&lt;/span&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.63&lt;/span&gt;   &lt;span class="mf"&gt;13.70&lt;/span&gt;   &lt;span class="mf"&gt;91.36&lt;/span&gt;

&lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                               &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="o"&gt;(&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|)&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;                   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;226.2894&lt;/span&gt;    &lt;span class="mf"&gt;23.4350&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;9.656&lt;/span&gt; &lt;span class="mf"&gt;3.33&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;Waist&lt;/span&gt;                            &lt;span class="mf"&gt;3.5060&lt;/span&gt;     &lt;span class="mf"&gt;0.2376&lt;/span&gt;  &lt;span class="mf"&gt;14.757&lt;/span&gt;  &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;runif&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;    &lt;span class="mf"&gt;0.1397&lt;/span&gt;     &lt;span class="mf"&gt;0.1181&lt;/span&gt;   &lt;span class="mf"&gt;1.183&lt;/span&gt;    &lt;span class="mf"&gt;0.239&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;106&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.6743&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.6682&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;109.7&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;106&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;



&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Multiple R-squared value increases irrespective of quality of explanation, which is incorrect. We should penalize our model performance if the quality of explanation is poor, that is why we need to adjust our R-squared value.&lt;/p&gt;
&lt;p&gt;To penalize the explained part of AT, we inflate the unexplained part of AT with &lt;span class="math"&gt;\(\frac{Total \ degrees \ of \ freedom}{Error \ degrees \ of \ freedom}\)&lt;/span&gt;. That is,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Adjusted \ R-squared = 1 - (1 - R^2) \frac{n-1}{n-p-1}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, n = Total number of observations; p = Total number of predictors (excluding intercept)&lt;/p&gt;
&lt;p&gt;Adding a new independent variable will increase &lt;span class="math"&gt;\(\frac{n-1}{n-p-1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R^2\)&lt;/span&gt;. If the amount of increment in &lt;span class="math"&gt;\(R^2\)&lt;/span&gt; is less than the amount of increment in &lt;span class="math"&gt;\(\frac{n-1}{n-p-1}\)&lt;/span&gt; than it will decrease the Adjusted R-squared value.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;fit_lm2&lt;/code&gt; model Adjusted R-squared decreases when we add randomly generated variable into the model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Let&amp;#39;s compute adjusted R-squared  for our example&lt;/span&gt;
&lt;span class="n"&gt;TDF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Total&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;EDF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Waist_AT&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;where&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;predictors&lt;/span&gt;
&lt;span class="n"&gt;AdjRSq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;MulRSq&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TDF&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;EDF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Compute Multiple R-squared: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdjRSq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Note that computed Adjusted R-squared value is matching with lm() Adjusted R-squared value in the above output &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Note: We are comparing with fit_lm model, not fit_lm2 &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.6669531&lt;/span&gt;

&lt;span class="n"&gt;Note&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;computed&lt;/span&gt; &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;matching&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;above&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;

&lt;span class="nl"&gt;Note:&lt;/span&gt; &lt;span class="n"&gt;We&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;comparing&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;fit_lm&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;fit_lm2&lt;/span&gt;
&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Aforementioned measures (Multiple R-squared &amp;amp; Adjusted R-squared) for &lt;strong&gt;Goodness of fit&lt;/strong&gt; are functions of sample and these will vary as sample changes. Similar to &lt;code&gt;t-test&lt;/code&gt; for regression coefficeints we need some statistical test to test model's performance for population.&lt;/p&gt;
&lt;p&gt;Objective is to compare the Mean sum of squares due to regression and Mean sum of squares due to error. &lt;code&gt;F-test&lt;/code&gt; is very helpful to compare the variations.&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$ F-test = \frac{\frac{1}{p-1}\sum_{i=1}^n (\hat{AT_i} - \bar{AT})^2}{\frac{1}{n-p-1} \sum_{i=1}^n (\hat{AT_i} - AT_i)^2}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Above expression follows F distribution only if, AT follows Normal Distribution&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;RDF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TDF&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;EDF&lt;/span&gt;
&lt;span class="n"&gt;SSE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SST&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;SSR&lt;/span&gt;
&lt;span class="n"&gt;MSR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;RDF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;SSR&lt;/span&gt;
&lt;span class="n"&gt;MSE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;EDF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;SSE&lt;/span&gt;
&lt;span class="n"&gt;F_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MSR&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;MSE&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Compute F statistic: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F_value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Note that computed F-statistic is matching with lm() F-statistic value in the above output &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt; &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Note: We are comparing with fit_lm model, not fit_lm2 &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;======================================================================= &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Compute&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;217.2787&lt;/span&gt;

&lt;span class="n"&gt;Note&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;computed&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;matching&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;above&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;

&lt;span class="nl"&gt;Note:&lt;/span&gt; &lt;span class="n"&gt;We&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;comparing&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;fit_lm&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;fit_lm2&lt;/span&gt;
&lt;span class="o"&gt;=======================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2. Multiple Linear Regression (MLR)&lt;/h2&gt;
&lt;p&gt;In multiple linear regression we consider more than one predictor and one dependent variable. Most of the above explanation is valid for MLR too.&lt;/p&gt;
&lt;h3&gt;Example: Car's MPG (Miles Per Gallon) prediction&lt;/h3&gt;
&lt;p&gt;Our interest is to model the MPG of a car based on the other variables.&lt;/p&gt;
&lt;p&gt;Variable Description:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VOL = cubic feet of cab space&lt;/li&gt;
&lt;li&gt;HP = engine horsepower&lt;/li&gt;
&lt;li&gt;MPG = average miles per gallon&lt;/li&gt;
&lt;li&gt;SP = top speed, miles per hour&lt;/li&gt;
&lt;li&gt;WT = vehicle weight, hundreds of pounds&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Reading Boston housing prices data&lt;/span&gt;
&lt;span class="n"&gt;car&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cars.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of rows: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Number of variables: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;81&lt;/span&gt;
 &lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;variables&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;&lt;th scope=col&gt;HP&lt;/th&gt;&lt;th scope=col&gt;MPG&lt;/th&gt;&lt;th scope=col&gt;VOL&lt;/th&gt;&lt;th scope=col&gt;SP&lt;/th&gt;&lt;th scope=col&gt;WT&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;&lt;td&gt;49      &lt;/td&gt;&lt;td&gt;53.70068&lt;/td&gt;&lt;td&gt;89      &lt;/td&gt;&lt;td&gt;104.1854&lt;/td&gt;&lt;td&gt;28.76206&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;55      &lt;/td&gt;&lt;td&gt;50.01340&lt;/td&gt;&lt;td&gt;92      &lt;/td&gt;&lt;td&gt;105.4613&lt;/td&gt;&lt;td&gt;30.46683&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;55      &lt;/td&gt;&lt;td&gt;50.01340&lt;/td&gt;&lt;td&gt;92      &lt;/td&gt;&lt;td&gt;105.4613&lt;/td&gt;&lt;td&gt;30.19360&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;70      &lt;/td&gt;&lt;td&gt;45.69632&lt;/td&gt;&lt;td&gt;92      &lt;/td&gt;&lt;td&gt;113.4613&lt;/td&gt;&lt;td&gt;30.63211&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;53      &lt;/td&gt;&lt;td&gt;50.50423&lt;/td&gt;&lt;td&gt;92      &lt;/td&gt;&lt;td&gt;104.4613&lt;/td&gt;&lt;td&gt;29.88915&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;70      &lt;/td&gt;&lt;td&gt;45.69632&lt;/td&gt;&lt;td&gt;89      &lt;/td&gt;&lt;td&gt;113.1854&lt;/td&gt;&lt;td&gt;29.59177&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Our objective is to model the variation in &lt;code&gt;MPG&lt;/code&gt; using other independent variables. That is,&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$MPG = \beta_0 + \beta_1 VOL + \beta_2 HP + \beta_3 SP + \beta_4 WT + \epsilon$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\(\beta_1\)&lt;/span&gt; represents the amount of change in &lt;code&gt;MPG&lt;/code&gt; per one unit change in &lt;code&gt;VOL&lt;/code&gt; provided other variables are fixed. Let's consider below two cases,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case1:&lt;/strong&gt; HP = 49; VOL = 89; SP = 104.1854; WT = 28.76206 =&amp;gt; MPG = 104.1854&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case2:&lt;/strong&gt; HP = 49; VOL = 90; SP = 104.1854; WT = 28.76206 =&amp;gt; MPG = 105.2453&lt;/p&gt;
&lt;p&gt;then &lt;span class="math"&gt;\(\beta_1 = 105.2453 - 104.1854 = 1.0599\)&lt;/span&gt;. Similarly, &lt;span class="math"&gt;\(\beta_2, \beta_3, \beta_4\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above effect is called as &lt;a href="https://en.wikipedia.org/wiki/Ceteris_paribus"&gt;&lt;code&gt;Ceteris Paribus Effect&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But in real world it is very difficult to collect records in above manner. That's why we compute (function of) partial correlation coefficients to quantify the effect of one variable, keeping others constant.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Let&amp;#39;s build MLR model to predict MPG based using other variables&lt;/span&gt;
&lt;span class="n"&gt;fit_mlr_actual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_mlr_actual&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Call&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="o"&gt;.,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="n"&gt;Min&lt;/span&gt;       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;   &lt;span class="n"&gt;Median&lt;/span&gt;       &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;      &lt;span class="n"&gt;Max&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.94530&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.32792&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.04058&lt;/span&gt;  &lt;span class="mf"&gt;0.24256&lt;/span&gt;  &lt;span class="mf"&gt;1.71034&lt;/span&gt;

&lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="o"&gt;(&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|)&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;  &lt;span class="mf"&gt;7.100&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;  &lt;span class="mf"&gt;5.461&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;   &lt;span class="mf"&gt;0.000&lt;/span&gt;   &lt;span class="mf"&gt;1.0000&lt;/span&gt;
&lt;span class="n"&gt;HP&lt;/span&gt;          &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.285&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;  &lt;span class="mf"&gt;2.453&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;5.239&lt;/span&gt;  &lt;span class="mf"&gt;1.4&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;VOL&lt;/span&gt;         &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.207&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;1.389&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.591&lt;/span&gt;   &lt;span class="mf"&gt;0.5563&lt;/span&gt;
&lt;span class="n"&gt;SP&lt;/span&gt;           &lt;span class="mf"&gt;6.144&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;2.458&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;   &lt;span class="mf"&gt;2.500&lt;/span&gt;   &lt;span class="mf"&gt;0.0146&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="n"&gt;WT&lt;/span&gt;           &lt;span class="mf"&gt;3.287&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;1.390&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;   &lt;span class="mf"&gt;0.237&lt;/span&gt;   &lt;span class="mf"&gt;0.8136&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4915&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;76&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.7705&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.7585&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;63.8&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;76&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One key observation from above output is, Std. Error for &lt;code&gt;VOL&lt;/code&gt; and &lt;code&gt;WT&lt;/code&gt; is very huge comparing to others and this inflates &lt;code&gt;t values&lt;/code&gt; and &lt;code&gt;p value&lt;/code&gt;. Hence, these two variables becomes very insignificant for the model.&lt;/p&gt;
&lt;p&gt;Let's go into deep, what happened to &lt;span class="math"&gt;\(Var(\hat{\beta_{VOL}})\)&lt;/span&gt; and &lt;span class="math"&gt;\(Var(\hat{\beta_{WT}})\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Analogy for &lt;span class="math"&gt;\(Var(\hat{\beta})\)&lt;/span&gt; in MLR is as follows:&lt;/p&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$Var(\hat{\beta_{VOL}}) = \frac{\sigma^2}{n\sum_{i=1}^n (VOL_i - \bar{VOL})^2 (1 - R_{VOL}^2)}$$&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Where, &lt;span class="math"&gt;\(R_{VOL}^2\)&lt;/span&gt; = Multiple R-squared value obtained by regressing VOL on all other independent variables&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; To understand it more clearly, take few random samples from cars data and run the MLR model and observe the variation in &lt;span class="math"&gt;\(\hat{\beta_{VOL}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{\beta_{WT}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Let&amp;#39;s regress VOL on all other independent variables&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;fit_mlr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VOL&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;HP&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SP&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_mlr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Call&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VOL&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;HP&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;SP&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;Min&lt;/span&gt;        &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;    &lt;span class="n"&gt;Median&lt;/span&gt;        &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;       &lt;span class="n"&gt;Max&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.068938&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.031641&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.008794&lt;/span&gt;  &lt;span class="mf"&gt;0.032018&lt;/span&gt;  &lt;span class="mf"&gt;0.077931&lt;/span&gt;

&lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="o"&gt;(&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|)&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;6.155&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;  &lt;span class="mf"&gt;4.481&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;   &lt;span class="mf"&gt;0.000&lt;/span&gt;    &lt;span class="mf"&gt;1.000&lt;/span&gt;
&lt;span class="n"&gt;HP&lt;/span&gt;           &lt;span class="mf"&gt;2.331&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;  &lt;span class="mf"&gt;1.995&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;   &lt;span class="mf"&gt;1.168&lt;/span&gt;    &lt;span class="mf"&gt;0.246&lt;/span&gt;
&lt;span class="n"&gt;SP&lt;/span&gt;          &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.294&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;  &lt;span class="mf"&gt;2.000&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.147&lt;/span&gt;    &lt;span class="mf"&gt;0.255&lt;/span&gt;
&lt;span class="n"&gt;WT&lt;/span&gt;           &lt;span class="mf"&gt;9.998&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;4.557&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt; &lt;span class="mf"&gt;219.396&lt;/span&gt;   &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.04033&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;77&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.9984&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.9984&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.637&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;04&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;77&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It's surprising that, &lt;span class="math"&gt;\(R_{VOL}^2\)&lt;/span&gt; is 0.9984 and also only &lt;code&gt;WT&lt;/code&gt; is significant. That is, these two predictors (&lt;code&gt;VOL&lt;/code&gt; and &lt;code&gt;WT&lt;/code&gt;) are highly correlated. This inflates &lt;span class="math"&gt;\(Var(\hat{\beta_{VOL}})\)&lt;/span&gt; and thus &lt;code&gt;t value&lt;/code&gt;. We might be missing some of the important information because of high correlation between predictors. This problem is called as &lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;Multicollinearity&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One quick solution for this problem is to remove either &lt;code&gt;VOL&lt;/code&gt; or &lt;code&gt;WT&lt;/code&gt; from the model. Let's compute partial correlation coeficient between &lt;code&gt;MPG&lt;/code&gt; and &lt;code&gt;VOL&lt;/code&gt; by removing the effect of &lt;code&gt;WT&lt;/code&gt; (say, &lt;span class="math"&gt;\(r_{MV.W}\)&lt;/span&gt;) and partial correlation coeficient between &lt;code&gt;MPG&lt;/code&gt; and &lt;code&gt;WT&lt;/code&gt; by removing the effect of &lt;code&gt;VOL&lt;/code&gt; (say, &lt;span class="math"&gt;\(r_{MW.V}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To compute &lt;span class="math"&gt;\(r_{MV.W}\)&lt;/span&gt; we need to compute the correlation between (a) part of &lt;code&gt;VOL&lt;/code&gt; which cannot be explained by &lt;code&gt;WT&lt;/code&gt; (regress &lt;code&gt;VOL&lt;/code&gt; on &lt;code&gt;WT&lt;/code&gt; and take the residuals) and (b) the part of &lt;code&gt;MPG&lt;/code&gt; which cannot be explained by &lt;code&gt;WT&lt;/code&gt; (regress &lt;code&gt;MPG&lt;/code&gt; on &lt;code&gt;WT&lt;/code&gt; and take the residuals)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;fit_partial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VOL&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;fit_partial2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;res1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_partial&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;residual&lt;/span&gt;
&lt;span class="n"&gt;res2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_partial2&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;residual&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Partial correlation coefficient between MPG and VOL by removing the effect of WT is: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;res2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Partial&lt;/span&gt; &lt;span class="n"&gt;correlation&lt;/span&gt; &lt;span class="n"&gt;coefficient&lt;/span&gt; &lt;span class="n"&gt;between&lt;/span&gt; &lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;VOL&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="n"&gt;removing&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;effect&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.08008873&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;fit_partial3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WT&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;VOL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;fit_partial4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;VOL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;res1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_partia3&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;residual&lt;/span&gt;
&lt;span class="n"&gt;res2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fit_partial4&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;residual&lt;/span&gt;
&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Partial correlation coefficient between MPG and WT by removing the effect of VOL is: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;res2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Partial&lt;/span&gt; &lt;span class="n"&gt;correlation&lt;/span&gt; &lt;span class="n"&gt;coefficient&lt;/span&gt; &lt;span class="n"&gt;between&lt;/span&gt; &lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="n"&gt;removing&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;effect&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;VOL&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.05538241&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since, &lt;span class="math"&gt;\(abs(r_{MV.W}) &amp;gt;= abs(r_{MW.V})\)&lt;/span&gt; we may remove &lt;code&gt;WT&lt;/code&gt; from the model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Remove WT and rerun the model&lt;/span&gt;
&lt;span class="n"&gt;fit_mlr_actual2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fit_mlr_actual2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Call&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MPG&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;WT&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;car&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="n"&gt;Min&lt;/span&gt;       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;   &lt;span class="n"&gt;Median&lt;/span&gt;       &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;      &lt;span class="n"&gt;Max&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.94036&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.31695&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.03457&lt;/span&gt;  &lt;span class="mf"&gt;0.23316&lt;/span&gt;  &lt;span class="mf"&gt;1.71570&lt;/span&gt;

&lt;span class="n"&gt;Coefficients&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
              &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="o"&gt;(&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|)&lt;/span&gt;
&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;  &lt;span class="mf"&gt;7.910&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;  &lt;span class="mf"&gt;5.427&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;   &lt;span class="mf"&gt;0.000&lt;/span&gt;   &lt;span class="mf"&gt;1.0000&lt;/span&gt;
&lt;span class="n"&gt;HP&lt;/span&gt;          &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.293&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;  &lt;span class="mf"&gt;2.415&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;5.353&lt;/span&gt; &lt;span class="mf"&gt;8.64&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;07&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;VOL&lt;/span&gt;         &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;4.925&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;5.516&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.928&lt;/span&gt; &lt;span class="mf"&gt;1.65&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;SP&lt;/span&gt;           &lt;span class="mf"&gt;6.222&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;  &lt;span class="mf"&gt;2.421&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;   &lt;span class="mf"&gt;2.571&lt;/span&gt;   &lt;span class="mf"&gt;0.0121&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4884&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;77&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.7704&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.7614&lt;/span&gt;
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;86.11&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;77&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After eliminating &lt;code&gt;WT&lt;/code&gt; from the model there is an increment of ~0.3% in Adjusted R-squared and more importantly, &lt;code&gt;VOL&lt;/code&gt; becomes significant at 0 &lt;a href="https://en.wikipedia.org/wiki/Statistical_significance"&gt;los&lt;/a&gt; (level of significance)&lt;/p&gt;
&lt;p&gt;&lt;a id='Assumptions'&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;3. Assumptions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Linear in Parameters:&lt;/strong&gt; We assume that there is a linear relation between dependent and set of independent variables&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zero conditional mean:&lt;/strong&gt; &lt;span class="math"&gt;\(E(\epsilon \mid X) = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Homoskedasticity:&lt;/strong&gt; &lt;span class="math"&gt;\(Var(\epsilon \mid X) = \sigma^2\)&lt;/span&gt; (Constant)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No perfect Collinearity:&lt;/strong&gt; All predecitors must be independent among themselves&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No serial correlation in errors:&lt;/strong&gt; Erros must be uncorrelated among themselves. In otherwords, observations or records must be independent of each other.&lt;/p&gt;
&lt;p&gt;We discussed first 4 assumptions in section 1 and 2.&lt;/p&gt;
&lt;p&gt;Here is a book that I recommend to learn more about this:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank"  href="https://www.amazon.com/gp/product/1111531048/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1111531048&amp;linkCode=as2&amp;tag=nkaveti-20&amp;linkId=83f6e694209869322f8bfad406883d2f"&gt;&lt;img border="0" src="//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1111531048&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=nkaveti-20" &gt;&lt;/a&gt;&lt;img src="//ir-na.amazon-adsystem.com/e/ir?t=nkaveti-20&amp;l=am2&amp;o=1&amp;a=1111531048" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" /&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processClass: 'mathjax', " +
        "        ignoreClass: 'no-mathjax', " +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="SLR"></category><category term="MLR"></category><category term="Linear Regression"></category><category term="Statistic"></category><category term="Basics"></category><category term="Layman Explanations"></category></entry></feed>