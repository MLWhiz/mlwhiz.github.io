<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.github.io/" rel="alternate"></link><link href="http://mlwhiz.github.io/feeds/python-kaggle-coursera.atom.xml" rel="self"></link><id>http://mlwhiz.github.io/</id><updated>2018-09-06T04:43:00-03:00</updated><entry><title>Multi-input Transfer Learning Model - One model to rule them all</title><link href="http://mlwhiz.github.io/blog/2018/09/06/multi_input_transfer_learning/" rel="alternate"></link><updated>2018-09-06T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2018-09-06:blog/2018/09/06/multi_input_transfer_learning/</id><summary type="html">&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/lotr_rules.png"  height="1000" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;So in my day to day I am working on a image classification task. Now I wn't go much into explaining transfer learning here. Those of you that want to understand it can read it at my blog &lt;a href="http://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyways this post is intended for folks who understand how transfer learning works and are trying to squeeze out extra bits from the same. Till now we normally do something like this:&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/before_ensemble.png"  height="800" width="600" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;For example: Lets say you are working for a kaggle competition and you create multiple models for the image classification task using precomputed features from all these different pretrained models. You might then stack these different models or create a mean ensemble out of these models. And really that is a pretty good idea. I myself went with this idea for a lot of stuff not long back.&lt;/p&gt;
&lt;p&gt;Than an epiphany stuck me. Why not let the neural network itself learn the weights while training the additional layers itself. That would be pretty cool. So i tried the same. So, why don't we do this:&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/now_lotr.png"  height="800" width="600" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In my experiments I have noticed that it works great.&lt;/p&gt;
&lt;h2&gt;Why it works?&lt;/h2&gt;
&lt;p&gt;Why it works? From an understanding point of view, I believe that when we give input from different architectures , the network is able to extract value using interactions between different architectures on its own.&lt;/p&gt;
&lt;h2&gt;Code:&lt;/h2&gt;
&lt;p&gt;Just the snippets of codes that are necessary:&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;# create input Layers for precomputed feats from different pretrained models
Xception_i = Input(shape=train_precomputed_Xception.shape[1:],  name='Xception')
ResNet50_i = Input(shape=train_precomputed_ResNet50.shape[1:],  name='ResNet50')
InceptionV3_i = Input(shape=train_precomputed_InceptionV3.shape[1:],name='InceptionV3')
InceptionResNetV2_i = Input(shape=train_precomputed_InceptionResNetV2.shape[1:],name='InceptionResNetV2')
DenseNet201_i = Input(shape=train_precomputed_DenseNet201.shape[1:],  name='DenseNet201')
DenseNet169_i = Input(shape=train_precomputed_DenseNet169.shape[1:],  name='DenseNet169')
VGG19_i = Input(shape=train_precomputed_VGG19.shape[1:],  name='VGG19')

# Add additional layers on top of each input layer
last_layers = [Xception_i,ResNet50_i,InceptionV3_i,InceptionResNetV2_i,DenseNet201_i,DenseNet169_i,VGG19_i]
last_layers_name = ['Xception_3d','ResNet50_3d','InceptionV3_3d','InceptionResNetV2_3d','DenseNet201_3d','DenseNet169_3d','VGG19_3d']
new_last_layers = {}
for i, last_layer in enumerate(last_layers):
    x = GlobalAveragePooling2D()(last_layer)
    x = Dense(1024, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    new_last_layers[last_layers_name[i]] = x

# Concatenate the output of the additional layers
x = concatenate([new_last_layers['Xception_3d'],new_last_layers['ResNet50_3d'],new_last_layers['InceptionV3_3d'],new_last_layers['InceptionResNetV2_3d'],new_last_layers['DenseNet201_3d'],new_last_layers['DenseNet169_3d'],new_last_layers['VGG19_3d']])

# Add a softmax layer
main_output = Dense(nb_classes, activation='softmax', name='main_output')(x)

# create the model using Keras Functional API
model = Model(inputs=[Xception_i,ResNet50_i,InceptionV3_i,InceptionResNetV2_i,DenseNet201_i,DenseNet169_i,VGG19_i], outputs=[main_output])

# compile
model.compile(optimizer="adam",loss="categorical_crossentropy",metrics =["accuracy"])
&lt;/code&gt;&lt;/pre&gt;</summary><category term="deeplearning"></category><category term="pretrained models"></category></entry><entry><title>Using XGBoost for time series prediction tasks</title><link href="http://mlwhiz.github.io/blog/2017/12/26/How_to_win_a_data_science_competition/" rel="alternate"></link><updated>2017-12-26T04:43:00-02:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-12-26:blog/2017/12/26/How_to_win_a_data_science_competition/</id><summary type="html">&lt;p&gt;Recently Kaggle master Kazanova along with some of his friends released a &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;"How to win a data science competition"&lt;/a&gt; Coursera course. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/lboard.png"  height="800" width="600" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;Description of the Problem:&lt;/h2&gt;
&lt;p&gt;In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.&lt;/p&gt;
&lt;p&gt;We were asked you to predict total sales for every product and store in the next month.&lt;/p&gt;
&lt;p&gt;The evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.&lt;/p&gt;
&lt;p&gt;The main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.&lt;/p&gt;
&lt;h2&gt;1. Created a dataframe of all Date_block_num, Store and  Item combinations:&lt;/h2&gt;
&lt;p&gt;This is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from itertools import product
# Create "grid" with columns
index_cols = ['shop_id', 'item_id', 'date_block_num']

# For every month we create a grid from all shops/items combinations from that month
grid = []
for block_num in sales['date_block_num'].unique():
    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()
    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()
    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))
grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;2. Cleaned up a little of sales data after some basic EDA:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales = sales[sales.item_price&lt;100000]
sales = sales[sales.item_cnt_day&lt;=1000]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;3. Created Mean Encodings:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_m = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()
sales_m = pd.merge(grid,sales_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)
# adding the category id too
sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')

for type_id in ['item_id','shop_id','item_category_id']:
    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:

        mean_df = sales.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]
        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']

        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num',type_id],how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
These above lines add the following 9 features :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;4. Create Lag Features:&lt;/h2&gt;
&lt;p&gt;Next we create lag features with diferent lag periods on the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;lag_variables  = list(sales_m.columns[7:])+['item_cnt_day']
lags = [1 ,2 ,3 ,4, 5, 12]
for lag in lags:
    sales_new_df = sales_m.copy()
    sales_new_df.date_block_num+=lag
    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]
    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]
    sales_means = pd.merge(sales_means, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;5. Fill NA with zeros:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;for feat in sales_means.columns:
    if 'item_cnt' in feat:
        sales_means[feat]=sales_means[feat].fillna(0)
    elif 'item_price' in feat:
        sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;6. Drop the columns that we are not going to use in training:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;cols_to_drop = lag_variables[:-1] + ['item_name','item_price']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;7. Take a recent bit of data only:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_means = sales_means[sales_means['date_block_num']&gt;12]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;8. Split in train and CV :&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;X_train = sales_means[sales_means['date_block_num']&lt;33].drop(cols_to_drop, axis=1)
X_cv =  sales_means[sales_means['date_block_num']==33].drop(cols_to_drop, axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;9. THE MAGIC SAUCE:&lt;/h2&gt;
&lt;p&gt;In the start I told that the clipping aspect of [0,20] will be important.
In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def clip(x):
    if x&gt;40:
        return 40
    elif x&lt;0:
        return 0
    else:
        return x
train['item_cnt_day'] = train.apply(lambda x: clip(x['item_cnt_day']),axis=1)
cv['item_cnt_day'] = cv.apply(lambda x: clip(x['item_cnt_day']),axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;10: Modelling:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Created a XGBoost model to get the most important features(Top 42 features)&lt;/li&gt;
&lt;li&gt;Use hyperopt to tune xgboost&lt;/li&gt;
&lt;li&gt;Used top 10 models from tuned XGBoosts to generate predictions.&lt;/li&gt;
&lt;li&gt;clipped the predictions to [0,20] range&lt;/li&gt;
&lt;li&gt;Final solution was the average of these 10 predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learned a lot of new things from this &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;awesome course&lt;/a&gt;. Most recommended.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry></feed>