<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.com/" rel="alternate"></link><link href="http://mlwhiz.github.io/feeds/python-kaggle-coursera.atom.xml" rel="self"></link><id>http://mlwhiz.com/</id><updated>2017-12-26T04:43:00-02:00</updated><entry><title>Using XGBoost for time series prediction tasks</title><link href="http://mlwhiz.com/blog/2017/12/26/How_to_win_a_data_science_competition/" rel="alternate"></link><updated>2017-12-26T04:43:00-02:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.com,2017-12-26:blog/2017/12/26/How_to_win_a_data_science_competition/</id><summary type="html">&lt;p&gt;Recently Kaggle master Kazanova along with some of his friends released a &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;"How to win a data science competition"&lt;/a&gt; Coursera course. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/lboard.png"  height="800" width="600" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2&gt;Description of the Problem:&lt;/h2&gt;
&lt;p&gt;In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.&lt;/p&gt;
&lt;p&gt;We were asked you to predict total sales for every product and store in the next month.&lt;/p&gt;
&lt;p&gt;The evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.&lt;/p&gt;
&lt;p&gt;The main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.&lt;/p&gt;
&lt;h2&gt;1. Created a dataframe of all Date_block_num, Store and  Item combinations:&lt;/h2&gt;
&lt;p&gt;This is important because in the months we don't have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from itertools import product
# Create "grid" with columns
index_cols = ['shop_id', 'item_id', 'date_block_num']

# For every month we create a grid from all shops/items combinations from that month
grid = []
for block_num in sales['date_block_num'].unique():
    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()
    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()
    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))
grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;2. Cleaned up a little of sales data after some basic EDA:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales = sales[sales.item_price&lt;100000]
sales = sales[sales.item_cnt_day&lt;=1000]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;3. Created Mean Encodings:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_m = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()
sales_m = pd.merge(grid,sales_m,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)
# adding the category id too
sales_m = pd.merge(sales_m,items,on=['item_id'],how='left')

for type_id in ['item_id','shop_id','item_category_id']:
    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:

        mean_df = sales.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]
        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']

        sales_m = pd.merge(sales_m,mean_df,on=['date_block_num',type_id],how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
These above lines add the following 9 features :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;4. Create Lag Features:&lt;/h2&gt;
&lt;p&gt;Next we create lag features with diferent lag periods on the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;'item_id_avg_item_price',&lt;/li&gt;
&lt;li&gt;'item_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'shop_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'shop_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_price'&lt;/li&gt;
&lt;li&gt;'item_category_id_sum_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_category_id_avg_item_cnt_day'&lt;/li&gt;
&lt;li&gt;'item_cnt_day'&lt;/li&gt;
&lt;/ul&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;lag_variables  = list(sales_m.columns[7:])+['item_cnt_day']
lags = [1 ,2 ,3 ,4, 5, 12]
for lag in lags:
    sales_new_df = sales_m.copy()
    sales_new_df.date_block_num+=lag
    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]
    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]
    sales_means = pd.merge(sales_means, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;5. Fill NA with zeros:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;for feat in sales_means.columns:
    if 'item_cnt' in feat:
        sales_means[feat]=sales_means[feat].fillna(0)
    elif 'item_price' in feat:
        sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;6. Drop the columns that we are not going to use in training:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;cols_to_drop = lag_variables[:-1] + ['item_name','item_price']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;7. Take a recent bit of data only:&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;sales_means = sales_means[sales_means['date_block_num']&gt;12]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;8. Split in train and CV :&lt;/h2&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;X_train = sales_means[sales_means['date_block_num']&lt;33].drop(cols_to_drop, axis=1)
X_cv =  sales_means[sales_means['date_block_num']==33].drop(cols_to_drop, axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;9. THE MAGIC SAUCE:&lt;/h2&gt;
&lt;p&gt;In the start I told that the clipping aspect of [0,20] will be important.
In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.&lt;/p&gt;
&lt;pre style="font-size:80%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;def clip(x):
    if x&gt;40:
        return 40
    elif x&lt;0:
        return 0
    else:
        return x
train['item_cnt_day'] = train.apply(lambda x: clip(x['item_cnt_day']),axis=1)
cv['item_cnt_day'] = cv.apply(lambda x: clip(x['item_cnt_day']),axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;10: Modelling:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Created a XGBoost model to get the most important features(Top 42 features)&lt;/li&gt;
&lt;li&gt;Use hyperopt to tune xgboost&lt;/li&gt;
&lt;li&gt;Used top 10 models from tuned XGBoosts to generate predictions.&lt;/li&gt;
&lt;li&gt;clipped the predictions to [0,20] range&lt;/li&gt;
&lt;li&gt;Final solution was the average of these 10 predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learned a lot of new things from this &lt;a href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0"&gt;awesome course&lt;/a&gt;. Most recommended.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry></feed>