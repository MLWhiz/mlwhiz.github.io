<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="https://mlwhiz.com/" rel="alternate"></link><link href="http://mlwhiz.com/feeds/python-nlp-algorithms-kaggle-tilt.atom.xml" rel="self"></link><id>https://mlwhiz.com/</id><updated>2017-04-09T04:43:00-03:00</updated><entry><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link href="https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/" rel="alternate"></link><updated>2017-04-09T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:https://mlwhiz.com,2017-04-09:blog/2017/04/09/word_vec_embeddings_examples_understanding/</id><summary type="html">&lt;p&gt;Recently Quora put out a &lt;a href="https://www.kaggle.com/c/quora-question-pairs"&gt;Question similarity&lt;/a&gt; competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p&gt;
&lt;p&gt;Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.&lt;/p&gt;
&lt;p&gt;For those who don't know &lt;em&gt;bag of words&lt;/em&gt;:
If we have a series of sentences(documents)&lt;/p&gt;
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. This is good       - [1,1,1,0,0]&lt;br&gt;
2. This is bad        - [1,1,0,1,0]&lt;br&gt;
3. This is awesome    - [1,1,0,0,1]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;Bag of words would encode it using &lt;em&gt;0:This 1:is 2:good 3:bad 4:awesome&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But it is much more powerful than that.&lt;/p&gt;
&lt;p&gt;What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.&lt;/p&gt;
&lt;h2&gt;How does that help?&lt;/h2&gt;
&lt;p&gt;We can use this for multiple scenarios but the most common are:&lt;/p&gt;
&lt;p&gt;A. &lt;em&gt;Using word2vec embeddings we can find out similarity between words&lt;/em&gt;.
Assume you have to answer if these two statements signify the same thing:
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. President greets press in Chicago&lt;br&gt;
2. Obama speaks to media in Illinois.
&lt;/div&gt;
If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/word2vecembed.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;But with a word encoding we can say that&lt;/p&gt;
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. President is similar to Obama&lt;br&gt;
2. greets is similar to speaks&lt;br&gt;
3. press is similar to media&lt;br&gt;
4. Chicago is similar to Illinois&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;B. &lt;em&gt;Encode Sentences&lt;/em&gt;: I read a &lt;a href="https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur"&gt;post&lt;/a&gt; from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.&lt;/p&gt;
&lt;p&gt;His Approach: Lets say the sentence is "What is this"
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)
&lt;div style="margin-left: 10px;margin-bottom:9px;color: green"&gt;
1. what : [.25 ,.25 ,.25 ,.25]&lt;br&gt;
2. is   : [  1 ,  0 ,  0 ,  0]&lt;br&gt;
3. this : [ .5 ,  0 ,  0 , .5]&lt;br&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.&lt;/p&gt;
&lt;div style="margin-bottom:9px;margin-left: 10px"&gt;
Elementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
&lt;br&gt;
divided by
&lt;br&gt;
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
&lt;br&gt;
gives:[1.16, .17, .17, 0.5]
&lt;/div&gt;

&lt;p&gt;Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.&lt;/p&gt;
&lt;p&gt;C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?&lt;br&gt;
Guess what it comes out to be : &lt;em&gt;Queen&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Application/Coding:&lt;/h2&gt;
&lt;p&gt;Now lets get down to the coding part as we know a little bit of fundamentals.&lt;/p&gt;
&lt;p&gt;First of all we download a custom word embedding from Google. There are many other embeddings too.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above file is pretty big. Might take some time. Then moving on to coding.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from gensim.models import word2vec
model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;1. Starting simple, lets find out similar words. Want to find similar words to python?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar('python')
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'pythons', 0.6688377261161804),&lt;br&gt;
 (u'Burmese_python', 0.6680364608764648),&lt;br&gt;
 (u'snake', 0.6606293320655823),&lt;br&gt;
 (u'crocodile', 0.6591362953186035),&lt;br&gt;
 (u'boa_constrictor', 0.6443519592285156),&lt;br&gt;
 (u'alligator', 0.6421656608581543),&lt;br&gt;
 (u'reptile', 0.6387745141983032),&lt;br&gt;
 (u'albino_python', 0.6158879995346069),&lt;br&gt;
 (u'croc', 0.6083582639694214),&lt;br&gt;
 (u'lizard', 0.601341724395752)]&lt;br&gt;
 &lt;/div&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;2. Now we can use this model to find the solution to the equation:&lt;/h3&gt;
&lt;p&gt;What is king - man + woman?&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['king','woman'],negative = ['man'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'queen', 0.7118192315101624),&lt;br&gt;
 (u'monarch', 0.6189674139022827),&lt;br&gt;
 (u'princess', 0.5902431011199951),&lt;br&gt;
 (u'crown_prince', 0.5499460697174072),&lt;br&gt;
 (u'prince', 0.5377321839332581),&lt;br&gt;
 (u'kings', 0.5236844420433044),&lt;br&gt;
 (u'Queen_Consort', 0.5235946178436279),&lt;br&gt;
 (u'queens', 0.5181134343147278),&lt;br&gt;
 (u'sultan', 0.5098593235015869),&lt;br&gt;
 (u'monarchy', 0.5087412595748901)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;You can do plenty of freaky/cool things using this:&lt;/p&gt;
&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;

&lt;h3&gt;3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['emma','he','male','mr'],negative = ['she','mrs','female'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'sanchez', 0.4920658469200134),&lt;br&gt;
 (u'kenny', 0.48300960659980774),&lt;br&gt;
 (u'alves', 0.4684845209121704),&lt;br&gt;
 (u'gareth', 0.4530612826347351),&lt;br&gt;
 (u'bellamy', 0.44884198904037476),&lt;br&gt;
 (u'gibbs', 0.445194810628891),&lt;br&gt;
 (u'dos_santos', 0.44508373737335205),&lt;br&gt;
 (u'gasol', 0.44387346506118774),&lt;br&gt;
 (u'silva', 0.4424275755882263),&lt;br&gt;
 (u'shaun', 0.44144102931022644)]&lt;br&gt;&lt;br&gt;
&lt;/div&gt;

&lt;h3&gt;4. Find which word doesn't belong to a &lt;a href="https://github.com/dhammack/Word2VecExample/blob/master/main.py"&gt;list&lt;/a&gt;?&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.doesnt_match("math shopping reading science".split(" "))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think staple doesnt belong in this list!&lt;/p&gt;
&lt;h2&gt;Other Cool Things&lt;/h2&gt;
&lt;h3&gt;1. Recommendations:&lt;/h3&gt;
&lt;div style="margin-top: 4px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/recommendationpaper.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href="https://arxiv.org/abs/1603.04259"&gt;paper&lt;/a&gt;, the authors have shown that itembased CF can be cast in the same framework of word embedding.&lt;/p&gt;
&lt;h3&gt;2. Some other &lt;a href="http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html"&gt;examples&lt;/a&gt; that people have seen after using their own embeddings:&lt;/h3&gt;
&lt;p&gt;Library - Books = Hall&lt;br&gt;
Obama + Russia - USA = Putin&lt;br&gt;
Iraq - Violence = Jordan&lt;br&gt;
President - Power = Prime Minister (Not in India Though)&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;3.Seeing the above I started playing with it a little.&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Is this model sexist?&lt;/strong&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ["donald_trump"],negative = ['brain'])
&lt;/code&gt;&lt;/pre&gt;

&lt;div style="font-size:60%;color:blue;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px"&gt;
[(u'novak', 0.40405112504959106),&lt;br&gt;
 (u'ozzie', 0.39440611004829407),&lt;br&gt;
 (u'democrate', 0.39187556505203247),&lt;br&gt;
 (u'clinton', 0.390536367893219),&lt;br&gt;
 (u'hillary_clinton', 0.3862358033657074),&lt;br&gt;
 (u'bnp', 0.38295692205429077),&lt;br&gt;
 (u'klaar', 0.38228923082351685),&lt;br&gt;
 (u'geithner', 0.380607008934021),&lt;br&gt;
 (u'bafana_bafana', 0.3801495432853699),&lt;br&gt;
 (u'whitman', 0.3790769875049591)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry></feed>