<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlwhiz</title><link href="http://mlwhiz.github.io/" rel="alternate"></link><link href="http://mlwhiz.github.io/feeds/python-nlp-algorithms-kaggle-tilt.atom.xml" rel="self"></link><id>http://mlwhiz.github.io/</id><updated>2017-04-09T04:43:00-03:00</updated><entry><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link href="http://mlwhiz.github.io/blog/2017/04/09/word_vec_embeddings_examples_understanding/" rel="alternate"></link><updated>2017-04-09T04:43:00-03:00</updated><author><name>Rahul Agarwal</name></author><id>tag:mlwhiz.github.io,2017-04-09:blog/2017/04/09/word_vec_embeddings_examples_understanding/</id><summary type="html">&lt;p&gt;Recently Quora put out a &lt;a href="https://www.kaggle.com/c/quora-question-pairs"&gt;Question similarity&lt;/a&gt; competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p&gt;
&lt;p&gt;Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.&lt;/p&gt;
&lt;p&gt;For those who don't know bag of words:
If we have a series of sentences(documents)
1. This is good       - [1,1,1,0,0]
2. This is bad        - [1,1,0,1,0]
3. This is awesome    - [1,1,0,0,1]&lt;/p&gt;
&lt;p&gt;Bag of words would encode it using 0:This 1:is 2:good 3:bad 4:awesome&lt;/p&gt;
&lt;p&gt;But it is much more powerful than that.&lt;/p&gt;
&lt;p&gt;What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.&lt;/p&gt;
&lt;h2&gt;How does that help?&lt;/h2&gt;
&lt;p&gt;We can use this for multiple scenarios but the most common are:&lt;/p&gt;
&lt;p&gt;A. Using word2vec embeddings we can find out similarity between words.
Assume you have to answer if these two statements signify the same thing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;President greets press in Chicago&lt;/li&gt;
&lt;li&gt;Obama speaks to media in Illinois.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.&lt;/p&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/word2vecembed.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;But with a word encoding we can say that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;President is similar to Obama&lt;/li&gt;
&lt;li&gt;greets is similar to speaks&lt;/li&gt;
&lt;li&gt;press is similar to media&lt;/li&gt;
&lt;li&gt;Chicago is similar to Illinois&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;B. I read a &lt;a href="https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur"&gt;post&lt;/a&gt; from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.&lt;/p&gt;
&lt;p&gt;His Approach: Lets say the sentence is "What is this"
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;what : [.25 ,.25 ,.25 ,.25]&lt;/li&gt;
&lt;li&gt;is   : [  1 ,  0 ,  0 ,  0]&lt;/li&gt;
&lt;li&gt;this : [ .5 ,  0 ,  0 , .5]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.&lt;/p&gt;
&lt;p&gt;Elementwise addition :
[.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
divided by
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
gives:
[1.16, .17, .17, 0.5]&lt;/p&gt;
&lt;p&gt;Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.&lt;/p&gt;
&lt;p&gt;C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?
Guess what it comes out to be : Queen&lt;/p&gt;
&lt;h2&gt;Application/Coding:&lt;/h2&gt;
&lt;p&gt;Now lets get down to the coding part as we know a little bit of fundamentals.&lt;/p&gt;
&lt;p&gt;First of all we download a custom word embedding from Google. There are many other embeddings too.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;
wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The above file is pretty big. Might take some time. Then moving on to coding.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;from gensim.models import word2vec
model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Starting simple, lets find out similar words. Want to find similar words to python?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar('python')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[(u'pythons', 0.6688377261161804),
 (u'Burmese_python', 0.6680364608764648),
 (u'snake', 0.6606293320655823),
 (u'crocodile', 0.6591362953186035),
 (u'boa_constrictor', 0.6443519592285156),
 (u'alligator', 0.6421656608581543),
 (u'reptile', 0.6387745141983032),
 (u'albino_python', 0.6158879995346069),
 (u'croc', 0.6083582639694214),
 (u'lizard', 0.601341724395752)]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Now we can use this model to find the solution to the equation:
What is king - man + woman?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['king','woman'],negative = ['man'])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[(u'queen', 0.7118192315101624),
 (u'monarch', 0.6189674139022827),
 (u'princess', 0.5902431011199951),
 (u'crown_prince', 0.5499460697174072),
 (u'prince', 0.5377321839332581),
 (u'kings', 0.5236844420433044),
 (u'Queen_Consort', 0.5235946178436279),
 (u'queens', 0.5181134343147278),
 (u'sultan', 0.5098593235015869),
 (u'monarchy', 0.5087412595748901)]&lt;/p&gt;
&lt;p&gt;You can do plenty of freaky/cool things using this:&lt;/p&gt;
&lt;script src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e"&gt;&lt;/script&gt;

&lt;ol&gt;
&lt;li&gt;Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ['emma','he','male','mr'],negative = ['she','mrs','female'])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[(u'sanchez', 0.4920658469200134),
 (u'kenny', 0.48300960659980774),
 (u'alves', 0.4684845209121704),
 (u'gareth', 0.4530612826347351),
 (u'bellamy', 0.44884198904037476),
 (u'gibbs', 0.445194810628891),
 (u'dos_santos', 0.44508373737335205),
 (u'gasol', 0.44387346506118774),
 (u'silva', 0.4424275755882263),
 (u'shaun', 0.44144102931022644)]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find which word doesn't belong to a &lt;a href="https://github.com/dhammack/Word2VecExample/blob/master/main.py"&gt;list&lt;/a&gt;?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.doesnt_match("math shopping reading science".split(" "))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I think staple doesnt belong in this list!&lt;/p&gt;
&lt;h2&gt;Other Cool Things&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Recommendations:&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="margin-top: 9px; margin-bottom: 10px;"&gt;
&lt;center&gt;&lt;img src="/images/recommendationpaper.png"  height="400" width="700" &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href="https://arxiv.org/abs/1603.04259"&gt;paper&lt;/a&gt;, the authors have shown that itembased CF can be cast in the same framework of word embedding.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Some other &lt;a href="http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html"&gt;examples&lt;/a&gt; that people have seen after using their own embeddings:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Library - Books = Hall
Obama + Russia - USA = Putin
Iraq - Violence = Jordan
President - Power = Prime Minister (Not in India Though)&lt;/p&gt;
&lt;p&gt;3.Seeing the above I started playing with it a little.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is this model sexist?&lt;/strong&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="python"&gt;model.most_similar(positive = ["donald_trump"],negative = ['brain'])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[(u'novak', 0.40405112504959106),
 (u'ozzie', 0.39440611004829407),
 (u'democrate', 0.39187556505203247),
 (u'clinton', 0.390536367893219),
 (u'hillary_clinton', 0.3862358033657074),
 (u'bnp', 0.38295692205429077),
 (u'klaar', 0.38228923082351685),
 (u'geithner', 0.380607008934021),
 (u'bafana_bafana', 0.3801495432853699),
 (u'whitman', 0.3790769875049591)]&lt;/p&gt;
&lt;p&gt;Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.&lt;/p&gt;</summary><category term="Python"></category><category term="NLP"></category><category term="Algorithms"></category><category term="Kaggle"></category></entry></feed>