<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Nlp on MLWhiz</title>
    <link>https://mlwhiz.com/tags/nlp/</link>
    <description>Recent content in Nlp on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 09 Apr 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/nlp/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      
      

      
      <description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;amp;rsquo;t know bag of words: If we have a series of sentences(documents)</description>

      <content:encoded>  
        
        <![CDATA[  Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;rsquo;t know bag of words: If we have a series of sentences(documents)
 This is good - [1,1,1,0,0] This is bad - [1,1,0,1,0] This is awesome - [1,1,0,0,1]  Bag of words would encode it using 0:This 1:is 2:good 3:bad 4:awesome
But it is much more powerful than that.
What word2vec does is that it creates vectors for words. What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.
How does that help? We can use this for multiple scenarios but the most common are:
A. Using word2vec embeddings we can find out similarity between words. Assume you have to answer if these two statements signify the same thing:
 President greets press in Chicago Obama speaks to media in Illinois.  If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.
  But with a word encoding we can say that
 President is similar to Obama greets is similar to speaks press is similar to media Chicago is similar to Illinois  B. Encode Sentences: I read a post from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.
His Approach: Lets say the sentence is &amp;ldquo;What is this&amp;rdquo; And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)
 what : [.25 ,.25 ,.25 ,.25] is : [ 1 , 0 , 0 , 0] this : [ .5 , 0 , 0 , .5]  Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.
Elementwise addition : [.25&#43;1&#43;0.5, 0.25&#43;0&#43;0 , 0.25&#43;0&#43;0, .25&#43;0&#43;.5] = [1.75, .25, .25, .75] divided by math.sqrt(1.25^2 &#43; .25^2 &#43; .25^2 &#43; .75^2) = 1.5 gives:[1.16, .17, .17, 0.5]  Thus I can convert any sentence to a vector of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.
C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man &#43; woman ?
Guess what it comes out to be : Queen
Application/Coding: Now lets get down to the coding part as we know a little bit of fundamentals.
First of all we download a custom word embedding from Google. There are many other embeddings too.
wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz The above file is pretty big. Might take some time. Then moving on to coding.
from gensim.models import word2vec model = gensim.models.KeyedVectors.load_word2vec_format(&amp;#39;data/GoogleNews-vectors-negative300.bin.gz&amp;#39;, binary=True) 1. Starting simple, lets find out similar words. Want to find similar words to python? model.most_similar(&amp;#39;python&amp;#39;) [(u&#39;pythons&#39;, 0.6688377261161804),
(u&#39;Burmese_python&#39;, 0.6680364608764648),
(u&#39;snake&#39;, 0.6606293320655823),
(u&#39;crocodile&#39;, 0.6591362953186035),
(u&#39;boa_constrictor&#39;, 0.6443519592285156),
(u&#39;alligator&#39;, 0.6421656608581543),
(u&#39;reptile&#39;, 0.6387745141983032),
(u&#39;albino_python&#39;, 0.6158879995346069),
(u&#39;croc&#39;, 0.6083582639694214),
(u&#39;lizard&#39;, 0.601341724395752)]
 2. Now we can use this model to find the solution to the equation: What is king - man &#43; woman?
model.most_similar(positive = [&amp;#39;king&amp;#39;,&amp;#39;woman&amp;#39;],negative = [&amp;#39;man&amp;#39;]) [(u&#39;queen&#39;, 0.7118192315101624),
(u&#39;monarch&#39;, 0.6189674139022827),
(u&#39;princess&#39;, 0.5902431011199951),
(u&#39;crown_prince&#39;, 0.5499460697174072),
(u&#39;prince&#39;, 0.5377321839332581),
(u&#39;kings&#39;, 0.5236844420433044),
(u&#39;Queen_Consort&#39;, 0.5235946178436279),
(u&#39;queens&#39;, 0.5181134343147278),
(u&#39;sultan&#39;, 0.5098593235015869),
(u&#39;monarchy&#39;, 0.5087412595748901)]
 You can do plenty of freaky/cool things using this:
3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma? model.most_similar(positive = [&amp;#39;emma&amp;#39;,&amp;#39;he&amp;#39;,&amp;#39;male&amp;#39;,&amp;#39;mr&amp;#39;],negative = [&amp;#39;she&amp;#39;,&amp;#39;mrs&amp;#39;,&amp;#39;female&amp;#39;]) [(u&#39;sanchez&#39;, 0.4920658469200134),
(u&#39;kenny&#39;, 0.48300960659980774),
(u&#39;alves&#39;, 0.4684845209121704),
(u&#39;gareth&#39;, 0.4530612826347351),
(u&#39;bellamy&#39;, 0.44884198904037476),
(u&#39;gibbs&#39;, 0.445194810628891),
(u&#39;dos_santos&#39;, 0.44508373737335205),
(u&#39;gasol&#39;, 0.44387346506118774),
(u&#39;silva&#39;, 0.4424275755882263),
(u&#39;shaun&#39;, 0.44144102931022644)]
 4. Find which word doesn&amp;rsquo;t belong to a list? model.doesnt_match(&amp;#34;math shopping reading science&amp;#34;.split(&amp;#34; &amp;#34;)) I think staple doesn&amp;rsquo;t belong in this list!
Other Cool Things 1. Recommendations:   In this paper, the authors have shown that itembased CF can be cast in the same framework of word embedding.
2. Some other examples that people have seen after using their own embeddings: Library - Books = Hall
Obama &#43; Russia - USA = Putin
Iraq - Violence = Jordan
President - Power = Prime Minister (Not in India Though)
3.Seeing the above I started playing with it a little. Is this model sexist?
model.most_similar(positive = [&amp;#34;donald_trump&amp;#34;],negative = [&amp;#39;brain&amp;#39;]) [(u&#39;novak&#39;, 0.40405112504959106),
(u&#39;ozzie&#39;, 0.39440611004829407),
(u&#39;democrate&#39;, 0.39187556505203247),
(u&#39;clinton&#39;, 0.390536367893219),
(u&#39;hillary_clinton&#39;, 0.3862358033657074),
(u&#39;bnp&#39;, 0.38295692205429077),
(u&#39;klaar&#39;, 0.38228923082351685),
(u&#39;geithner&#39;, 0.380607008934021),
(u&#39;bafana_bafana&#39;, 0.3801495432853699),
(u&#39;whitman&#39;, 0.3790769875049591)]
 Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>