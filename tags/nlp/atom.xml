<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on MLWhiz</title>
    <link>/tags/nlp/</link>
    <description>Recent content in Nlp on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/nlp/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>A Layman guide to moving from Keras to Pytorch</title>
      <link>/blog/2019/01/06/pytorch_keras_conversion/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/01/06/pytorch_keras_conversion/</guid>
      <description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds.</description>
    </item>
    
    <item>
      <title>What Kagglers are using for Text Classification</title>
      <link>/blog/2018/12/17/text_classification/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/12/17/text_classification/</guid>
      <description>With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. How could you use that?
 To find sentiment of a review. Find toxic comments in a platform like Facebook Find Insincere questions on Quora.</description>
    </item>
    
    <item>
      <title>Using XGBoost for time series prediction tasks</title>
      <link>/blog/2017/12/26/how_to_win_a_data_science_competition/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017/12/26/how_to_win_a_data_science_competition/</guid>
      <description>Recently Kaggle master Kazanova along with some of his friends released a &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.
  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.</description>
    </item>
    
    <item>
      <title>Good Feature Building Techniques - Tricks for Kaggle -  My Kaggle Code Repository</title>
      <link>/blog/2017/09/14/kaggle_tricks/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017/09/14/kaggle_tricks/</guid>
      <description>Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing).</description>
    </item>
    
    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      <description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;rsquo;t know bag of words: If we have a series of sentences(documents)</description>
    </item>
    
  </channel>
</rss>