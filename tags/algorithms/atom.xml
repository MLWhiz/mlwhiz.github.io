<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Algorithms on MLWhiz</title>
    <link>https://mlwhiz.com/tags/algorithms/</link>
    <description>Recent content in Algorithms on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/algorithms/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Handling Trees in Data Science Algorithmic Interview</title>
      <link>https://mlwhiz.com/blog/2020/01/29/altr/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/29/altr/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/altr/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are crucial all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are crucial all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.
This post is about fast-tracking this study and explaining tree concepts for the data scientists so that you breeze through the next time you get asked these in an interview.
But First, Why are Trees important for Data Science? To data scientists, Trees mean a different thing than they mean for a Software Engineer.
For a software engineer, a tree is just a simple Data Structure they can use to manage hierarchical relationships while for a Data Scientists trees form the basis of some of the most useful classification and regression algorithms.
So where do these two meet?
They are necessarily the same thing. Don’t be surprised. Below is how data scientists and software engineer’s look at trees.
The only difference is that Data science tree nodes keep much more information that helps us in identifying how to traverse the tree. For example, in the case of Data science tree for prediction, we will look at the feature in the node and determine which way we want to move based on the split value.
If you want to write your decision tree from scratch, you might need to understand how trees work from a software engineering perspective too.
Types of Trees: In this post, I will only be talking about two kinds of trees that get asked a lot in Data Science interview questions. Binary Trees(BT) and an extension of Binary Trees called Binary Search Trees(BST).
1. Binary Trees: A binary tree is simply a tree in which each node has up to two children. A decision tree is an example we see in our day to day lives.
2. Binary Search Tree(BST): A binary search tree is a binary tree in which:
 All left descendants of a node are less than or equal to the node, and
 All right descendants of the node are greater than the node.
  There are variations to this definition when it comes to equalities. Sometimes the equalities are on the right-hand side or either side. Sometimes only distinct values are allowed in the tree.
Source
8 is greater than all the elements in the left subtree and smaller than all elements in the right subtree. The same could be said for any node in the tree.
Creating a Simple Tree: So How do we construct a simple tree?
By definition, a tree is made up of nodes. So we start by defining the node class which we will use to create nodes. Our node class is pretty simple as it holds value for a node, the location of the left child and the location of the right child.
class node: def __init__(self,val): self.val = val self.left = None self.right = None We can create a simple tree now as:
root = node(1) root.left = node(2) root.right = node(3) Now I have noticed that we cannot really get the hang of Tree-based questions without doing some coding ourselves.
So let us get a little deeper into the coding part with some problems I found most interesting when it comes to trees.
Inorder Tree Traversal: There are a variety of ways to traverse a tree, but I find the inorder traversal to be most intuitive.
When we do an inorder traversal on the root node on a Binary Search tree, it visits/prints the node in ascending order.
def inorder(node): if node: inorder(node.left) print(node.val) inorder(node.right) This above method is pretty important as it allows us to visit all the nodes.
So if we want to search for a node in any binary tree, we might try to use inorder tree traversal.
Creating a Binary Search Tree from a Sorted array What kind of coders will we be if we need to create a tree piece by piece manually as we did above?
So can we create a BST from a sorted array of unique elements?
def create_bst(array,min_index,max_index): if max_index&amp;lt;min_index: return None mid = int((min_index&#43;max_index)/2) root = node(array[mid]) leftbst = create_bst(array,min_index,mid-1) rightbst = create_bst(array,mid&#43;1,max_index) root.left = leftbst root.right = rightbst return root a = [2,4,5,6,7] root = create_bst(a,0,len(a)-1) Trees are inherently recursive, and so we use recursion here. We take the mid element of the array and assign it as the node. We then apply the create_bst function to the left part of the array and assign it to node.left and do the same with the right part of the array.
And we get our BST.
Have we done it right? We can check it by creating the BST and then doing an inorder traversal.
inorder(root) 2 4 5 6 7  Seems Right!
Let’s check if our tree is a Valid BST But again what sort of coders are we if we need to print all the elements and check manually for the BST property being satisfied?
Here is a simple code to check if our BST is valid or not. We assume strict inequality in our Binary Search Tree.
def isValidBST(node, minval, maxval): if node: # Base case if node.val&amp;lt;=minval or node.val&amp;gt;=maxval: return False # Check the subtrees changing the min and max values return isValidBST(node.left,minval,node.val) &amp;amp; isValidBST(node.right,node.val,maxval) return True isValidBST(root,-float(&amp;#39;inf&amp;#39;),float(&amp;#39;inf&amp;#39;)) True  We check the subtrees recursively if they satisfy the Binary Search tree property or not. At each recursive call, we change the minval or maxval for the call to provide the function with the range of allowed values for the subtree.
Conclusion In this post, I talked about Trees from a software engineering perspective. If you want to see trees from a data science perspective, you might take a look at this post. The Simple Math behind 3 Decision Tree Splitting criterions
Trees form the basis of some of the most asked questions in Data Science algorithmic interviews. I used to despair such tree-based questions in the past, but now I have grown to like the mental exercise involved in them. And I love the recursive structure involved in such problems.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Here is a small notebook for you where I have put all these small concepts for you to try and run.
Take a look at my other posts in the Algorithmic Interviews Series, if you want to learn about Recursion, Dynamic Programming or Linked Lists.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A simple introduction to Linked Lists for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/01/28/ll/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/28/ll/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/ll/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.
This post is about fast-tracking this study and explaining linked list concepts for the data scientists in an easy to understand way.
What are Linked Lists? The linked list is just a very simple data structure that represents a sequence of nodes.
Each node is just an object that contains a value and a pointer to the next node. For example, In the example here we have a node that contains the data 12 and points to the next node 99. Then 99 points to node 37 and so on until we encounter a NULL Node.
There are also doubly linked lists in which each node contains the address of the next as well as the previous node.
But why would we ever need Linked Lists? We all have worked with Lists in Python.But have you ever thought of the insertion time for the list data structure?
Lets say we need to insert an element at the start of a list. Inserting or removing elements at the start in a python list requires an O(n) copy operation.
What if we are faced with the problem in which there are a lot of such inserts and we need a data structure that actually does inserts in constant O(1) time?
There are many practical applications of a linked list that you can think about. One can use a doubly-linked list to implement a system where only the location of previous and next nodes are needed. For example, the previous page and next page functionality in the chrome browser. Or the previous and next photo in a photo editor.
Another benefit of using a linked list is that we don’t need to have contiguous space requirements for a linked list i.e. the nodes can reside anywhere in the memory while for a data structure like an array the nodes need to be allocated a sequence of memory.
How do we create a Linked list in Python? We first start by defining a class that can be used to create a single node.
class Node: def __init__(self,val): self.val = val self.next = None We then use this class object to create multiple nodes and stitch them together to form a linked list.
head = Node(12) a = Node(99) b = Node(37) head.next = a a.next = b And we have our linked list, starting at head. In most cases, we only keep the variable head to define our linked list as it contains all the information we require to access the whole list.
Common Operations or Interview Questions with the Linked Lists 1. Insert a new Node In the start, we said that we can insert an element at the start of the linked list in a constant O(1) time. Let’s see how we can do that.
def insert(head,val): new_head = Node(val) new_head.next = head return new_head So given the head of the node, we just create a new_head object and set its pointer to the previous head of the linked list. We just create a new node and just update a pointer.
2. Print/Traverse the linked list Printing the elements of a linked list is pretty simple. We just go through the linked list in an iterative fashion till we encounter the None node(or the end).
def print(head): while head: print(head.val) head = head.next 3. Reverse a singly linked list This is more of a very common interview question on linked lists. If you are given a linked list, can you reverse that linked list in O(n) time?
For Example: Input: 1-&amp;gt;2-&amp;gt;3-&amp;gt;4-&amp;gt;5-&amp;gt;NULL Output: 5-&amp;gt;4-&amp;gt;3-&amp;gt;2-&amp;gt;1-&amp;gt;NULL  So how do we deal with this?
We start by iterating through the linked list and reversing the pointer direction while moving the head to the next node until there is a next node.
def reverseList(head): newhead = None while head: tmp = head.next head.next = newhead newhead = head head = tmp return newhead Conclusion In this post, I talked about Linked List and its implementation.
Linked lists form the basis of some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Here is a small notebook for you where I have put all these small concepts.
I will leave you with solving this problem by yourself — Implement a function to check if a linked list is a palindrome.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Dynamic Programming for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/01/28/dp/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/28/dp/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dp/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.
This post is about fast-tracking this study and explaining Dynamic Programming concepts for the data scientists in an easy to understand way.
How Dynamic Programming Works? Let’s say that we need to find the nth Fibonacci Number.
Fibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:
def fib(n): if n&amp;lt;=1: return 1 return fib(n-1) &#43; fib(n-2) This problem relates well to a recursive approach. But can you spot the problem here?
If you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.
Source
Now, Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.
In Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.
def fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n&#43;1): dp_sols[i] = dp_sols[i-1] &#43; dp_sols[i-2] return dp_sols[n] Why Dynamic Programming is Hard? Recursion is a mathematical concept and it comes naturally to us. We try to find a solution to a bigger problem by breaking it into smaller ones.
Now Dynamic Programming entails exactly the same idea but in the case of Dynamic programming, we precompute all the subproblems that might need to be calculated in a bottom-up manner.
We human beings are essentially hard-wired to work in a top-down manner. Be it our learning where most people try to go into the breadth of things before going in-depth. Or be it the way we think.
So how does one start thinking in a bottom-up way?
I found out that solving the below problem gives a lot of intuition in how DP works. I myself got highly comfortable with DP once I was able to solve this one and hope it helps you too.
Basically the idea is if you can derive/solve a bigger subproblem if you know the solution to a smaller one?
Maximum Path Sum Given a m x n grid filled with gold, find a path from top left to bottom right which maximizes the sum of gold along its path. We can only move down or right starting from (0,0)
Now there can be decidedly many paths. We can go all the way to the right and then the bottom. Or we can take a zigzag path?
But only one/few paths are going to make you rich.
So how do you even start thinking about such a problem?
When we think of Dynamic Programming questions, we take a bottom-up approach. So we start by thinking about the simplest of problems. In our case, the simplest of problems to solve is the base case. What is the maximum value of Gold we can acquire if we just had to reach cell (0,0)?
And the answer to that is pretty simple — It is the cell value itself.
So we move on to a little harder problem.
What about cell (0,1) and cell (1,0)?
These are also pretty simple. We can reach (0,1)and (1,0) through only (0,0) and hence the maximum gold we can obtain is the value in cell (0,1)/(1,0) plus the maximum gold we can have when we reach cell(0,0)
What about cell(0,2)? Again only one path. So if we know the solution to (0,1) we can just add the value of cell (0,2) to get the solution for (0,2)
Let’s now try to do the same for an arbitrary cell. We want to derive a relation here.
So in the case of an arbitrary cell, we can reach it from the top or from the left.If we know the solutions to the top and left of the cell, we can definitely compute the solution to the arbitrary current target cell.
Coding Once we have the intuition the coding exercise is pretty straightforward. We start by calculating the solutions for the first row and first column. And then we continue to calculate the other values in the grid using the relation we got previously.
def maxPathSum(grid): m = len(grid) n = len(grid[0]) # sol keeps the solutions for each point in the grid. sol = list(grid) # we start by calculating solutions for the first row for i in range(1,n): sol[0][i] &#43;= sol[0][i-1] # we then calculate solutions for the first column for i in range(1,m): sol[i][0] &#43;= sol[i-1][0] # we then calculate all the solutions in the grid for i in range(1,m): for j in range(1,n): sol[i][j] &#43;= max(sol[i-1][j],sol[i][j-1]) # return the last element return sol[-1][-1] Conclusion In this post, I talked about how I think about Dynamic Programming questions.
I start by asking myself the simplest problem I could solve and if I can solve the bigger problem by using the solutions to the simpler problem.
Dynamic Programming forms the basis of some of the most asked questions in Data Science/Machine Learning job interviews, and a good understanding of these might help you land your dream job.
So go out there and do some problems with Leetcode/HackerRank. The problems are surely interesting.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Using Gradient Boosting for Time Series prediction tasks</title>
      <link>https://mlwhiz.com/blog/2019/12/28/timeseries/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/28/timeseries/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/timeseries/main.png"></media:content>
      

      
      <description>Time series prediction problems are pretty frequent in the retail domain.
Companies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.
While there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.</description>

      <content:encoded>  
        
        <![CDATA[  Time series prediction problems are pretty frequent in the retail domain.
Companies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.
While there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.
In this post, we will try to solve the time series problem using XGBoost.
The main things I am going to focus on are the sort of features such a setup takes and how to create such features.
Dataset Kaggle master Kazanova along with some of his friends released a “How to win a data science competition” Coursera course. The Course involved a final project which itself was a time series prediction problem.
In this competition, we are given a challenging time-series dataset consisting of daily sales data, provided by one of the largest Russian software firms — 1C Company.
We have to predict total sales for every product and store in the next month.
Here is how the data looks like:
We are given the data at a daily level, and we want to build a model which predicts total sales for every product and store in the next month.
The variable date_block_num is a consecutive month number, used for convenience. January 2013 is 0, and October 2015 is 33. You can think of it as a proxy to month variable. I think all the other variables are self-explanatory.
So how do we approach this sort of a problem?
Data Preparation The main thing that I noticed is that the data preparation and feature generation aspect is by far the most important thing when we attempt to solve the time series problem using regression.
1. Do Basic EDA and remove outliers sales = sales[sales[&amp;#39;item_price&amp;#39;]&amp;lt;100000] sales = sales[sales[&amp;#39;item_cnt_day&amp;#39;]&amp;lt;=1000] 2. Group data at a level you want your predictions to be: We start with creating a dataframe of distinct date_block_num, store and item combinations.
This is important because in the months we don’t have a data for an item store combination, the machine learning algorithm needs to be told explicitly that the sales are zero.
from itertools import product # Create &amp;#34;grid&amp;#34; with columns index_cols = [&amp;#39;shop_id&amp;#39;, &amp;#39;item_id&amp;#39;, &amp;#39;date_block_num&amp;#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&amp;#39;date_block_num&amp;#39;].unique(): cur_shops = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;shop_id&amp;#39;].unique() cur_items = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;item_id&amp;#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&amp;#39;int32&amp;#39;)) grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) grid.head() The grid dataFrame contains all the shop, items and month combinations.
We then merge the Grid with Sales to get the monthly sales DataFrame. We also replace all the NA’s with zero for months that didn’t have any sales.
sales_m = sales.groupby([&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]).agg({&amp;#39;item_cnt_day&amp;#39;: &amp;#39;sum&amp;#39;,&amp;#39;item_price&amp;#39;: np.mean}).reset_index() # Merging sales numbers with the grid dataframe sales_m = pd.merge(grid,sales_m,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;).fillna(0) # adding the category id too from the items table. sales_m = pd.merge(sales_m,items,on=[&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;) 3. Create Target Encodings To create target encodings, we group by a particular column and take the mean/min/sum etc. of the target column on it. These features are the first features we create in our model.
Please note that these features may induce a lot of leakage/overfitting in our system and thus we don’t use them directly in our models. We will use the lag based version of these features in our models which we will create next.
groupcollist = [&amp;#39;item_id&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_category_id&amp;#39;] aggregationlist = [(&amp;#39;item_price&amp;#39;,np.mean,&amp;#39;avg&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.sum,&amp;#39;sum&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.mean,&amp;#39;avg&amp;#39;)] for type_id in groupcollist: for column_id,aggregator,aggtype in aggregationlist: # get numbers from sales data and set column names mean_df = sales_m.groupby([type_id,&amp;#39;date_block_num&amp;#39;]).aggregate(aggregator).reset_index()[[column_id,type_id,&amp;#39;date_block_num&amp;#39;]] mean_df.columns = [type_id&#43;&amp;#39;_&amp;#39;&#43;aggtype&#43;&amp;#39;_&amp;#39;&#43;column_id,type_id,&amp;#39;date_block_num&amp;#39;] # merge new columns on sales_m data sales_m = pd.merge(sales_m,mean_df,on=[&amp;#39;date_block_num&amp;#39;,type_id],how=&amp;#39;left&amp;#39;) We group by item_id, shop_id, and item_category_id and aggregate on the item_price and item_cnt_day column to create the following new features:
We could also have used featuretools for this. Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.
4. Create Lag Features The next set of features our model needs are the lag based Features.
When we create regular classification models, we treat training examples as fairly independent of each other. But in case of time series problems, at any point in time, the model needs information on what happened in the past.
We can’t do this for all the past days, but we can provide the models with the most recent information nonetheless using our target encoded features.
lag_variables = [&amp;#39;item_id_avg_item_price&amp;#39;,&amp;#39;item_id_sum_item_cnt_day&amp;#39;,&amp;#39;item_id_avg_item_cnt_day&amp;#39;,&amp;#39;shop_id_avg_item_price&amp;#39;,&amp;#39;shop_id_sum_item_cnt_day&amp;#39;,&amp;#39;shop_id_avg_item_cnt_day&amp;#39;,&amp;#39;item_category_id_avg_item_price&amp;#39;,&amp;#39;item_category_id_sum_item_cnt_day&amp;#39;,&amp;#39;item_category_id_avg_item_cnt_day&amp;#39;,&amp;#39;item_cnt_day&amp;#39;] lags = [1 ,2 ,3 ,4, 5, 12] # we will keep the results in thsi dataframe sales_means = sales_m.copy() for lag in lags: sales_new_df = sales_m.copy() sales_new_df.date_block_num&#43;=lag # subset only the lag variables we want sales_new_df = sales_new_df[[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43;lag_variables] sales_new_df.columns = [&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43; [lag_feat&#43;&amp;#39;_lag_&amp;#39;&#43;str(lag) for lag_feat in lag_variables] # join with date_block_num,shop_id and item_id sales_means = pd.merge(sales_means, sales_new_df,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;] ,how=&amp;#39;left&amp;#39;) So we aim to add past information for a few features in our data. We do it for all the new features we created and the item_cnt_day feature.
We fill the NA’s with zeros once we have the lag features.
for feat in sales_means.columns: if &amp;#39;item_cnt&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(0) elif &amp;#39;item_price&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median()) We end up creating a lot of lag features with different lags:
&#39;item_id_avg_item_price_lag_1&#39;,&#39;item_id_sum_item_cnt_day_lag_1&#39;, &#39;item_id_avg_item_cnt_day_lag_1&#39;,&#39;shop_id_avg_item_price_lag_1&#39;, &#39;shop_id_sum_item_cnt_day_lag_1&#39;,&#39;shop_id_avg_item_cnt_day_lag_1&#39;,&#39;item_category_id_avg_item_price_lag_1&#39;,&#39;item_category_id_sum_item_cnt_day_lag_1&#39;,&#39;item_category_id_avg_item_cnt_day_lag_1&#39;, &#39;item_cnt_day_lag_1&#39;, &#39;item_id_avg_item_price_lag_2&#39;, &#39;item_id_sum_item_cnt_day_lag_2&#39;,&#39;item_id_avg_item_cnt_day_lag_2&#39;, &#39;shop_id_avg_item_price_lag_2&#39;,&#39;shop_id_sum_item_cnt_day_lag_2&#39;, &#39;shop_id_avg_item_cnt_day_lag_2&#39;,&#39;item_category_id_avg_item_price_lag_2&#39;,&#39;item_category_id_sum_item_cnt_day_lag_2&#39;,&#39;item_category_id_avg_item_cnt_day_lag_2&#39;, &#39;item_cnt_day_lag_2&#39;, ...  Modelling 1. Drop the unrequired columns As previously said, we are going to drop the target encoded features as they might induce a lot of overfitting in the model. We also lose the item_name and item_price feature.
cols_to_drop = lag_variables[:-1] &#43; [&amp;#39;item_name&amp;#39;,&amp;#39;item_price&amp;#39;] for col in cols_to_drop: del sales_means[col] 2. Take a recent bit of data only When we created the lag variables, we induced a lot of zeroes in the system. We used the maximum lag as 12. To counter that we remove the first 12 months indexes.
sales_means = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;gt;11] 3. Train and CV Split When we do a time series split, we usually don’t take a cross-sectional split as the data is time-dependent. We want to create a model that sees till now and can predict the next month well.
X_train = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;lt;33] X_cv = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]==33] Y_train = X_train[&amp;#39;item_cnt_day&amp;#39;] Y_cv = X_cv[&amp;#39;item_cnt_day&amp;#39;] del X_train[&amp;#39;item_cnt_day&amp;#39;] del X_cv[&amp;#39;item_cnt_day&amp;#39;] 4. Create Baseline Before we proceed with modelling steps, lets check the RMSE of a naive model, as we want to have an RMSE to compare to. We assume that we are going to predict the last month sales as current month sale for our baseline model. We can quantify the performance of our model using this baseline RMSE.
from sklearn.metrics import mean_squared_error sales_m_test = sales_m[sales_m[&amp;#39;date_block_num&amp;#39;]==33] preds = sales_m.copy() preds[&amp;#39;date_block_num&amp;#39;]=preds[&amp;#39;date_block_num&amp;#39;]&#43;1 preds = preds[preds[&amp;#39;date_block_num&amp;#39;]==33] preds = preds.rename(columns={&amp;#39;item_cnt_day&amp;#39;:&amp;#39;preds_item_cnt_day&amp;#39;}) preds = pd.merge(sales_m_test,preds,on = [&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;)[[&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;,&amp;#39;preds_item_cnt_day&amp;#39;,&amp;#39;item_cnt_day&amp;#39;]].fillna(0) # We want our predictions clipped at (0,20). Competition Specific preds[&amp;#39;item_cnt_day&amp;#39;] = preds[&amp;#39;item_cnt_day&amp;#39;].clip(0,20) preds[&amp;#39;preds_item_cnt_day&amp;#39;] = preds[&amp;#39;preds_item_cnt_day&amp;#39;].clip(0,20) baseline_rmse = np.sqrt(mean_squared_error(preds[&amp;#39;item_cnt_day&amp;#39;],preds[&amp;#39;preds_item_cnt_day&amp;#39;])) print(baseline_rmse) 1.1358170090812756  5. Train XGB We use the XGBRegressor object from the xgboost scikit API to build our model. Parameters are taken from this kaggle kernel. If you have time, you can use hyperopt to automatically find out the hyperparameters yourself.
from xgboost import XGBRegressor model = XGBRegressor( max_depth=8, n_estimators=1000, min_child_weight=300, colsample_bytree=0.8, subsample=0.8, eta=0.3, seed=42) model.fit( X_train, Y_train, eval_metric=&amp;#34;rmse&amp;#34;, eval_set=[(X_train, Y_train), (X_cv, Y_cv)], verbose=True, early_stopping_rounds = 10) After running this, we can see RMSE in ranges of 0.93 on the CV set. And that is pretty impressive based on our baseline validation RMSE of 1.13. And so we work on deploying this model as part of our continuous integration effort.
5. Plot Feature Importance We can also see the important features that come from XGB.
feature_importances = pd.DataFrame({&amp;#39;col&amp;#39;: columns,&amp;#39;imp&amp;#39;:model.feature_importances_}) feature_importances = feature_importances.sort_values(by=&amp;#39;imp&amp;#39;,ascending=False) px.bar(feature_importances,x=&amp;#39;col&amp;#39;,y=&amp;#39;imp&amp;#39;) Conclusion In this post, we talked about how we can use trees for even time series modelling. The purpose was not to get perfect scores on the kaggle leaderboard but to gain an understanding of how such models work.
When I took part in this competition as part of the course, a couple of years back, using trees I reached near the top of the leaderboard.
Over time people have worked a lot on tweaking the model, hyperparameter tuning and creating even more informative features. But the basic approach has remained the same.
You can find the whole running code on GitHub.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of ways to improve your models using feature engineering and hyperparameter tuning.
I am going to be writing more beginner-friendly posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>3 Programming concepts for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/12/09/pc/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/09/pc/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pc/main.png"></media:content>
      

      
      <description>Algorithms are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms.
This post is about fast-tracking that study and panning some essential algorithms concepts for the data scientists in an easy to understand way.
1. Recursion/Memoization Recursion is where a function being defined is applied within its own definition. Put simply; recursion is when a function calls itself. Google does something pretty interesting when you search for recursion there.
Hope you get the joke. While recursion may seem a little bit daunting to someone just starting, it is pretty simple to understand. And it is a beautiful concept once you know it.
The best example I find for explaining recursion is to calculate the factorial of a number.
def factorial(n): if n==0: return 1 return n*factorial(n-1) We can see how factorial is a recursive function quite easily.
Factorial(n) = n*Factorial(n-1)
So how does it translates to programming?
A function for a recursive call generally consists of two things:
 A base case — The case where the recursion ends.
 A recursive formulation- a formulaic way to move towards the base case.
  A lot of problems you end up solving are recursive. It applies to data science, as well.
For example, A decision tree is just a binary tree, and tree algorithms are generally recursive. Or, we do use sort in a lot of times. The algorithm responsible for that is called mergesort, which in itself is a recursive algorithm. Another one is binary search, which includes finding an element in an array.
Now we have got a basic hang of recursion, let us try to find the nth Fibonacci Number. Fibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:
def fib(n): if n&amp;lt;=1: return 1 return fib(n-1) &#43; fib(n-2) But do you spot the problem here?
If you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.
Can we do better? Yes, we can. We can change our implementation a little bit an add a dictionary to add some storage to our method. Now, this memo dictionary gets updated any time a number has been calculated. If that number appears again, we don’t calculate it again but give results from the memo dictionary. This addition of storage is called Memoization.
memo = {} def fib_memo(n): if n in memo: return memo[n] if n&amp;lt;=1: memo[n]=1 return 1 memo[n] = fib_memo(n-1) &#43; fib_memo(n-2) return memo[n] Usually, I like to write the recursive function first, and if it is making a lot of calls to the same parameters again and again, I add a dictionary to memorize solutions.
How much does it help?
This is the run time comparison for different values of n. We can see the runtime for Fibonacci without Memoization increases exponentially, while for the memoized function, the time is linear.
2. Dynamic programming Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.
In Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.
def fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n&#43;1): dp_sols[i] = dp_sols[i-1] &#43; dp_sols[i-2] return dp_sols[n] Above is the comparison of runtimes for DP vs. Memoization. We can see that they are both linear, but DP still is a little bit faster.
Why? Because Dynamic Programming made only one call exactly to each subproblem in this case.
There is an excellent story on how Bellman who developed Dynamic Programming framed the term:
 Where did the name, dynamic programming, come from? The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word research. What title, what name, could I choose? In the first place, I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.
 3. Binary Search Let us say we have a sorted array of numbers, and we want to find out a number from this array. We can go the linear route that checks every number one by one and stops if it finds the number. The problem is that it takes too long if the array contains millions of elements. Here we can use a Binary search.
  Source:mathwarehouse.com|Finding 37 — There are 3.7 trillion fish in the ocean, they’re looking for one    # Returns index of target in nums array if present, else -1 def binary_search(nums, left, right, target): # Base case if right &amp;gt;= left: mid = int((left &#43; right)/2) # If target is present at the mid, return if nums[mid] == target: return mid # Target is smaller than mid search the elements in left elif nums[mid] &amp;gt; target: return binary_search(nums, left, mid-1, target) # Target is larger than mid, search the elements in right else: return binary_search(nums, mid&#43;1, right, target) else: # Target is not in nums return -1 nums = [1,2,3,4,5,6,7,8,9] print(binary_search(nums, 0, len(nums)-1,7)) This is an advanced case of a recursion based algorithm where we make use of the fact that the array is sorted. Here we recursively look at the middle element and see if we want to search in the left or right of the middle element. This makes our searching space go down by a factor of 2 every step.
And thus the run time of this algorithm is O(logn) as opposed to O(n) for linear search.
How much does that matter? Below is a comparison in run times. We can see that the Binary search is pretty fast compared to Linear search.
Conclusion In this post, I talked about some of the most exciting algorithms that form the basis for programming.
These algorithms are behind some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend to learn the basics of algorithms.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Sampling Algorithms every Data Scientist need to know</title>
      <link>https://mlwhiz.com/blog/2019/07/30/sampling/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/30/sampling/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/sampling/1.jpg"></media:content>
      

      
      <description>Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.</description>

      <content:encoded>  
        
        <![CDATA[  Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.
Below we select 100 sample points from a dataset.
sample_df = df.sample(100) Stratified Sampling Assume that we need to estimate the average number of votes for each candidate in an election. Assume that the country has 3 towns:
Town A has 1 million factory workers,
Town B has 2 million workers, and
Town C has 3 million retirees.
We can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation.
Instead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of the sample.
You can do something like this pretty easily with Python:
from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25) Reservoir Sampling I love this problem statement:
Say you have a stream of items of large and unknown length that we can only iterate over once.
Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.
How can we do that?
Let us assume we have to sample 5 objects out of an infinite stream such that each element has an equal probability of getting selected.
import random def generator(max): number = 1 while number &amp;lt; max: number &#43;= 1 yield number # Create as stream generator stream = generator(10000) # Doing Reservoir Sampling from the stream k=5 reservoir = [] for i, element in enumerate(stream): if i&#43;1&amp;lt;= k: reservoir.append(element) else: probability = k/(i&#43;1) if random.random() &amp;lt; probability: # Select item in stream and remove one of the k items already selected reservoir[random.choice(range(0,k))] = element print(reservoir) [1369, 4108, 9986, 828, 5589]  It can be mathematically proved that in the sample each element has the same probability of getting selected from the stream.
How?
It always helps to think of a smaller problem when it comes to mathematics.
So, let us think of a stream of only 3 items and we have to keep 2 of them.
We see the first item, we hold it in the list as our reservoir has space. We see the second item, we hold it in the list as our reservoir has space.
We see the third item. Here is where things get interesting. We choose the third item to be in the list with probability 2&amp;frasl;3.
Let us now see the probability of first item getting selected:
The probability of removing the first item is the probability of element 3 getting selected multiplied by the probability of Element 1 getting randomly chosen as the replacement candidate from the 2 elements in the reservoir. That probability is:
2&amp;frasl;3*1&amp;frasl;2 = 1&amp;frasl;3
Thus the probability of 1 getting selected is:
1–1/3 = 2&amp;frasl;3
We can have the exact same argument for the Second Element and we can extend it for many elements.
Thus each item has the same probability of getting selected: 2&amp;frasl;3 or in general k/n
Random Undersampling and Oversampling It is too often that we encounter an imbalanced dataset.
A widely adopted technique for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).
Let us first create some example imbalanced data.
from sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[&amp;#39;target&amp;#39;] = y We can now do random oversampling and undersampling using:
num_0 = len(X[X[&amp;#39;target&amp;#39;]==0]) num_1 = len(X[X[&amp;#39;target&amp;#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0].sample(num_1) , X[X[&amp;#39;target&amp;#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0] , X[X[&amp;#39;target&amp;#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180  Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.
It provides a variety of methods to undersample and oversample.
a. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.
In this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier.
from imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=&amp;#39;majority&amp;#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.
from imblearn.over_sampling import SMOTE smote = SMOTE(ratio=&amp;#39;minority&amp;#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.
Conclusion Algorithms are the lifeblood of data science.
Sampling is an important topic in data science and we really don’t talk about it as much as we should.
A good sampling strategy sometimes could pull the whole project forward. A bad sampling strategy could give us incorrect results. So one should be careful while selecting a sampling strategy.
So use sampling, be it at work or at bars.
If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Bayesian Bandits explained simply</title>
      <link>https://mlwhiz.com/blog/2019/07/21/bandits/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/21/bandits/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/bandits/1.png"></media:content>
      

      
      <description>Exploration and Exploitation play a key role in any business.
And any good business will try to “explore” various opportunities where it can make a profit.
Any good business at the same time also tries to focus on a particular opportunity it has found already and tries to “exploits” it.
Let me explain this further with a thought experiment.
Thought Experiment: Assume that we have infinite slot machines. Every slot machine has some win probability.</description>

      <content:encoded>  
        
        <![CDATA[  Exploration and Exploitation play a key role in any business.
And any good business will try to “explore” various opportunities where it can make a profit.
Any good business at the same time also tries to focus on a particular opportunity it has found already and tries to “exploits” it.
Let me explain this further with a thought experiment.
Thought Experiment: Assume that we have infinite slot machines. Every slot machine has some win probability. But we don’t know these probability values.
You have to operate these slot machines one by one. How do you come up with a strategy to maximize your outcome from these slot machines in minimum time.
You will most probably start by trying out some machines.
Would you stick to a particular machine that has an okayish probability(exploitation) or would you keep searching for better machines(exploration)?
It is the exploration-exploitation tradeoff.
And the question is how do we balance this tradeoff such that we get maximum profits?
The answer is Bayesian Bandits.
Why? Business Use-cases: There are a lot of places where such a thought experiment could fit.
 AB Testing: You have a variety of assets that you can show at the website. Each asset has a particular probability of success(getting clicked by the user).
 Ad Clicks: You have a variety of ads that you can show to the user. Each advert has a particular probability of clickthrough
 Finance: which stock pick gives the highest return.
 We as human beings are faced with the exact same problem — Explore or exploit and we handle it quite brilliantly mostly. Should we go find a new job or should we earn money doing the thing we know would give us money?
  In this post, we will focus on AB Testing but this experiment will work for any of the above problems.
Problem Statement: The problem we have is that we have different assets which we want to show on our awesome website but we really don’t know which one to show.
One asset is blue(B), another red&amp;reg; and the third one green(G).
Our UX team say they like the blue one. But you like the green one.
Which one to show on our website?
Bayes Everywhere: Before we delve down into the algorithm we will use to solve this, let us revisit the Bayes theorem.
Just remember that Bayes theorem says that Posterior ~ likelihood*Prior
Beta Distribution The beta distribution is a continuous probability distribution defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by α and β.The PDF of the beta distribution is:
And, the pdf looks like below for different values of α and β:
The beta distribution is frequently applied to model the behavior of probabilities as it lies in the range [0,1]. The beta distribution is a suitable model for the random behavior of percentages and proportions too.
Bayesian Bandits So after knowing the above concepts let us come back to our present problem.
We have three assets.
For the sake of this problem, let&amp;rsquo;s assume that we know the click probabilities of these assets also.
The win probability of blue is 0.3, red is 0.8 and green is 0.4. Please note that in real life, we won’t know this.
These probabilities are going to be hidden from our algorithm and we will see how our algorithm will still converge to these real probabilities.
So, what are our priors(beliefs) about the probability of each of these assets?
Since we have not observed any data we cannot have prior beliefs about any of our three assets.
We need to model our prior probabilities and we will use beta distribution to do that. See the curve for beta distribution above for α = 1 and β=1.
It is actually just a uniform distribution over the range [0,1]. And that is what we want for our prior probabilities for our assets. We don’t have any information yet so we start with a uniform probability distribution over our probability values.
So we can denote the prior probabilities of each of our asset using a beta distribution.
Strategy:  We will sample a random variable from each of the 3 distributions for assets.
 We will find out which random variable is maximum and will show the one asset which gave the maximum random variable.
 We will get to know if that asset is clicked or not.
 We will update the prior for the asset using the information in step 3.
 Repeat.
  Updating the Prior: The reason we took beta distribution to model our probabilities is because of its great mathematical properties.
If the prior is f(α,β), then the posterior distribution is again beta, given by f(α&#43;#success, β&#43;#failures)
where #success is the number of clicks and #failures are the number of views minus the number of clicks.
Let us Code We have every bit of knowledge we require for writing some code now. I will be using pretty much simple and standard Python functionality to do this but there exist tools like pyMC and such for this sort of problem formulations.
Let us work through this problem step by step.
We have three assets with different probabilities.
real_probs_dict = {&amp;#39;R&amp;#39;:0.8,&amp;#39;G&amp;#39;:0.4,&amp;#39;B&amp;#39;:0.3} assets = [&amp;#39;R&amp;#39;,&amp;#39;G&amp;#39;,&amp;#39;B&amp;#39;] We will be trying to see if our strategy given above works or not.
&amp;#39;&amp;#39;&amp;#39; This function takes as input three tuples for alpha,beta that specify priorR,priorG,priorB And returns R,G,B along with the maximum value sampled from these three distributions. We can sample from a beta distribution using scipy. &amp;#39;&amp;#39;&amp;#39; def find_asset(priorR,priorG,priorB): red_rv = scipy.stats.beta.rvs(priorR[0],priorR[1]) green_rv = scipy.stats.beta.rvs(priorG[0],priorG[1]) blue_rv = scipy.stats.beta.rvs(priorB[0],priorB[1]) return assets[np.argmax([red_rv,green_rv,blue_rv])] &amp;#39;&amp;#39;&amp;#39; This is a helper function that simulates the real world using the actual probability value of the assets. In real life we won&amp;#39;t have this function and our user click input will be the proxy for this function. &amp;#39;&amp;#39;&amp;#39; def simulate_real_website(asset, real_probs_dict): #simulate a coin toss with probability. Asset clicked or not. if real_probs_dict[asset]&amp;gt; scipy.stats.uniform.rvs(0,1): return 1 else: return 0 &amp;#39;&amp;#39;&amp;#39; This function takes as input the selected asset and returns the posteriors for the selected asset. &amp;#39;&amp;#39;&amp;#39; def update_posterior(asset,priorR,priorG,priorB,outcome): if asset==&amp;#39;R&amp;#39;: priorR=(priorR[0]&#43;outcome,priorR[1]&#43;1-outcome) elif asset==&amp;#39;G&amp;#39;: priorG=(priorG[0]&#43;outcome,priorG[1]&#43;1-outcome) elif asset==&amp;#39;B&amp;#39;: priorB=(priorB[0]&#43;outcome,priorB[1]&#43;1-outcome) return priorR,priorG,priorB &amp;#39;&amp;#39;&amp;#39; This function runs the strategy once. &amp;#39;&amp;#39;&amp;#39; def run_strategy_once(priorR,priorG,priorB): # 1. get the asset asset = find_asset(priorR,priorG,priorB) # 2. get the outcome from the website/users outcome = simulate_real_website(asset, real_probs_dict) # 3. update prior based on outcome priorR,priorG,priorB = update_posterior(asset,priorR,priorG,priorB,outcome) return asset,priorR,priorG,priorB Let us run this strategy multiple times and collect the data.
priorR,priorG,priorB = (1,1),(1,1),(1,1) data = [(&amp;#34;_&amp;#34;,priorR,priorG,priorB)] for i in range(50): asset,priorR,priorG,priorB = run_strategy_once(priorR,priorG,priorB) data.append((asset,priorR,priorG,priorB)) This is the result of our runs. You can see the functions I used to visualize the posterior distributions here at kaggle. As you can see below, we have pretty much converged to the best asset by the end of 20 runs. And the probabilities are also estimated roughly what they should be.
At the start, we have a uniform prior. As we go through with the runs we see that the “red” asset’s posterior distribution converges towards a higher mean and as such a higher probability of pick. But remember that doesn’t mean that the green asset and blue asset are not going to be picked ever.
Let us also see how many times each asset is picked in the 50 runs we did.
Pick 1 : G ,Pick 2 : R ,Pick 3 : R ,Pick 4 : B ,Pick 5 : R ,Pick 6 : R ,Pick 7 : R ,Pick 8 : R ,Pick 9 : R ,Pick 10 : R ,Pick 11 : R ,Pick 12 : R ,Pick 13 : R ,Pick 14 : R ,Pick 15 : R ,Pick 16 : R ,Pick 17 : R ,Pick 18 : R ,Pick 19 : R ,Pick 20 : R ,Pick 21 : R ,Pick 22 : G ,Pick 23 : R ,Pick 24 : R ,Pick 25 : G ,Pick 26 : G ,Pick 27 : R ,Pick 28 : R ,Pick 29 : R ,Pick 30 : R ,Pick 31 : R ,Pick 32 : R ,Pick 33 : R ,Pick 34 : R ,Pick 35 : R ,Pick 36 : R ,Pick 37 : R ,Pick 38 : R ,Pick 39 : G ,Pick 40 : B ,Pick 41 : R ,Pick 42 : R ,Pick 43 : R ,Pick 44 : R ,Pick 45 : B ,Pick 46 : R ,Pick 47 : R ,Pick 48 : R ,Pick 49 : R ,Pick 50 : R  We can see that although we are mostly picking R we still end up sometimes picking B(Pick 45) and G(Pick 44) in the later runs too. Overall we can see that in the first few runs, we are focussing on exploration and as we go towards later runs we focus on exploitation.
End Notes: We saw how solving this problem using the Bayesian approach could help us converge to a good solution while maximizing our profit and not discarding any asset.
An added advantage of such an approach is that it is self-learning and could self-correct by itself if the probability of click on the red decreases and blue asset increases. This case might happen when the user preferences change for example.
The whole code is posted in the Kaggle Kernel.
Also, if you want to learn more about Bayesian Statistics, one of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization
I am going to be writing more of such posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</guid>
      
      

      
      <description>As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.</description>

      <content:encoded>  
        
        <![CDATA[    As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.  Simple Random Sampling : Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen. Stratified Sampling: Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns : Town A has 1 million factory workers, Town B has 2 million workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample. Reservoir Sampling :Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.  2. Map-Reduce. If you want to work with the whole data. Can be used for feature creation. For Example: I had a use case where I had a graph of 60 Million customers and 130 Million accounts. Each account was connected to other account if they had the Same SSN or Same Name&#43;DOB&#43;Address. I had to find customer ID’s for each of the accounts. On a single node parsing such a graph took more than 2 days. On a Hadoop cluster of 80 nodes running a Connected Component Algorithm took less than 24 minutes. On Spark it is even faster.
3. Graph Algorithms. Recently I was working on an optimization problem which was focussed on finding shortest distance and routes between two points in a store layout. Routes which don’t pass through different aisles, so we cannot use euclidean distances. We solved this problem by considering turning points in the store layout and the djikstra’s Algorithm.
 4. Feature Selection.  Univariate Selection. Statistical tests can be used to select those features that have the strongest relationship with the output variable. VarianceThreshold. Feature selector that removes all low-variance features. Recursive Feature Elimination. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Feature Importance: Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.  5. Algorithms to work efficiently. Apart from these above algorithms sometimes you may need to write your own algorithms. Now I think of big algorithms as a combination of small but powerful algorithms. You just need to have idea of these algorithms to make a more better/efficient product. So some of these powerful algorithms which can help you are:
 Recursive Algorithms:Binary search algorithm. Divide and Conquer Algorithms: Merge-Sort. Dynamic Programming:Solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.  6. Classification/Regression Algorithms. The usual suspects. Minimum you must know:  Linear Regression - Ridge Regression, Lasso Regression, ElasticNet Logistic Regression From there you can build upon:  Decision Trees - ID3, CART, C4.5, C5.0 KNN SVM ANN - Back Propogation, CNN  And then on to Ensemble based algorithms:  Boosting: Gradient Boosted Trees Bagging: Random Forests Blending: Prediction outputs of different learning algorithms are fed into another learning algorithm.   7 . Clustering Methods.For unsupervised learning.  k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering  8. Other algorithms you can learn about:  Apriori algorithm- Association Rule Mining Eclat algorithm - Association Rule Mining Item/User Based Similarity - Recommender Systems Reinforcement learning - Build your own robot. Graphical Models Bayesian Algorithms NLP - For language based models. Chatbots.  Hope this has been helpful&amp;hellip;..
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>