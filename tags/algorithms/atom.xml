<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on MLWhiz</title>
    <link>https://mlwhiz.com/tags/algorithms/</link>
    <description>Recent content in Algorithms on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Dec 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mlwhiz.com/tags/algorithms/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Using XGBoost for time series prediction tasks</title>
      <link>https://mlwhiz.com/blog/2017/12/26/how_to_win_a_data_science_competition/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/26/how_to_win_a_data_science_competition/</guid>
      <description>Recently Kaggle master Kazanova along with some of his friends released a &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.
  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.</description>
    </item>
    
    <item>
      <title>Good Feature Building Techniques - Tricks for Kaggle -  My Kaggle Code Repository</title>
      <link>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</guid>
      <description>Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing).</description>
    </item>
    
    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      <description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;rsquo;t know bag of words: If we have a series of sentences(documents)</description>
    </item>
    
    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/machine_learning_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/machine_learning_algorithms_for_data_scientist/</guid>
      <description>As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.</description>
    </item>
    
  </channel>
</rss>