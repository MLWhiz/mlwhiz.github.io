<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>
    Hadoop on 
    MLWhiz
    </title>
    <link>https://mlwhiz.com/tags/hadoop/</link>
    <description>Recent content in Hadoop 
    on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
    
    <lastBuildDate>Mon, 07 Sep 2015 00:00:00 +0000</lastBuildDate>
    
    
        <atom:link href="https://mlwhiz.com/tags/hadoop/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Learning Spark using Python: Basics and Applications</title>
      <link>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</link>
      <pubDate>Mon, 07 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</guid>
      <description>

&lt;p&gt;I generally have a use case for &lt;a href=&#34;https://hadoop.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Hadoop&lt;/a&gt; in my daily job. It has made my life easier in a sense that I am able to get results which I was not able to see with SQL queries. But still I find it painfully slow.
I have to write procedural programs while I work. As in merge these two datasets and then filter and then merge another dataset and then filter using some condition and yada-yada.
You get the gist. And in hadoop its painstakingly boring to do this. You have to write more than maybe 3 Mapreduce Jobs. One job will read the data line by line and write to the disk.&lt;/p&gt;

&lt;p&gt;There is a lot of data movement that happens in between that further affects the speed.
Another thing I hate is that there is no straight way to pass files to mappers and reducers and that generally adds up another mapreduce job to the whole sequence.&lt;/p&gt;

&lt;p&gt;And that is just procedural tasks. To implement an iterative algorithm even after geting the whole logic of parallelization is again a challenge. There would be a lot of mapreduce tasks, a shell based driver program and a lot of unique thinking to bring everything together. And the running times are like crazy. Though sometimes it has its benefits:&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/compiling.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;That makes me think about the whole way Hadoop is implemented. While at the time Hadoop appeared the RAM was costly.
Now that is not the case. We already have 64GB machines in our Hadoop cluster. So is it really a good idea to not use a larger chunk of memory and read line by line.
Also can we have something that allows us to keep a particular piece of data in the memory, So that the next time our program needs it it doesnt have to read it again and waste time.
Wouldnt it be better if we have some variable that lets us keep the state our iterative algorithm is in.&lt;/p&gt;

&lt;h2 id=&#34;the-solution&#34;&gt;The Solution?&lt;/h2&gt;

&lt;p&gt;And here is where &lt;a href=&#34;http://spark.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spark&lt;/a&gt; comes to rescue. Now working on Spark is very different from Hadoop but when you start using it you find that it makes things so much easier. You still do have to think in the mapreduce way sort of but the way the map and reduce steps are done are a little bit different.&lt;/p&gt;

&lt;p&gt;So lets first get Spark on our System (But keep in mind that for running spark in production environments you will
need whole clusters set up. A liberty which you may or may not have at present)&lt;/p&gt;

&lt;p&gt;The best way that I found to install Spark is following the Apache Spark installation guidelines with the Apache Spark eDx &lt;a href=&#34;https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;course&lt;/a&gt;. It lets you get Spark in your system and work with Spark with iPython notebooks. Something I prefer a lot and find the best way to code in Python.&lt;/p&gt;

&lt;p&gt;The installation instructions can be found &lt;a href=&#34;https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/920d3370060540c8b21d56f05c64bdda/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;HERE&lt;/a&gt;. You may have to login in to an edX account to follow these instructions, but it is worth it.&lt;/p&gt;

&lt;p&gt;So once you have gone through all the steps mentioned there and installed spark using these instructions, you would see something like this in your browser.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/ipython_startup.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Ahh! so you have got Spark up and running now. That&amp;rsquo;s actually like half the process. I like to learn by examples so let&amp;rsquo;s get done with the &amp;ldquo;Hello World&amp;rdquo; of Distributed computing: The WordCount Program.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lines &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shakespeare.txt&amp;#34;&lt;/span&gt;)                   &lt;span style=&#34;color:#75715e&#34;&gt;# Distribute the data - Create a RDD &lt;/span&gt;

counts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (lines&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;))          &lt;span style=&#34;color:#75715e&#34;&gt;# Create a list with all words&lt;/span&gt;
                  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))                 &lt;span style=&#34;color:#75715e&#34;&gt;# Create tuple (word,1)&lt;/span&gt;
                  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y : x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y))      &lt;span style=&#34;color:#75715e&#34;&gt;# reduce by key i.e. the word&lt;/span&gt;
output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; counts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)                                 &lt;span style=&#34;color:#75715e&#34;&gt;# get the output on local&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (word, count) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; output:                             &lt;span style=&#34;color:#75715e&#34;&gt;# print output&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; (word, count))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/wordcount_result.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;So that is a small example. Pretty small code when you compare it with Hadoop. And most of the work gets done in the second command.
Don&amp;rsquo;t worry if you are not able to follow this yet as I need to tell you about the things that make Spark work.&lt;/p&gt;

&lt;p&gt;But before we get into Spark basics, Let us refresh some of our python Basics. Understanding Spark becomes a lot easier if you have used Lambda functions in Python.&lt;/p&gt;

&lt;p&gt;For those of you who haven&amp;rsquo;t used it, below is a brief intro.&lt;/p&gt;

&lt;h2 id=&#34;lambda-functions-in-python&#34;&gt;Lambda Functions in Python&lt;/h2&gt;

&lt;h4 id=&#34;map&#34;&gt;Map&lt;/h4&gt;

&lt;p&gt;Map is used to map a function to a array or a list. Say you want to apply some function to every element in a list. You can do this by simply using a for loop but python lambda functions let you do this in a single line in Python.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;my_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Lets say I want to square each term in my_list.&lt;/span&gt;
squared_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,my_list)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; squared_list&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In the above example you could think of map as a function which takes two arguments - A function and a list. It then applies the function to every element of the list. What lambda allows you to do is write an inline function. In here the part &lt;strong&gt;&amp;ldquo;lambda x:x**2&amp;rdquo;&lt;/strong&gt; defines a function that takes x as input and returns x^2.&lt;/p&gt;

&lt;p&gt;You could have also provided a proper function in place of lambda. For Example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;squared&lt;/span&gt;(x):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;br&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;my_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Lets say I want to square each term in my_list.&lt;/span&gt;
squared_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; map(squared,my_list)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; squared_list&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The same result, but the lambda expressions make the code compact and a lot more readable.&lt;/p&gt;

&lt;h4 id=&#34;filter&#34;&gt;Filter&lt;/h4&gt;

&lt;p&gt;The other function that is used extensively is the filter function. This function takes two arguments - A condition and the list to filter. If you want to filter your list using some condition you use filter.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;my_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Lets say I want only the even numbers in my list.&lt;/span&gt;
filtered_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:x&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,my_list)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; filtered_list&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[2, 4, 6, 8, 10]&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&#34;reduce&#34;&gt;Reduce&lt;/h4&gt;

&lt;p&gt;The next function is the reduce function. This function will be the workhorse in Spark. This function takes two arguments - a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;my_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Lets say I want to sum all elements in my list.&lt;/span&gt;
sum_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reduce(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y:x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y,my_list)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; sum_list&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;15&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reduce function first sends 1,2    ; the lambda function returns 3
Reduce function then sends 3,3     ; the lambda function returns 6
Reduce function then sends 6,4     ; the lambda function returns 10
Reduce function finally sends 10,5 ; the lambda function returns 15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A condition on the lambda function we use in reduce is that it must be commutative that is a + b = b + a and associative that is (a + b) + c == a + (b + c).
In the above case we used sum which is &lt;strong&gt;commutative as well as associative&lt;/strong&gt;. Other functions that we could have used are &lt;strong&gt;max, min, multiplication&lt;/strong&gt; etc.&lt;/p&gt;

&lt;h2 id=&#34;moving-again-to-spark&#34;&gt;Moving Again to Spark&lt;/h2&gt;

&lt;p&gt;As we have now got the fundamentals of Python Functional Programming out of the way, lets again head to Spark.&lt;/p&gt;

&lt;p&gt;But first let us delve a little bit into how spark works. Spark actually consists of two things a driver and workers. Workers normally do all the work and the driver makes them do that work.&lt;/p&gt;

&lt;p&gt;An RDD is defined a parallelized data structure that gets distributed across the worker nodes. In our wordcount example, in the first line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lines = sc.textFile(&amp;quot;data/cs100/lab1/shakespeare.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We took a text file and distributed it across worker nodes so that they can work on it in parallel.
We could also parallelize lists using the function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sc.parallelize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
new_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
new_rdd&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;ParallelCollectionRDD[15] at parallelize at PythonRDD.scala:392&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In Spark we classify the operations into two Basic Types: Transformations and Actions.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transformations&lt;/strong&gt; : Create new datasets from existing RDDs&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt; : Mechanism to get results out of Spark&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;understanding-transformations&#34;&gt;Understanding Transformations&lt;/h2&gt;

&lt;p&gt;So lets say you have got your data in the form of an RDD. To requote your data is now accesible b all the worker machines. You want to do some transformations on the data now. You may want to filter, Apply some function etc. In Spark this is done using Transformation functions. Spark provides many transformation functions. You can see a comprehensive list &lt;a href=&#34;http://spark.apache.org/docs/latest/programming-guide.html#transformations&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
Some of the main ones that I use frequently are:&lt;/p&gt;

&lt;h5 id=&#34;1-map&#34;&gt;1. Map:&lt;/h5&gt;

&lt;p&gt;Applies a given function to an RDD. Note that the syntax is a little bit different from python, but it necessarily does the same thing. Don&amp;rsquo;t worry about collet yet. For now just think of it as a function that collects the data in squared_rdd back to a list.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
squared_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
squared_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;2-filter&#34;&gt;2. Filter:&lt;/h5&gt;

&lt;p&gt;Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
filtered_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:x&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
filtered_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[2, 4, 6, 8, 10]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;3-distinct&#34;&gt;3. Distinct:&lt;/h5&gt;

&lt;p&gt;Returns only distinct elements in an RDD&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
distinct_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distinct()
distinct_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[8, 4, 1, 5, 9, 2, 10, 6, 3, 7]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;4-flatmap&#34;&gt;4. Flatmap:&lt;/h5&gt;

&lt;p&gt;Similar to map, but each input item can be mapped to 0 or more output items&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
flat_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;flatMap(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:[x,x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
flat_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[1, 1, 2, 8, 3, 27, 4, 64]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;5-reduce-by-key&#34;&gt;5. Reduce By Key:&lt;/h5&gt;

&lt;p&gt;The analogue to the reduce in Hadoop Mapreduce. Now Spark cannot provide the value if it just worked with Lists. In Spark there is a concept of pair RDDs that makes it a lot more flexible. Lets assume we have a data in which we have product, its category and its selling price. We can still parallelize the data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Apple&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Banana&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Tomato&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Potato&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Vegetable&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;103&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Carrot&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Vegetable&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;)]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Right now our RDD rdd holds tuples. Now we want to find out the total sum of revenue that we got from each category. To do that we have to transform our rdd to a pair rdd so that it only contatins key-value pairs/tuples.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;category_price_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]))
category_price_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[(&amp;lsquo;Fruit&amp;rsquo;, 200), (&amp;lsquo;Fruit&amp;rsquo;, 24), (&amp;lsquo;Fruit&amp;rsquo;, 56), (&amp;lsquo;Vegetable&amp;rsquo;, 103), (&amp;lsquo;Vegetable&amp;rsquo;, 34)]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here we used the map function to get it in the format we wanted. When working with textfile, the rdd that gets formed has got a lot of strings. We use map to convert it into a format that we want.&lt;/p&gt;

&lt;p&gt;So now our category_price_rdd contains the product category and the price at which the prouct sold. Now we want to reduce on the key and sum the prices. We can do this by:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;category_total_price_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; category_price_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y:x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)
category_total_price_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[(&amp;lsquo;Vegetable&amp;rsquo;, 137), (&amp;lsquo;Fruit&amp;rsquo;, 280)]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;6-group-by-key&#34;&gt;6. Group By Key:&lt;/h5&gt;

&lt;p&gt;Similar to reduce by key but does not reduce just puts all the elements in an iterator. For example if we wanted to keep as key the category and as the value all the products we would use this function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Apple&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Banana&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Tomato&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Fruit&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Potato&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Vegetable&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;103&lt;/span&gt;),(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Carrot&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Vegetable&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;)]
rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize(data,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
category_product_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
category_product_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[(&amp;lsquo;Fruit&amp;rsquo;,&amp;lsquo;Apple&amp;rsquo;),(&amp;lsquo;Fruit&amp;rsquo;,&amp;lsquo;Banana&amp;rsquo;),(&amp;lsquo;Fruit&amp;rsquo;,&amp;lsquo;Tomato&amp;rsquo;),(&amp;lsquo;Vegetable&amp;rsquo;,&amp;lsquo;Potato&amp;rsquo;),(&amp;lsquo;Vegetable&amp;rsquo;,&amp;lsquo;Carrot&amp;rsquo;)]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;grouped_products_by_category_rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; category_product_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupByKey()
findata &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grouped_products_by_category_rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; findata:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; data[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],list(data[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;Vegetable [&amp;lsquo;Potato&amp;rsquo;, &amp;lsquo;Carrot&amp;rsquo;]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;Fruit [&amp;lsquo;Apple&amp;rsquo;, &amp;lsquo;Banana&amp;rsquo;, &amp;lsquo;Tomato&amp;rsquo;]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here the grouped by function worked and it returned the category and the list of products in that category.&lt;/p&gt;

&lt;h2 id=&#34;understanding-actions&#34;&gt;Understanding Actions&lt;/h2&gt;

&lt;p&gt;Now you have filtered your data, mapped some functions on it. Done your computation. Now you want to get the data on your local machine or save it to a file. You will have to use actions for that. A comprehensive list of actions is provided &lt;a href=&#34;http://spark.apache.org/docs/latest/programming-guide.html#actions&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some of the most common actions that I tend to use are:&lt;/p&gt;

&lt;h5 id=&#34;1-collect&#34;&gt;1. Collect:&lt;/h5&gt;

&lt;p&gt;We have already used this actio many times. It takes the whole rdd and brings it back to the driver program.&lt;/p&gt;

&lt;h5 id=&#34;2-reduce&#34;&gt;2. Reduce:&lt;/h5&gt;

&lt;p&gt;Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduce(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y : x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;15&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;3-take&#34;&gt;3.take:&lt;/h5&gt;

&lt;p&gt;Return an list with the first n elements of the dataset.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[1, 2, 3]&lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&#34;4-takeordered&#34;&gt;4. takeOrdered:&lt;/h5&gt;

&lt;p&gt;Return the first n elements of the RDD using either their natural order or a custom comparator.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;takeOrdered(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; s:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;s)      &lt;span style=&#34;color:#75715e&#34;&gt;# descending order&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[23, 12, 5]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;rdd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;),(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;),(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;344&lt;/span&gt;),(&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;)])
rdd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;takeOrdered(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; s:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;s[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])      &lt;span style=&#34;color:#75715e&#34;&gt;# descending order&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span style=&#34;background-color: #FFF122; color:#000000&#34;&gt;[(12, 344), (3, 34), (23, 29)]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So now lets take a look at the Wordcount Again&lt;/p&gt;

&lt;h2 id=&#34;understanding-the-wordcount-example&#34;&gt;Understanding The WordCount Example&lt;/h2&gt;

&lt;p&gt;Now we sort of understand the transformations and the actions provided to us by Spark. It should not be difficult to understand the work count program now. Lets go through the program niw line by line.&lt;/p&gt;

&lt;p&gt;The first lines creates a RDD and distributeds to the workers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lines = sc.textFile(&amp;quot;data/cs100/lab1/shakespeare.txt&amp;quot;)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This RDD lines contains a list of strings that are actually the line in file. This RDD is of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&#39;word1 word2 word3&#39;,&#39;word4 word3 word2&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This next line is actually the workhorse function in the whole script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;counts = (lines.flatMap(lambda x: x.split(&#39; &#39;))          
                  .map(lambda x: (x, 1))                 
                  .reduceByKey(lambda x,y : x + y))      
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It contains a series of transformations that we do to the lines RDD. First of all we do a flatmap transformation. The flatmap transformation takes as input the lines and gives words as output. So after the flatmap transformation the RDD is of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&#39;word1&#39;,&#39;word2&#39;,&#39;word3&#39;,&#39;word4&#39;,&#39;word3&#39;,&#39;word2&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we do a map transformation on the flatmap output which converts the rdd to :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[(&#39;word1&#39;,1),(&#39;word2&#39;,1),(&#39;word3&#39;,1),(&#39;word4&#39;,1),(&#39;word3&#39;,1),(&#39;word2&#39;,1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we do a reduceByKey transformation which counts the number of time each word appeared. After which the rdd approaches the final desirable form.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[(&#39;word1&#39;,1),(&#39;word2&#39;,2),(&#39;word3&#39;,2),(&#39;word4&#39;,1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This next line is an action that takes the first 10 elements of the resulting RDD locally.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output = counts.take(10)                                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line just prints the output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (word, count) in output:                 
    print(&amp;quot;%s: %i&amp;quot; % (word, count))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;getting-serious&#34;&gt;Getting Serious&lt;/h2&gt;

&lt;p&gt;So till now we have talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don&amp;rsquo;t do wordcount in real life. We have to work on bigger problems which are much more complex. Worry not! whatever we have learned till now will let us do that and more.&lt;/p&gt;

&lt;p&gt;Lets work with a concrete example:
I will work on an example in which Greg Rada Worked on &lt;a href=&#34;http://grouplens.org/datasets/movielens/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Movielens&lt;/a&gt;
Data with &lt;a href=&#34;http://www.gregreda.com/2013/10/26/using-pandas-on-the-movielens-dataset/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Pandas&lt;/a&gt; (BTW a great resource to learn Pandas). This example takes care of every sort of transformation that you may like to do with this data.&lt;/p&gt;

&lt;p&gt;So lets first talk about the dataset. The movielens dataset contains a lot of files but we are going to be working with 3 files only:&lt;/p&gt;

&lt;p&gt;1) Users: This file name is kept as &amp;ldquo;u.user&amp;rdquo;, The columns in this file are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&#39;user_id&#39;, &#39;age&#39;, &#39;sex&#39;, &#39;occupation&#39;, &#39;zip_code&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) Ratings: This file name is kept as &amp;ldquo;u.data&amp;rdquo;, The columns in this file are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;unix_timestamp&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3) Movies: This file name is kept as &amp;ldquo;u.item&amp;rdquo;, The columns in this file are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[&#39;movie_id&#39;, &#39;title&#39;, &#39;release_date&#39;, &#39;video_release_date&#39;, &#39;imdb_url&#39;, and 18 more columns.....]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##What are the 25 most rated movies?
First of all lets load the data in different rdds. And see what the data contains.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;userRDD &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/vagrant/ml-100k/u.user&amp;#34;&lt;/span&gt;) 
ratingRDD &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/vagrant/ml-100k/u.data&amp;#34;&lt;/span&gt;) 
movieRDD &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;textFile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/vagrant/ml-100k/u.item&amp;#34;&lt;/span&gt;) 
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;userRDD:&amp;#34;&lt;/span&gt;,userRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ratingRDD:&amp;#34;&lt;/span&gt;,ratingRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;movieRDD:&amp;#34;&lt;/span&gt;,movieRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/data_def.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Seeing the data we note that to answer this question we will need to use the ratingRdd. But the ratingRDD does not have movie name. So we would have to merge movieRDD and ratingRDD. So lets see how we would do that in Spark.
Lets first do it step by step.Read the comments.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating.&lt;/span&gt;
RDD_movid_rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ratingRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x : (x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;RDD_movid_rating:&amp;#34;&lt;/span&gt;,RDD_movid_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title.&lt;/span&gt;
RDD_movid_title &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; movieRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x : (x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;RDD_movid_title:&amp;#34;&lt;/span&gt;,RDD_movid_title&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin()&lt;/span&gt;
rdd_movid_title_rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; RDD_movid_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;leftOuterJoin(RDD_movid_title)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_movid_title_rating:&amp;#34;&lt;/span&gt;,rdd_movid_title_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# use the RDD in previous step to create (movie,1) tuple pair RDD&lt;/span&gt;
rdd_title_rating &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd_movid_title_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; ))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_title_rating:&amp;#34;&lt;/span&gt;,rdd_title_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Use the reduceByKey transformation to reduce on the basis of movie_title&lt;/span&gt;
rdd_title_ratingcnt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rdd_title_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y: x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_title_ratingcnt:&amp;#34;&lt;/span&gt;,rdd_title_ratingcnt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Get the final answer by using takeOrdered Transformation&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#####################################&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;25 most rated movies:&amp;#34;&lt;/span&gt;,rdd_title_ratingcnt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;takeOrdered(&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#####################################&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/result_rating_cnt_25.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We could have done all this in a single command using the below command but the code is a little messy now. I did this to show that you can do things sequentially with Spark and you could bypass the process of variable creation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (((ratingRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x : (x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;])))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
     leftOuterJoin(movieRDD&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x : (x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
     map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
     reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y: x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
     takeOrdered(&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))


&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;div style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;margin-top: 9px; margin-bottom: 10px;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;center&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;lt;&lt;/span&gt;img src&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/images/result_rating_cnt_25_2.png&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;lt;/&lt;/span&gt;center&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/&lt;/span&gt;div&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##Which movies are most highly rated?&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we want to find the most highly rated 25 movvies using the same dataset. We actually want only those movies which have been rated atleast 100 times.
Lets do this using Spark:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We already have the RDD rdd_movid_title_rating: [(u&amp;#39;429&amp;#39;, (u&amp;#39;5&amp;#39;, u&amp;#39;Day the Earth Stood Still, The (1951)&amp;#39;))]&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# We create an RDD that contains sum of all the ratings for a particular movie&lt;/span&gt;

rdd_title_ratingsum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rdd_movid_title_rating&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
                        map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],int(x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
                        reduceByKey(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x,y:x&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;y))
                        
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_title_ratingsum:&amp;#34;&lt;/span&gt;,rdd_title_ratingsum&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Merge this data with the RDD rdd_title_ratingcnt we created in the last step &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# And use Map function to divide ratingsum by rating count.&lt;/span&gt;

rdd_title_ratingmean_rating_count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rdd_title_ratingsum&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
                                    leftOuterJoin(rdd_title_ratingcnt)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
                                    map(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:(x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],(float(x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))))
                                    
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_title_ratingmean_rating_count:&amp;#34;&lt;/span&gt;,rdd_title_ratingmean_rating_count&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# We could use take ordered here only but we want to only get the movies which have count&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# of ratings more than or equal to 100 so lets filter the data RDD.&lt;/span&gt;
rdd_title_rating_rating_count_gt_100 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (rdd_title_ratingmean_rating_count&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;
                                        filter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;))
                                        
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rdd_title_rating_rating_count_gt_100:&amp;#34;&lt;/span&gt;,rdd_title_rating_rating_count_gt_100&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;take(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Get the final answer by using takeOrdered Transformation&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#####################################&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;25 highly rated movies:&amp;#34;&lt;/span&gt;,
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; rdd_title_rating_rating_count_gt_100&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;takeOrdered(&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#####################################&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/result_top25_rating.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So Spark has Already provided an interface where we could apply transformations sequentially much easily than Hadoop.
And it is fast. While in hadoop things are a pain to do sequentially, the infrastructure that Spark provides seem to fit naturally into the analytics use case.&lt;/p&gt;

&lt;p&gt;Hopefully I&amp;rsquo;ve covered the basics well enough to pique your interest and help you get started with Spark. If I&amp;rsquo;ve missed something critical, feel free to let me know on Twitter or in the comments - I&amp;rsquo;d love constructive feedback.&lt;/p&gt;

&lt;p&gt;You can find the Jupyter notebook &lt;a href=&#34;http://nbviewer.ipython.org/github/MLWhiz/Spark_blog/blob/master/Spark_Part1.ipynb&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One of the newest and best resources that you can keep an eye on is the &lt;a href=&#34;https://www.coursera.org/specializations/big-data?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-NVLix0UOweGz5rWmJlt8.A&amp;siteID=lVarvwc5BD0-NVLix0UOweGz5rWmJlt8.A&amp;utm_content=3&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Big Data&lt;/a&gt; course in the &lt;a href=&#34;https://www.coursera.org/specializations/big-data?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-NVLix0UOweGz5rWmJlt8.A&amp;siteID=lVarvwc5BD0-NVLix0UOweGz5rWmJlt8.A&amp;utm_content=3&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Big Data Specialization&lt;/a&gt; from UCSanDiego&lt;/p&gt;

&lt;p&gt;Look out for these two books to learn more about Spark.&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34; href=&#34;https://www.amazon.com/gp/product/1491912766/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491912766&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=916f1678fb802e13211b4b1c648be75e&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491912766&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491912766&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34; href=&#34;https://www.amazon.com/gp/product/1617292605/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1617292605&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=89da1866198268847438c42ef14c4380&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1617292605&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1617292605&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;The first one of these is a bestseller. It presents 9 case studies of data analysis applications in various domains. The topics are diverse and the authors always use real world datasets. Beside learning Spark and a data science you will also have the opportunity to gain insight about topics like taxi traffic in NYC, deforestation or neuroscience. The second one is more of a reference that takes the reader on a tour of the Spark fundamentals, explaining the RDD data model in detail, after which it dives into the main functionality of Spark: Spark SQL, Spark Streaming, MLLib, SparkML, and GraphX. Later on, it covers the operational aspects of setting up a standalone Spark cluster, as well as running it on YARN and Mesos.&lt;/p&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop Mapreduce Streaming Tricks and Techniques</title>
      <link>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</link>
      <pubDate>Sat, 09 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</guid>
      <description>

&lt;p&gt;I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.&lt;/p&gt;

&lt;h2 id=&#34;using-shell-scripts-to-run-your-programs&#34;&gt;Using Shell Scripts to run your Programs&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://mlwhiz.com/images/I-love-bash-1024x220.png&#34; &gt;&lt;/p&gt;

&lt;p&gt;I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. &lt;em&gt;You can effectively organize your workflow by using shell scripts.&lt;/em&gt; Now Shell scripts are not as formidable as they sound. We wont be doing programming perse using these shell scripts(Though they are pretty good at that too), we will just use them to store commands that we need to use sequentially.&lt;/p&gt;

&lt;p&gt;Below is a sample of the shell script I use to run my Mapreduce Codes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/bin/bash
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Defining program variables&lt;/span&gt;
IP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/input&amp;#34;&lt;/span&gt;
OP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/output&amp;#34;&lt;/span&gt;
HADOOP_JAR_PATH&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&amp;#34;&lt;/span&gt;
MAPPER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test_m.py&amp;#34;&lt;/span&gt;
REDUCER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test_r.py&amp;#34;&lt;/span&gt;

hadoop fs -rmr -skipTrash&amp;amp;nbsp;$OP
hadoop jar&amp;amp;nbsp;$HADOOP_JAR_PATH &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-file&amp;amp;nbsp;$MAPPER -mapper &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python test_m.py&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-file&amp;amp;nbsp;$REDUCER -reducer &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python test_r.py&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;-input&amp;amp;nbsp;$IP -output&amp;amp;nbsp;$OP&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I generally save them as test_s.sh and whenever i need to run them i simply type &lt;code&gt;sh test_s.sh&lt;/code&gt;. This helps in three ways.
&lt;ul&gt;&lt;li&gt; It helps me to store hadoop commands in a manageable way. &lt;/li&gt;
&lt;li&gt; It is easy to run the mapreduce code using the shell script. &lt;/li&gt;
&lt;li&gt; &lt;em&gt;&lt;strong&gt;If the code fails, I do not have to manually delete the output directory&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;em&gt;
The simplification of anything is always sensational.
&lt;br&gt;&lt;/em&gt;
&lt;small&gt;Gilbert K. Chesterton&lt;/small&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;using-distributed-cache-to-provide-mapper-with-a-dictionary&#34;&gt;Using Distributed Cache to provide mapper with a dictionary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://mlwhiz.com/images/Game-Of-Thrones-Wallpaper-House-Sigils-1.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Often times it happens that you want that your Hadoop Mapreduce program is able to access some static file. This static file could be a dictionary, could be parameters for the program or could be anything. What distributed cache does is that it provides this file to all the mapper nodes so that you can use that file in any way across all your mappers.
Now this concept although simple would help you to think about Mapreduce in a whole new light.
Lets start with an example.
Supppose you have to create a sample Mapreduce program that reads a big file containing the information about all the characters in &lt;a href=&#34;http://www.hbo.com/game-of-thrones&#34;&gt;Game of Thrones&lt;/a&gt; stored as &lt;strong&gt;&lt;code&gt;&amp;rdquo;/data/characters/&amp;rdquo;&lt;/code&gt;&lt;/strong&gt;:
&lt;div style=&#34;width: 50%; margin: 0 auto;&#34;&gt;
&lt;table class=&#34;table&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Cust_ID&lt;/th&gt;
&lt;th&gt;User_Name&lt;/th&gt;
&lt;th&gt;House&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Daenerys Targaryen&lt;/td&gt;
&lt;td&gt;Targaryen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Tyrion Lannister&lt;/td&gt;
&lt;td&gt;Lannister&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Cersei Lannister&lt;/td&gt;
&lt;td&gt;Lannister&lt;/td&gt;
&lt;/tr&gt;
&lt;tr &gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Robert Baratheon&lt;/td&gt;
&lt;td&gt;Baratheon&lt;/td&gt;
&lt;/tr&gt;
&lt;tr &gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Robb Stark&lt;/td&gt;
&lt;td&gt;Stark&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;But you dont want to use the dead characters in the file for the analysis you want to do. &lt;em&gt;You want to count the number of living characters in Game of Thrones grouped by their House&lt;/em&gt;. (I know its easy!!!!!)
One thing you could do is include an if statement in your Mapper Code which checks if the persons ID is 4 then exclude it from the mapper and such.
But the problem is that you would have to do it again and again for the same analysis as characters die like flies when it comes to George RR Martin.(Also where is the fun in that)
So you create a file which contains the Ids of all the dead characters at &lt;strong&gt;&lt;code&gt;&amp;rdquo;/data/dead_characters.txt&amp;rdquo;&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;div style=&#34;width: 50%; margin: 0 auto;&#34;&gt;
&lt;table class=&#34;table&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Died&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Whenever you have to run the analysis you can just add to this file and you wont have to change anything in the code.
Also sometimes this file would be long and you would not want to clutter your code with IDs and such.&lt;/p&gt;

&lt;p&gt;So How Would we do it.
Let&amp;rsquo;s go in a step by step way around this.
We will create a shell script, a mapper script and a reducer script for this task.&lt;/p&gt;

&lt;h2 id=&#34;1-shell-script&#34;&gt;1) Shell Script&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/bin/bash&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#Defining program variables&lt;/span&gt;
DC&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/dead_characters.txt&amp;#34;&lt;/span&gt;
IP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/characters&amp;#34;&lt;/span&gt;
OP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data/output&amp;#34;&lt;/span&gt;
HADOOP_JAR_PATH&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&amp;#34;&lt;/span&gt;
MAPPER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;got_living_m.py&amp;#34;&lt;/span&gt;
REDUCER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;got_living_r.py&amp;#34;&lt;/span&gt;

hadoop jar&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;HADOOP_JAR_PATH \
&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;file&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;MAPPER &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;mapper &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python got_living_m.py&amp;#34;&lt;/span&gt; \
&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;file&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;REDUCER &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;reducer &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python got_living_r.py&amp;#34;&lt;/span&gt; \
&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;cacheFile&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;DC&lt;span style=&#34;color:#75715e&#34;&gt;#ref \&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;input&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;IP &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;output&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;nbsp;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;OP&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how we use the &lt;code&gt;&amp;rdquo;-cacheFile&amp;rdquo;&lt;/code&gt; option here. We have specified that we will refer to the file that has been provided in the Distributed cache as &lt;code&gt;#ref&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next is our Mapper Script.&lt;/p&gt;

&lt;h2 id=&#34;2-mapper-script&#34;&gt;2) Mapper Script&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
dead_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; set()

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;read_cache&lt;/span&gt;():
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ref&amp;#39;&lt;/span&gt;):
		id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()
		dead_ids&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add(id)

read_cache()

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
	rec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# Split using Delimiter &amp;#34;|&amp;#34;&lt;/span&gt;
	id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rec[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
    house &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rec[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; id &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dead_ids:
    	&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; (house,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And our Reducer Script.&lt;/p&gt;

&lt;h2 id=&#34;3-reducer-script&#34;&gt;3) Reducer Script&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
current_key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
	line &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()
	rec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
	key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rec[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]	
	value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(rec[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
	
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; current_key &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; key:
		count &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; value
	&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; current_key:
			&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;(key,str(count))		
		current_key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
		count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; value

&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; current_key &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; key:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;(key,str(count))	&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This was a simple program and the output will be just what you expected and not very exciting. &lt;em&gt;&lt;strong&gt;But the Technique itself solves a variety of common problems. You can use it to pass any big dictionary to your Mapreduce Program&lt;/strong&gt;&lt;/em&gt;. Atleast thats what I use this feature mostly for.
Hope You liked it. Will try to expand this post with more tricks.&lt;/p&gt;

&lt;p&gt;The codes for this post are posted at github &lt;a href=&#34;https://github.com/MLWhiz/Hadoop-Mapreduce-Tricks&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Other Great Learning Resources For Hadoop:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CB0QFjAA&amp;url=http%3A%2F%2Fwww.michael-noll.com%2Ftutorials%2Fwriting-an-hadoop-mapreduce-program-in-python%2F&amp;ei=8RRVVdP2IMe0uQShsYDYBg&amp;usg=AFQjCNH3DqrlSIG8D-K8jgQWTALic1no5A&amp;sig2=BivwTW6mdJs5c9w9VaSK2Q&amp;bvm=bv.93112503,d.c2E&#34;&gt;Michael Noll&amp;rsquo;s Hadoop Mapreduce Tutorial&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0CCMQFjAB&amp;url=http%3A%2F%2Fhadoop.apache.org%2Fdocs%2Fr1.2.1%2Fstreaming.html&amp;ei=8RRVVdP2IMe0uQShsYDYBg&amp;usg=AFQjCNEIB4jmqcBs-GepHdn7DRxqTI9zXA&amp;sig2=nYkAnDjjjaum5YVlYuMUJQ&amp;bvm=bv.93112503,d.c2E&#34;&gt;Apache&amp;rsquo;s Hadoop Streaming Documentation&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;Also I like these books a lot. Must have for a Hadooper&amp;hellip;.&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/1785887211/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1785887211&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=a0e7b4f0b2ea4a5146042890e1c04f7e&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1785887211&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1785887211&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;

&lt;/t&gt;&lt;/t&gt;

&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491901632&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=4122280e94f7bbd0ceebc9d13e60d103&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491901632&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491901632&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;The first book is a guide for using Hadoop as well as spark with Python. While the second one contains a detailed overview of all the things in Hadoop. Its the definitive guide.&lt;/p&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Learning pyspark  Installation  Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</guid>
      <description>

&lt;p&gt;This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.&lt;/p&gt;

&lt;p&gt;The installation is pretty simple. These steps were done on Mac OS Mavericks but should work for Linux too. Here are the steps for the installation:&lt;/p&gt;

&lt;h2 id=&#34;1-download-the-binaries&#34;&gt;1. Download the Binaries:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;Spark : http:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apache&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;org&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;downloads&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;html
Scala : http:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;www&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scala&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;lang&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;org&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;download&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;

Dont use Latest Version of Scala, Use Scala &lt;span style=&#34;color:#ae81ff&#34;&gt;2.10&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;2-add-these-lines-to-your-bash-profile&#34;&gt;2. Add these lines to your .bash_profile:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;export SCALA_HOME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;your_path_to_scala
export SPARK_HOME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;your_path_to_spark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;3-build-spark-this-will-take-time&#34;&gt;3. Build Spark(This will take time):&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;brew install sbt
cd $SPARK_HOME
sbt/sbt assembly&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;4-start-the-pyspark-shell&#34;&gt;4. Start the Pyspark Shell:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$SPARK_HOME/bin/pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And Voila. You are running pyspark on your Machine&lt;/p&gt;

&lt;p&gt;To check that everything is properly installed, Lets run a simple program:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should return 3.
So Now Just Run Hadoop On your Machine and then run pyspark Using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /usr/local/hadoop/
bin/start-all.sh
jps
$SPARK_HOME/bin/pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop, Mapreduce and More  Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</link>
      <pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</guid>
      <description>

&lt;p&gt;It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all.
What I understood finally is that Hadoop is basically comprised of 3 elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A File System&lt;/li&gt;
&lt;li&gt;Map  Reduce&lt;/li&gt;
&lt;li&gt;Its many individual Components.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets go through each of them one by one.&lt;/p&gt;

&lt;h2 id=&#34;1-hadoop-as-a-file-system&#34;&gt;1. Hadoop as a File System:&lt;/h2&gt;

&lt;p&gt;One of the main things that Hadoop provides is cheap data storage. What happens intrinsically is that the Hadoop system takes a file, cuts it into chunks and keeps those chunks at different places in a cluster. Suppose you have a big big file in your local system and you want that file to be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;On the cloud for easy access&lt;/li&gt;
&lt;li&gt;Processable in human time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The one thing you can look forward to is Hadoop.&lt;/p&gt;

&lt;p&gt;Assuming that you have got hadoop installed on the amazon cluster you are working on.&lt;/p&gt;

&lt;h3 id=&#34;start-the-hadoop-cluster&#34;&gt;Start the Hadoop Cluster:&lt;/h3&gt;

&lt;p&gt;You need to run the following commands to start the hadoop cluster(Based on location of hadoop installation directory):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /usr/local/hadoop/
bin/start-all.sh
jps&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Adding File to HDFS:&lt;/strong&gt; Every command in Hadoop starts with hadoop fs and the rest of it works like the UNIX syntax. To add a file purchases.txt to the hdfs system:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;hadoop fs -put purchases.txt /usr/purchases.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;2-hadoop-for-map-reduce&#34;&gt;2. Hadoop for Map-Reduce:&lt;/h2&gt;

&lt;p&gt;MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.&lt;/p&gt;

&lt;p&gt;While Hadoop is implemented in Java, you can use almost any language to do map-reduce in hadoop using hadoop streaming. Suppose you have a big file containing the Name of store and sales of store each hour. And you want to find out the sales per store using map-reduce. Lets Write a sample code for that:&lt;/p&gt;

&lt;p&gt;InputFile&lt;/p&gt;

&lt;pre style=&#34;font-family:courier new,monospace; background-color:#f6c6529c; color:#000000&#34;&gt;A,300,12:00
B,234,1:00
C,234,2:00
D,123,3:00
A,123,1:00
B,346,2:00
&lt;/pre&gt;

&lt;p&gt;Mapper.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mapper&lt;/span&gt;():
    &lt;span style=&#34;color:#75715e&#34;&gt;# The Mapper takes inputs from stdin and prints out store name and value&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
        data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;)
        storeName,Value,time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(storeName,Value)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Reducer.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;reducer&lt;/span&gt;():
    &lt;span style=&#34;color:#75715e&#34;&gt;# The reducer takes inputs from mapper and prints out aggregated store name and value&lt;/span&gt;
    salesTotal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    oldKey &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
        data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;#Adding a little bit of Defensive programming&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(data) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        curKey,curVal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; oldKey adn oldKey &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; curKey:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(oldKey,salesTotal)
            salesTotal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        oldKey&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;curKey
        salesTotal &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; curVal
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; oldkey&lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt;None:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(oldKey,salesTotal)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running the program on shell using pipes&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;textfile.txt | ./mapper.py | sort | ./reducer.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running the program on mapreduce using Hadoop Streaming&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;hadoop jar contrib/streaming/hadoop-*streaming*.jar /
-file mapper.py -mapper mapper.py /
-file reducer.py -reducer reducer.py /
-input /inputfile -output /outputfile&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;3-hadoop-components&#34;&gt;3. Hadoop Components:&lt;/h2&gt;

&lt;p&gt;Now if you have been following Hadoop you might have heard about Apache, Cloudera, HortonWorks etc. All of these are Hadoop vendors who provide Hadoop Along with its components. I will talk about the main component of Hadoop here  Hive.
So what exactly is Hive: Hive is a SQL like interface to map-reduce queries. So if you dont understand all the hocus-pocus of map-reduce but know SQL, you can do map-reduce via Hive.
Seems Promising? It is.
While the syntax is mainly SQL, it is still a little different and there are some quirks that we need to understand to work with Hive.
First of all lets open hive command prompt: For that you just have to type hive, and voila you are in.
Here are some general commands&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;show databases  &lt;span style=&#34;color:#75715e&#34;&gt;#   -- See all Databases&lt;/span&gt;
use database     &lt;span style=&#34;color:#75715e&#34;&gt;#     -- Use a particular Database&lt;/span&gt;
show tables       &lt;span style=&#34;color:#75715e&#34;&gt;#     -- See all tables in a particular Database&lt;/span&gt;
describe table    &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Creating an external table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;CREATE EXTERNAL TABLE IF NOT EXISTS BXDataSet
&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;ISBN STRING,BookTitle STRING, ImageURLL STRING&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
ROW FORMAT DELIMITED  FIELDS TERMINATED BY ; STORED AS TEXTFILE;
LOAD DATA INPATH /user/book.csv OVERWRITE INTO TABLE BXDataSet;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The query commands work the same way as in SQL. You can do all the group by and hive will automatically convert it in map-reduce:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;select&lt;/span&gt; * from tablename;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Stay Tuned for Part 2  Where we will talk about another components of Hadoop  PIG
To learn more about hadoop in the meantime these are the books I recommend:&lt;/p&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491901632&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=4122280e94f7bbd0ceebc9d13e60d103&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491901632&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491901632&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
  </channel>
</rss>