<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Python on MLWhiz</title>
    <link>https://mlwhiz.com/tags/python/</link>
    <description>Recent content in Python on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/python/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Be careful with using for loops</title>
      <link>https://mlwhiz.com/blog/2019/04/22/python_forloops/</link>
      <pubDate>Tue, 23 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/22/python_forloops/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/python2/loops.jpeg"></media:content>
      

      
      <description>Python provides us with many styles of coding.
In a way, it is pretty inclusive.
One can come from any language and start writing Python.
However, learning to write a language and writing a language in an optimized way are two different things.
In this series of posts named Python Shorts, I will explain some simple but very useful constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.</description>

      <content:encoded>  
        
        <![CDATA[    Python provides us with many styles of coding.
In a way, it is pretty inclusive.
One can come from any language and start writing Python.
However, learning to write a language and writing a language in an optimized way are two different things.
In this series of posts named Python Shorts, I will explain some simple but very useful constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.
In this post, I am going to talk about for loops in Python and how you should avoid them whenever possible.
3 Ways of writing a for loop: Let me explain this with a simple example statement.
Suppose you want to take the sum of squares in a list.
This is a valid problem we all face in machine learning whenever we want to calculate the distance between two points in n dimension.
You can do this using loops easily.
In fact, I will show you** three ways to do the same task which I have seen people use and let you choose for yourself which you find the best.**
x = [1,3,5,7,9] sum_squared = 0 for i in range(len(x)): sum_squared&#43;=x[i]**2 Whenever I see the above code in a python codebase, I understand that the person has come from C or Java background.
A **slightly more pythonic way **of doing the same thing is:
x = [1,3,5,7,9] sum_squared = 0 for y in x: sum_squared&#43;=y**2 Better.
I didn’t index the list. And my code is more readable.
But still, the pythonic way to do it is in one line.
x = [1,3,5,7,9] sum_squared = sum([y**2 for y in x]) This approach is called List Comprehension, and this may very well be one of the reasons that I love Python.
You can also use if in a list comprehension.
Let’s say we wanted a list of squared numbers for even numbers only.
x = [1,2,3,4,5,6,7,8,9] even_squared = [y**2 for y in x if y%2==0] -------------------------------------------- [4,16,36,64] if-else?
What if we wanted to have the number squared for even and cubed for odd?
x = [1,2,3,4,5,6,7,8,9] squared_cubed = [y**2 if y%2==0 else y**3 for y in x] -------------------------------------------- [1, 4, 27, 16, 125, 36, 343, 64, 729] Great!!!
  So basically follow specific guidelines: Whenever you feel like writing a for statement, you should ask yourself the following questions,
 Can it be done without a for loop? Most Pythonic
 Can it be done using list comprehension? If yes, use it.
 Can I do it without indexing arrays? if not, think about using enumerate
  What is enumerate?
Sometimes we need both the index in an array as well as the value in an array.
In such cases, I prefer to use enumerate rather than indexing the list.
L = [&amp;#39;blue&amp;#39;, &amp;#39;yellow&amp;#39;, &amp;#39;orange&amp;#39;] for i, val in enumerate(L): print(&amp;#34;index is %dand value is %s&amp;#34; % (i, val)) --------------------------------------------------------------- index is 0 and value is blue index is 1 and value is yellow index is 2 and value is orange The rule is:
 Never index a list, if you can do without it.  Try Using Dictionary Comprehension Also try using dictionary comprehension, which is a relatively new addition in Python. The syntax is pretty similar to List comprehension.
Let me explain using an example. I want to get a dictionary with (key: squared value) for every value in x.
x = [1,2,3,4,5,6,7,8,9] {k:k**2 for k in x} --------------------------------------------------------- {1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} What if I want a dict only for even values?
x = [1,2,3,4,5,6,7,8,9] {k:k**2 for k in x if x%2==0} --------------------------------------------------------- {2: 4, 4: 16, 6: 36, 8: 64} What if we want squared value for even key and cubed number for the odd key?
x = [1,2,3,4,5,6,7,8,9] {k:k**2 if k%2==0 else k**3 for k in x} --------------------------------------------------------- {1: 1, 2: 4, 3: 27, 4: 16, 5: 125, 6: 36, 7: 343, 8: 64, 9: 729} Conclusion To conclude, I will say that while it might seem easy to transfer the knowledge you acquired from other languages to Python, you won’t be able to appreciate the beauty of Python if you keep doing that. Python is much more powerful when we use its ways and decidedly much more fun.
So, use List Comprehensions and Dict comprehensions when you need afor loop. Use enumerate if you need array index.
 Avoid for loops like plague  Your code will be much more readable and maintainable in the long run.
Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog to be informed about them.
As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Python Pro Tip: Start using Python defaultdict and Counter in place of dictionary</title>
      <link>https://mlwhiz.com/blog/2019/04/22/python_defaultdict/</link>
      <pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/22/python_defaultdict/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/python1/likeaboss.jpeg"></media:content>
      

      
      <description>Learning a language is easy. Whenever I start with a new language, I focus on a few things in below order, and it is a breeze to get started with writing code in any language.
 Operators and Data Types: &#43;,-,int,float,str
 Conditional statements: if,else,case,switch
 Loops: For, while
 Data structures: List, Array, Dict, Hashmaps
 Define Function
  However, learning to write a language and writing a language in an optimized way are two different things.</description>

      <content:encoded>  
        
        <![CDATA[    Learning a language is easy. Whenever I start with a new language, I focus on a few things in below order, and it is a breeze to get started with writing code in any language.
 Operators and Data Types: &#43;,-,int,float,str
 Conditional statements: if,else,case,switch
 Loops: For, while
 Data structures: List, Array, Dict, Hashmaps
 Define Function
  However, learning to write a language and writing a language in an optimized way are two different things.
Every Language has some ingredients which make it unique.
Yet, a new programmer to any language will always do some forced overfitting. A Java programmer, new to python, for example, might write this code to add numbers in a list.
x=[1,2,3,4,5] sum_x = 0 for i in range(len(x)): sum_x&#43;=x[i] While a python programmer will naturally do this:
sum_x = sum(x) In this series of posts named ‘Python Shorts’, I will explain some simple constructs that Python provides, some essential tips and some use cases I come up with regularly in my Data Science work.
This series is about efficient and readable code.
Counter and defaultdict — Use Cases   Let’s say I need to count the number of word occurrences in a piece of text. Maybe for a book like Hamlet. How could I do that?
Python always provides us with multiple ways to do the same thing. But only one way that I find elegant.
This is a Naive Python implementation using the dict object.
text = &amp;#34;I need to count the number of word occurrences in a piece of text. How could I do that? Python provides us with multiple ways to do the same thing. But only one way I find beautiful.&amp;#34; word_count_dict = {} for w in text.split(&amp;#34; &amp;#34;): if w in word_count_dict: word_count_dict[w]&#43;=1 else: word_count_dict[w]=1 We could use defaultdict to reduce the number of lines in the code.
from Collections import defaultdict word_count_dict = defaultdict(int) for w in text.split(&amp;#34; &amp;#34;): word_count_dict[w]&#43;=1 We could also have used Counter to do this.
from Collections import Counter word_count_dict = Counter() for w in text.split(&amp;#34; &amp;#34;): word_count_dict[w]&#43;=1 If we use Counter, we can also get the most common words using a simple function.
word_count_dict.most_common(10) --------------------------------------------------------------- [(&amp;#39;I&amp;#39;, 3), (&amp;#39;to&amp;#39;, 2), (&amp;#39;the&amp;#39;, 2)] Other use cases of Counter:
# Count Characters Counter(&amp;#39;abccccccddddd&amp;#39;) --------------------------------------------------------------- Counter({&amp;#39;a&amp;#39;: 1, &amp;#39;b&amp;#39;: 1, &amp;#39;c&amp;#39;: 6, &amp;#39;d&amp;#39;: 5}) # Count List elements Counter([1,2,3,4,5,1,2]) --------------------------------------------------------------- Counter({1: 2, 2: 2, 3: 1, 4: 1, 5: 1}) So, why ever use defaultdict ? Notice that in Counter, the value is always an integer.
What if we wanted to parse through a list of tuples and wanted to create a dictionary of key and list of values.
The main functionality provided by a defaultdict is that it defaults a key to empty/zero if it is not found in the defaultdict.
s = [(&amp;#39;color&amp;#39;, &amp;#39;blue&amp;#39;), (&amp;#39;color&amp;#39;, &amp;#39;orange&amp;#39;), (&amp;#39;color&amp;#39;, &amp;#39;yellow&amp;#39;), (&amp;#39;fruit&amp;#39;, &amp;#39;banana&amp;#39;), (&amp;#39;fruit&amp;#39;, &amp;#39;orange&amp;#39;),(&amp;#39;fruit&amp;#39;,&amp;#39;banana&amp;#39;)] d = defaultdict(list) for k, v in s: d[k].append(v) print(d) --------------------------------------------------------------- defaultdict(&amp;lt;class &amp;#39;list&amp;#39;&amp;gt;, {&amp;#39;color&amp;#39;: [&amp;#39;blue&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;yellow&amp;#39;], &amp;#39;fruit&amp;#39;: [&amp;#39;banana&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;banana&amp;#39;]}) banana comes two times in fruit, we could use set
d = defaultdict(set) for k, v in s: d[k].add(v) print(d) --------------------------------------------------------------- defaultdict(&amp;lt;class &amp;#39;set&amp;#39;&amp;gt;, {&amp;#39;color&amp;#39;: {&amp;#39;yellow&amp;#39;, &amp;#39;blue&amp;#39;, &amp;#39;orange&amp;#39;}, &amp;#39;fruit&amp;#39;: {&amp;#39;banana&amp;#39;, &amp;#39;orange&amp;#39;}}) Conclusion To conclude, I will say that there is always a beautiful way to do anything in Python. Search for it before you write code. Going to StackOverflow is okay. I go there a lot of times when I get stuck. Always Remember:
 Creating a function for what already is provided is not pythonic.
 Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
If you liked this post do share. It will help increase coverage for this post. I am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>3 Awesome Visualization Techniques for every dataset</title>
      <link>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/visualizations/football.jpeg"></media:content>
      

      
      <description>Visualizations are awesome. However, a good visualization is annoyingly hard to make.
Moreover, it takes time and effort when it comes to present these visualizations to a bigger audience.
We all know how to make Bar-Plots, Scatter Plots, and Histograms, yet we don&amp;amp;rsquo;t pay much attention to beautify them.
This hurts us - our credibility with peers and managers. You won&amp;amp;rsquo;t feel it now, but it happens.</description>

      <content:encoded>  
        
        <![CDATA[    Visualizations are awesome. However, a good visualization is annoyingly hard to make.
Moreover, it takes time and effort when it comes to present these visualizations to a bigger audience.
We all know how to make Bar-Plots, Scatter Plots, and Histograms, yet we don&amp;rsquo;t pay much attention to beautify them.
This hurts us - our credibility with peers and managers. You won&amp;rsquo;t feel it now, but it happens.
Also, I find it essential to reuse my code. Every time I visit a new dataset do I need to start again? Some reusable ideas of graphs that can help us to find information about the data FAST.
In this post, I am also going to talk about 3 cool visual tools:
 Categorical Correlation with Graphs, Pairplots, Swarmplots and Graph Annotations using Seaborn.  In short, this post is about useful and presentable graphs.
I will be using data from FIFA 19 complete player dataset on kaggle - Detailed attributes for every player registered in the latest edition of FIFA 19 database.
Since the Dataset has many columns, we will only focus on a subset of categorical and continuous columns.
import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) player_df = pd.read_csv(&amp;#34;../input/data.csv&amp;#34;) numcols = [ &amp;#39;Overall&amp;#39;, &amp;#39;Potential&amp;#39;, &amp;#39;Crossing&amp;#39;,&amp;#39;Finishing&amp;#39;, &amp;#39;ShortPassing&amp;#39;, &amp;#39;Dribbling&amp;#39;,&amp;#39;LongPassing&amp;#39;, &amp;#39;BallControl&amp;#39;, &amp;#39;Acceleration&amp;#39;, &amp;#39;SprintSpeed&amp;#39;, &amp;#39;Agility&amp;#39;, &amp;#39;Stamina&amp;#39;, &amp;#39;Value&amp;#39;,&amp;#39;Wage&amp;#39;] catcols = [&amp;#39;Name&amp;#39;,&amp;#39;Club&amp;#39;,&amp;#39;Nationality&amp;#39;,&amp;#39;Preferred Foot&amp;#39;,&amp;#39;Position&amp;#39;,&amp;#39;Body Type&amp;#39;] # Subset the columns player_df = player_df[numcols&#43; catcols] # Few rows of data player_df.head(5)   Player Data    This is a nicely formatted data, yet we need to do some preprocessing to the Wage and Value columns(as they are in Euro and contain strings) to make them numeric for our subsequent analysis.
def wage_split(x): try: return int(x.split(&amp;#34;K&amp;#34;)[0][1:]) except: return 0 player_df[&amp;#39;Wage&amp;#39;] = player_df[&amp;#39;Wage&amp;#39;].apply(lambda x : wage_split(x)) def value_split(x): try: if &amp;#39;M&amp;#39; in x: return float(x.split(&amp;#34;M&amp;#34;)[0][1:]) elif &amp;#39;K&amp;#39; in x: return float(x.split(&amp;#34;K&amp;#34;)[0][1:])/1000 except: return 0 player_df[&amp;#39;Value&amp;#39;] = player_df[&amp;#39;Value&amp;#39;].apply(lambda x : value_split(x)) Categorical Correlation with Graphs: In Simple terms, Correlation is a measure of how two variables move together.
For example, In the real world, Income and Spend are positively correlated. If one increases the other also increases.
Academic Performance and Video Games Usage is negatively correlated. Increase in one predicts a decrease in another.
So if our predictor variable is positively or negatively correlated with our target variable, it is valuable.
I feel that Correlations among different variables are a pretty good thing to do when we try to understand our data.
We can create a pretty good correlation plot using Seaborn easily.
corr = player_df.corr() g = sns.heatmap(corr, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&amp;#34;shrink&amp;#34;: .5}, annot=True, fmt=&amp;#39;.2f&amp;#39;, cmap=&amp;#39;coolwarm&amp;#39;) sns.despine() g.figure.set_size_inches(14,10) plt.show()   Where did all the categorical variables go?    But do you notice any problem?
Yes, this graph only calculates Correlation between Numerical columns. What if my target variable is Club or Position?
I want to be able to get a correlation among three different cases, and we use the following metrics of correlation to calculate these:
1. Numerical Variables We already have this in the form of Pearson&amp;rsquo;s Correlation which is a measure of how two variables move together. This Ranges from [-1,1]
2. Categorical Variables We will use Cramer&amp;rsquo;s V for categorical-categorical cases. It is the intercorrelation of two discrete variables and used with variables having two or more levels. It is a symmetrical measure as in the order of variable does not matter. Cramer(A,B) == Cramer(B,A).
For Example: In our dataset, Club and Nationality must be somehow correlated.
Let us check this using a stacked graph which is an excellent way to understand distribution between categorical vs. categorical variables. Note that we use a subset of data since there are a lot of nationalities and club in this data.
We keep only the best teams(Kept FC Porto just for more diversity in the sample)and the most common nationalities.
  Note that Club preference says quite a bit about Nationality: knowing the former helps a lot in predicting the latter.
We can see that if a player belongs to England, it is more probable that he plays in Chelsea or Manchester United and not in FC Barcelona or Bayern Munchen or Porto.
So there is some information present here. Cramer&amp;rsquo;s V captures the same information.
If all clubs have the same proportion of players from every nationality, Cramer&amp;rsquo;s V is 0.
If Every club prefers a single nationality Cramer&amp;rsquo;s V ==1, for example, all England player play in Manchester United, All Germans in Bayern Munchen and so on.
In all other cases, it ranges from [0,1]
3. Numerical and Categorical variables We will use the Correlation Ratio for categorical-continuous cases.
Without getting into too much Maths, it is a measure of Dispersion.
 Given a number can we find out which category it belongs to?
 For Example:
Suppose we have two columns from our dataset: SprintSpeed and Position:
 GK: 58(De Gea),52(T. Courtois), 58(M. Neuer), 43(G. Buffon) CB: 68(D. Godin), 59(V. Kompany), 73(S. Umtiti), 75(M. Benatia) ST: 91(C.Ronaldo), 94(G. Bale), 80(S.Aguero), 76(R. Lewandowski)  As you can see these numbers are pretty predictive of the bucket they fall into and thus high Correlation Ratio.
If I know the sprint speed is more than 85, I can definitely say this player plays at ST.
This ratio also ranges from [0,1]
The code to do this is taken from the dython package. I won&amp;rsquo;t write too much into code which you can anyway find in my Kaggle Kernel. The final result looks something like:
player_df = player_df.fillna(0) results = associations(player_df,nominal_columns=catcols,return_results=True)   Categorical vs. Categorical, Categorical vs. Numeric, Numeric vs. Numeric. Much more interesting    Isn&amp;rsquo;t it Beautiful?
We can understand so much about Football just by looking at this data. For Example:
 The position of a player is highly correlated with dribbling ability. You won&amp;rsquo;t play Messi at the back. Right?
 Value is more highly correlated with passing and ball control than dribbling. The rule is to pass the ball always. Neymar I am looking at you.
 Club and Wage have high Correlation. To be expected.
 Body Type and Preferred Foot is correlated highly. Does that mean if you are Lean, you are most likely left-footed? Doesn&amp;rsquo;t make much sense. One can investigate further.
  Moreover, so much info we could find with this simple graph which was not visible in the typical correlation plot without Categorical Variables.
I leave it here at that. One can look more into the chart and find more meaningful results, but the point is that this makes life so much easier to find patterns.
Pairplots While I talked a lot about correlation, it is a fickle metric.
To understand what I mean let us see one example.
Anscombe&amp;rsquo;s quartet comprises four datasets that have nearly identical Correlation of 1, yet have very different distributions and appear very different when graphed.
  Anscombe Quartet - Correlations can be fickle.    Thus sometimes it becomes crucial to plot correlated data. And see the distributions individually.
Now we have many columns in our dataset. Graphing them all would be so much effort.
No, it is a single line of code.
filtered_player_df = player_df[(player_df[&amp;#39;Club&amp;#39;].isin([&amp;#39;FC Barcelona&amp;#39;, &amp;#39;Paris Saint-Germain&amp;#39;, &amp;#39;Manchester United&amp;#39;, &amp;#39;Manchester City&amp;#39;, &amp;#39;Chelsea&amp;#39;, &amp;#39;Real Madrid&amp;#39;,&amp;#39;FC Porto&amp;#39;,&amp;#39;FC Bayern München&amp;#39;])) &amp;amp; (player_df[&amp;#39;Nationality&amp;#39;].isin([&amp;#39;England&amp;#39;, &amp;#39;Brazil&amp;#39;, &amp;#39;Argentina&amp;#39;, &amp;#39;Brazil&amp;#39;, &amp;#39;Italy&amp;#39;,&amp;#39;Spain&amp;#39;,&amp;#39;Germany&amp;#39;])) ] # Single line to create pairplot g = sns.pairplot(filtered_player_df[[&amp;#39;Value&amp;#39;,&amp;#39;SprintSpeed&amp;#39;,&amp;#39;Potential&amp;#39;,&amp;#39;Wage&amp;#39;]])   Pretty Good. We can see so much in this graph.
 Wage and Value are highly correlated.
 Most of the other values are correlated too. However, the trend of potential vs. value is unusual. We can see how the value increases exponentially as we reach a particular potential threshold. This information can be helpful in modeling. Can use some transformation on Potential to make it more correlated?
  Caveat: No categorical columns.
Can we do better? We always can.
g = sns.pairplot(filtered_player_df[[&amp;#39;Value&amp;#39;,&amp;#39;SprintSpeed&amp;#39;,&amp;#39;Potential&amp;#39;,&amp;#39;Wage&amp;#39;,&amp;#39;Club&amp;#39;]],hue = &amp;#39;Club&amp;#39;)   So much more info. Just by adding the hue parameter as a categorical variable Club.
 Porto&amp;rsquo;s Wage distribution is too much towards the lower side. I don&amp;rsquo;t see that steep distribution in value of Porto players. Porto&amp;rsquo;s players would always be looking out for an opportunity. See how a lot of pink points(Chelsea) form sort of a cluster on Potential vs. wage graph. Chelsea has a lot of high potential players with lower wages. Needs more attention.  I already know some of the points on the Wage/Value Subplot.
The blue point for wage 500k is Messi. Also, the orange point having more value than Messi is Neymar.
Although this hack still doesn&amp;rsquo;t solve the Categorical problem, I have something cool to look into categorical variables distribution. Though individually.
SwarmPlots How to see the relationship between categorical and numerical data?
Enter into picture Swarmplots, just like their name. A swarm of points plotted for each category with a little dispersion on the y-axis to make them easier to see.
They are my current favorite for plotting such relationships.
g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding  size = 7) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(14,10) plt.show()   Swarmplot...    Why don&amp;rsquo;t I use Boxplots? Where are the median values? Can I plot that? Obviously. Overlay a bar plot on top, and we have a great looking graph.
g = sns.boxplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding  size = 7,color = &amp;#39;black&amp;#39;) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(12,8) plt.show()   Swarmplot&#43;Boxplot, Interesting    Pretty good. We can see the individual points on the graph, see some statistics and understand the wage difference categorically.
The far right point is Messi. However, I should not have to tell you that in a text below the chart. Right?
This graph is going to go in a presentation. Your boss says. I want to write Messi on this graph. Comes into picture annotations.
max_wage = filtered_player_df.Wage.max() max_wage_player = filtered_player_df[(player_df[&amp;#39;Wage&amp;#39;] == max_wage)][&amp;#39;Name&amp;#39;].values[0] g = sns.boxplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding  size = 7,color=&amp;#39;black&amp;#39;) # remove the top and right line in graph sns.despine() # Annotate. xy for coordinate. max_wage is x and 0 is y. In this plot y ranges from 0 to 7 for each level # xytext for coordinates of where I want to put my text plt.annotate(s = max_wage_player, xy = (max_wage,0), xytext = (500,1), # Shrink the arrow to avoid occlusion arrowprops = {&amp;#39;facecolor&amp;#39;:&amp;#39;gray&amp;#39;, &amp;#39;width&amp;#39;: 3, &amp;#39;shrink&amp;#39;: 0.03}, backgroundcolor = &amp;#39;white&amp;#39;) g.figure.set_size_inches(12,8) plt.show()   Annotated, Statistical Info and point swarm. To the presentation, I go.     See Porto Down there. Competing with the giants with such a small wage budget. So many Highly paid players in Real and Barcelona. Manchester City has the highest median Wage. Manchester United and Chelsea believes in equality. Many players clustered in around the same wage scale. I am happy that while Neymar is more valued than Messi, Messi and Neymar have a huge Wage difference.  A semblance of normalcy in this crazy world.
So to recap, in this post, we talked about calculating and reading correlations between different variable types, plotting correlations between numerical data and Plotting categorical data with Numerical data using Swarmplots. I love how we can overlay chart elements on top of each other in Seaborn.
Also if you want to learn more about Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out
If you liked this post, do look at my other post on Seaborn too where I have created some more straightforward reusable graphs. I am going to be writing more beginner friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
Code for this post in this kaggle kernel.
References:  The Search for Categorical Correlation Seaborn Swarmplot Documentation Seaborn Pairplot Documentation  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman guide to moving from Keras to Pytorch</title>
      <link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/artificial-neural-network.png"></media:content>
      

      
      <description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds.</description>

      <content:encoded>  
        
        <![CDATA[    Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: Sentiment Analysis, summarization, dialogue state tracking, to name a few.
Also take a look at my other post: Text Preprocessing Methods for Deep Learning, which talks about different preprocessing techniques you can use for your NLP task and What Kagglers are using for Text Classification, which talks about various deep learning models in use in NLP.
Ok back to the task at hand. While Keras is great to start with deep learning, with time you are going to resent some of its limitations. I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole session.run commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.
Pytorch helps in that since it seems like the python way to do things. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:
I&amp;#39;ve been using PyTorch a few months now and I&amp;#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.
&amp;mdash; Andrej Karpathy (@karpathy) May 26, 2017 
So without further ado let me translate Keras to Pytorch for you.
The Classy way to write your network?   Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch take any network you have and try porting it to Pytorch. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post here.
def get_model(features,clipvalue=1.,num_filters=40,dropout=0.1,embed_size=501): features_input = Input(shape=(features.shape[1],)) inp = Input(shape=(maxlen, )) # Layer 1: Word2Vec Embeddings. x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) # Layer 2: SpatialDropout1D(0.1) x = SpatialDropout1D(dropout)(x) # Layer 3: Bidirectional CuDNNLSTM x = Bidirectional(LSTM(num_filters, return_sequences=True))(x) # Layer 4: Bidirectional CuDNNGRU x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x) # Layer 5: some pooling operations avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) # Layer 6: A concatenation of the last state, maximum pool, average pool and  # additional features x = concatenate([avg_pool, x_h, max_pool,features_input]) # Layer 7: A dense layer x = Dense(16, activation=&amp;#34;relu&amp;#34;)(x) # Layer 8: A dropout layer x = Dropout(0.1)(x) # Layer 9: Output dense layer with one output for our Binary Classification problem. outp = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(x) # Some keras model creation and compiling model = Model(inputs=[inp,features_input], outputs=outp) adam = optimizers.adam(clipvalue=clipvalue) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=adam, metrics=[&amp;#39;accuracy&amp;#39;]) return model So a model in pytorch is defined as a class(therefore a little more classy) which inherits from nn.module . Every class necessarily contains an __init__ procedure block and a block for the forward pass.
 In the __init__ part the user defines all the layers the network is going to have but doesn&amp;rsquo;t yet define how those layers would be connected to each other
 In the forward pass block, the user defines how data flows from one layer to another inside the network.
  Why is this Classy? Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:
1) It gives you a lot of control on how your network is built.
2) You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)
3) Easy to debug networks. Any time you find any problem with the network just use something like print(&amp;quot;avg_pool&amp;quot;, avg_pool.size()) in the forward pass to check the sizes of the layer and you will debug the network easily
4) You can return multiple outputs from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.
5) Pytorch tensors work in a very similar manner to numpy arrays. For example, I could have used Pytorch Maxpool function to write the maxpool layer but max_pool, _ = torch.max(h_gru, 1) will also work.
6) You can set up different layers with different initialization schemes. Something you won&amp;rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.
7) Wait until you see the training loop in Pytorch You will be amazed at the sort of control it provides.
Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.
class Alex_NeuralNet_Meta(nn.Module): def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix): super(Alex_NeuralNet_Meta, self).__init__() # Initialize some parameters for your model self.hidden_size = hidden_size drp = 0.1 # Layer 1: Word2Vec Embeddings. self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False # Layer 2: Dropout1D(0.1) self.embedding_dropout = nn.Dropout2d(0.1) # Layer 3: Bidirectional CuDNNLSTM self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True) for name, param in self.lstm.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 4: Bidirectional CuDNNGRU self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True) for name, param in self.gru.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 7: A dense layer self.linear = nn.Linear(hidden_size*6 &#43; features.shape[1], lin_size) self.relu = nn.ReLU() # Layer 8: A dropout layer  self.dropout = nn.Dropout(drp) # Layer 9: Output dense layer with one output for our Binary Classification problem. self.out = nn.Linear(lin_size, 1) def forward(self, x): &amp;#39;&amp;#39;&amp;#39; here x[0] represents the first element of the input that is going to be passed. We are going to pass a tuple where first one contains the sequences(x[0]) and the second one is a additional feature vector(x[1]) &amp;#39;&amp;#39;&amp;#39; h_embedding = self.embedding(x[0]) # Based on comment by Ivank to integrate spatial dropout.  embeddings = h_embedding.unsqueeze(2) # (N, T, 1, K) embeddings = embeddings.permute(0, 3, 2, 1) # (N, K, 1, T) embeddings = self.embedding_dropout(embeddings) # (N, K, 1, T), some features are masked embeddings = embeddings.permute(0, 3, 2, 1) # (N, T, 1, K) h_embedding = embeddings.squeeze(2) # (N, T, K) #h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0))) #print(&amp;#34;emb&amp;#34;, h_embedding.size()) h_lstm, _ = self.lstm(h_embedding) #print(&amp;#34;lst&amp;#34;,h_lstm.size()) h_gru, hh_gru = self.gru(h_lstm) hh_gru = hh_gru.view(-1, 2*self.hidden_size ) #print(&amp;#34;gru&amp;#34;, h_gru.size()) #print(&amp;#34;h_gru&amp;#34;, hh_gru.size()) # Layer 5: is defined dynamically as an operation on tensors. avg_pool = torch.mean(h_gru, 1) max_pool, _ = torch.max(h_gru, 1) #print(&amp;#34;avg_pool&amp;#34;, avg_pool.size()) #print(&amp;#34;max_pool&amp;#34;, max_pool.size()) # the extra features you want to give to the model f = torch.tensor(x[1], dtype=torch.float).cuda() #print(&amp;#34;f&amp;#34;, f.size()) # Layer 6: A concatenation of the last state, maximum pool, average pool and  # additional features conc = torch.cat(( hh_gru, avg_pool, max_pool,f), 1) #print(&amp;#34;conc&amp;#34;, conc.size()) # passing conc through linear and relu ops conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) # return the final output return out Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.
Tailored or Readymade: The Best Fit with a highly customizable Training Loop   In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.
This training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.
So a brief summary of this loop are as follows:
 Create stratified splits using train data Loop through the splits.  Convert your train and CV data to tensor and load your data to the GPU using the X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() command Load the model onto the GPU using the model.cuda() command Define Loss function, Scheduler and Optimizer create train_loader and valid_loader` to iterate through batches. Start running epochs. In each epoch  Set the model mode to train using model.train(). Go through the batches in train_loader and run the forward pass Run a scheduler step to change the learning rate Compute loss Set the existing gradients in the optimizer to zero Backpropagate the losses through the network Clip the gradients Take an optimizer step to change the weights in the whole network Set the model mode to eval using model.eval(). Get predictions for the validation data using valid_loader and store in variable valid_preds_fold Calculate Loss and print  After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final test_preds Get Out-of-fold(OOF) predictions for train set using train_preds[valid_idx] = valid_preds_fold These OOF predictions can then be used to calculate the Local CV score for your model.   def pytorch_model_run_cv(x_train,y_train,features,x_test, model_obj, feats = False,clip = True): seed_everything() avg_losses_f = [] avg_val_losses_f = [] # matrix for the out-of-fold predictions train_preds = np.zeros((len(x_train))) # matrix for the predictions on the test set test_preds = np.zeros((len(x_test))) splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train)) for i, (train_idx, valid_idx) in enumerate(splits): seed_everything(i*1000&#43;i) x_train = np.array(x_train) y_train = np.array(y_train) if feats: features = np.array(features) x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() if feats: kfold_X_features = features[train_idx.astype(int)] kfold_X_valid_features = features[valid_idx.astype(int)] x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda() y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() model = copy.deepcopy(model_obj) model.cuda() loss_fn = torch.nn.BCEWithLogitsLoss(reduction=&amp;#39;sum&amp;#39;) step_size = 300 base_lr, max_lr = 0.001, 0.003 optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr) ################################################################################################ scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode=&amp;#39;exp_range&amp;#39;, gamma=0.99994) ############################################################################################### train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold)) valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold)) train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False) print(f&amp;#39;Fold {i &#43; 1}&amp;#39;) for epoch in range(n_epochs): start_time = time.time() model.train() avg_loss = 0. for i, (x_batch, y_batch, index) in enumerate(train_loader): if feats: f = kfold_X_features[index] y_pred = model([x_batch,f]) else: y_pred = model(x_batch) if scheduler: scheduler.batch_step() # Compute and print loss. loss = loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() if clip: nn.utils.clip_grad_norm_(model.parameters(),1) optimizer.step() avg_loss &#43;= loss.item() / len(train_loader) model.eval() valid_preds_fold = np.zeros((x_val_fold.size(0))) test_preds_fold = np.zeros((len(x_test))) avg_val_loss = 0. for i, (x_batch, y_batch,index) in enumerate(valid_loader): if feats: f = kfold_X_valid_features[index] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() avg_val_loss &#43;= loss_fn(y_pred, y_batch).item() / len(valid_loader) valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0] elapsed_time = time.time() - start_time print(&amp;#39;Epoch {}/{} \tloss={:.4f} \tval_loss={:.4f} \ttime={:.2f}s&amp;#39;.format( epoch &#43; 1, n_epochs, avg_loss, avg_val_loss, elapsed_time)) avg_losses_f.append(avg_loss) avg_val_losses_f.append(avg_val_loss) # predict all samples in the test set batch per batch for i, (x_batch,) in enumerate(test_loader): if feats: f = test_features[i * batch_size:(i&#43;1) * batch_size] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() test_preds_fold[i * batch_size:(i&#43;1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0] train_preds[valid_idx] = valid_preds_fold test_preds &#43;= test_preds_fold / len(splits) print(&amp;#39;All \tloss={:.4f} \tval_loss={:.4f} \t&amp;#39;.format(np.average(avg_losses_f),np.average(avg_val_losses_f))) return train_preds, test_preds But Why? Why so much code? Okay. I get it. That was probably a handful. What you could have done with a simple.fit in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:
 While in Keras you have prespecified schedulers like ReduceLROnPlateau (and it is a task to write them), in Pytorch you can experiment like crazy. If you know how to write Python you are going to get along just fine Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly. And much more. It is only your imagination that will stop you.  Wanna Run it Yourself?   So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the seed_everything, MyDataset and CyclicLR (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a Kaggle Kernel with the whole running code. You can see the code here and include it in your projects.
If you liked this post, please don&amp;rsquo;t forget to upvote the Kernel too. I will be obliged.
Endnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.
 Discussion on 3rd Place winner model in Toxic comment 3rd Place model in Keras by Larry Freeman Pytorch starter Capsule model How to: Preprocessing when using embeddings Improve your Score with some Text Preprocessing Pytorch baseline Pytorch starter  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>What Kagglers are using for Text Classification</title>
      <link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/text_convolution.png"></media:content>
      

      
      <description>With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning. For those who don&amp;amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. How could you use that?
 To find sentiment of a review. Find toxic comments in a platform like Facebook Find Insincere questions on Quora.</description>

      <content:encoded>  
        
        <![CDATA[  With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence of text of indefinite length into a category of text. How could you use that?
 To find sentiment of a review. Find toxic comments in a platform like Facebook Find Insincere questions on Quora. A current ongoing competition on kaggle Find fake reviews on websites Will a text advert get clicked or not  And much more. The whole internet is filled with text and to categorise that information algorithmically will only give us incremental benefits to say the least in the field of AI.
Here I am going to use the data from Quora&amp;rsquo;s Insincere questions to talk about the different models that people are building and sharing to perform this task. Obviously these standalone models are not going to put you on the top of the leaderboard, yet I hope that this ensuing discussion would be helpful for people who want to learn more about text classification. This is going to be a long post in that regard.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few.
Also take a look at my other post: Text Preprocessing Methods for Deep Learning, which talks about different preprocessing techniques you can use for your NLP task and how to switch from Keras to Pytorch.
So let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them.
1. TextCNN: The idea of using a CNN to classify text was first presented in the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim. Instead of image pixels, the input to the tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one word vector. That is, each row is word-vector that represents a word. Thus a sequence of max length 70 gives us a image of 70(max sequence length)x300(embedding size)
  Now for some intuition. While for a image we move our conv filter horizontally also since here we have fixed our kernel size to filter_size x embed_size i.e. (3,300) we are just going to move down for the convolution taking look at three words at once since our filter size is 3 in this case.Also one can think of filter sizes as unigrams, bigrams, trigrams etc. Since we are looking at a context window of 1,2,3, and 5 words respectively. Here is the text classification network coded in Keras:
# https://www.kaggle.com/yekenot/2dcnn-textclassifier def model_cnn(embedding_matrix): filter_sizes = [1,2,3,5] num_filters = 36 inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) x = Reshape((maxlen, embed_size, 1))(x) maxpool_pool = [] for i in range(len(filter_sizes)): conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size), kernel_initializer=&amp;#39;he_normal&amp;#39;, activation=&amp;#39;elu&amp;#39;)(x) maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] &#43; 1, 1))(conv)) z = Concatenate(axis=1)(maxpool_pool) z = Flatten()(z) z = Dropout(0.1)(z) outp = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(z) model = Model(inputs=inp, outputs=outp) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=&amp;#39;adam&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.661 on the public leaderboard.
2. BiDirectional RNN(LSTM/GRU): TextCNN takes care of a lot of things. For example it takes care of words in close range. It is able to see &amp;ldquo;new york&amp;rdquo; together. But it still can&amp;rsquo;t take care of all the context provided in a particular text sequence. It still does not learn the seem to learn the sequential structure of the data, where every word is dependednt on the previous word. Or a word in the previous sentence.
RNN help us with that. They are able to remember previous information using hidden states and connect it to the current task.
Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for a long period of time. More over the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won&amp;rsquo;t work for a time sweries prediction task).
  For a most simplistic explanation of Bidirectional RNN, think of RNN cell as taking as input a hidden state(a vector) and the word vector and giving out an output vector and the next hidden state.
 Hidden state, Word vector -&amp;gt;(RNN Cell) -&amp;gt; Output Vector , Next Hidden state  For a sequence of length 4 like &amp;lsquo;you will never believe&amp;rsquo;, The RNN cell will give 4 output vectors. Which can be concatenated and then used as part of a dense feedforward architecture.
In the Bidirectional RNN the only change is that we read the text in the normal fashion as well in reverse. So we stack two RNNs in parallel and hence we get 8 output vectors to append.
Once we get the output vectors we send them through a series of dense layers and finally a softmax layer to build a text classifier.
Due to the limitations of RNNs like not remembering long term dependencies, in practice we almost always use LSTM/GRU to model long term dependencies. In such a case you can just think of the RNN cell being replaced by a LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangably with CuDNNLSTM, when you build models.
# BiDirectional LSTM def model_lstm_du(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) &amp;#39;&amp;#39;&amp;#39; Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer? 64*70(maxlen)*2(bidirection concat) CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU &amp;#39;&amp;#39;&amp;#39; x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) conc = concatenate([avg_pool, max_pool]) conc = Dense(64, activation=&amp;#34;relu&amp;#34;)(conc) conc = Dropout(0.1)(conc) outp = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(conc) model = Model(inputs=inp, outputs=outp) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=&amp;#39;adam&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.671 on the public leaderboard.
3. Attention Models The concept of Attention is relatively new as it comes from Hierarchical Attention Networks for Document Classification paper written jointly by CMU and Microsoft guys in 2016.
So in the past we used to find features from text by doing a keyword extraction. Some word are more helpful in determining the category of a text than others. But in this method we sort of lost the sequential structure of text. With LSTM and deep learning methods while we are able to take case of the sequence structure we lose the ability to give higher weightage to more important words. Can we have the best of both worlds?
And that is attention for you. In the author&amp;rsquo;s words:
 Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector
   In essense we want to create scores for every word in the text, which are the attention similarity score for a word.
To do this we start with a weight matrix(W), a bias vector(b) and a context vector u. All of them will be learned by the optimmization algorithm.
Then there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as non linearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to an exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v’s to get the Final Scores,s
These final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.
def dot_product(x, kernel): &amp;#34;&amp;#34;&amp;#34; Wrapper for dot product operation, in order to be compatible with both Theano and Tensorflow Args: x (): input kernel (): weights Returns: &amp;#34;&amp;#34;&amp;#34; if K.backend() == &amp;#39;tensorflow&amp;#39;: return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1) else: return K.dot(x, kernel) class AttentionWithContext(Layer): &amp;#34;&amp;#34;&amp;#34; Attention operation, with a context/query vector, for temporal data. Supports Masking. Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] &amp;#34;Hierarchical Attention Networks for Document Classification&amp;#34; by using a context vector to assist the attention # Input shape 3D tensor with shape: `(samples, steps, features)`. # Output shape 2D tensor with shape: `(samples, features)`. How to use: Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. The dimensions are inferred based on the output shape of the RNN. Note: The layer has been tested with Keras 2.0.6 Example: model.add(LSTM(64, return_sequences=True)) model.add(AttentionWithContext()) # next add a Dense layer (for classification/regression) or whatever... &amp;#34;&amp;#34;&amp;#34; def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None, W_constraint=None, u_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get(&amp;#39;glorot_uniform&amp;#39;) self.W_regularizer = regularizers.get(W_regularizer) self.u_regularizer = regularizers.get(u_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.u_constraint = constraints.get(u_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias super(AttentionWithContext, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1], input_shape[-1],), initializer=self.init, name=&amp;#39;{}_W&amp;#39;.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) if self.bias: self.b = self.add_weight((input_shape[-1],), initializer=&amp;#39;zero&amp;#39;, name=&amp;#39;{}_b&amp;#39;.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) self.u = self.add_weight((input_shape[-1],), initializer=self.init, name=&amp;#39;{}_u&amp;#39;.format(self.name), regularizer=self.u_regularizer, constraint=self.u_constraint) super(AttentionWithContext, self).build(input_shape) def compute_mask(self, input, input_mask=None): # do not pass the mask to the next layers return None def call(self, x, mask=None): uit = dot_product(x, self.W) if self.bias: uit &#43;= self.b uit = K.tanh(uit) ait = dot_product(uit, self.u) a = K.exp(ait) # apply mask after the exp. will be re-normalized next if mask is not None: # Cast the mask to floatX to avoid float64 upcasting in theano a *= K.cast(mask, K.floatx()) # in some cases especially in the early stages of training the sum may be almost zero # and this results in NaN&amp;#39;s. A workaround is to add a very small positive number ε to the sum. # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) &#43; K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], input_shape[-1] def model_lstm_atten(embedding_matrix): inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x) x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) x = AttentionWithContext()(x) x = Dense(64, activation=&amp;#34;relu&amp;#34;)(x) x = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(x) model = Model(inputs=inp, outputs=x) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=&amp;#39;adam&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) return model I have written a simplified and well commented code to run this network(taking input from a lot of other kernels) on a kaggle kernel for this competition. Do take a look there to learn the preprocessing steps, and the word to vec embeddings usage in this model. You will learn something. Please do upvote the kernel if you find it helpful. This kernel scored around 0.682 on the public leaderboard.
Hope that Helps! Do checkout the kernels for all the networks and see the comments too. I will try to write a part 2 of this post where I would like to talk about capsule networks and more techniques as they get used in this competition.
Here are the kernel links again: TextCNN,BiLSTM/GRU,Attention
Do upvote the kenels if you find them helpful.
References:  CNN for NLP https://en.diveintodeeplearning.org/d2l-en.pdf https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 http://univagora.ro/jour/index.php/ijccc/article/view/3142 Shujian&amp;rsquo;s kernel on Kaggle  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>To all Data Scientists - The one Graph Algorithm you need to know</title>
      <link>https://mlwhiz.com/blog/2018/12/07/connected_components/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/12/07/connected_components/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://upload.wikimedia.org/wikipedia/commons/8/85/Pseudoforest.svg"></media:content>
      

      
      <description>Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that.
Here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.</description>

      <content:encoded>  
        
        <![CDATA[  Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that.
Here is a Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory. You can start for free with the 7-day Free Trial.
One of the algorithms I am going to focus in the current post is called Connected Components. Why it is important. We all know clustering.
You can think of Connected Components in very layman&amp;rsquo;s terms as sort of a hard clustering algorithm which finds clusters/islands in related/connected data. As a concrete example: Say you have data about roads joining any two cities in the world. And you need to find out all the continents in the world and which city they contain.
How will you achieve that? Come on give some thought.
To put a Retail Perspective: Lets say, we have a lot of customers using a lot of accounts. One way in which we can use the Connected components algorithm is to find out distinct families in our dataset. We can assume edges(roads) between CustomerIDs based on same credit card usage, or same address or same mobile number etc. Once we have those connections, we can then run the connected component algorithm on the same to create individual clusters to which we can then assign a family ID. We can use these family IDs to provide personalized recommendations based on a family needs. We can also use this family ID to fuel our classification algorithms by creating grouped features based on family.
In Finance Perspective: Another use case would be to capture fraud using these family IDs. If an account has done fraud in past, it is highly probable that the connected accounts are also susceptible to fraud.
So enough of use cases. Lets start with a simple graph class written in Python to start up our exploits with code.
This post will revolve more around code from here onwards.
&amp;#34;&amp;#34;&amp;#34; A Python Class A simple Python graph class, demonstrating the essential facts and functionalities of graphs. Taken from https://www.python-course.eu/graphs_python.php Changed the implementation a little bit to include weighted edges &amp;#34;&amp;#34;&amp;#34; class Graph(object): def __init__(self, graph_dict=None): &amp;#34;&amp;#34;&amp;#34; initializes a graph object If no dictionary or None is given, an empty dictionary will be used &amp;#34;&amp;#34;&amp;#34; if graph_dict == None: graph_dict = {} self.__graph_dict = graph_dict def vertices(self): &amp;#34;&amp;#34;&amp;#34; returns the vertices of a graph &amp;#34;&amp;#34;&amp;#34; return list(self.__graph_dict.keys()) def edges(self): &amp;#34;&amp;#34;&amp;#34; returns the edges of a graph &amp;#34;&amp;#34;&amp;#34; return self.__generate_edges() def add_vertex(self, vertex): &amp;#34;&amp;#34;&amp;#34; If the vertex &amp;#34;vertex&amp;#34; is not in self.__graph_dict, a key &amp;#34;vertex&amp;#34; with an empty dict as a value is added to the dictionary. Otherwise nothing has to be done. &amp;#34;&amp;#34;&amp;#34; if vertex not in self.__graph_dict: self.__graph_dict[vertex] = {} def add_edge(self, edge,weight=1): &amp;#34;&amp;#34;&amp;#34; assumes that edge is of type set, tuple or list &amp;#34;&amp;#34;&amp;#34; edge = set(edge) (vertex1, vertex2) = tuple(edge) if vertex1 in self.__graph_dict: self.__graph_dict[vertex1][vertex2] = weight else: self.__graph_dict[vertex1] = {vertex2:weight} if vertex2 in self.__graph_dict: self.__graph_dict[vertex2][vertex1] = weight else: self.__graph_dict[vertex2] = {vertex1:weight} def __generate_edges(self): &amp;#34;&amp;#34;&amp;#34; A static method generating the edges of the graph &amp;#34;graph&amp;#34;. Edges are represented as sets with one (a loop back to the vertex) or two vertices &amp;#34;&amp;#34;&amp;#34; edges = [] for vertex in self.__graph_dict: for neighbour,weight in self.__graph_dict[vertex].iteritems(): if (neighbour, vertex, weight) not in edges: edges.append([vertex, neighbour, weight]) return edges def __str__(self): res = &amp;#34;vertices: &amp;#34; for k in self.__graph_dict: res &#43;= str(k) &#43; &amp;#34; &amp;#34; res &#43;= &amp;#34;\nedges: &amp;#34; for edge in self.__generate_edges(): res &#43;= str(edge) &#43; &amp;#34; &amp;#34; return res def adj_mat(self): return self.__graph_dict You can certainly play with our new graph class.Here we try to build some graphs.
g = { &amp;#34;a&amp;#34; : {&amp;#34;d&amp;#34;:2}, &amp;#34;b&amp;#34; : {&amp;#34;c&amp;#34;:2}, &amp;#34;c&amp;#34; : {&amp;#34;b&amp;#34;:5, &amp;#34;d&amp;#34;:3, &amp;#34;e&amp;#34;:5} } graph = Graph(g) print(&amp;#34;Vertices of graph:&amp;#34;) print(graph.vertices()) print(&amp;#34;Edges of graph:&amp;#34;) print(graph.edges()) print(&amp;#34;Add vertex:&amp;#34;) graph.add_vertex(&amp;#34;z&amp;#34;) print(&amp;#34;Vertices of graph:&amp;#34;) print(graph.vertices()) print(&amp;#34;Add an edge:&amp;#34;) graph.add_edge({&amp;#34;a&amp;#34;,&amp;#34;z&amp;#34;}) print(&amp;#34;Vertices of graph:&amp;#34;) print(graph.vertices()) print(&amp;#34;Edges of graph:&amp;#34;) print(graph.edges()) print(&amp;#39;Adding an edge {&amp;#34;x&amp;#34;,&amp;#34;y&amp;#34;} with new vertices:&amp;#39;) graph.add_edge({&amp;#34;x&amp;#34;,&amp;#34;y&amp;#34;}) print(&amp;#34;Vertices of graph:&amp;#34;) print(graph.vertices()) print(&amp;#34;Edges of graph:&amp;#34;) print(graph.edges()) Vertices of graph: [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;] Edges of graph: [[&#39;a&#39;, &#39;d&#39;, 2], [&#39;c&#39;, &#39;b&#39;, 5], [&#39;c&#39;, &#39;e&#39;, 5], [&#39;c&#39;, &#39;d&#39;, 3], [&#39;b&#39;, &#39;c&#39;, 2]] Add vertex: Vertices of graph: [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;z&#39;] Add an edge: Vertices of graph: [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;z&#39;] Edges of graph: [[&#39;a&#39;, &#39;z&#39;, 1], [&#39;a&#39;, &#39;d&#39;, 2], [&#39;c&#39;, &#39;b&#39;, 5], [&#39;c&#39;, &#39;e&#39;, 5], [&#39;c&#39;, &#39;d&#39;, 3], [&#39;b&#39;, &#39;c&#39;, 2], [&#39;z&#39;, &#39;a&#39;, 1]] Adding an edge {&#34;x&#34;,&#34;y&#34;} with new vertices: Vertices of graph: [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;y&#39;, &#39;x&#39;, &#39;z&#39;] Edges of graph: [[&#39;a&#39;, &#39;z&#39;, 1], [&#39;a&#39;, &#39;d&#39;, 2], [&#39;c&#39;, &#39;b&#39;, 5], [&#39;c&#39;, &#39;e&#39;, 5], [&#39;c&#39;, &#39;d&#39;, 3], [&#39;b&#39;, &#39;c&#39;, 2], [&#39;y&#39;, &#39;x&#39;, 1], [&#39;x&#39;, &#39;y&#39;, 1], [&#39;z&#39;, &#39;a&#39;, 1]]  Lets do something interesting now.
We will use the above graph class for our understanding purpose. There are many Modules in python which we can use to do whatever I am going to do next,but to understand the methods we will write everything from scratch. Lets start with an example graph which we can use for our purpose.
  g = {&amp;#39;Frankfurt&amp;#39;: {&amp;#39;Mannheim&amp;#39;:85, &amp;#39;Wurzburg&amp;#39;:217, &amp;#39;Kassel&amp;#39;:173}, &amp;#39;Mannheim&amp;#39;: {&amp;#39;Frankfurt&amp;#39;:85, &amp;#39;Karlsruhe&amp;#39;:80}, &amp;#39;Karlsruhe&amp;#39;: {&amp;#39;Augsburg&amp;#39;:250, &amp;#39;Mannheim&amp;#39;:80}, &amp;#39;Augsburg&amp;#39;: {&amp;#39;Karlsruhe&amp;#39;:250, &amp;#39;Munchen&amp;#39;:84}, &amp;#39;Wurzburg&amp;#39;: {&amp;#39;Erfurt&amp;#39;:186, &amp;#39;Numberg&amp;#39;:103,&amp;#39;Frankfurt&amp;#39;:217}, &amp;#39;Erfurt&amp;#39;: {&amp;#39;Wurzburg&amp;#39;:186}, &amp;#39;Numberg&amp;#39;: {&amp;#39;Wurzburg&amp;#39;:103, &amp;#39;Stuttgart&amp;#39;:183,&amp;#39;Munchen&amp;#39;:167}, &amp;#39;Munchen&amp;#39;: {&amp;#39;Numberg&amp;#39;:167, &amp;#39;Augsburg&amp;#39;:84,&amp;#39;Kassel&amp;#39;:502}, &amp;#39;Kassel&amp;#39;: {&amp;#39;Frankfurt&amp;#39;:173, &amp;#39;Munchen&amp;#39;:502}, &amp;#39;Stuttgart&amp;#39;: {&amp;#39;Numberg&amp;#39;:183} } graph = Graph(g) print(&amp;#34;Vertices of graph:&amp;#34;) print(graph.vertices()) print(&amp;#34;Edges of graph:&amp;#34;) print(graph.edges()) Vertices of graph: [&#39;Mannheim&#39;, &#39;Erfurt&#39;, &#39;Munchen&#39;, &#39;Numberg&#39;, &#39;Stuttgart&#39;, &#39;Augsburg&#39;, &#39;Kassel&#39;, &#39;Frankfurt&#39;, &#39;Wurzburg&#39;, &#39;Karlsruhe&#39;] Edges of graph: [[&#39;Mannheim&#39;, &#39;Frankfurt&#39;, 85], [&#39;Mannheim&#39;, &#39;Karlsruhe&#39;, 80], [&#39;Erfurt&#39;, &#39;Wurzburg&#39;, 186], [&#39;Munchen&#39;, &#39;Numberg&#39;, 167], [&#39;Munchen&#39;, &#39;Augsburg&#39;, 84], [&#39;Munchen&#39;, &#39;Kassel&#39;, 502], [&#39;Numberg&#39;, &#39;Stuttgart&#39;, 183], [&#39;Numberg&#39;, &#39;Wurzburg&#39;, 103], [&#39;Numberg&#39;, &#39;Munchen&#39;, 167], [&#39;Stuttgart&#39;, &#39;Numberg&#39;, 183], [&#39;Augsburg&#39;, &#39;Munchen&#39;, 84], [&#39;Augsburg&#39;, &#39;Karlsruhe&#39;, 250], [&#39;Kassel&#39;, &#39;Munchen&#39;, 502], [&#39;Kassel&#39;, &#39;Frankfurt&#39;, 173], [&#39;Frankfurt&#39;, &#39;Mannheim&#39;, 85], [&#39;Frankfurt&#39;, &#39;Wurzburg&#39;, 217], [&#39;Frankfurt&#39;, &#39;Kassel&#39;, 173], [&#39;Wurzburg&#39;, &#39;Numberg&#39;, 103], [&#39;Wurzburg&#39;, &#39;Erfurt&#39;, 186], [&#39;Wurzburg&#39;, &#39;Frankfurt&#39;, 217], [&#39;Karlsruhe&#39;, &#39;Mannheim&#39;, 80], [&#39;Karlsruhe&#39;, &#39;Augsburg&#39;, 250]]  Lets say we are given a graph with the cities of Germany and respective distance between them. You want to find out how to go from Frankfurt (The starting node) to Munchen. There might be many ways in which you can traverse the graph but you need to find how many cities you will need to visit on a minimum to go from frankfurt to Munchen) This problem is analogous to finding out distance between nodes in an unweighted graph.
The algorithm that we use here is called as Breadth First Search.
def min_num_edges_between_nodes(graph,start_node): distance = 0 shortest_path = [] queue = [start_node] #FIFO levels = {} levels[start_node] = 0 shortest_paths = {} shortest_paths[start_node] = &amp;#34;:&amp;#34; visited = [start_node] while len(queue)!=0: start = queue.pop(0) neighbours = graph[start] for neighbour,_ in neighbours.iteritems(): if neighbour not in visited: queue.append(neighbour) visited.append(neighbour) levels[neighbour] = levels[start]&#43;1 shortest_paths[neighbour] = shortest_paths[start] &#43;&amp;#34;-&amp;gt;&amp;#34;&#43; start return levels, shortest_paths What we do in the above piece of code is create a queue and traverse it based on levels. We start with Frankfurt as starting node. We loop through its neighbouring cities(Menheim, Wurzburg and Kassel) and push them into the queue. We keep track of what level they are at and also the path through which we reached them. Since we are popping a first element of a queue we are sure we will visit cities in the order of their level.
Checkout this good post about BFS to understand more about queues and BFS.
min_num_edges_between_nodes(g,&amp;#39;Frankfurt&amp;#39;) ({&#39;Augsburg&#39;: 3, &#39;Erfurt&#39;: 2, &#39;Frankfurt&#39;: 0, &#39;Karlsruhe&#39;: 2, &#39;Kassel&#39;: 1, &#39;Mannheim&#39;: 1, &#39;Munchen&#39;: 2, &#39;Numberg&#39;: 2, &#39;Stuttgart&#39;: 3, &#39;Wurzburg&#39;: 1}, {&#39;Augsburg&#39;: &#39;:-Frankfurt-Mannheim-Karlsruhe&#39;, &#39;Erfurt&#39;: &#39;:-Frankfurt-Wurzburg&#39;, &#39;Frankfurt&#39;: &#39;:&#39;, &#39;Karlsruhe&#39;: &#39;:-Frankfurt-Mannheim&#39;, &#39;Kassel&#39;: &#39;:-Frankfurt&#39;, &#39;Mannheim&#39;: &#39;:-Frankfurt&#39;, &#39;Munchen&#39;: &#39;:-Frankfurt-Kassel&#39;, &#39;Numberg&#39;: &#39;:-Frankfurt-Wurzburg&#39;, &#39;Stuttgart&#39;: &#39;:-Frankfurt-Wurzburg-Numberg&#39;, &#39;Wurzburg&#39;: &#39;:-Frankfurt&#39;})  I did this example to show how BFS algorithm works. We can extend this algorithm to find out connected components in an unconnected graph. Lets say we need to find groups of unconnected vertices in the graph.
For example: the below graph has 3 unconnected sub-graphs. Can we find what nodes belong to a particular subgraph?
  #We add another countries in the loop  graph = Graph(g) graph.add_edge((&amp;#34;Mumbai&amp;#34;, &amp;#34;Delhi&amp;#34;),400) graph.add_edge((&amp;#34;Delhi&amp;#34;, &amp;#34;Kolkata&amp;#34;),500) graph.add_edge((&amp;#34;Kolkata&amp;#34;, &amp;#34;Bangalore&amp;#34;),600) graph.add_edge((&amp;#34;TX&amp;#34;, &amp;#34;NY&amp;#34;),1200) graph.add_edge((&amp;#34;ALB&amp;#34;, &amp;#34;NY&amp;#34;),800) g = graph.adj_mat() def bfs_connected_components(graph): connected_components = [] nodes = graph.keys() while len(nodes)!=0: start_node = nodes.pop() queue = [start_node] #FIFO visited = [start_node] while len(queue)!=0: start = queue[0] queue.remove(start) neighbours = graph[start] for neighbour,_ in neighbours.iteritems(): if neighbour not in visited: queue.append(neighbour) visited.append(neighbour) nodes.remove(neighbour) connected_components.append(visited) return connected_components print bfs_connected_components(g) The above code is similar to the previous BFS code. We keep all the vertices of the graph in the nodes list. We take a node from the nodes list and start BFS on it. as we visit a node we remove that node from the nodes list. Whenever the BFS completes we start again with another node in the nodes list until the nodes list is empty.
[[&#39;Kassel&#39;, &#39;Munchen&#39;, &#39;Frankfurt&#39;, &#39;Numberg&#39;, &#39;Augsburg&#39;, &#39;Mannheim&#39;, &#39;Wurzburg&#39;, &#39;Stuttgart&#39;, &#39;Karlsruhe&#39;, &#39;Erfurt&#39;], [&#39;Bangalore&#39;, &#39;Kolkata&#39;, &#39;Delhi&#39;, &#39;Mumbai&#39;], [&#39;NY&#39;, &#39;ALB&#39;, &#39;TX&#39;]]  As you can see we are able to find distinct components in our data. Just by using Edges and Vertices. This algorithm could be run on different data to satisfy any use case I presented above.
But Normally using Connected Components for a retail case will involve a lot of data and you will need to scale this algorithm.
Connected Components in PySpark Below is an implementation from this paper on Connected Components in MapReduce and Beyond from Google Research. Read the PPT to understand the implementation better. Some ready to use code for you.
def create_edges(line): a = [int(x) for x in line.split(&amp;#34; &amp;#34;)] edges_list=[] for i in range(0, len(a)-1): for j in range(i&#43;1 ,len(a)): edges_list.append((a[i],a[j])) edges_list.append((a[j],a[i])) return edges_list # adj_list.txt is a txt file containing adjacency list of the graph. adjacency_list = sc.textFile(&amp;#34;adj_list.txt&amp;#34;) edges_rdd = adjacency_list.flatMap(lambda line : create_edges(line)).distinct() def largeStarInit(record): a, b = record yield (a,b) yield (b,a) def largeStar(record): a, b = record t_list = list(b) t_list.append(a) list_min = min(t_list) for x in b: if a &amp;lt; x: yield (x,list_min) def smallStarInit(record): a, b = record if b&amp;lt;=a: yield (a,b) else: yield (b,a) def smallStar(record): a, b = record t_list = list(b) t_list.append(a) list_min = min(t_list) for x in t_list: if x!=list_min: yield (x,list_min) #Handle case for single nodes def single_vertex(line): a = [int(x) for x in line.split(&amp;#34; &amp;#34;)] edges_list=[] if len(a)==1: edges_list.append((a[0],a[0])) return edges_list iteration_num =0 while 1==1: if iteration_num==0: print &amp;#34;iter&amp;#34;, iteration_num large_star_rdd = edges_rdd.groupByKey().flatMap(lambda x : largeStar(x)) small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct() iteration_num &#43;= 1 else: print &amp;#34;iter&amp;#34;, iteration_num large_star_rdd = small_star_rdd.flatMap(lambda x: largeStarInit(x)).groupByKey().flatMap(lambda x : largeStar(x)).distinct() small_star_rdd = large_star_rdd.flatMap(lambda x : smallStarInit(x)).groupByKey().flatMap(lambda x : smallStar(x)).distinct() iteration_num &#43;= 1 #check Convergence changes = (large_star_rdd.subtract(small_star_rdd).union(small_star_rdd.subtract(large_star_rdd))).collect() if len(changes) == 0 : break single_vertex_rdd = adjacency_list.flatMap(lambda line : single_vertex(line)).distinct() answer = single_vertex_rdd.collect() &#43; large_star_rdd.collect() print answer[:10] Or Use GraphFrames in PySpark To Install graphframes:
I ran on command line: pyspark &amp;ndash;packages graphframes:graphframes:0.5.0-spark2.1-s_2.11 which opened up my notebook and installed graphframes after i try to import in my notebook.
The string to be formatted as : graphframes:(latest version)-spark(your spark version)-s_(your scala version).
Checkout this guide on how to use GraphFrames for more information.
from graphframes import * def vertices(line): vert = [int(x) for x in line.split(&amp;#34; &amp;#34;)] return vert vertices = adjacency_list.flatMap(lambda x: vertices(x)).distinct().collect() vertices = sqlContext.createDataFrame([[x] for x in vertices], [&amp;#34;id&amp;#34;]) def create_edges(line): a = [int(x) for x in line.split(&amp;#34; &amp;#34;)] edges_list=[] if len(a)==1: edges_list.append((a[0],a[0])) for i in range(0, len(a)-1): for j in range(i&#43;1 ,len(a)): edges_list.append((a[i],a[j])) edges_list.append((a[j],a[i])) return edges_list edges = adjacency_list.flatMap(lambda x: create_edges(x)).distinct().collect() edges = sqlContext.createDataFrame(edges, [&amp;#34;src&amp;#34;, &amp;#34;dst&amp;#34;]) g = GraphFrame(vertices, edges) sc.setCheckpointDir(&amp;#34;.&amp;#34;) # graphframes uses the same paper we referenced apparently cc = g.connectedComponents() print cc.show() The GraphFrames library implements the CC algorithm as well as a variety of other graph algorithms.
The above post was a lot of code but hope it was helpful. It took me a lot of time to implement the algorithm so wanted to make it easy for the folks.
If you want to read up more on Graph Algorithms here is an Graph Analytics for Big Data course on Coursera by UCSanDiego which I highly recommend to learn the basics of graph theory.
References  Graphs in Python A Gentle Intoduction to Graph Theory Blog Graph Analytics for Big Data course on Coursera by UCSanDiego  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Using XGBoost for time series prediction tasks</title>
      <link>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/lboard.png"></media:content>
      

      
      <description>Recently Kaggle master Kazanova along with some of his friends released a &amp;amp;ldquo;How to win a data science competition&amp;amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.
  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.</description>

      <content:encoded>  
        
        <![CDATA[  Recently Kaggle master Kazanova along with some of his friends released a &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.
  Description of the Problem: In this competition we were given a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.
We were asked you to predict total sales for every product and store in the next month.
The evaluation metric was RMSE where True target values are clipped into [0,20] range. This target range will be a lot important in understanding the submissions that I will prepare.
The main thing that I noticed was that the data preparation aspect of this competition was by far the most important thing. I creted a variety of features. Here are the steps I took and the features I created.
1. Created a dataframe of all Date_block_num, Store and Item combinations: This is important because in the months we don&amp;rsquo;t have a data for an item store combination, the machine learning algorithm needs to be specifically told that the sales is zero.
from itertools import product # Create &amp;#34;grid&amp;#34; with columns index_cols = [&amp;#39;shop_id&amp;#39;, &amp;#39;item_id&amp;#39;, &amp;#39;date_block_num&amp;#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&amp;#39;date_block_num&amp;#39;].unique(): cur_shops = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;shop_id&amp;#39;].unique() cur_items = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;item_id&amp;#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&amp;#39;int32&amp;#39;)) grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) 2. Cleaned up a little of sales data after some basic EDA: sales = sales[sales.item_price&amp;lt;100000] sales = sales[sales.item_cnt_day&amp;lt;=1000] 3. Created Mean Encodings: sales_m = sales.groupby([&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]).agg({&amp;#39;item_cnt_day&amp;#39;: &amp;#39;sum&amp;#39;,&amp;#39;item_price&amp;#39;: np.mean}).reset_index() sales_m = pd.merge(grid,sales_m,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;).fillna(0) # adding the category id too sales_m = pd.merge(sales_m,items,on=[&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;) for type_id in [&amp;#39;item_id&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_category_id&amp;#39;]: for column_id,aggregator,aggtype in [(&amp;#39;item_price&amp;#39;,np.mean,&amp;#39;avg&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.sum,&amp;#39;sum&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.mean,&amp;#39;avg&amp;#39;)]: mean_df = sales.groupby([type_id,&amp;#39;date_block_num&amp;#39;]).aggregate(aggregator).reset_index()[[column_id,type_id,&amp;#39;date_block_num&amp;#39;]] mean_df.columns = [type_id&#43;&amp;#39;_&amp;#39;&#43;aggtype&#43;&amp;#39;_&amp;#39;&#43;column_id,type_id,&amp;#39;date_block_num&amp;#39;] sales_m = pd.merge(sales_m,mean_df,on=[&amp;#39;date_block_num&amp;#39;,type_id],how=&amp;#39;left&amp;#39;) These above lines add the following 9 features :
 &amp;lsquo;item_id_avg_item_price&amp;rsquo; &amp;lsquo;item_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;item_id_avg_item_cnt_day&amp;rsquo; &amp;lsquo;shop_id_avg_item_price&amp;rsquo;, &amp;lsquo;shop_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;shop_id_avg_item_cnt_day&amp;rsquo; &amp;lsquo;item_category_id_avg_item_price&amp;rsquo; &amp;lsquo;item_category_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;item_category_id_avg_item_cnt_day&amp;rsquo;  4. Create Lag Features: Next we create lag features with diferent lag periods on the following features:
 &amp;lsquo;item_id_avg_item_price&amp;rsquo;, &amp;lsquo;item_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;item_id_avg_item_cnt_day&amp;rsquo; &amp;lsquo;shop_id_avg_item_price&amp;rsquo; &amp;lsquo;shop_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;shop_id_avg_item_cnt_day&amp;rsquo; &amp;lsquo;item_category_id_avg_item_price&amp;rsquo; &amp;lsquo;item_category_id_sum_item_cnt_day&amp;rsquo; &amp;lsquo;item_category_id_avg_item_cnt_day&amp;rsquo; &amp;lsquo;item_cnt_day&amp;rsquo;  lag_variables = list(sales_m.columns[7:])&#43;[&amp;#39;item_cnt_day&amp;#39;] lags = [1 ,2 ,3 ,4, 5, 12] for lag in lags: sales_new_df = sales_m.copy() sales_new_df.date_block_num&#43;=lag sales_new_df = sales_new_df[[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43;lag_variables] sales_new_df.columns = [&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43; [lag_feat&#43;&amp;#39;_lag_&amp;#39;&#43;str(lag) for lag_feat in lag_variables] sales_means = pd.merge(sales_means, sales_new_df,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;] ,how=&amp;#39;left&amp;#39;) 5. Fill NA with zeros: for feat in sales_means.columns: if &amp;#39;item_cnt&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(0) elif &amp;#39;item_price&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median()) 6. Drop the columns that we are not going to use in training: cols_to_drop = lag_variables[:-1] &#43; [&amp;#39;item_name&amp;#39;,&amp;#39;item_price&amp;#39;] 7. Take a recent bit of data only: sales_means = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;gt;12] 8. Split in train and CV : X_train = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;lt;33].drop(cols_to_drop, axis=1) X_cv = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]==33].drop(cols_to_drop, axis=1) 9. THE MAGIC SAUCE: In the start I told that the clipping aspect of [0,20] will be important. In the next few lines I clipped the days to range[0,40]. You might ask me why 40. An intuitive answer is if I had clipped to range [0,20] there would be very few tree nodes that could give 20 as an answer. While if I increase it to 40 having a 20 becomes much more easier. Please note that We will clip our predictions in the [0,20] range in the end.
def clip(x): if x&amp;gt;40: return 40 elif x&amp;lt;0: return 0 else: return x train[&amp;#39;item_cnt_day&amp;#39;] = train.apply(lambda x: clip(x[&amp;#39;item_cnt_day&amp;#39;]),axis=1) cv[&amp;#39;item_cnt_day&amp;#39;] = cv.apply(lambda x: clip(x[&amp;#39;item_cnt_day&amp;#39;]),axis=1) 10: Modelling:  Created a XGBoost model to get the most important features(Top 42 features) Use hyperopt to tune xgboost Used top 10 models from tuned XGBoosts to generate predictions. clipped the predictions to [0,20] range Final solution was the average of these 10 predictions.  Learned a lot of new things from this awesome course. Most recommended.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Good Feature Building Techniques - Tricks for Kaggle -  My Kaggle Code Repository</title>
      <link>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/kaggle_tricks/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://storage.googleapis.com/kaggle-organizations/4/thumbnail.png"></media:content>
      

      
      <description>Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing).</description>

      <content:encoded>  
        
        <![CDATA[  Often times it happens that we fall short of creativity. And creativity is one of the basic ingredients of what we do. Creating features needs creativity. So here is the list of ideas I gather in day to day life, where people have used creativity to get great results on Kaggle leaderboards.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova(Number 3 Kaggler at the time of writing). You can start for free with the 7-day Free Trial.
This post is inspired by a Kernel on Kaggle written by Beluga, one of the top Kagglers, for a knowledge based competition.
Some of the techniques/tricks I am sharing have been taken directly from that kernel so you could take a look yourself. Otherwise stay here and read on.
1. Don&amp;rsquo;t try predicting the future when you don&amp;rsquo;t have to: If both training/test comes from the same timeline, we can get really crafty with features. Although this is a case with Kaggle only, we can use this to our advantage. For example: In the Taxi Trip duration challenge the test data is randomly sampled from the train data. In this case we can use the target variable averaged over different categorical variable as a feature. Like in this case Beluga actually used the averaged the target variable over different weekdays. He then mapped the same averaged value as a variable by mapping it to test data too.
2. logloss clipping Technique: Something that I learned in the Neural Network course by Jeremy Howard. Its based on a very simple Idea. Logloss penalises a lot if we are very confident and wrong. So in case of Classification problems where we have to predict probabilities, it would be much better to clip our probabilities between 0.05-0.95 so that we are never very sure about our prediction.
3. kaggle submission in gzip format: A small piece of code that will help you save countless hours of uploading. Enjoy. df.to_csv(&amp;lsquo;submission.csv.gz&amp;rsquo;, index=False, compression=&amp;lsquo;gzip&amp;rsquo;)
4. How best to use Latitude and Longitude features - Part 1: One of the best things that I liked about the Beluga Kernel is how he used the Lat/Lon Data. So in the example we had pickup Lat/Lon and Dropoff Lat/Lon. We created features like:
A. Haversine Distance Between the Two Lat/Lons: def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 &#43; np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h B. Manhattan Distance Between the two Lat/Lons: def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a &#43; b C. Bearing Between the two Lat/Lons: def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) D. Center Latitude and Longitude between Pickup and Dropoff: train.loc[:, &amp;#39;center_latitude&amp;#39;] = (train[&amp;#39;pickup_latitude&amp;#39;].values &#43; train[&amp;#39;dropoff_latitude&amp;#39;].values) / 2 train.loc[:, &amp;#39;center_longitude&amp;#39;] = (train[&amp;#39;pickup_longitude&amp;#39;].values &#43; train[&amp;#39;dropoff_longitude&amp;#39;].values) / 2 5. How best to use Latitude and Longitude features - Part 2: The Second way he used the Lat/Lon Feats was to create clusters for Pickup and Dropoff Lat/Lons. The way it worked was it created sort of Boroughs in the data by design.
from sklearn.cluster import MiniBatchKMeans coords = np.vstack((train[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]].values, train[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]].values, test[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]].values, test[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]].values)) sample_ind = np.random.permutation(len(coords))[:500000] kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind]) train.loc[:, &amp;#39;pickup_cluster&amp;#39;] = kmeans.predict(train[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]]) train.loc[:, &amp;#39;dropoff_cluster&amp;#39;] = kmeans.predict(train[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]]) test.loc[:, &amp;#39;pickup_cluster&amp;#39;] = kmeans.predict(test[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]]) test.loc[:, &amp;#39;dropoff_cluster&amp;#39;] = kmeans.predict(test[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]]) He then used these Clusters to create features like counting no of trips going out and coming in on a particular day.
6. How best to use Latitude and Longitude features - Part 3 He used PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since he transformed 2D-&amp;gt; 2D. The rotation could help for decision tree splits, and it did actually.
pca = PCA().fit(coords) train[&amp;#39;pickup_pca0&amp;#39;] = pca.transform(train[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]])[:, 0] train[&amp;#39;pickup_pca1&amp;#39;] = pca.transform(train[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]])[:, 1] train[&amp;#39;dropoff_pca0&amp;#39;] = pca.transform(train[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]])[:, 0] train[&amp;#39;dropoff_pca1&amp;#39;] = pca.transform(train[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]])[:, 1] test[&amp;#39;pickup_pca0&amp;#39;] = pca.transform(test[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]])[:, 0] test[&amp;#39;pickup_pca1&amp;#39;] = pca.transform(test[[&amp;#39;pickup_latitude&amp;#39;, &amp;#39;pickup_longitude&amp;#39;]])[:, 1] test[&amp;#39;dropoff_pca0&amp;#39;] = pca.transform(test[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]])[:, 0] test[&amp;#39;dropoff_pca1&amp;#39;] = pca.transform(test[[&amp;#39;dropoff_latitude&amp;#39;, &amp;#39;dropoff_longitude&amp;#39;]])[:, 1] 7. Lets not forget the Normal Things you can do with your features:  Scaling by Max-Min Normalization using Standard Deviation Log based feature/Target: use log based features or log based target function. One Hot Encoding  8. Creating Intuitive Additional Features: A) Date time Features: Time based Features like &amp;ldquo;Evening&amp;rdquo;, &amp;ldquo;Noon&amp;rdquo;, &amp;ldquo;Night&amp;rdquo;, &amp;ldquo;Purchases_last_month&amp;rdquo;, &amp;ldquo;Purchases_last_week&amp;rdquo; etc.
B) Thought Features: Suppose you have shopping cart data and you want to categorize TripType (See Walmart Recruiting: Trip Type Classification on Kaggle for some background).
You could think of creating a feature like &amp;ldquo;Stylish&amp;rdquo; where you create this variable by adding together number of items that belong to category Men&amp;rsquo;s Fashion, Women&amp;rsquo;s Fashion, Teens Fashion.
You could create a feature like &amp;ldquo;Rare&amp;rdquo; which is created by tagging some items as rare, based on the data we have and then counting the number of those rare items in the shopping cart. Such features might work or might not work. From what I have observed they normally provide a lot of value.
I feel this is the way that Target&amp;rsquo;s &amp;ldquo;Pregnant Teen model&amp;rdquo; was made. They would have had a variable in which they kept all the items that a pregnant teen could buy and put it into a classification algorithm.
9 . The not so Normal Things which people do: These features are highly unintuitive and should not be created where the machine learning model needs to be interpretable.
A) Interaction Features: If you have features A and B create features A*B, A&#43;B, A/B, A-B. This explodes the feature space. If you have 10 features and you are creating two variable interactions you will be adding 10C2 * 4 features = 180 features to your model. And most of us have a lot more than 10 features.
B) Bucket Feature Using Hashing: Suppose you have a lot of features. In the order of Thousands but you don&amp;rsquo;t want to use all the thousand features because of the training times of algorithms involved. People bucket their features using some hashing algorithm to achieve this.Mostly done for text classification tasks. For example: If we have 6 features A,B,C,D,E,F. And the row of data is: A:1,B:1,C:1,D:0,E:1,F:0 I may decide to use a hashing function so that these 6 features correspond to 3 buckets and create the data using this feature hashing vector. After processing my data might look like: Bucket1:2,Bucket2:2,Bucket3:0 Which happened because A and B fell in bucket1, C and E fell in bucket2 and D and F fell in bucket 3. I summed up the observations here, but you could substitute addition with any math function you like. Now i would use Bucket1,Bucket2,Bucket3 as my variables for machine learning.
Will try to keep on expanding. Wait for more&amp;hellip;.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Maths Beats Intuition probably every damn time</title>
      <link>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</guid>
      
      

      
      <description>Newton once said that &amp;amp;ldquo;God does not play dice with the universe&amp;amp;rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.
One of such ways intuition plays with us is with the Birthday problem.
Problem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?</description>

      <content:encoded>  
        
        <![CDATA[  Newton once said that &amp;ldquo;God does not play dice with the universe&amp;rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.
One of such ways intuition plays with us is with the Birthday problem.
Problem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?
By the pigeonhole principle, the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).
However, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.
Mathematical Proof: Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event. We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement) Since there are k ways in which birthdays can be chosen with replacement.
$P(birthday Match) = 1 - \dfrac{(365).364&amp;hellip;(365−k&#43;1)}{365^k}$
Simulation: Lets try to build around this result some more by trying to simulate this result:
%matplotlib inline import matplotlib import numpy as np import matplotlib.pyplot as plt import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes import random def sim_bithday_problem(num_people_room, trials =1000): &amp;#39;&amp;#39;&amp;#39;This function takes as input the number of people in the room. Runs 1000 trials by default and returns (number of times same brthday found)/(no of trials) &amp;#39;&amp;#39;&amp;#39; same_birthdays_found = 0 for i in range(trials): # randomly sample from the birthday space which could be any of a number from 1 to 365 birthdays = [random.randint(1,365) for x in range(num_people_room)] if len(birthdays) - len(set(birthdays))&amp;gt;0: same_birthdays_found&#43;=1 return same_birthdays_found/float(trials) num_people = range(2,100) probs = [sim_bithday_problem(i) for i in num_people] data = pd.DataFrame() data[&amp;#39;num_peeps&amp;#39;] = num_people data[&amp;#39;probs&amp;#39;] = probs sns.set(style=&amp;#34;ticks&amp;#34;) g = sns.regplot(x=&amp;#34;num_peeps&amp;#34;, y=&amp;#34;probs&amp;#34;, data=data, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, marker=&amp;#34;x&amp;#34;,fit_reg=False) sns.despine() g.figure.set_size_inches(10,6) g.axes.set_title(&amp;#39;As the Number of people in room reaches 23 the probability reaches ~0.5\nAt more than 50 people the probability is reaching 1&amp;#39;, fontsize=15,color=&amp;#34;g&amp;#34;,alpha=0.5) g.set_xlabel(&amp;#34;# of people in room&amp;#34;,size = 30,color=&amp;#34;r&amp;#34;,alpha=0.5) g.set_ylabel(&amp;#34;Probability&amp;#34;,size = 30,color=&amp;#34;r&amp;#34;,alpha=0.5) g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   We can see from the graph that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.
Intuition: To understand it we need to think of this problem in terms of pairs. There are ${{23}\choose{2}} = 253$ pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at ${{70}\choose{2}} = 2450$ pairs.
So thats it for now. To learn more about this go to Wikipedia which has an awesome page on this topic.
References:  Introduction to Probability by Joseph K. Blitzstein Birthday Problem on Wikipedia  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      
      

      
      <description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;amp;rsquo;t know bag of words: If we have a series of sentences(documents)</description>

      <content:encoded>  
        
        <![CDATA[  Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.
Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.
For those who don&amp;rsquo;t know bag of words: If we have a series of sentences(documents)
 This is good - [1,1,1,0,0] This is bad - [1,1,0,1,0] This is awesome - [1,1,0,0,1]  Bag of words would encode it using 0:This 1:is 2:good 3:bad 4:awesome
But it is much more powerful than that.
What word2vec does is that it creates vectors for words. What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.
How does that help? We can use this for multiple scenarios but the most common are:
A. Using word2vec embeddings we can find out similarity between words. Assume you have to answer if these two statements signify the same thing:
 President greets press in Chicago Obama speaks to media in Illinois.  If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.
  But with a word encoding we can say that
 President is similar to Obama greets is similar to speaks press is similar to media Chicago is similar to Illinois  B. Encode Sentences: I read a post from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.
His Approach: Lets say the sentence is &amp;ldquo;What is this&amp;rdquo; And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)
 what : [.25 ,.25 ,.25 ,.25] is : [ 1 , 0 , 0 , 0] this : [ .5 , 0 , 0 , .5]  Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.
Elementwise addition : [.25&#43;1&#43;0.5, 0.25&#43;0&#43;0 , 0.25&#43;0&#43;0, .25&#43;0&#43;.5] = [1.75, .25, .25, .75] divided by math.sqrt(1.25^2 &#43; .25^2 &#43; .25^2 &#43; .75^2) = 1.5 gives:[1.16, .17, .17, 0.5]  Thus I can convert any sentence to a vector of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.
C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man &#43; woman ?
Guess what it comes out to be : Queen
Application/Coding: Now lets get down to the coding part as we know a little bit of fundamentals.
First of all we download a custom word embedding from Google. There are many other embeddings too.
wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz The above file is pretty big. Might take some time. Then moving on to coding.
from gensim.models import word2vec model = gensim.models.KeyedVectors.load_word2vec_format(&amp;#39;data/GoogleNews-vectors-negative300.bin.gz&amp;#39;, binary=True) 1. Starting simple, lets find out similar words. Want to find similar words to python? model.most_similar(&amp;#39;python&amp;#39;) [(u&#39;pythons&#39;, 0.6688377261161804),
(u&#39;Burmese_python&#39;, 0.6680364608764648),
(u&#39;snake&#39;, 0.6606293320655823),
(u&#39;crocodile&#39;, 0.6591362953186035),
(u&#39;boa_constrictor&#39;, 0.6443519592285156),
(u&#39;alligator&#39;, 0.6421656608581543),
(u&#39;reptile&#39;, 0.6387745141983032),
(u&#39;albino_python&#39;, 0.6158879995346069),
(u&#39;croc&#39;, 0.6083582639694214),
(u&#39;lizard&#39;, 0.601341724395752)]
 2. Now we can use this model to find the solution to the equation: What is king - man &#43; woman?
model.most_similar(positive = [&amp;#39;king&amp;#39;,&amp;#39;woman&amp;#39;],negative = [&amp;#39;man&amp;#39;]) [(u&#39;queen&#39;, 0.7118192315101624),
(u&#39;monarch&#39;, 0.6189674139022827),
(u&#39;princess&#39;, 0.5902431011199951),
(u&#39;crown_prince&#39;, 0.5499460697174072),
(u&#39;prince&#39;, 0.5377321839332581),
(u&#39;kings&#39;, 0.5236844420433044),
(u&#39;Queen_Consort&#39;, 0.5235946178436279),
(u&#39;queens&#39;, 0.5181134343147278),
(u&#39;sultan&#39;, 0.5098593235015869),
(u&#39;monarchy&#39;, 0.5087412595748901)]
 You can do plenty of freaky/cool things using this:
3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma? model.most_similar(positive = [&amp;#39;emma&amp;#39;,&amp;#39;he&amp;#39;,&amp;#39;male&amp;#39;,&amp;#39;mr&amp;#39;],negative = [&amp;#39;she&amp;#39;,&amp;#39;mrs&amp;#39;,&amp;#39;female&amp;#39;]) [(u&#39;sanchez&#39;, 0.4920658469200134),
(u&#39;kenny&#39;, 0.48300960659980774),
(u&#39;alves&#39;, 0.4684845209121704),
(u&#39;gareth&#39;, 0.4530612826347351),
(u&#39;bellamy&#39;, 0.44884198904037476),
(u&#39;gibbs&#39;, 0.445194810628891),
(u&#39;dos_santos&#39;, 0.44508373737335205),
(u&#39;gasol&#39;, 0.44387346506118774),
(u&#39;silva&#39;, 0.4424275755882263),
(u&#39;shaun&#39;, 0.44144102931022644)]
 4. Find which word doesn&amp;rsquo;t belong to a list? model.doesnt_match(&amp;#34;math shopping reading science&amp;#34;.split(&amp;#34; &amp;#34;)) I think staple doesn&amp;rsquo;t belong in this list!
Other Cool Things 1. Recommendations:   In this paper, the authors have shown that itembased CF can be cast in the same framework of word embedding.
2. Some other examples that people have seen after using their own embeddings: Library - Books = Hall
Obama &#43; Russia - USA = Putin
Iraq - Violence = Jordan
President - Power = Prime Minister (Not in India Though)
3.Seeing the above I started playing with it a little. Is this model sexist?
model.most_similar(positive = [&amp;#34;donald_trump&amp;#34;],negative = [&amp;#39;brain&amp;#39;]) [(u&#39;novak&#39;, 0.40405112504959106),
(u&#39;ozzie&#39;, 0.39440611004829407),
(u&#39;democrate&#39;, 0.39187556505203247),
(u&#39;clinton&#39;, 0.390536367893219),
(u&#39;hillary_clinton&#39;, 0.3862358033657074),
(u&#39;bnp&#39;, 0.38295692205429077),
(u&#39;klaar&#39;, 0.38228923082351685),
(u&#39;geithner&#39;, 0.380607008934021),
(u&#39;bafana_bafana&#39;, 0.3801495432853699),
(u&#39;whitman&#39;, 0.3790769875049591)]
 Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</guid>
      
      

      
      <description>As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.</description>

      <content:encoded>  
        
        <![CDATA[    As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.  Simple Random Sampling : Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen. Stratified Sampling: Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns : Town A has 1 million factory workers, Town B has 2 million workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample. Reservoir Sampling :Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.  2. Map-Reduce. If you want to work with the whole data. Can be used for feature creation. For Example: I had a use case where I had a graph of 60 Million customers and 130 Million accounts. Each account was connected to other account if they had the Same SSN or Same Name&#43;DOB&#43;Address. I had to find customer ID’s for each of the accounts. On a single node parsing such a graph took more than 2 days. On a Hadoop cluster of 80 nodes running a Connected Component Algorithm took less than 24 minutes. On Spark it is even faster.
3. Graph Algorithms. Recently I was working on an optimization problem which was focussed on finding shortest distance and routes between two points in a store layout. Routes which don’t pass through different aisles, so we cannot use euclidean distances. We solved this problem by considering turning points in the store layout and the djikstra’s Algorithm.
 4. Feature Selection.  Univariate Selection. Statistical tests can be used to select those features that have the strongest relationship with the output variable. VarianceThreshold. Feature selector that removes all low-variance features. Recursive Feature Elimination. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Feature Importance: Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.  5. Algorithms to work efficiently. Apart from these above algorithms sometimes you may need to write your own algorithms. Now I think of big algorithms as a combination of small but powerful algorithms. You just need to have idea of these algorithms to make a more better/efficient product. So some of these powerful algorithms which can help you are:
 Recursive Algorithms:Binary search algorithm. Divide and Conquer Algorithms: Merge-Sort. Dynamic Programming:Solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.  6. Classification/Regression Algorithms. The usual suspects. Minimum you must know:  Linear Regression - Ridge Regression, Lasso Regression, ElasticNet Logistic Regression From there you can build upon:  Decision Trees - ID3, CART, C4.5, C5.0 KNN SVM ANN - Back Propogation, CNN  And then on to Ensemble based algorithms:  Boosting: Gradient Boosted Trees Bagging: Random Forests Blending: Prediction outputs of different learning algorithms are fed into another learning algorithm.   7 . Clustering Methods.For unsupervised learning.  k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering  8. Other algorithms you can learn about:  Apriori algorithm- Association Rule Mining Eclat algorithm - Association Rule Mining Item/User Based Similarity - Recommender Systems Reinforcement learning - Build your own robot. Graphical Models Bayesian Algorithms NLP - For language based models. Chatbots.  Hope this has been helpful&amp;hellip;..
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Pandas For All - Some Basic Pandas Functions</title>
      <link>https://mlwhiz.com/blog/2016/10/27/baby_panda/</link>
      <pubDate>Thu, 27 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/10/27/baby_panda/</guid>
      
      

      
      <description>It has been quite a few days I have been working with Pandas and apparently I feel I have gotten quite good at it. (Quite a Braggard I know) So thought about adding a post about Pandas usage here. I intend to make this post quite practical and since I find the pandas syntax quite self explanatory, I won&amp;amp;rsquo;t be explaining much of the codes. Just the use cases and the code to achieve them.</description>

      <content:encoded>  
        
        <![CDATA[  It has been quite a few days I have been working with Pandas and apparently I feel I have gotten quite good at it. (Quite a Braggard I know) So thought about adding a post about Pandas usage here. I intend to make this post quite practical and since I find the pandas syntax quite self explanatory, I won&amp;rsquo;t be explaining much of the codes. Just the use cases and the code to achieve them.
1. Import Pandas We Start by importing the libraries that we will need to use.
import pandas as pd 2. Read a Datasource: # Read from csv data files # With Header df = pd.read_csv(&amp;#34;/Users/ragarw5/Downloads/SalesJan2009.csv&amp;#34;) # Without Header. sep param to provide the delimiter df = pd.read_csv(&amp;#34;/Users/ragarw5/Downloads/SalesJan2009.csv&amp;#34;, header=None, sep= &amp;#34;,&amp;#34;) # Reading from SQL Datasource import MySQLdb from pandas import DataFrame from pandas.io.sql import read_sql db = MySQLdb.connect(host=&amp;#34;localhost&amp;#34;, # your host, usually localhost user=&amp;#34;root&amp;#34;, # your username passwd=&amp;#34;password&amp;#34;, # your password db=&amp;#34;dbname&amp;#34;) # name of the data base query = &amp;#34;SELECT * FROM tablename&amp;#34; data = read_sql(query, db) # Reading from ExcelFile data = pd.read_excel(filename) For now, we will be working with the file at http://samplecsvs.s3.amazonaws.com/SalesJan2009.csv. The Sales Jan 2009 file contains some “sanitized” sales transactions during the month of January. If you want to work along you can download this file from that location.
df = pd.read_csv(&amp;#34;/Users/ragarw5/Downloads/SalesJan2009.csv&amp;#34;) 3. See few rows of data: # top 5 rows df.head() # top 50 rows df.head(50) # last 5 rows df.tail() # last 50 rows df.tail(50) 4. Getting Column Names in a list: columnnames = df.columns 5. Specifying user defined Column Names: Sometimes you want to change the column names:
df.columns = [&amp;#39;Transdate&amp;#39;, &amp;#39;Product&amp;#39;, &amp;#39;Price&amp;#39;, &amp;#39;PaymentType&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;City&amp;#39;, &amp;#39;State&amp;#39;, &amp;#39;Country&amp;#39;, &amp;#39;AccountCreated&amp;#39;, &amp;#39;LastLogin&amp;#39;, &amp;#39;Latitude&amp;#39;, &amp;#39;Longitude&amp;#39;] 6. Subsetting specific columns: Sometimes you only need to work with specific columns in a dataframe only. You can subset the columns in the dataframe using
newDf = df[[&amp;#39;Product&amp;#39;, &amp;#39;Price&amp;#39;, &amp;#39;PaymentType&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;City&amp;#39;, &amp;#39;State&amp;#39;, &amp;#39;Country&amp;#39;]] 7. Seeing column types: newDf.dtypes 8. Change type of a column First thing i try is this.
newDf[&amp;#39;Price&amp;#39;] = newDf[&amp;#39;Price&amp;#39;].astype(&amp;#39;int&amp;#39;) It gives error : ValueError: invalid literal for long() with base 10: &amp;lsquo;13,000&amp;rsquo;. That is you cannot cast a string with &amp;ldquo;,&amp;rdquo; to an int. To do that we first have to get rid of the comma. For that we use a particular lambda-apply functionality which lets us apply functions to each row in the data.
newDf[&amp;#39;Price&amp;#39;] = newDf.apply(lambda x: int(x[&amp;#39;Price&amp;#39;].replace(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;)),axis=1)  9. Simple Dataframe Statistics: # To get statistics of numerical columns newDf.describe() # To get maximum value of a column. When you take a single column you can think of it as a list and apply functions you would apply to a list max(newDf[&amp;#39;Price&amp;#39;]) # no of rows in dataframe len(newDf) # Shape of Dataframe newDf.shape 10. Creating a new column: # Create a column Address containing City,State and Country. Simply concat the columns. newDf[&amp;#39;Address&amp;#39;] = newDf[&amp;#39;City&amp;#39;] &#43;&amp;#34;,&amp;#34;&#43; newDf[&amp;#39;State&amp;#39;] &#43;&amp;#34;,&amp;#34;&#43; newDf[&amp;#39;Country&amp;#39;] # I like to use a function defined approach with lambda-apply as it gives me more flexibility and more options. Like if i want to create a column which is 1 if the price is greater than 1200 and 0 otherwise. def gt(x): if x&amp;gt;1200: return 1 else: return 0 newDf[&amp;#39;Pricegt1200&amp;#39;] = newDf.apply(lambda x: gt(x[&amp;#39;Price&amp;#39;]),axis=1) 11. Subset a DataFrame: # Single condition: dataframe with all entries priced greater than 1500 df_gt_1500 = newDf[newDf[&amp;#39;Price&amp;#39;]&amp;gt;1500] # Multiple conditions: AND - dataframe with all entries priced greater than 1500 and from London And_df = newDf[(newDf[&amp;#39;Price&amp;#39;]&amp;gt;1500) &amp;amp; (newDf[&amp;#39;City&amp;#39;]==&amp;#39;London&amp;#39;)] # Multiple conditions: OR - dataframe with all entries priced greater than 1500 or from London Or_df = newDf[(newDf[&amp;#39;Price&amp;#39;]&amp;gt;1500) | (newDf[&amp;#39;City&amp;#39;]==&amp;#39;London&amp;#39;)] # Multiple conditions: NOT - dataframe with all entries priced greater than 1500 or from London have to be excluded Not_df = newDf[~((newDf[&amp;#39;Price&amp;#39;]&amp;gt;1500) | (newDf[&amp;#39;City&amp;#39;]==&amp;#39;London&amp;#39;))] 12. Change the Column at particular places or impute: # In the state column the state is abbreviated as &amp;#39;TX&amp;#39;. We want the whole name &amp;#39;Texas&amp;#39; in there newDf.loc[newDf[&amp;#39;State&amp;#39;]==&amp;#39;TX&amp;#39;,&amp;#39;State&amp;#39;] = &amp;#39;Texas&amp;#39; # When City is Monaco State is not given. You want to impute &amp;#39;Monaco State&amp;#39; as state also. newDf.loc[newDf[&amp;#39;City&amp;#39;]==&amp;#39;Monaco&amp;#39;,&amp;#39;State&amp;#39;] = &amp;#39;Monaco State&amp;#39; 13. GroupBy: One of the most used functionality. One simple example
# Find out the sum of transactions by a state. reset_index() is a function that resets the index of a dataframe. I apply this function ALWAYS whenever I do a groupby and you might think of it as a default syntax for groupby operations import numpy as np newDf.groupby([&amp;#39;State&amp;#39;]).aggregate(np.sum).reset_index() # You might get a few extra columns that you dont need. Just subset the columns in the dataframe. You could just chain the commands to subset for the columns you need. newDf.groupby([&amp;#39;State&amp;#39;]).aggregate(np.sum).reset_index()[[&amp;#39;State&amp;#39;,&amp;#39;Price&amp;#39;]] # Find minimum transaction in each state newDf.groupby([&amp;#39;State&amp;#39;]).aggregate(np.min).reset_index()[[&amp;#39;State&amp;#39;,&amp;#39;Price&amp;#39;]] # You might want to groupby more than one column newDf.groupby([&amp;#39;State&amp;#39;,&amp;#39;City&amp;#39;]).aggregate(np.sum).reset_index()[[&amp;#39;State&amp;#39;,&amp;#39;City&amp;#39;,&amp;#39;Price&amp;#39;]] 14. Concat: You have two datarames df1 and df2 you need to concat. Means append one below the other you can do it using:
pd.concat([df1,df2]) 15. Merge: #Suppose in the start, you had two dataframes. One which contains city and price information: City_Price = newwDf[[&amp;#39;City&amp;#39;,&amp;#39;Price&amp;#39;]] #And another which contains &amp;#39;City&amp;#39; and &amp;#39;State&amp;#39; insformation City_State = newDf[[&amp;#39;City&amp;#39;,&amp;#39;State&amp;#39;]].drop_duplicates(keep=False).reset_index() #You need to merge these datatframes on basis of city. You need to do: City_Price_State_df = pd.merge(City_Price,City_State,on=[&amp;#39;City&amp;#39;],how=&amp;#39;left&amp;#39;) 16. Save a Dataframe to external File: # To Csv file newDf.to_csv(&amp;#34;NewDfData.csv&amp;#34;,index=False) # To Excel File from pandas import ExcelWriter writer = ExcelWriter(&amp;#39;NewDfData.xlsx&amp;#39;) newDf.to_excel(writer,&amp;#39;Sheet1&amp;#39;) writer.save() 17. Pushing Pandas Df to a sql database: from pandas.io import sql import MySQLdb db = MySQLdb.connect(host=&amp;#34;localhost&amp;#34;, # your host, usually localhost user=&amp;#34;root&amp;#34;, # your username passwd=&amp;#34;password&amp;#34;, # your password db=&amp;#34;dbname&amp;#34;) # name of the data base newDf.to_sql(con = db, name=&amp;#39;tablename&amp;#39;,if_exists=&amp;#39;append&amp;#39;,flavor=&amp;#39;mysql&amp;#39;, chunksize=10000,index=False) Hope you found this post useful and worth your time. I tried to make this as simple as possible but You may always ask me or see the documentation for doubts.
If you have any more ideas on how to use Pandas or other usecases, please suggest in the comments section.
Till then ciao!!
References  Intro to Pandas By Greg Rada What I have written is in a condensed form, If you want to get a detailed description visit Greg Rada&amp;rsquo;s 3 posts series. Pandas Documentation  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know - Part II(AWK)</title>
      <link>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</link>
      <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</guid>
      
      

      
      <description>Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.
And it is easy to learn.</description>

      <content:encoded>  
        
        <![CDATA[  Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.
And it is easy to learn.
In this post, I will try to give you a brief intro about how you could add awk to your daily work-flow.
Please see my previous post if you want some background or some basic to intermediate understanding of shell commands.
Basics/ Fundamentals So let me start with an example first. Say you wanted to sum a column in a comma delimited file. How would you do that in shell?
Here is the command. The great thing about awk is that it took me nearly 5 sec to write this command. I did not have to open any text editor to write a python script.
It lets you do adhoc work quickly.
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum }&amp;#39; data.txt 44662539172  See the command one more time. There is a basic structure to the awk command
BEGIN {action} pattern {action} pattern {action} . . pattern { action} END {action}   An awk program consists of:
 An optional BEGIN segment : In the begin part we initialize our variables before we even start reading from the file or the standard input.
 pattern - action pairs: In the middle part we Process the input data. You put multiple pattern action pairs when you want to do multiple things with the same line.
 An optional END segment: In the end part we do something we want to do when we have reached the end of file.
  An awk command is called on a file using:
awk &amp;#39;BEGIN{SOMETHING HERE} {SOMETHING HERE: could put Multiple Blocks Like this} END {SOMETHING HERE}&amp;#39; file.txt You also need to know about these preinitialized variables that awk keeps track of.:
 FS : field separator. Default is whitespace (1 or more spaces or tabs). If you are using any other seperator in the file you should specify it in the Begin Part. RS : record separator. Default record separator is newline. Can be changed in BEGIN action. NR : NR is the variable whose value is the number of the current record. You normally use it in the action blocks in the middle. NF : The Number of Fields after the single line has been split up using FS. Dollar variables : awk splits up the line which is coming to it by using the given FS and keeps the split parts in the $ variables. For example column 1 is in $1, column 2 is in $2. $0 is the string representation of the whole line. Note that if you want to access last column you don&amp;rsquo;t have to count. You can just use $NF. For second last column you can use $(NF-1). Pretty handy. Right.  So If you are with me till here, the hard part is done. Now the fun part starts. Lets look at the first awk command again and try to understand it.
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum }&amp;#39; data.txt So there is a begin block. Remember before we read any line. We initialize sum to 0 and FS to &amp;ldquo;,&amp;rdquo;.
Now as awk reads its input line by line it increments sum by the value in column 5(as specified by $5).
Note that there is no pattern specified here so awk will do the action for every line.
When awk has completed reading the file it prints out the sum.
What if you wanted mean?
We could create a cnt Variable:
awk &amp;#39;BEGIN{ sum=0;cnt=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5; cnt&#43;=1 } END { print sum/cnt }&amp;#39; data.txt 1.86436e&#43;06  or better yet, use our friend NR which bash is already keeping track of:
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum/NR }&amp;#39; data.txt 1.86436e&#43;06  Filter a file In the mean and sum awk commands we did not put any pattern in our middle commands. Let us use a simple pattern now. Suppose we have a file Salaries.csv which contains:
head salaries.txt yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000 1985,BAL,AL,lacyle01,725000 1985,BAL,AL,flanami01,641667 1985,BAL,AL,boddimi01,625000 1985,BAL,AL,stewasa01,581250 1985,BAL,AL,martide01,560000 1985,BAL,AL,roeniga01,558333  I want to filter records for players who who earn more than 22 M in 2013 just because I want to. You just do:
awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} $5&amp;gt;=22000000 &amp;amp;&amp;amp; $1==2013{print $0}&amp;#39; Salaries.csv 2013,DET,AL,fieldpr01,23000000 2013,MIN,AL,mauerjo01,23000000 2013,NYA,AL,rodrial01,29000000 2013,NYA,AL,wellsve01,24642857 2013,NYA,AL,sabatcc01,24285714 2013,NYA,AL,teixema01,23125000 2013,PHI,NL,leecl02,25000000 2013,SFN,NL,linceti01,22250000  Cool right. Now let me explain it a little bit. The part in the command &amp;ldquo;$5&amp;gt;=22000000 &amp;amp;&amp;amp; $1==2013&amp;rdquo; is called a pattern. It says that print this line($0) if and only if the Salary($5) is more than 22M and(&amp;amp;&amp;amp;) year($1) is equal to 2013. If the incoming record(line) does not satisfy this pattern it never reaches the inner block.
So Now you could do basic Select SQL at the command line only if you had:
The logic Operators:
 == equality operator; returns TRUE is both sides are equal
 != inverse equality operator
 &amp;amp;&amp;amp; logical AND
 || logical OR
 ! logical NOT
 &amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;= relational operators
  Normal Arithmetic Operators: &#43;, -, /, *, %, ^
Some String Functions: length, substr, split
GroupBy Now you will say: &amp;ldquo;Hey Dude SQL without groupby is incomplete&amp;rdquo;. You are right and for that we can use the associative array. Lets just see the command first and then I will explain. So lets create another useless use case(or may be something useful to someone :)) We want to find out the number of records for each year in the file. i.e we want to find the distribution of years in the file. Here is the command:
awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} {my_array[$1]=my_array[$1]&#43;1} END{ for (k in my_array){if(k!=&amp;#34;yearID&amp;#34;)print k&amp;#34;|&amp;#34;my_array[k]}; }&amp;#39; Salaries.csv 1990|867 1991|685 1996|931 1997|925 ...  Now I would like to tell you a secret. You don&amp;rsquo;t really need to declare the variables you want to use in awk. So you did not really needed to define sum, cnt variables before. I only did that because it is good practice. If you don&amp;rsquo;t declare a user defined variable in awk, awk assumes it to be null or zero depending on the context. So in the command above we don&amp;rsquo;t declare our myarray in the begin block and that is fine.
Associative Array: The variable myarray is actually an associative array. i.e. It stores data in a key value format.(Python dictionaries anyone). The same array could keep integer keys and String keys. For example, I can do this in a single code.
myarray[1]=&#34;key&#34; myarray[&#39;mlwhiz&#39;] = 1   For Loop for associative arrays: I could use a for loop to read associative array
for (k in array) { DO SOMETHING } # Assigns to k each Key of array (unordered) # Element is array[k]   If Statement:Uses a syntax like C for the if statement. the else block is optional:
if (n  0){ DO SOMETHING } else{ DO SOMETHING }   So lets dissect the above command now.
I set the File separator to &amp;ldquo;,&amp;rdquo; in the beginning. I use the first column as the key of myarray. If the key exists I increment the value by 1.
At the end, I loop through all the keys and print out key value pairs separated by &amp;ldquo;|&amp;rdquo;
I know that the header line in my file contains &amp;ldquo;yearID&amp;rdquo; in column 1 and I don&amp;rsquo;t want &amp;lsquo;yearID|1&amp;rsquo; in the output. So I only print when Key is not equal to &amp;lsquo;yearID&amp;rsquo;.
GroupBy with case statement: cat Salaries.csv | awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} $5&amp;lt;100000{array5[&amp;#34;[0-100000)&amp;#34;]&#43;=1} $5&amp;gt;=100000&amp;amp;&amp;amp;$5&amp;lt;250000{array5[&amp;#34;[100000,250000)&amp;#34;]=array5[&amp;#34;[100000,250000)&amp;#34;]&#43;1} $5&amp;gt;=250000&amp;amp;&amp;amp;$5&amp;lt;500000{array5[&amp;#34;[250000-500000)&amp;#34;]=array5[&amp;#34;[250000-500000)&amp;#34;]&#43;1} $5&amp;gt;=500000&amp;amp;&amp;amp;$5&amp;lt;1000000{array5[&amp;#34;[500000-1000000)&amp;#34;]=array5[&amp;#34;[500000-1000000)&amp;#34;]&#43;1} $5&amp;gt;=1000000{array5[&amp;#34;[1000000)&amp;#34;]=array5[&amp;#34;[1000000)&amp;#34;]&#43;1} END{ print &amp;#34;VAR Distrib:&amp;#34;; for (v in array5){print v&amp;#34;|&amp;#34;array5[v]} }&amp;#39; VAR Distrib: [250000-500000)|8326 [0-100000)|2 [1000000)|23661 [100000,250000)|9480  Here we used multiple pattern-action blocks to create a case statement.
For The Brave: This is a awk code that I wrote to calculate the Mean,Median,min,max and sum of a column simultaneously. Try to go through the code and understand it.I have added comments too. Think of this as an exercise. Try to run this code and play with it. You may learn some new tricks in the process. If you don&amp;rsquo;t understand it do not worry. Just get started writing your own awk codes, you will be able to understand it in very little time.
# Create a New file named A.txt to keep only the salary column. cat Salaries.csv | cut -d &amp;#34;,&amp;#34; -f 5 &amp;gt; A.txt FILENAME=&amp;#34;A.txt&amp;#34; # The first awk counts the number of lines which are numeric. We use a regex here to check if the column is numeric or not. # &amp;#39;;&amp;#39; stands for Synchronous execution i.e sort only runs after the awk is over. # The output of both commands are given to awk command which does the whole work. # So Now the first line going to the second awk is the number of lines in the file which are numeric. # and from the second to the end line the file is sorted. (awk &amp;#39;BEGIN {c=0} $1 ~ /^[-0-9]*(\.[0-9]*)?$/ {c=c&#43;1;} END {print c;}&amp;#39; &amp;#34;$FILENAME&amp;#34;; \  sort -n &amp;#34;$FILENAME&amp;#34;) | awk &amp;#39; BEGIN { c = 0; sum = 0; med1_loc = 0; med2_loc = 0; med1_val = 0; med2_val = 0; min = 0; max = 0; } NR==1 { LINES = $1 # We check whether numlines is even or odd so that we keep only # the locations in the array where the median might be. if (LINES%2==0) {med1_loc = LINES/2-1; med2_loc = med1_loc&#43;1;} if (LINES%2!=0) {med1_loc = med2_loc = (LINES-1)/2;} } $1 ~ /^[-0-9]*(\.[0-9]*)?$/ &amp;amp;&amp;amp; NR!=1 { # setting min value if (c==0) {min = $1;} # middle two values in array if (c==med1_loc) {med1_val = $1;} if (c==med2_loc) {med2_val = $1;} c&#43;&#43; sum &#43;= $1 max = $1 } END { ave = sum / c median = (med1_val &#43; med2_val ) / 2 print &amp;#34;sum:&amp;#34; sum print &amp;#34;count:&amp;#34; c print &amp;#34;mean:&amp;#34; ave print &amp;#34;median:&amp;#34; median print &amp;#34;min:&amp;#34; min print &amp;#34;max:&amp;#34; max } &amp;#39; &amp;lt;pre style=&amp;#34;font-size:50%; padding:7px; margin:0em; background-color:#FFF112&amp;#34;&amp;gt;sum:44662539172 count:23956 mean:1.86436e&#43;06 median:507950 min:0 max:33000000 &amp;lt;/pre&amp;gt; Endnote: awk is an awesome tool and there are a lot of use-cases where it can make your life simple. There is a sort of a learning curve, but I think that it would be worth it in the long term. I have tried to give you a taste of awk and I have covered a lot of ground here in this post. To tell you a bit more there, awk is a full programming language. There are for loops, while loops, conditionals, booleans, functions and everything else that you would expect from a programming language. So you could look more still.
To learn more about awk you can use this book. This book is a free resource and you could learn more about awk and use cases.
Or if you like to have your book binded and in paper like me you can buy this book, which is a gem:
Do leave comments in case you find more use-cases for awk or if you want me to write on new use-cases. Or just comment weather you liked it or not and how I could improve as I am also new and trying to learn more of this.
Till then Ciao !!!
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know -Part I</title>
      <link>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</guid>
      
      

      
      <description>Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).
Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.</description>

      <content:encoded>  
        
        <![CDATA[  Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).
Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.
Probably whatever you do it won&amp;rsquo;t be simpler/less time consuming than this
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 1 | sort | uniq | wc -l 30  And this will run way faster than whatever you do with perl/python script.
Now this command says
 Use the cat command to print/stream the contents of the file to stdout. Pipe the streaming contents from our cat command to the next command cut. The cut commands specifies the delimiter by the argument -d and the column by the argument -f and streams the output to stdout. Pipe the streaming content to the sort command which sorts the input and streams only the distinct values to the stdout. It takes the argument -u that specifies that we only need unique values. Pipe the output to the wc -l command which counts the number of lines in the input.  There is a lot going on here and I will try my best to ensure that you will be able to understand most of it by the end of this Blog post.Although I will also try to explain more advanced concepts than the above command in this post.
Now, I use shell commands extensively at my job. I will try to explain the usage of each of the commands based on use cases that I counter nearly daily at may day job as a data scientist.
Some Basic Commands in Shell: There are a lot of times when you just need to know a little bit about the data. You just want to see may be a couple of lines to inspect a file. One way of doing this is opening the txt/csv file in the notepad. And that is probably the best way for small files. But you could also do it in the shell using:
1. cat cat data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  Now the cat command prints the whole file in the terminal window for you.I have not shown the whole file here.
But sometimes the files will be so big that you wont be able to open them up in notepad&#43;&#43; or any other software utility and there the cat command will shine.
2. Head and Tail Now you might ask me why would you print the whole file in the terminal itself? Generally I won&amp;rsquo;t. But I just wanted to tell you about the cat command. For the use case when you want only the top/bottom n lines of your data you will generally use the head/tail commands. You can use them as below.
head data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  head -n 3 data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000  tail data.txt 2013|WAS|NL|bernaro01|1212500 2013|WAS|NL|tracych01|1000000 2013|WAS|NL|stammcr01|875000 2013|WAS|NL|dukeza01|700000 2013|WAS|NL|espinda01|526250 2013|WAS|NL|matthry01|504500 2013|WAS|NL|lombast02|501250 2013|WAS|NL|ramoswi01|501250 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  tail -n 2 data.txt 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  Notice the structure of the shell command here.
CommandName [-arg1name] [arg1value] [-arg2name] [arg2value] filename   3. Piping Now we could have also written the same command as:
cat data.txt | head yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  This brings me to one of the most important concepts of Shell usage - piping. You won&amp;rsquo;t be able to utilize the full power the shell provides without using this concept. And the concept is actually simple.
Just read the &amp;ldquo;|&amp;rdquo; in the command as &amp;ldquo;pass the data on to&amp;rdquo;
So I would read the above command as:
cat(print) the whole data to stream, pass the data on to head so that it can just give me the first few lines only.
So did you understood what piping did? It is providing us a way to use our basic commands in a consecutive manner. There are a lot of commands that are fairly basic and it lets us use these basic commands in sequence to do some fairly non trivial things.
Now let me tell you about a couple of more commands before I show you how we can chain them to do fairly advanced tasks.
4. wc wc is a fairly useful shell utility/command that lets us count the number of lines(-l), words(-w) or characters(-c) in a given file
wc -l data.txt 23957 data.txt  5. grep You may want to print all the lines in your file which have a particular word. Or as a Data case you might like to see the salaries for the team BAL in 2000. In this case we have printed all the lines in the file which contain &amp;ldquo;2000|BAL&amp;rdquo;. grep is your friend.
grep &amp;#34;2000|BAL&amp;#34; data.txt | head 2000|BAL|AL|belleal01|12868670 2000|BAL|AL|anderbr01|7127199 2000|BAL|AL|mussimi01|6786032 2000|BAL|AL|ericksc01|6620921 2000|BAL|AL|ripkeca01|6300000 2000|BAL|AL|clarkwi02|6000000 2000|BAL|AL|johnsch04|4600000 2000|BAL|AL|timlimi01|4250000 2000|BAL|AL|deshide01|4209324 2000|BAL|AL|surhobj01|4146789  you could also use regular expressions with grep.
6. sort You may want to sort your dataset on a particular column.Sort is your friend. Say you want to find out the top 10 maximum salaries given to any player in your dataset.
sort -t &amp;#34;|&amp;#34; -k 5 -r -n data.txt | head -10 2010|NYA|AL|rodrial01|33000000 2009|NYA|AL|rodrial01|33000000 2011|NYA|AL|rodrial01|32000000 2012|NYA|AL|rodrial01|30000000 2013|NYA|AL|rodrial01|29000000 2008|NYA|AL|rodrial01|28000000 2011|LAA|AL|wellsve01|26187500 2005|NYA|AL|rodrial01|26000000 2013|PHI|NL|leecl02|25000000 2013|NYA|AL|wellsve01|24642857  So there are certainly a lot of options in this command. Lets go through them one by one.
 -t: Which delimiter to use? -k: Which column to sort on? -n: If you want Numerical Sorting. Dont use this option if you want Lexographical sorting. -r: I want to sort Descending. Sorts Ascending by Default.  7. cut This command lets you select certain columns from your data. Sometimes you may want to look at just some of the columns in your data. As in you may want to look only at the year, team and salary and not the other columns. cut is the command to use.
cut -d &amp;#34;|&amp;#34; -f 1,2,5 data.txt | head yearID|teamID|salary 1985|BAL|1472819 1985|BAL|1090000 1985|BAL|800000 1985|BAL|725000 1985|BAL|641667 1985|BAL|625000 1985|BAL|581250 1985|BAL|560000 1985|BAL|558333  The options are:
 -d: Which delimiter to use? -f: Which column/columns to cut?  8. uniq uniq is a little bit tricky as in you will want to use this command in sequence with sort. This command removes sequential duplicates. So in conjunction with sort it can be used to get the distinct values in the data. For example if I wanted to find out 10 distinct teamIDs in data, I would use:
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 2 | sort | uniq | head ANA ARI ATL BAL BOS CAL CHA CHN CIN CLE  This command could be used with argument -c to count the occurrence of these distinct values. Something akin to count distinct.
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 2 | sort | uniq -c | head 247 ANA 458 ARI 838 ATL 855 BAL 852 BOS 368 CAL 812 CHA 821 CHN 46 CIN 867 CLE  Some Other Utility Commands for Other Operations Some Other command line tools that you could use without going in the specifics as the specifics are pretty hard.
1. Change delimiter in a file Find and Replace Magic.: You may want to replace certain characters in file with something else using the tr command.
cat data.txt | tr &amp;#39;|&amp;#39; &amp;#39;,&amp;#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  or the sed command
cat data.txt | sed -e &amp;#39;s/|/,/g&amp;#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  2. Sum of a column in a file Using the awk command you could find the sum of column in file. Divide it by the number of lines and you can get the mean.
cat data.txt | awk -F &amp;#34;|&amp;#34; &amp;#39;{ sum &#43;= $5 } END { printf sum }&amp;#39; 44662539172  awk is a powerful command which is sort of a whole language in itself. Do see the wiki page for awk for a lot of great usecases of awk. I also wrote a post on awk as a second part in this series. Check it HERE
3. Find the files in a directory that satisfy a certain condition You can do this by using the find command. Lets say you want to find all the .txt files in the current working dir that start with lowercase h.
find . -name &amp;#34;h*.txt&amp;#34; ./hamlet.txt  To find all .txt files starting with h regarless of case we could use regex.
find . -name &amp;#34;[Hh]*.txt&amp;#34; ./hamlet.txt ./Hamlet1.txt  4. Passing file list as Argument. xargs was suggested by Gaurav in the comments, so I read about it and it is actually a very nice command which you could use in a variety of use cases.
So if you just use a pipe, any command/utility receives data on STDIN (the standard input stream) as a raw pile of data that it can sort through one line at a time. However some programs don&amp;rsquo;t accept their commands on standard in. For example the rm command(which is used to remove files), touch command(used to create file with a given name) or a certain python script you wrote(which takes command line arguments). They expect it to be spelled out in the arguments to the command.
For example: rm takes a file name as a parameter on the command line like so: rm file1.txt. If I wanted to delete all &amp;lsquo;.txt&amp;rsquo; files starting with &amp;ldquo;h/H&amp;rdquo; from my working directory, the below command won&amp;rsquo;t work because rm expects a file as an input.
find . -name &amp;#34;[hH]*.txt&amp;#34; | rm usage: rm [-f | -i] [-dPRrvW] file ... unlink file  To get around it we can use the xargs command which reads the STDIN stream data and converts each line into space separated arguments to the command.
find . -name &amp;#34;[hH]*.txt&amp;#34; | xargs ./hamlet.txt ./Hamlet1.txt  Now you could use rm to remove all .txt files that start with h/H. A word of advice: Always see the output of xargs first before using rm.
find . -name &amp;#34;[hH]*.txt&amp;#34; | xargs rm Another usage of xargs could be in conjunction with grep to find all files that contain a given string.
find . -name &amp;#34;*.txt&amp;#34; | xargs grep &amp;#39;honest soldier&amp;#39; ./Data1.txt:O, farewell, honest soldier; ./Data2.txt:O, farewell, honest soldier; ./Data3.txt:O, farewell, honest soldier;  Hopefully You could come up with varied uses building up on these examples. One other use case could be to use this for passing arguments to a python script.
Other Cool Tricks Sometimes you want your data that you got by some command line utility(Shell commands/ Python scripts) not to be shown on stdout but stored in a textfile. You can use the &amp;rdquo;&amp;gt;&amp;rdquo; operator for that. For Example: You could have stored the file after replacing the delimiters in the previous example into anther file called newdata.txt as follows:
cat data.txt | tr &amp;#39;|&amp;#39; &amp;#39;,&amp;#39; &amp;gt; newdata.txt I really got confused between &amp;rdquo;|&amp;rdquo; (piping) and &amp;rdquo;&amp;gt;&amp;rdquo; (to_file) operations a lot in the beginning. One way to remember is that you should only use &amp;rdquo;&amp;gt;&amp;rdquo; when you want to write something to a file. &amp;rdquo;|&amp;rdquo; cannot be used to write to a file. Another operation you should know about is the &amp;rdquo;&amp;gt;&amp;gt;&amp;rdquo; operation. It is analogous to &amp;rdquo;&amp;gt;&amp;rdquo; but it appends to an existing file rather that replacing the file and writing over.
If you would like to know more about commandline, which I guess you would, here are some books that I would recommend for a beginner:
The first book is more of a fun read at leisure type of book. THe second book is a little more serious. Whatever suits you.
So, this is just the tip of the iceberg. Although I am not an expert in shell usage, these commands reduced my workload to a large extent. If there are some shell commands you use on a regular basis or some shell command that are cool, do tell in the comments. I would love to include it in the blogpost.
I wrote a blogpost on awk as a second part of this post. Check it Here
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title>
      <link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid>
      
      

      
      <description>When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph.</description>

      <content:encoded>  
        
        <![CDATA[  When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph. This led me to think that a Blog post to create common Graph types in Python is in order. But being the procrastinator that I am it always got pushed to the back of my head.
One thing that helped me in pursuit of my data visualization needs in Python was this awesome course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Highly Recommended.
But, yesterday I got introduced to Seaborn and I must say I am quite impressed with it. It makes beautiful graphs that are in my opinion better than R&amp;rsquo;s ggplot2. Gives you enough options to customize and the best part is that it is so easy to learn.
So I am finally writing this blog post with a basic purpose of creating a code base that provides me with ready to use codes which could be put into analysis in a fairly straight-forward manner.
Right. So here Goes.
We Start by importing the libraries that we will need to use.
import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes To create a use case for our graphs, we will be working with the Tips data that contains the following information.
tips = sns.load_dataset(&amp;#34;tips&amp;#34;) tips.head()   Scatterplot With Regression Line Now let us work on visualizing this data. We will use the regplot option in seaborn.
# We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) # Here we create a matplotlib axes object. The extra parameters we use # &amp;#34;ci&amp;#34; to remove confidence interval # &amp;#34;marker&amp;#34; to have a x as marker. # &amp;#34;scatter_kws&amp;#34; to provide style info for the points.[s for size] # &amp;#34;line_kws&amp;#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;, data=tips, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, line_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;alpha&amp;#34;:0.5,&amp;#34;lw&amp;#34;:4},marker=&amp;#34;x&amp;#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Tip&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Total Bill&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Now that required a bit of a code but i feel that it looks much better than what either Matplotlib or ggPlot2 could have rendered. We got a lot of customization without too much code.
But that is not really what actually made me like Seaborn. The plot type that actually got my attention was lmplot, which lets us use regplot in a faceted mode.
# So this function creates a faceted plot. The plot is parameterized by the following: # col : divides the data points into days and creates that many plots # palette: deep, muted, pastel, bright, dark, and colorblind. change the colors in graph. Experiment with these # col_wrap: we want 2 graphs in a row? Yes.We do # scatter_kws: attributes for points # hue: Colors on a particular column. # size: controls the size of graph g = sns.lmplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;,ci=None,data=tips, col=&amp;#34;day&amp;#34;, palette=&amp;#34;muted&amp;#34;,col_wrap=2,scatter_kws={&amp;#34;s&amp;#34;: 100,&amp;#34;alpha&amp;#34;:.5}, line_kws={&amp;#34;lw&amp;#34;:4,&amp;#34;alpha&amp;#34;:0.5},hue=&amp;#34;day&amp;#34;,x_jitter=1.0,y_jitter=1.0,size=6) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabels(&amp;#34;Tip&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabels(&amp;#34;Total Bill&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here titles = [&amp;#39;Thursday&amp;#39;,&amp;#39;Friday&amp;#39;,&amp;#39;Saturday&amp;#39;,&amp;#39;Sunday&amp;#39;] for ax,title in zip(g.axes.flat,titles): ax.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   A side Note on Palettes:
You can build your own color palettes using color_palette() function. color_palette() will accept the name of any seaborn palette or matplotlib colormap(except jet, which you should never use). It can also take a list of colors specified in any valid matplotlib format (RGB tuples, hex color codes, or HTML color names). The return value is always a list of RGB tuples. This allows you to use your own color palettes in graph.   Barplots sns.set(style=&amp;#34;ticks&amp;#34;) flatui = [&amp;#34;#9b59b6&amp;#34;, &amp;#34;#3498db&amp;#34;, &amp;#34;#95a5a6&amp;#34;, &amp;#34;#e74c3c&amp;#34;, &amp;#34;#34495e&amp;#34;, &amp;#34;#2ecc71&amp;#34;] # This Function takes as input a custom palette g = sns.barplot(x=&amp;#34;sex&amp;#34;, y=&amp;#34;tip&amp;#34;, hue=&amp;#34;day&amp;#34;, palette=sns.color_palette(flatui),data=tips,ci=None) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Do We tend to \nTip high on Weekends?&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Gender&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Mean Tips&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Histograms and Distribution Diagrams They form another part of my workflow. Lets plot the normal Histogram using seaborn. For this we will use the distplot function. This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() function. It can also fit scipy.stats distributions and plot the estimated PDF over the data.
# Create a list of 1000 Normal RVs x = np.random.normal(size=1000) sns.set_context(&amp;#34;poster&amp;#34;) sns.set_style(&amp;#34;ticks&amp;#34;) # This Function creates a normed Histogram by default. # If we use the parameter kde=False and norm_hist=False then # we will be using a count histogram g=sns.distplot(x, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Normal Simulation&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   import scipy.stats as stats a = 1.5 b = 1.5 x = np.arange(0.01, 1, 0.01) y = stats.beta.rvs(a,b,size=10000) y_act = stats.beta.pdf(x,a,b) g=sns.distplot(y,kde=False,norm_hist=True, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # Note that we plotted on the graph using plt matlabplot function plt.plot(x,y_act) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title((&amp;#34;Beta Simulation vs. Calculated Beta Density\nFor a=%s,b=%s&amp;#34;) %(a,b),fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   PairPlots You need to see how variables vary with one another. What is the distribution of variables in the dataset. This is the graph to use with the pairplot function. Very helpful And Seaborn males it a joy to use. We will use Iris Dataset here for this example.
iris = sns.load_dataset(&amp;#34;iris&amp;#34;) iris.head()   # Create a Pairplot g = sns.pairplot(iris,hue=&amp;#34;species&amp;#34;,palette=&amp;#34;muted&amp;#34;,size=5, vars=[&amp;#34;sepal_width&amp;#34;, &amp;#34;sepal_length&amp;#34;],kind=&amp;#39;reg&amp;#39;,markers=[&amp;#39;o&amp;#39;,&amp;#39;x&amp;#39;,&amp;#39;&#43;&amp;#39;]) # To change the size of the scatterpoints in graph g = g.map_offdiag(plt.scatter, s=35,alpha=0.5) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Relation between Sepal Width and Sepal Length&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3)   Hope you found this post useful and worth your time. You can find the iPython notebook at github
I tried to make this as simple as possible but You may always ask me or see the documentation for doubts.
If you have any more ideas on how to use Seaborn or which graphs should i add here, please suggest in the comments section.
I will definitely try to add to this post as I start using more visualizations and encounter other libraries as good as seaborn.
Also since this is my first visualization post on this blog, I would like to call out a good course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Behold the power of MCMC</title>
      <link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link>
      <pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid>
      
      

      
      <description>Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;amp;rsquo;t know the normalizing constant.
So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.</description>

      <content:encoded>  
        
        <![CDATA[    Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.
So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.
So here goes. This time we will cover some applications of MCMC in various areas of Computer Science using Python. If you feel the problems difficult to follow with, I would advice you to go back and read the previous post, which tries to explain MCMC Methods. We Will try to solve the following two problems:
 Breaking the Code - This problem has got somewhat of a great pedigree as this method was suggested by Persi Diaconis- The Mathemagician. So Someone comes to you with the below text. This text looks like gibberish but this is a code, Could you decrypyt it?
XZ STAVRK HXVR MYAZ OAKZM JKSSO SO MYR OKRR XDP JKSJRK XBMASD SO YAZ TWDHZ MYR JXMBYNSKF BSVRKTRM NYABY NXZ BXKRTRZZTQ OTWDH SVRK MYR AKSD ERPZMRXP KWZMTRP MYR JXTR OXBR SO X QSWDH NSIXD NXZ KXAZRP ORRETQ OKSI MYR JATTSN XDP X OXADM VSABR AIJRKORBMTQ XKMABWTXMRP MYR NSKPZ TRM IR ZRR MYR BYATP XDP PAR MYR ZWKHRSD YXP ERRD ZAMMADH NAMY YAZ OXBR MWKDRP MSNXKPZ MYR OAKR HAVADH MYR JXTIZ SO YAZ YXDPZ X NXKI XDP X KWE XTMRKDXMRTQ XZ MYR QSWDH NSIXD ZJSFR YR KSZR XDP XPVXDBADH MS MYR ERP Z YRXP ZXAP NAMY ISKR FADPDRZZ MYXD IAHYM YXVR ERRD RGJRBMRP SO YAI
 The Knapsack Problem - This problem comes from Introduction to Probability by Joseph Blitzstein. You should check out his courses STAT110 and CS109 as they are awesome. Also as it turns out Diaconis was the advisor of Joseph. So you have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution? This is known as the Knapsack Problem in Computer Science.
  Breaking the Code   So we look at the data and form a hypothesis that the data has been scrambled using a Substitution Cipher. We don&amp;rsquo;t know the encryption key, and we would like to know the Decryption Key so that we can decrypt the data and read the code.
To create this example, this data has actually been taken from Oliver Twist. We scrambled the data using a random encryption key, which we forgot after encrypting and we would like to decrypt this encrypted text using MCMC Chains. The real decryption key actually is &amp;ldquo;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;rdquo;
So lets think about this problem for a little bit. The decryption key could be any 26 letter string with all alphabets appearing exactly once. How many string permutations are there like that? That number would come out to be $26! \approx 10^{26}$ permutations. That is a pretty large number. If we go for using a brute force approach we are screwed. So what could we do? MCMC Chains come to rescue.
We will devise a Chain whose states theoritically could be any of these permutations. Then we will:
 Start by picking up a random current state. Create a proposal for a new state by swapping two random letters in the current state. Use a Scoring Function which calculates the score of the current state $Score_C$ and the proposed State $Score_P$. If the score of the proposed state is more than current state, Move to Proposed State. Else flip a coin which has a probability of Heads $Score_P/Score_C$. If it comes heads move to proposed State. Repeat from 2nd State.  If we get lucky we may reach a steady state where the chain has the stationary distribution of the needed states and the state that the chain is at could be used as a solution.
So the Question is what is the scoring function that we will want to use. We want to use a scoring function for each state(Decryption key) which assigns a positive score to each decryption key. This score intuitively should be more if the encrypted text looks more like actual english if decrypted using this decryption key.
So how can we quantify such a function. We will check a long text and calculate some statistics. See how many times one alphabet comes after another in a legitimate long text like War and Peace. For example we want to find out how many times does &amp;lsquo;BA&amp;rsquo; appears in the text or how many times &amp;lsquo;TH&amp;rsquo; occurs in the text.
For each pair of characters $\beta_1$ and $\beta_2$ (e.g. $\beta_1$ = T and $\beta_2$ =H), we let $R(\beta_1,\beta_2)$ record the number of times that specific pair(e.g. &amp;ldquo;TH&amp;rdquo;) appears consecutively in the reference text.
Similarly, for a putative decryption key x, we let $F_x(\beta_1,\beta_2)$ record the number of times that pair appears when the cipher text is decrypted using the decryption key x.
We then Score a particular decryption key x using:
$$Score(x) = \prod R(\beta_1,\beta_2)^{F_x(\beta_1,\beta_2)}$$ This function can be thought of as multiplying, for each consecutive pair of letters in the decrypted text, the number of times that pair occurred in the reference text. Intuitively, the score function is higher when the pair frequencies in the decrypted text most closely match those of the reference text, and the decryption key is thus most likely to be correct.
To make life easier with calculations we will calculate $log(Score(x))$
So lets start working through the problem step by step.
# AIM: To Decrypt a text using MCMC approach. i.e. find decryption key which we will call cipher from now on. import string import math import random # This function takes as input a decryption key and creates a dict for key where each letter in the decryption key # maps to a alphabet For example if the decryption key is &amp;#34;DGHJKL....&amp;#34; this function will create a dict like {D:A,G:B,H:C....}  def create_cipher_dict(cipher): cipher_dict = {} alphabet_list = list(string.ascii_uppercase) for i in range(len(cipher)): cipher_dict[alphabet_list[i]] = cipher[i] return cipher_dict # This function takes a text and applies the cipher/key on the text and returns text. def apply_cipher_on_text(text,cipher): cipher_dict = create_cipher_dict(cipher) text = list(text) newtext = &amp;#34;&amp;#34; for elem in text: if elem.upper() in cipher_dict: newtext&#43;=cipher_dict[elem.upper()] else: newtext&#43;=&amp;#34; &amp;#34; return newtext # This function takes as input a path to a long text and creates scoring_params dict which contains the  # number of time each pair of alphabet appears together # Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..} def create_scoring_params_dict(longtext_path): scoring_params = {} alphabet_list = list(string.ascii_uppercase) with open(longtext_path) as fp: for line in fp: data = list(line.strip()) for i in range(len(data)-1): alpha_i = data[i].upper() alpha_j = data[i&#43;1].upper() if alpha_i not in alphabet_list and alpha_i != &amp;#34; &amp;#34;: alpha_i = &amp;#34; &amp;#34; if alpha_j not in alphabet_list and alpha_j != &amp;#34; &amp;#34;: alpha_j = &amp;#34; &amp;#34; key = alpha_i&#43;alpha_j if key in scoring_params: scoring_params[key]&#43;=1 else: scoring_params[key]=1 return scoring_params # This function takes as input a text and creates scoring_params dict which contains the  # number of time each pair of alphabet appears together # Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..} def score_params_on_cipher(text): scoring_params = {} alphabet_list = list(string.ascii_uppercase) data = list(text.strip()) for i in range(len(data)-1): alpha_i =data[i].upper() alpha_j = data[i&#43;1].upper() if alpha_i not in alphabet_list and alpha_i != &amp;#34; &amp;#34;: alpha_i = &amp;#34; &amp;#34; if alpha_j not in alphabet_list and alpha_j != &amp;#34; &amp;#34;: alpha_j = &amp;#34; &amp;#34; key = alpha_i&#43;alpha_j if key in scoring_params: scoring_params[key]&#43;=1 else: scoring_params[key]=1 return scoring_params # This function takes the text to be decrypted and a cipher to score the cipher. # This function returns the log(score) metric def get_cipher_score(text,cipher,scoring_params): cipher_dict = create_cipher_dict(cipher) decrypted_text = apply_cipher_on_text(text,cipher) scored_f = score_params_on_cipher(decrypted_text) cipher_score = 0 for k,v in scored_f.iteritems(): if k in scoring_params: cipher_score &#43;= v*math.log(scoring_params[k]) return cipher_score # Generate a proposal cipher by swapping letters at two random location def generate_cipher(cipher): pos1 = random.randint(0, len(list(cipher))-1) pos2 = random.randint(0, len(list(cipher))-1) if pos1 == pos2: return generate_cipher(cipher) else: cipher = list(cipher) pos1_alpha = cipher[pos1] pos2_alpha = cipher[pos2] cipher[pos1] = pos2_alpha cipher[pos2] = pos1_alpha return &amp;#34;&amp;#34;.join(cipher) # Toss a random coin with robability of head p. If coin comes head return true else false. def random_coin(p): unif = random.uniform(0,1) if unif&amp;gt;=p: return False else: return True # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states  def MCMC_decrypt(n_iter,cipher_text,scoring_params): current_cipher = string.ascii_uppercase # Generate a random cipher to start state_keeper = set() best_state = &amp;#39;&amp;#39; score = 0 for i in range(n_iter): state_keeper.add(current_cipher) proposed_cipher = generate_cipher(current_cipher) score_current_cipher = get_cipher_score(cipher_text,current_cipher,scoring_params) score_proposed_cipher = get_cipher_score(cipher_text,proposed_cipher,scoring_params) acceptance_probability = min(1,math.exp(score_proposed_cipher-score_current_cipher)) if score_current_cipher&amp;gt;score: best_state = current_cipher if random_coin(acceptance_probability): current_cipher = proposed_cipher if i%500==0: print &amp;#34;iter&amp;#34;,i,&amp;#34;:&amp;#34;,apply_cipher_on_text(cipher_text,current_cipher)[0:99] return state_keeper,best_state ## Run the Main Program: scoring_params = create_scoring_params_dict(&amp;#39;war_and_peace.txt&amp;#39;) plain_text = &amp;#34;As Oliver gave this first proof of the free and proper action of his lungs, \ the patchwork coverlet which was carelessly flung over the iron bedstead, rustled; \ the pale face of a young woman was raised feebly from the pillow; and a faint voice imperfectly \ articulated the words, Let me see the child, and die. \ The surgeon had been sitting with his face turned towards the fire: giving the palms of his hands a warm \ and a rub alternately. As the young woman spoke, he rose, and advancing to the bed&amp;#39;s head, said, with more kindness \ than might have been expected of him: &amp;#34; encryption_key = &amp;#34;XEBPROHYAUFTIDSJLKZMWVNGQC&amp;#34; cipher_text = apply_cipher_on_text(plain_text,encryption_key) decryption_key = &amp;#34;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;#34; print&amp;#34;Text To Decode:&amp;#34;, cipher_text print &amp;#34;\n&amp;#34; states,best_state = MCMC_decrypt(10000,cipher_text,scoring_params) print &amp;#34;\n&amp;#34; print &amp;#34;Decoded Text:&amp;#34;,apply_cipher_on_text(cipher_text,best_state) print &amp;#34;\n&amp;#34; print &amp;#34;MCMC KEY FOUND:&amp;#34;,best_state print &amp;#34;ACTUAL DECRYPTION KEY:&amp;#34;,decryption_key   This chain converges around the 2000th iteration and we are able to unscramble the code. That&amp;rsquo;s awesome!!! Now as you see the MCMC Key found is not exactly the encryption key. So the solution is not a deterministic one, but we can see that it does not actually decrease any of the value that the MCMC Methods provide. Now Lets Help Bilbo :)
The Knapsack Problem Restating, we have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution?
So in this problem we have an $1$x$M$ array of Weight Values W, Gold Values G and a value for the maximum weight $w_{MAX}$ that Bilbo can carry. We want to find out an $1$x$M$ array $X$ of 1&amp;rsquo;s and 0&amp;rsquo;s, which holds weather Bilbo Carries a particular treasure or not. This array needs to follow the constraint $WX^T &amp;lt; w_{MAX}$ and we want to maximize $GX^T$ for a particular state X.(Here the T means transpose)
So lets first discuss as to how we will create a proposal from a previous state.
 Pick a random index from the state and toggle the index value. Check if we satisfy our constraint. If yes this state is the proposal state. Else pick up another random index and repeat.  We also need to think about the Scoring Function. We need to give high values to states with high gold value. We will use: $$Score(X)=e^{\beta GX^T}$$ We give exponentially more value to higher score. The Beta here is a &#43;ve constant. But how to choose it? If $\beta$ is big we will give very high score to good solutions and the chain will not be able to try new solutions as it can get stuck in local optimas. If we give a small value the chain will not converge to very good solutions. So weuse an Optimization Technique called Simulated Annealing i.e. we will start with a small value of $\beta$ and increase as no of iterations go up. That way the chain will explore in the starting stages and stay at the best solution in the later stages.
So now we have everything we need to get started
import numpy as np W = [20,40,60,12,34,45,67,33,23,12,34,56,23,56] G = [120,420,610,112,341,435,657,363,273,812,534,356,223,516] W_max = 150 # This function takes a state X , The gold vector G and a Beta Value and return the Log of score def score_state_log(X,G,Beta): return Beta*np.dot(X,G) # This function takes as input a state X and the number of treasures M, The weight vector W and the maximum weight W_max # and returns a proposal state def create_proposal(X,W,W_max): M = len(W) random_index = random.randint(0,M-1) #print random_index proposal = list(X) proposal[random_index] = 1 - proposal[random_index] #Toggle #print proposal if np.dot(proposal,W)&amp;lt;=W_max: return proposal else: return create_proposal(X,W,W_max) # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states  def MCMC_Golddigger(n_iter,W,G,W_max, Beta_start = 0.05, Beta_increments=.02): M = len(W) Beta = Beta_start current_X = [0]*M # We start with all 0&amp;#39;s state_keeper = [] best_state = &amp;#39;&amp;#39; score = 0 for i in range(n_iter): state_keeper.append(current_X) proposed_X = create_proposal(current_X,W,W_max) score_current_X = score_state_log(current_X,G,Beta) score_proposed_X = score_state_log(proposed_X,G,Beta) acceptance_probability = min(1,math.exp(score_proposed_X-score_current_X)) if score_current_X&amp;gt;score: best_state = current_X if random_coin(acceptance_probability): current_X = proposed_X if i%500==0: Beta &#43;= Beta_increments # You can use these below two lines to tune value of Beta #if i%20==0: # print &amp;#34;iter:&amp;#34;,i,&amp;#34; |Beta=&amp;#34;,Beta,&amp;#34; |Gold Value=&amp;#34;,np.dot(current_X,G) return state_keeper,best_state Running the Main program:
max_state_value =0 Solution_MCMC = [0] for i in range(10): state_keeper,best_state = MCMC_Golddigger(50000,W,G,W_max,0.0005, .0005) state_value=np.dot(best_state,G) if state_value&amp;gt;max_state_value: max_state_value = state_value Solution_MCMC = best_state print &amp;#34;MCMC Solution is :&amp;#34; , str(Solution_MCMC) , &amp;#34;with Gold Value:&amp;#34;, str(max_state_value) MCMC Solution is : [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0] with Gold Value: 2435  Now I won&amp;rsquo;t say that this is the best solution. The deterministic solution using DP will be the best for such use case but sometimes when the problems gets large, having such techniques at disposal becomes invaluable.
So tell me What do you think about MCMC Methods?
Also, If you find any good applications or would like to apply these techniques to some area, I would really be glad to know about them and help if possible.
The codes for both examples are sourced at Github
References and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia The Markov Chain Monte Carlo Revolution, Persi Diaconis Decrypting Classical Cipher Text Using Markov Chain Monte Carlo, Jian Chen and Jeffrey S. Rosenthal  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)
Apart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.
Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:
  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>My Tryst With MCMC Algorithms</title>
      <link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link>
      <pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid>
      
      

      
      <description>The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.
  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.</description>

      <content:encoded>  
        
        <![CDATA[  The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.
  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.
So what really are MCMC Methods? First of all we have to understand what are Monte Carlo Methods!!!
Monte Carlo methods derive their name from Monte Carlo Casino in Monaco. There are many card games that need probability of winning against the dealer. Sometimes calculating this probability can be mathematically complex or highly intractable. But we can always run a computer simulation to simulate the whole game many times and see the probability as the number of wins divided by the number of games played.
So that is all you need to know about Monte carlo Methods. Yes it is just a simple simulation technique with a Fancy Name.
So as we have got the first part of MCMC, we also need to understand what are Markov Chains. Before Jumping onto Markov Chains let us learn a little bit about Markov Property.
Suppose you have a system of $M$ possible states, and you are hopping from one state to another. Markov Property says that given a process which is at a state $X_n$ at a particular point of time, the probability of $X_{n&#43;1} = k$, where $k$ is any of the $M$ states the process can hop to, will only be dependent on which state it is at the given moment of time. And not on how it reached the current state.
Mathematically speaking:
 $$P(X_{n&#43;1}=k | X_n=k_n,X_{n-1}=k_{n-1},....,X_1=k_1) = P(X_{n&#43;1}=k|X_n=k_n)$$ If a process exhibits the Markov Property than it is known as a Markov Process.
Now Why is a Markov Chain important? It is important because of its stationary distribution.
So what is a Stationary Distribution?
Assume you have a markov process like below. You start from any state $X_i$ and want to find out the state Probability distribution at $X_{i&#43;1}$.
  You have a matrix of transition probability  
which defines the probability of going from a state $X_i$ to $X_j$. You start calculating the Probability distribution for the next state. If you are at Bull Market State at time $i$ , you have a state Probability distribution as [0,1,0]
you want to get the state pdf at $X_{i&#43;1}$. That is given by
$$s_{i&#43;1} = s_{i}Q$$ $$ s_{i&#43;1}=\left[ {\begin{array}{cc} .15 &amp; .8 &amp; .05 \end{array} } \right]$$ And the next state distribution could be found out by $$s_{i&#43;1} = s_iQ^2$$div and so on. Eventually you will reach a stationary state s where: $$sQ=s$$ For this transition matrix Q the Stationary distribution $s$ is $$ s_{i&#43;1}=\left[ {\begin{array}{cc} .625 &amp; .3125 &amp; .0625 \end{array} } \right]$$ The stationary state distribution is important because it lets you define the probability for every state of a system at a random time. That is for this particular example we can say that 62.5% of the times market will be in a bull market state, 31.25% of weeks it will be a bear market and 6.25% of weeks it will be stagnant
Intuitively you can think of it as an random walk on a chain. You might visit some nodes more often than others based on node probabilities. In the Google Pagerank problem you might think of a node as a page, and the probability of a page in the stationary distribution as its relative importance.
Woah! That was a lot of information and we have yet not started talking about the MCMC Methods. Well if you are with me till now, we can now get on to the real topic now.
So What is MCMC? According to Wikipedia:
 **Markov Chain Monte Carlo** (MCMC) methods are a class of algorithms for **sampling from a probability distribution** based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.  So let&amp;rsquo;s explain this with an example: Assume that we want to sample from a Beta distribution. The PDF is:
$$f(x) = Cx^{\alpha -1}(1-x)^{\beta -1}$$ where $C$ is the normalizing constant (which we actually don&amp;rsquo;t need to Sample from the distribution as we will see later).
This is a fairly difficult problem with the Beta Distribution if not intractable. In reality you might need to work with a lot harder Distribution Functions and sometimes you won&amp;rsquo;t actually know the normalizing constants.
MCMC methods make life easier for us by providing us with algorithms that could create a Markov Chain which has the Beta distribution as its stationary distribution given that we can sample from a uniform distribution(which is fairly easy).
If we start from a random state and traverse to the next state based on some algorithm repeatedly, we will end up creating a Markov Chain which has the Beta distribution as its stationary distribution and the states we are at after a long time could be used as sample from the Beta Distribution.
One such MCMC Algorithm is the Metropolis Hastings Algorithm
Metropolis Hastings Algorithm Let $s=(s_1,s_2,&amp;hellip;.,s_M)$ be the desired stationary distribution. We want to create a Markov Chain that has this stationary distribution. We start with an arbitrary Markov Chain $P$ with $M$ states with transition matrix $Q$, so that $Q_{ij}$ represents the probability of going from state $i$ to $j$. Intuitively we know how to wander around this Markov Chain but this Markov Chain does not have the required Stationary Distribution. This chain does have some stationary distribution(which is not of our use)
Our Goal is to change the way we wander on the this Markov Chain $P$ so that this chain has the desired Stationary distribution.
To do this we:
 Start at a random initial State $i$. Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Compute an measure called the Acceptance Probability which is defined as: $a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$ Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  After a long time this chain will converge and will have a stationary distribution $s$. We can then use the states of the chain as the sample from any distribution.
While doing this to sample the Beta Distribution, the only time we are using the PDF is to find the acceptance probability and in that we divide $s_j$ by $s_i$, i.e. the normalizing constant $C$ gets cancelled.
Now Let&amp;rsquo;s Talk about the intuition. For the Intuition I am quoting an Answer from the site Stack Exchange,as this was the best intuitive explanation that I could find:  I think there&amp;rsquo;s a nice and simple intuition to be gained from the (independence-chain) Metropolis-Hastings algorithm. First, what&amp;rsquo;s the goal? The goal of MCMC is to draw samples from some probability distribution without having to know its exact height at any point(We don&amp;rsquo;t need to know C). The way MCMC achieves this is to &amp;ldquo;wander around&amp;rdquo; on that distribution in such a way that the amount of time spent in each location is proportional to the height of the distribution. If the &amp;ldquo;wandering around&amp;rdquo; process is set up correctly, you can make sure that this proportionality (between time spent and height of the distribution) is achieved. Intuitively, what we want to do is to to walk around on some (lumpy) surface in such a way that the amount of time we spend (or # samples drawn) in each location is proportional to the height of the surface at that location. So, e.g., we&amp;rsquo;d like to spend twice as much time on a hilltop that&amp;rsquo;s at an altitude of 100m as we do on a nearby hill that&amp;rsquo;s at an altitude of 50m. The nice thing is that we can do this even if we don&amp;rsquo;t know the absolute heights of points on the surface: all we have to know are the relative heights. e.g., if one hilltop A is twice as high as hilltop B, then we&amp;rsquo;d like to spend twice as much time at A as we spend at B. The simplest variant of the Metropolis-Hastings algorithm (independence chain sampling) achieves this as follows: assume that in every (discrete) time-step, we pick a random new &amp;ldquo;proposed&amp;rdquo; location (selected uniformly across the entire surface). If the proposed location is higher than where we&amp;rsquo;re standing now, move to it. If the proposed location is lower, then move to the new location with probability p, where p is the ratio of the height of that point to the height of the current location. (i.e., flip a coin with a probability p of getting heads; if it comes up heads, move to the new location; if it comes up tails, stay where we are). Keep a list of the locations you&amp;rsquo;ve been at on every time step, and that list will (asyptotically) have the right proportion of time spent in each part of the surface. (And for the A and B hills described above, you&amp;rsquo;ll end up with twice the probability of moving from B to A as you have of moving from A to B). There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: (1) pick a new &amp;ldquo;proposed&amp;rdquo; location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location. 
Sampling from Beta Distribution Now Let&amp;rsquo;s Move on to the problem of Simulating from Beta Distribution. Now Beta Distribution is a continuous Distribution on [0,1] and it can have infinite states on [0,1].
Lets Assume an arbitrary Markov Chain P with infinite states on [0,1] having transition Matrix Q such that $Q_{ij} = Q_{ji} = $ All entries in Matrix. We don&amp;rsquo;t really need the Matrix Q as we will see later, But I want to keep the problem description as close to the algorihm we suggested.
 Start at a random initial State $i$ given by Unif(0,1). Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Lets say we pick up another Unif(0,1) state as a proposal state $j$. Compute an measure called the Acceptance Probability :  $$a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$$ which is, $$a_{ij} = min(s_j/s_i,1)$$ where, $$s_i = Ci^{\alpha -1}(1-i)^{\beta -1}$$ and, $$s_j = Cj^{\alpha -1}(1-j)^{\beta -1}$$  Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  So enough with theory, Let&amp;rsquo;s Move on to python to create our Beta Simulations Now&amp;hellip;.
import random # Lets define our Beta Function to generate s for any particular state. We don&amp;#39;t care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&amp;gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain Let us check our results of the MCMC Sampled Beta distribution against the actual beta distribution.
import numpy as np import pylab as pl import scipy.special as ss %matplotlib inline pl.rcParams[&amp;#39;figure.figsize&amp;#39;] = (17.0, 4.0) # Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a &#43; b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label=&amp;#34;Real Distribution: a=&amp;#34;&#43;str(a)&#43;&amp;#34;, b=&amp;#34;&#43;str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype=&amp;#39;step&amp;#39;,label=&amp;#34;Simulated_MCMC: a=&amp;#34;&#43;str(a)&#43;&amp;#34;, b=&amp;#34;&#43;str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)  As we can see our sampled beta values closely resemble the beta distribution.
So MCMC Methods are useful for the following basic problems.
 Simulating from a Random Variable PDF. Example: Simulate from a Beta(0.5,0.5) or from a Normal(0,1). Solve problems with a large state space.For Example: Knapsack Problem, Encrytion Cipher etc. We will work on this in the Next Blog Post as this one has already gotten bigger than what I expected.  Till Then Ciao!!!!!!
References and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia StackExchange  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)
Apart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.
Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:
  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Hadoop Mapreduce Streaming Tricks and Techniques</title>
      <link>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</link>
      <pubDate>Sat, 09 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</guid>
      
      

      
      <description>I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.
Using Shell Scripts to run your Programs I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. You can effectively organize your workflow by using shell scripts.</description>

      <content:encoded>  
        
        <![CDATA[  I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.
Using Shell Scripts to run your Programs I am not a fan of large bash commands. The ones where you have to specify the whole path of the jar files and the such. You can effectively organize your workflow by using shell scripts. Now Shell scripts are not as formidable as they sound. We wont be doing programming perse using these shell scripts(Though they are pretty good at that too), we will just use them to store commands that we need to use sequentially.
Below is a sample of the shell script I use to run my Mapreduce Codes.
#!/bin/bash #Defining program variables IP=&amp;#34;/data/input&amp;#34; OP=&amp;#34;/data/output&amp;#34; HADOOP_JAR_PATH=&amp;#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&amp;#34; MAPPER=&amp;#34;test_m.py&amp;#34; REDUCER=&amp;#34;test_r.py&amp;#34; hadoop fs -rmr -skipTrash&amp;amp;nbsp;$OP hadoop jar&amp;amp;nbsp;$HADOOP_JAR_PATH \ -file&amp;amp;nbsp;$MAPPER -mapper &amp;#34;python test_m.py&amp;#34; \ -file&amp;amp;nbsp;$REDUCER -reducer &amp;#34;python test_r.py&amp;#34; \ -input&amp;amp;nbsp;$IP -output&amp;amp;nbsp;$OP I generally save them as test_s.sh and whenever i need to run them i simply type sh test_s.sh. This helps in three ways.  It helps me to store hadoop commands in a manageable way.   It is easy to run the mapreduce code using the shell script.   If the code fails, I do not have to manually delete the output directory 
  The simplification of anything is always sensational.  Gilbert K. Chesterton  Using Distributed Cache to provide mapper with a dictionary Often times it happens that you want that your Hadoop Mapreduce program is able to access some static file. This static file could be a dictionary, could be parameters for the program or could be anything. What distributed cache does is that it provides this file to all the mapper nodes so that you can use that file in any way across all your mappers. Now this concept although simple would help you to think about Mapreduce in a whole new light. Lets start with an example. Supppose you have to create a sample Mapreduce program that reads a big file containing the information about all the characters in Game of Thrones stored as &amp;rdquo;/data/characters/&amp;rdquo;:   Cust_ID User_Name House     1 Daenerys Targaryen Targaryen   2 Tyrion Lannister Lannister   3 Cersei Lannister Lannister  4 Robert Baratheon Baratheon  5 Robb Stark Stark    
But you dont want to use the dead characters in the file for the analysis you want to do. You want to count the number of living characters in Game of Thrones grouped by their House. (I know its easy!!!!!) One thing you could do is include an if statement in your Mapper Code which checks if the persons ID is 4 then exclude it from the mapper and such. But the problem is that you would have to do it again and again for the same analysis as characters die like flies when it comes to George RR Martin.(Also where is the fun in that) So you create a file which contains the Ids of all the dead characters at &amp;rdquo;/data/dead_characters.txt&amp;rdquo;:
  Died     4   5     Whenever you have to run the analysis you can just add to this file and you wont have to change anything in the code. Also sometimes this file would be long and you would not want to clutter your code with IDs and such.
So How Would we do it. Let&amp;rsquo;s go in a step by step way around this. We will create a shell script, a mapper script and a reducer script for this task.
1) Shell Script #!/bin/bash #Defining program variables DC=&amp;#34;/data/dead_characters.txt&amp;#34; IP=&amp;#34;/data/characters&amp;#34; OP=&amp;#34;/data/output&amp;#34; HADOOP_JAR_PATH=&amp;#34;/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.5.0.jar&amp;#34; MAPPER=&amp;#34;got_living_m.py&amp;#34; REDUCER=&amp;#34;got_living_r.py&amp;#34; hadoop jar&amp;amp;nbsp;$HADOOP_JAR_PATH \ -file&amp;amp;nbsp;$MAPPER -mapper &amp;#34;python got_living_m.py&amp;#34; \ -file&amp;amp;nbsp;$REDUCER -reducer &amp;#34;python got_living_r.py&amp;#34; \ -cacheFile&amp;amp;nbsp;$DC#ref \ -input&amp;amp;nbsp;$IP -output&amp;amp;nbsp;$OP Note how we use the &amp;rdquo;-cacheFile&amp;rdquo; option here. We have specified that we will refer to the file that has been provided in the Distributed cache as #ref.
Next is our Mapper Script.
2) Mapper Script import sys dead_ids = set() def read_cache(): for line in open(&amp;#39;ref&amp;#39;): id = line.strip() dead_ids.add(id) read_cache() for line in sys.stdin: rec = line.strip().split(&amp;#34;|&amp;#34;) # Split using Delimiter &amp;#34;|&amp;#34; id = rec[0] house = rec[2] if id not in dead_ids: print &amp;#34;%s\t%s&amp;#34; % (house,1) And our Reducer Script.
3) Reducer Script import sys current_key = None key = None count = 0 for line in sys.stdin: line = line.strip() rec = line.split(&amp;#39;\t&amp;#39;) key = rec[0]	value = int(rec[1]) if current_key == key: count &#43;= value else: if current_key: print &amp;#34;%s:%s&amp;#34; %(key,str(count))	current_key = key count = value if current_key == key: print &amp;#34;%s:%s&amp;#34; %(key,str(count))	 This was a simple program and the output will be just what you expected and not very exciting. But the Technique itself solves a variety of common problems. You can use it to pass any big dictionary to your Mapreduce Program. Atleast thats what I use this feature mostly for. Hope You liked it. Will try to expand this post with more tricks.
The codes for this post are posted at github here.
Other Great Learning Resources For Hadoop:   Michael Noll&amp;rsquo;s Hadoop Mapreduce Tutorial   Apache&amp;rsquo;s Hadoop Streaming Documentation  
Also I like these books a lot. Must have for a Hadooper&amp;hellip;.
  The first book is a guide for using Hadoop as well as spark with Python. While the second one contains a detailed overview of all the things in Hadoop. Its the definitive guide.
 ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Exploring Vowpal Wabbit with the Avazu Clickthrough Prediction Challenge</title>
      <link>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</guid>
      
      

      
      <description>In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.</description>

      <content:encoded>  
        
        <![CDATA[  In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.
Data Fields id: ad identifier click: 0/1 for non-click/click hour: format is YYMMDDHH, so 14091123 means 23:00 on Sept. 11, 2014 UTC. C1 -- anonymized categorical variable banner_pos site_id site_domain site_category app_id app_domain app_category device_id device_ip device_model device_type device_conn_type C14-C21 -- anonymized categorical variables  Loading Data ## Loading the data  import pandas as pd import numpy as np import string as stri #too large data not keeping it in memory. # will be using line by line scripting. #data = pd.read_csv(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/train&amp;#34;) Since the data is too large around 6 gb , we will proceed by doing line by line analysis of data. We will try to use vowpal wabbit first of all as it is an online model and it also gives us the option of minimizing log loss as a default. It is also very fast to run and will give us quite an intuition as to how good our prediction can be.
I will use all the variables in the first implementation and we will rediscover things as we move on
Running Vowpal Wabbit Creating data in vowpal format (One Time Only) from datetime import datetime def csv_to_vw(loc_csv, loc_output, train=True): start = datetime.now() print(&amp;#34;\nTurning %sinto %s. Is_train_set? %s&amp;#34;%(loc_csv,loc_output,train)) i = open(loc_csv, &amp;#34;r&amp;#34;) j = open(loc_output, &amp;#39;wb&amp;#39;) counter=0 with i as infile: line_count=0 for line in infile: # to counter the header if line_count==0: line_count=1 continue # The data has all categorical features #numerical_features = &amp;#34;&amp;#34; categorical_features = &amp;#34;&amp;#34; counter = counter&#43;1 #print counter line = line.split(&amp;#34;,&amp;#34;) if train: #working on the date column. We will take day , hour a = line[2] new_date= datetime(int(&amp;#34;20&amp;#34;&#43;a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(&amp;#34;%A&amp;#34;) hour= a[6:8] categorical_features &#43;= &amp;#34; |hr %s&amp;#34; % hour categorical_features &#43;= &amp;#34; |day %s&amp;#34; % day # 24 columns in data  for i in range(3,24): if line[i] != &amp;#34;&amp;#34;: categorical_features &#43;= &amp;#34;|c%s%s&amp;#34; % (str(i),line[i]) else: a = line[1] new_date= datetime(int(&amp;#34;20&amp;#34;&#43;a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(&amp;#34;%A&amp;#34;) hour= a[6:8] categorical_features &#43;= &amp;#34; |hr %s&amp;#34; % hour categorical_features &#43;= &amp;#34; |day %s&amp;#34; % day for i in range(2,23): if line[i] != &amp;#34;&amp;#34;: categorical_features &#43;= &amp;#34; |c%s%s&amp;#34; % (str(i&#43;1),line[i]) #Creating the labels #print &amp;#34;a&amp;#34; if train: #we care about labels if line[1] == &amp;#34;1&amp;#34;: label = 1 else: label = -1 #we set negative label to -1 #print (numerical_features) #print categorical_features j.write( &amp;#34;%s&amp;#39;%s%s\n&amp;#34; % (label,line[0],categorical_features)) else: #we dont care about labels #print ( &amp;#34;1 &amp;#39;%s |i%s |c%s\n&amp;#34; % (line[0],numerical_features,categorical_features) ) j.write( &amp;#34;1 &amp;#39;%s%s\n&amp;#34; % (line[0],categorical_features) ) #Reporting progress #print counter if counter % 1000000 == 0: print(&amp;#34;%s\t%s&amp;#34;%(counter, str(datetime.now() - start))) print(&amp;#34;\n%sTask execution time:\n\t%s&amp;#34;%(counter, str(datetime.now() - start))) #csv_to_vw(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/train&amp;#34;, &amp;#34;/Users/RahulAgarwal/kaggle_cpr/click.train_original_data.vw&amp;#34;,train=True) #csv_to_vw(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/test&amp;#34;, &amp;#34;/Users/RahulAgarwal/kaggle_cpr/click.test_original_data.vw&amp;#34;,train=False) Running Vowpal Wabbit on the data The Vowpal Wabbit will be run on the command line itself.
Training VW:
vw click.train_original_data.vw -f click.model.vw --loss_function logistic Testing VW:
vw click.test_original_data.vw -t -i click.model.vw -p click.preds.txt Creating Kaggle Submission File import math def zygmoid(x): return 1 / (1 &#43; math.exp(-x)) with open(&amp;#34;kaggle.click.submission.csv&amp;#34;,&amp;#34;wb&amp;#34;) as outfile: outfile.write(&amp;#34;id,click\n&amp;#34;) for line in open(&amp;#34;click.preds.txt&amp;#34;): row = line.strip().split(&amp;#34; &amp;#34;) try: outfile.write(&amp;#34;%s,%f\n&amp;#34;%(row[1],zygmoid(float(row[0])))) except: pass This solution ranked 211&amp;frasl;371 submissions at the time and the leaderboard score was 0.4031825 while the best leaderboard score was 0.3901120
Next Steps  Create a better VW model
 Shuffle the data before making the model as the VW algorithm is an online learner and might have given more preference to the latest data provide high weights for clicks as data is skewed. How Much? tune VW algorithm using vw-hypersearch. What should be tuned? Use categorical features like |C1 &amp;ldquo;C1&amp;rdquo;&amp;amp;&amp;ldquo;1&amp;rdquo;  Create a XGBoost Model.
 Create a Sofia-ML Model and see how it works on this data.
   ]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>