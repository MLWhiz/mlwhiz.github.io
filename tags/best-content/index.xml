<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Best Content on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/tags/best-content/</link><description>Recent content in Best Content on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 02 Dec 2022 23:21:54 +0000</lastBuildDate><atom:link href="https://mlwhiz.com/tags/best-content/index.xml" rel="self" type="application/rss+xml"/><item><title>The Most Complete Guide to pySpark DataFrames</title><link>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</guid><description>&lt;p>Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.&lt;/p></description></item><item><title>A Newspaper for COVID-19 — The CoronaTimes</title><link>https://mlwhiz.com/blog/2020/03/29/coronatimes/</link><pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/03/29/coronatimes/</guid><description>&lt;p>It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.&lt;/p></description></item><item><title>5 Ways to add a new column in a PySpark Dataframe</title><link>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</guid><description>&lt;p>&lt;em>&lt;strong>Too much data is getting generated day by day.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>Although sometimes we can manage our big data using tools like 

&lt;a href="https://towardsdatascience.com/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=---------5------------------" target="_blank" rel="nofollow noopener">Rapids&lt;/a>
 or 

&lt;a href="https://towardsdatascience.com/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8?source=---------11------------------" target="_blank" rel="nofollow noopener">Parallelization&lt;/a>
, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.&lt;/p></description></item><item><title>Become a Data Scientist in 2023 with these 10 resources</title><link>https://mlwhiz.com/blog/2020/02/21/ds2020/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/21/ds2020/</guid><description>&lt;p>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.&lt;/p></description></item><item><title>Confidence Intervals Explained Simply for Data Scientists</title><link>https://mlwhiz.com/blog/2020/02/21/ci/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/21/ci/</guid><description>&lt;p>Recently, I got asked about how to explain confidence intervals in simple terms to a layperson. I found that it is hard to do that.&lt;/p></description></item><item><title>The 5 most useful Techniques to Handle Imbalanced datasets</title><link>https://mlwhiz.com/blog/2020/01/28/imbal/</link><pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/01/28/imbal/</guid><description>&lt;p>Have you ever faced an issue where you have such a small sample for the positive class in your dataset that the model is unable to learn?&lt;/p></description></item><item><title>How to write Web apps using simple Python for Data Scientists?</title><link>https://mlwhiz.com/blog/2019/12/07/streamlit/</link><pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/07/streamlit/</guid><description>&lt;p>A Machine Learning project is never really complete if we don’t have a good way to showcase it.&lt;/p></description></item><item><title>The 5 Classification Evaluation metrics every Data Scientist must know</title><link>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</link><pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</guid><description>&lt;p>&lt;em>&lt;strong>What do we want to optimize for?&lt;/strong>&lt;/em> Most of the businesses fail to answer this simple question.&lt;/p></description></item><item><title>6 Important Steps to build a Machine Learning System</title><link>https://mlwhiz.com/blog/2019/09/26/building_ml_system/</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/09/26/building_ml_system/</guid><description>&lt;p>Creating a great machine learning system is an art.&lt;/p>
&lt;p>There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.&lt;/p></description></item><item><title>The Ultimate Guide to using the Python regex module</title><link>https://mlwhiz.com/blog/2019/09/01/regex/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/09/01/regex/</guid><description>&lt;p>One of the main tasks while working with text data is to create a lot of text-based features.&lt;/p></description></item><item><title>Minimal Pandas Subset for Data Scientists</title><link>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</guid><description>&lt;p>Pandas is a vast library.&lt;/p>
&lt;p>Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.&lt;/p></description></item><item><title>The Hitchhikers guide to handle Big Data using Spark</title><link>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</guid><description>&lt;p>Big Data has become synonymous with Data engineering.&lt;/p>
&lt;p>But the line between Data Engineering and Data scientists is blurring day by day.&lt;/p></description></item><item><title>The Hitchhiker’s Guide to Feature Extraction</title><link>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</guid><description>&lt;p>Good Features are the backbone of any machine learning model.&lt;/p></description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>&lt;p>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p></description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>&lt;p>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>&lt;p>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong>Quora Insincere questions classification&lt;/strong> in which I got a rank of &lt;strong>&lt;code>182/4037&lt;/code>&lt;/strong>. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p></description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>&lt;p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the various &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In this post, I will try to take you through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong>Deep learning models&lt;/strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>&lt;p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p></description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/artificial-neural-network.png" height="350" width="700" >&lt;/center>
&lt;/div>
&lt;p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the &lt;strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic&lt;/strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.&lt;/p></description></item><item><title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title><link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid><description>&lt;p>When it comes to data preparation and getting acquainted with data, the &lt;strong>one step we normally skip is the data visualization&lt;/strong>.
While a part of it could be attributed to the &lt;strong>lack of good visualization tools&lt;/strong> for the platforms we use, most of us also &lt;strong>get lazy&lt;/strong> at times.&lt;/p></description></item><item><title>Behold the power of MCMC</title><link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link><pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/mcmc.png">&lt;/center>
&lt;/div>
&lt;p>Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.&lt;/p></description></item><item><title>My Tryst With MCMC Algorithms</title><link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link><pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid><description>&lt;p>The things that I find hard to understand push me to my limits. One of the things that I have always found hard is &lt;strong>Markov Chain Monte Carlo Methods&lt;/strong>.
When I first encountered them, I read a lot about them but mostly it ended like this.&lt;/p></description></item></channel></rss>