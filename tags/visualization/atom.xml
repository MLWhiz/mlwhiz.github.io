<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Visualization on MLWhiz</title>
    <link>https://mlwhiz.com/tags/visualization/</link>
    <description>Recent content in Visualization on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Mar 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/visualization/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>A Newspaper for COVID-19 — The CoronaTimes</title>
      <link>https://mlwhiz.com/blog/2020/03/29/coronatimes/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/03/29/coronatimes/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/coronatimes/main.gif"></media:content>
      

      
      <description>It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea.</description>

      <content:encoded>  
        
        <![CDATA[  It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea. Why not create an international news dashboard for Corona? And here it is.
This post is about how I created the news dashboard using Streamlit and data from NewsApi and European CDC.
TLDR; Link to the App here.
Getting The Data The most important thing while creating this Dashboard was acquiring the data. I am using two data sources:
1. Data from the European Centre for Disease Prevention and Control. The downloadable data file is updated daily and contains the latest available public data on COVID-19. Here is a snapshot of this data.
def get_data(date): os.system(&amp;quot;rm cases.csv&amp;quot;) url = &amp;quot;[https://opendata.ecdc.europa.eu/covid19/casedistribution/csv](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)&amp;quot; filename = wget.download(url,&amp;quot;cases.csv&amp;quot;) casedata = pd.read_csv(filename, encoding=&#39;latin-1&#39;) return casedata  2. News API The second source of data comes from the News API, which lets me access articles from leading news outlets from various countries for free. The only caveat is that I could only hit the API 500 times a day, and there is a result limit of 100 results for a particular query for free accounts.
I tried to get around those limit barriers by using streamlit caching(So I don’t hit the API a lot). I also tried to get news data from last month using multiple filters to get a lot of data.
from newsapi import NewsApiClient newsapi = NewsApiClient(api_key=&#39;aedb6aa9bebb4011a4eb5447019dd592&#39;)  The primary way the API works is by giving us access to 3 functions.
a) A function to get Recent News from a country:
json_data = newsapi.get_top_headlines(q=q,language=&#39;en&#39;, country=&#39;us&#39;) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  b) A function to get “Everything” related to a query from the country. You can see the descriptions of API parameters here:
json_data = newsapi.get_everything(q=&#39;corona&#39;, language=&#39;en&#39;, from_param=str(date.today() -timedelta(days=29)), to= str(date.today()), sources = &#39;usa-today&#39;, page_size=100, page = 1, sort_by=&#39;relevancy&#39; ) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  c) A function to get a list of sources from a Country programmatically. We can then use these sources to pull data from the “everything” API
def get_sources(country): sources = newsapi.get_sources(country=country) sources = [x[&#39;id&#39;] for x in sources[&#39;sources&#39;]] return sources sources = get_sources(country=&#39;us&#39;) print(sources[:5]) ------------------------------------------------------------------- [&#39;abc-news&#39;, &#39;al-jazeera-english&#39;, &#39;ars-technica&#39;, &#39;associated-press&#39;, &#39;axios&#39;]  I used all the functions above to get data that refreshes at a particular cadence. You can see how I use these API functions in a loop to download the data by looking at my code at GitHub.
Creating the Dashboard I wanted to have a few important information in the Dashboard that I was interested in. So I started by creating various widgets.
1. Current World Snapshot: The first information was regarding the whole world situation. The Number of Cases and Deaths. The case and death curve in various countries? What are the fatality rates in various countries? Below is the current world situation on 28 Mar 2020.
Observations: We can see the deaths in Italy are still on the rise, while we are seeing the deaths shooting up in Spain, France, and the United States as well. The death rates in some countries are worrying with death rates of 10.56% in Italy and 8.7% in Iraq. I suspect that the death rate statistic of 2% in the starting days of CoronaVirus was misinformed if not wrong.
Technical Details — To create this part of the Dashboard, I used the ECDC data. I also used a lot of HTML hacks with Streamlit, where I used bootstrap widgets as well as custom HTML to get data in the way I wanted to display it. Here are a few of the hacks:
 Using Bootstrap Cards: You can use bootstrap or, in that case, any HTML element in Streamlit if you change the parameter unsafe_allow_html to True. Do note that I am also using python f string formatting here.  st.sidebar.markdown(f&amp;#39;&amp;#39;&amp;#39;&amp;lt;div class=&amp;#34;card text-white bg-info mb-3&amp;#34; style=&amp;#34;width: 18rem&amp;#34;&amp;gt; &amp;lt;div class=&amp;#34;card-body&amp;#34;&amp;gt; &amp;lt;h5 class=&amp;#34;card-title&amp;#34;&amp;gt;Total Cases&amp;lt;/h5&amp;gt; &amp;lt;p class=&amp;#34;card-text&amp;#34;&amp;gt;{sum(casedata[&amp;#39;cases&amp;#39;]):,d}&amp;lt;/p&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;&amp;#39;&amp;#39;&amp;#39;, unsafe_allow_html=True) The above code is behind the Dashboard styled cards in the streamlit app sidebar.
 Changed the width of the streamlit main page:  Again, there was no parameter given by streamlit to do this, and I was finding the page width a little too small for my use case. Adding the above code at the start of the app solved the issue.
st.markdown( f&amp;#34;&amp;#34;&amp;#34; &amp;lt;style&amp;gt; .reportview-container .main .block-container{{ max-width: 1000px; }} &amp;lt;/style&amp;gt; &amp;#34;&amp;#34;&amp;#34;, unsafe_allow_html=True, ) 2. Most Recent News from Country The primary purpose of creating this Dashboard was to get news from various outlets from top media outlets in the country.
Observations: As here you can see, here we have the top recent news from the United Kingdom concerning cases in Ireland and Boris Johnson’s corona woes.
Technical Details: As said before, I am using the News API to get this data. And here is how I am using a mashup of HTML and markdown to display the news results.
def create_most_recent_markdown(df,width=700): if len(df)&amp;gt;0: # img url img_path = df[&#39;urlToImage&#39;].iloc[0] if not img_path: images = [x for x in df.urlToImage.values if x is not None] if len(images)!=0: img_path = random.choice(images) else: img_path = &#39;[https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;](https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;) img_alt = df[&#39;title&#39;].iloc[0] df = df[:5] **markdown_str = f&amp;quot;&amp;lt;img src=&#39;{img_path}&#39; width=&#39;{width}&#39;/&amp;gt; &amp;lt;br&amp;gt; &amp;lt;br&amp;gt;&amp;quot;** for index, row in df.iterrows(): **markdown_str &#43;= f&amp;quot;[{row[&#39;title&#39;]}]({row[&#39;url&#39;]}) by {row[&#39;author&#39;]}&amp;lt;br&amp;gt; &amp;quot;** return markdown_str else: return &#39;&#39;  Few things to note here:
 The image width cannot be set using markdown so using custom HTML
 The usage of python f strings to create the article titles and URLs.
 If no image is found, we are defaulting to a custom image.
  3. News Sentiment Another thing that has been bothering me in these trying times is so much negativity everywhere. I wanted to see the news covered from a positive angle if it could be in any way. So I did some simple sentiment analysis using the custom sentiment analyzer from Textblob to do this.
I found out sentiments by news outlets as well as some of the most positive and negative news related to Coronavirus in the past 30 days. (Past 30 days because I cannot go more back with the free API).
Observations: As you can see that one of the most positive news is Trump changing his coronavirus stance on March 17th, and I agree. The second positive report seems to be regarding some sort of solution to the problem. While the first Negative news is regarding Cardi B slamming celebrities for sowing confusion about the Coronavirus. I won’t comment on this :)
Technical Details: To get the sentiment scores of an article I used TextBlob. Getting the sentiment scores that range from -1 to 1 is as simple as using the below function. I used a concatenation of title and description to find the sentiment as the content from the News API was truncated.
def textblob_sentiment(title,description): blob = TextBlob(str(title)&#43;&amp;quot; &amp;quot;&#43;str(description)) return blob.sentiment.polarity  The main difficulty here was to have a two-column layout to give both positive and negative news. For that again, I had to use a mashup of HTML and markdown. I used the HTML table to do this. Also, note how I used markdown to convert markdown to HTML using Python f strings.
import markdown md = markdown.Markdown() positive_results_markdown = create_most_recent_markdown(positivedata,400) negative_results_markdown = create_most_recent_markdown(negativedata,400) html = f&#39;&#39;&#39;&amp;lt;table style=&amp;quot;width:100%&amp;quot;&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Positive News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Negative News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(positive_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(negative_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/table&amp;gt;&#39;&#39;&#39; #print md.convert(&amp;quot;# sample heading text&amp;quot;) st.markdown(html,unsafe_allow_html=True)  4. News Source WordCloud A visualization dashboard that works with text is never really complete without a word cloud, so I thought of adding a word cloud to understand the word usage from a particular source.
Observations: We can see Vice news using words like “New” and “Tested” a lot of times. While Business Insider used “China” a lot.
      Technical Details:Here is what I used to create this masked word cloud:
import cv2 def create_mask(): mask = np.array(Image.open(&amp;quot;coronavirus.png&amp;quot;)) im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) _, mask = cv2.threshold(im_gray, thresh=20, maxval=255, type=cv2.THRESH_BINARY) mask = 255 - mask return mask mask = create_mask() def create_wc_by(source): data = fulldf[fulldf[&#39;source&#39;]==source] text = &amp;quot; &amp;quot;.join([x for x in data.content.values if x is not None]) stopwords = set(STOPWORDS) stopwords.add(&#39;chars&#39;) stopwords.add(&#39;coronavirus&#39;) stopwords.add(&#39;corona&#39;) stopwords.add(&#39;chars&#39;) wc = WordCloud(background_color=&amp;quot;white&amp;quot;, max_words=1000, mask=mask, stopwords=stopwords, max_font_size=90, random_state=42, contour_width=3, contour_color=&#39;steelblue&#39;) wc.generate(text) plt.figure(figsize=[30,30]) plt.imshow(wc, interpolation=&#39;bilinear&#39;) plt.axis(&amp;quot;off&amp;quot;) return plt st.pyplot(create_wc_by(source),use_container_width=True)  Other Technical Considerations 1. Advanced Caching: In new streamlit release notes for 0.57.0 which just came out yesterday, streamlit has made updates to st.cache. One notable change to this release is the “ability to set expiration options for cached functions by setting the max_entries and ttl arguments”. From the documentation:
 max_entries (int or None) — The maximum number of entries to keep in the cache, or None for an unbounded cache. (When a new entry is added to a full cache, the oldest cached entry will be removed.) The default is None.
 ttl (float or None) — The maximum number of seconds to keep an entry in the cache, or None if cache entries should not expire. The default is None.
  Two use cases where this might help would be:
 If you’re serving your app and don’t want the cache to grow forever.
 If you have a cached function that reads live data from a URL and should clear every few hours to fetch the latest data
  So this is what is being used in a lot of functions to avoid hitting APIs multiple times and to prevent them from getting stale at the same time.
For Example, Top results from a country are fetched at a period of 360 seconds i.e., 6 minutes.
st.cache(ttl=360,max_entries=20) def create_dataframe_top(queries,country): #Hits API Here  While full results from the everything API are fetched at a period of one day.
[@st](http://twitter.com/st).cache(ttl = 60*60*24,max_entries=20) def create_dataframe_last_30d(queries, sources): # hits API  2. Deployment: I used the amazon free ec2 instance to deploy this app at http://54.149.204.138:8501/. If you want to know the steps,read my post on How to Deploy a Streamlit App using an Amazon Free ec2 instance?
There are also a few caveats:
 Since it is a free server, it might not take too much load.
 I have not thoroughly tested the caching routine. I just hope that there are no memory errors with the limited memory on the server.
 The News API is also free. There might be rate limits that might kick in even after I have tried to handle that.
  3. Learning For folks who are lost, you might like to start with the basics first. Here is my introductory posts on Streamlit and Plotly express.
 How to write Web apps using simple Python for Data Scientists? Python’s One Liner graph creation library with animations Hans Rosling Style  Conclusion Here I have tried creating a dashboard for news on Coronavirus, but it is still in a nascent stage, and a lot needs to be done.
For one, it needs a large server. For another, a lot of time to improve the visualization and layouts. And also a lot of testing.
Also, we have done a few things in a roundabout way using HTML and few hacks. There are still a lot of things that I will love to have in Streamlit. I have been in talks with the Streamlit team over the new functionality that they are going to introduce, and I will try to keep you updated on the same. The good news is that Layout options are a part of the new functionality that Streamlit is working on.
You can find the full code for the final app here at my Github repo. And here is the full app on the web.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Bamboolib — Learn and use Pandas without Coding</title>
      <link>https://mlwhiz.com/blog/2020/02/23/bamboo/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/23/bamboo/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/bamboo/main.gif"></media:content>
      

      
      <description>Have you ever been frustrated by doing data exploration and manipulation with Pandas?
With so many ways to do the same thing, I get spoiled by choice and end up doing absolutely nothing.
And then for a beginner, the problem is just the opposite as in how to do even a simple thing is not appropriately documented. Understanding Pandas syntax can be a hard thing for the uninitiated.
So what should one do?</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever been frustrated by doing data exploration and manipulation with Pandas?
With so many ways to do the same thing, I get spoiled by choice and end up doing absolutely nothing.
And then for a beginner, the problem is just the opposite as in how to do even a simple thing is not appropriately documented. Understanding Pandas syntax can be a hard thing for the uninitiated.
So what should one do?
The creators of Bamboolib had an idea that solved this problem — Why not add a GUI to pandas?
The idea is to “Learn and use pandas without coding.” Now the idea may have started simple, but I found Bamboolib to be so much more when it comes to data exploration and data cleaning.
This post is about setting up and using Bamboolib for your data.
Installing Bamboolib Installation is pretty simple with:
pip install bamboolib  To get bamboolib to work with Jupyter and Jupyterlab, I will need to install some additional extensions. Since I like working with Jupyter Notebook, I installed the Jupyter Notebook extensions via the following command:
jupyter nbextension enable --py qgrid --sys-prefix jupyter nbextension enable --py widgetsnbextension --sys-prefix jupyter nbextension install --py bamboolib --sys-prefix jupyter nbextension enable --py bamboolib --sys-prefix  If you want the process to install for Jupyterlab, here is the process.
Verifying Bamboolib Installation To check if everything works as intended, you can open up a Jupyter notebook, and execute the following commands:
import bamboolib as bam import pandas as pd data = pd.read_csv(bam.titanic_csv) bam.show(data)  The first time you run this command, you will be asked to provide a Licence key. The key is needed if you want to use bamboolib over your own data. Since I wanted to use bamboolib for my own project, I got the key from one of Bamboolib founder Tobias Krabel who was gracious enough to provide it to me to review. You can, however, buy your own from https://bamboolib.8080labs.com/pricing/. If you want to see the library in action before purchasing the key, you can try out the live demo.
Once bamboolib is activated, the fun part starts. You can see the output of Bamboolib like this. You can choose to play with the options it provides.
So let’s try Bamboolib with our exciting data source, we all have seen Titanic data aplenty.
To do this, I will be using the Mobile Price Classification data from Kaggle. In this problem, we have to create a classifier that predicts the price range of mobile phones based on the features of a mobile phone. So lets start this up with Bamboolib.
train = pd.read_csv(&amp;quot;../Downloads/mobile-price-classification/train.csv&amp;quot;) bam.show(train)  We need to do a simple call to bam.show(train) to start Bamboolib.
Easy Data Exploration Bamboolib helps a great bit for Exploratory Data analysis. Now, Data exploration is an integral part of any data science pipeline. And writing the whole code for data exploration and creating all the charts is complicated and needs a lot of patience and effort to get right. I will admit sometimes I do slack off and am not able to give enough time for it.
Bamboolib makes the whole Data Exploration exercise a breeze.
For example. Here is a glimpse of your data, once you click on Visualize Dataframe.
You get to see the missing values in each column, as well as the number of unique values and a few instances.
But that’s not all. We can get univariate column-level statistics and information, as well. So lets get some information about our target variable — Price Range.
Here we deep-dive into the target column and can see univariate column statistics as well as the most important predictors for our target column. It looks like RAM and battery power are the most important predictors for the price range. Nice.
Let’s take a look at how RAM influences the price range. We can use bivariate plots for this.
Getting such beautiful plots with standard Python libraries like seaborn or plotly usually takes some amount of code. Although plotly_express helps a lot in this by giving simple functions for most charts, Bamboolib creates a lot of important charts for us automatically.
Above, we can see that as RAM increases, the price range increases. We also see a weighted F1 Score of 0.676 for the RAM Variable. You can do this for every variable in your dataset and try to get a sense of your data.
One can also export the code of these charts to use in some presentation/ export these charts as PNG.
To do this just copy the code fragment that shows above each graph. For example, you can copy and run the code to see price_range vs ram, and you will see an option to download these graphs as PNG. In the backend, they are all plotly graphs.
bam.plot(train, &#39;price_range&#39;, &#39;ram&#39;)  GUI Based Data Munging Have you ever faced the problem of forgetting pandas code to do something and going to stack overflow and getting lost in various threads? If yes, here is a Minimal Pandas refresher. Or you can use Bamboolib as per your preference.
Bamboolib makes it so easy to do things and not get lost in the code. You can drop columns, filter, sort, join, groupby, pivot, melt (Mostly everything you would like to do with a dataset) all by using the simple GUI provided.
For example, here I am dropping the missing values from the target column, if any. You can add multiple conditions, as well.
The best part is that it also gives us the code. Here the code to drop the missing values gets populated in the cell automatically.
train = train.loc[train[&#39;price_range&#39;].notna()] train.index = pd.RangeIndex(len(train))  It works just like Microsoft Excel for business users while providing all the code to slice and dice the data for the advanced ones. You can try to play with the other options to get familiar.
Here is another example of how to use groupby. It is actually pretty intuitive.
The code for this gets populated as:
train = train.groupby([&#39;price_range&#39;]).agg({&#39;battery_power&#39;: [&#39;mean&#39;], &#39;clock_speed&#39;: [&#39;std&#39;]}) train.columns = [&#39;_&#39;.join(multi_index) for multi_index in train.columns.ravel()] train = train.reset_index()  You can see how it takes care of multi_index as well as ravel for us, which are a bit difficult to understand and deal with.
Conclusion The GUI of Bamboolib is pretty intuitive, and I found it an absolute joy to work with. The project is still in its beginnings, but what a beginning it has been.
I can surely say that this library is pretty useful for beginners who want to learn to code in Pandas as it provides them access to all the necessary functions without being bothersome.
While I will still focus on understanding the basics of Pandas and would advise looking at the output of Bamboolib to learn Pandas as well, I would like to see how the adoption of Bamboolib happens in the future.
Let me know your thoughts as well in the comments.
If you want to learn more about Pandas, I would like to call out an excellent course on Introduction to Data Science in Python from the University of Michigan or check out my previous post on how to work with Pandas.
I am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Share your Projects even more easily with this New Streamlit Feature</title>
      <link>https://mlwhiz.com/blog/2020/02/23/streamlitrec/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/23/streamlitrec/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlitrec/main.png"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
As Sten Sootla says in his satire piece which I thoroughly enjoyed:</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
As Sten Sootla says in his satire piece which I thoroughly enjoyed:
 The secret: it’s not what you know, it’s what you show.
 This is where StreamLit comes in and provides a way to create web apps just using Python. I have been keeping close tabs on this excellent product for the past few months. In my last few posts, I talked about Working with Streamlit and how to Deploy the streamlit app using ec2. I have also been in constant touch with the Streamlit team while they have been working continuously to make the user experience even better by releasing additional features.
So, have you ever had a problem with explaining how the app works to the stakeholders/business partners? Having to set up multiple calls with different stakeholders in different countries and explaining the whole process again and again?
Or have you worked on a project that you want to share on social media? LinkedIn, Youtube, and the like?
With their new version, Streamlit has released a new feature called “Record a Screencast” which will solve this problem for you.
How? Read on.
Setting up So to check this new feature out, which is a part of Streamlit’s version 0.55.0 offering, we need to first install or upgrade streamlit. Do this by using this command:
pip install --upgrade streamlit  We also need to run Streamlit. Here I will use the demo app. You can also use any of your own apps.
streamlit hello  You should see something like below:
A tab also opens up in your browser, where you can try their demo. If that doesn’t open up in the browser, you can manually go to the Local URL http://localhost:8501/ too.
Recording the Screencast Now the time has come to record our screencast to share with the world. You can find the option to record the screencast using the top-right menu in Streamlit.
Once you click on that, you will get the option to record audio, and you can select the aptly named “Start Recording” button to start recording.
You can then choose what you want to share — just your streamlit app or your entire desktop. One can choose to share the whole desktop if they need to go forth between different programs like Excel sheets, powerpoints, and the streamlit app, for example. Here I choose to show just the “Streamlit” App and click share.
Your screencast has now started, and you can record the explanation session for your shareholders now. Once you are done with the recording, you can click on the top-right menu again and select stop recording. Or conveniently press escape to end the recording session.
You will be able to preview and save the session video you recorded as a .webm file, which you can aim to send to your shareholders and even share on LinkedIn/twitter/youtube for your personal projects.
And that’s it. The process is pretty simple and doesn’t need any additional software installation from our side.
Endnotes Streamlit has democratized the whole process of creating apps.
I honestly like the way Streamlit is working on developing its product, keeping in mind all the pain points of its users. With this iteration, they have resolved one more pain point where users struggle to showcase their work in a meaningful way on social media sites or to explain the workings of an app multiple times to the shareholders.
On top of that, Streamlit is a free and open-source rather than a proprietary web app that works out of the box. I couldn’t recommend it more.
Also, do let me know if you want to request any additional features in Streamlit in the comments section. I will make sure to pass it on to the Streamlit team.
If you want to learn more about using Streamlit to create and deploy apps, take a look at my other posts:
 How to write Web apps using simple Python for Data Scientists? How to Deploy a Streamlit App using an Amazon Free ec2 instance?  If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to Deploy a Streamlit App using an Amazon Free ec2 instance?</title>
      <link>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlitec2/main.png"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
In my last post on Streamlit, I talked about how to write Web apps using simple Python for Data Scientists.
But still, a major complaint, if you would check out the comment section of that post, was regarding the inability to deploy Streamlit apps over the web.
And it was a valid complaint.
 A developer can’t show up with his laptop every time the client wanted to use the app. What is the use of such an app?
 So in this post, we will go one step further deploy our Streamlit app over the Web using an Amazon Free ec2 instance.
Setting up the Amazon Instance Before we start with using the amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a simple sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine”. Here we are trying to set up the machine where we will deploy our Streamlit app.
 In the first step, you need to choose the AMI template for the machine. I select the 18.04 Ubuntu Server since it is applicable for the Free Tier. And Ubuntu.
   In the second step, I select the t2.micro instance as again it is the one which is eligible for the free tier. As you can see t2.micro is just a single CPU instance with 512 MB RAM. You can opt for a bigger machine if you are dealing with a powerful model or are willing to pay.   Keep pressing Next until you reach the “6. Configure Security Group” tab. You will need to add a rule with Type: “Custom TCP Rule”, Port Range:8501, and Source: Anywhere. We use the port 8501 here since it is the custom port used by Streamlit.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch you might need to create a new key pair. Here I am creating a new key pair named streamlit and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state, it should be showing “Running”   Select your instance, and copy the Public DNS(IPv4) Address from the description. It should be something starting with ec2.
 Once you have that run the following commands in the folder you saved the streamlit.pem file. I have masked some of the information here.
   chmod 400 streamlit.pem ssh -i &amp;quot;streamlit.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Installing Required Libraries Whoa, that was a handful. After all the above steps you should be able to see the ubuntu prompt for the virtual machine. We will need to set up this machine to run our app. I am going to be using the same streamlit_football_demo app that I used in my previous post.
We start by installing miniconda and adding its path to the environment variable.
sudo apt-get update wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p ~/miniconda echo &amp;quot;PATH=$PATH:$HOME/miniconda/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc  We then install additional dependencies for our app to run. That means I install streamlit and plotly_express.
pip install streamlit pip install plotly_express  And our machine is now prepped and ready to run.
Running Streamlit on Amazon ec2 As I am set up with the instance, I can get the code for my demo app from Github. Or you can choose to create or copy another app as you wish.
git clone https://github.com/MLWhiz/streamlit_football_demo.git cd streamlit_football_demo streamlit run helloworld.py  Now you can go to a browser and type the external URL to access your app. In my case the address is http://35.167.158.251:8501. Here is the output. This app will be up right now if you want to play with it.
A Very Small Problem Though We are up and running with our app for the world to see. But whenever you are going to close the SSH terminal window the process will stop and so will your app.
So what do we do?
TMUX to the rescue. TMUX allows us to keep running our sessions even after we leave the terminal window. It also helps with a lot of other things but I will just go through the steps we need.
First, we stop our app using Ctrl&#43;C and install tmux
sudo apt-get install tmux  We start a new tmux session using the below command. We keep the name of our session as StreamSession. You could use any name here.
tmux new -s StreamSession  You can see that the session name is “StreamSession” at the bottom of the screen. You can now start running streamlit in the tmux session.
streamlit run helloworld.py  You will be able to see your app at the External URL. The next step is to detach our TMUX session so that it continues running in the background when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)
You can now close your SSH session and the app will continue running at the External URL.
And Voila! We are up and running.
Pro TMUX Tip: You can reattach to the same session by using the attach command below. The best part is that you can close your SSH shell and then maybe come back after some hours and reattach to a session and keep working from wherever you were when you closed the SSH shell.
tmux attach -t StreamSession  Simple Troubleshooting: If your app is not hosting at 8501, it means that an instance of streamlit app is already running on your system and you will need to stop that. You can do so by first finding the process ID
ps aux | grep streamlit  You will see something like:
ubuntu 20927 2.4 18.8 713780 189580 pts/3 Sl&#43; 19:55 0:26 /home/ubuntu/miniconda/bin/python /home/ubuntu/miniconda/bin/streamlit run helloworld.py  You will need to kill this process. You can do this simply by
kill -9 20947  Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more. If you want to learn more about how to create awesome web apps with Streamlit then read up my last post.
In this post, we deployed a simple web app on AWS using amazon ec2.
In the process of doing this, we created our own Amazon ec2 instance, logged into the SSH shell, installed miniconda and dependencies, ran our Streamlit application and learned about TMUX. Enough learning for a day?
So go and show on these Mad skills. To end on a lighter note, as Sten Sootla says in his satire piece which I thoroughly enjoyed:
 The secret: it’s not what you know, it’s what you show.
 If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to write Web apps using simple Python for Data Scientists?</title>
      <link>https://mlwhiz.com/blog/2019/12/07/streamlit/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/07/streamlit/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlit/main.jpeg"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
So, are we doomed to learn web frameworks? Or to call our developer friend for silly doubts in the middle of the night?
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
 Zen of Python: Simple is better than complex and Streamlit makes it dead simple to create apps.
 This post is about understanding how to create apps that support data science projects using Streamlit.
To understand more about the architecture and the thought process that led to streamlit, have a look at this excellent post by one of the original developers/founder Adrien Treuille.
Installation Installation is as simple as running the command:
pip install streamlit
To see if our installation is successful, we can just run:
streamlit hello
This should show you a screen that says:
You can go to the local URL: localhost:8501 in your browser to see a Streamlit app in action. The developers have provided some cool demos that you can play with. Do take your time and feel the power of the tool before coming back.
Streamlit Hello World Streamlit aims to make app development easy using simple Python.
So let us write a simple app to see if it delivers on that promise.
Here I start with a simple app which we will call the Hello World of streamlit. Just paste the code given below in a file named helloworld.py
import streamlit as st x = st.slider(&amp;#39;x&amp;#39;) st.write(x, &amp;#39;squared is&amp;#39;, x * x) And, on the terminal run:
streamlit run helloworld.py And voila, you should be able to see a simple app in action in your browser at localhost:8501 that allows you to move a slider and gives the result.
It was pretty easy. In the above app, we used two features from Streamlit:
 the st.slider widget that we can slide to change the output of the web app.
 and the versatile st.write command. I am amazed at how it can write anything from charts, dataframes, and simple text. More on this later.
  Important: Remember that every time we change the widget value, the whole app runs from top to bottom.
Streamlit Widgets Widgets provide us a way to control our app. The best place to read about the widgets is the API reference documentation itself but I will describe some most prominent ones that you might end up using.
1. Slider streamlit.slider(label, min_value=None, max_value=None, value=None, step=None, format=None) We already saw st.slider in action above. It can be used with min_value,max_value, and step for getting inputs in a range.
2. Text Input The simplest way to get user input be it some URL input or some text input for sentiment analysis. It just needs a single label for naming the textbox.
import streamlit as st url = st.text_input(&amp;#39;Enter URL&amp;#39;) st.write(&amp;#39;The Entered URL is&amp;#39;, url) This is how the app looks:
Tip: You can just change the file helloworld.py and refresh the browser. The way I work is to open and change helloworld.py in sublime text and see the changes in the browser side by side.
3. Checkbox One use case for checkboxes is to hide or show/hide a specific section in an app. Another could be setting up a boolean value in the parameters for a function.st.checkbox() takes a single argument, which is the widget label. In this app, the checkbox is used to toggle a conditional statement.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) if st.checkbox(&amp;#39;Show dataframe&amp;#39;): st.write(df) 4. SelectBox We can use st.selectbox to choose from a series or a list. Normally a use case is to use it as a simple dropdown to select values from a list.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) option = st.selectbox( &amp;#39;Which Club do you like best?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, option) 5. MultiSelect We can also use multiple values from a dropdown. Here we use st.multiselect to get multiple values as a list in the variable options
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) options = st.multiselect( &amp;#39;What are your favorite clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, options) Creating Our Simple App Step by Step So much for understanding the important widgets. Now, we are going to create a simple app using multiple widgets at once.
To start simple, we will try to visualize our football data using streamlit. It is pretty much simple to do this with the help of the above widgets.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) # Filter dataframe new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] # write dataframe to screen st.write(new_df) Our simple app looks like:
That was easy. But it seems pretty basic right now. Can we add some charts?
Streamlit currently supports many libraries for plotting.Plotly, Bokeh, Matplotlib, Altair, and Vega charts being some of them. Plotly Express also works, although they didn’t specify it in the docs. It also has some inbuilt chart types that are “native” to Streamlit, like st.line_chart and st.area_chart.
We will work with plotly_express here. Here is the code for our simple app. We just used four calls to streamlit. Rest is all simple python.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # create figure using plotly express fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) Improvements In the start we said that each time we change any widget, the whole app runs from start to end. This is not feasible when we create apps that will serve deep learning models or complicated machine learning models. Streamlit covers us in this aspect by introducing Caching.
1. Caching In our simple app. We read the pandas dataframe again and again whenever a value changes. While it works for the small data we have, it will not work for big data or when we have to do a lot of processing on the data. Let us use caching using the st.cache decorator function in streamlit like below.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) Or for more complex and time taking functions that need to run only once(think loading big Deep Learning models), using:
@st.cache def complex_func(a,b): DO SOMETHING COMPLEX # Won&amp;#39;t run again and again. complex_func(a,b) When we mark a function with Streamlit’s cache decorator, whenever the function is called streamlit checks the input parameters that you called the function with.
If this is the first time Streamlit has seen these params, it runs the function and stores the result in a local cache.
When the function is called the next time, if those params have not changed, Streamlit knows it can skip executing the function altogether. It just uses the results from the cache.
2. Sidebar For a cleaner look based on your preference, you might want to move your widgets into a sidebar, something like Rshiny dashboards. This is pretty simple. Just add st.sidebar in your widget’s code.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) 3. Markdown? I love writing in Markdown. I find it less verbose than HTML and much more suited for data science work. So, can we use Markdown with the streamlit app?
Yes, we can. There are a couple of ways to do this. In my view, the best one is to use Magic commands. Magic commands allow you to write markdown as easily as comments. You could also have used the command st.markdown
import streamlit as st import pandas as pd import numpy as np import plotly_express as px &amp;#39;&amp;#39;&amp;#39; # Club and Nationality App This very simple webapp allows you to select and visualize players from certain clubs and certain nationalities. &amp;#39;&amp;#39;&amp;#39; df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) &amp;#39;&amp;#39;&amp;#39; ### Here is a simple chart between player age and overall &amp;#39;&amp;#39;&amp;#39; st.plotly_chart(fig) Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more.
In this post, we created a simple web app. But the possibilities are endless. To give an example here is face GAN from the streamlit site. And it works by just using the same guiding ideas of widgets and caching.
I love the default colors and styles that the developers have used, and I found it much more comfortable than using Dash, which I was using until now for my demos. You can also include audio and video in your streamlit apps.
On top of that, Streamlit is a free and open-source rather than a proprietary web app that just works out of the box.
In the past, I had to reach out to my developer friends for any single change in a demo or presentation; now it is relatively trivial to do that.
 I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
 I don’t have an idea if it will perform well in a production environment yet, but its a boon for the small proof of concept projects and demos. I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
You can find the full code for the final app here.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How did I learn Data Science?</title>
      <link>https://mlwhiz.com/blog/2019/08/12/resources/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/08/12/resources/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/resources/1.png"></media:content>
      

      
      <description>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
But I didn’t like it and so I left that.
I made it my goal to move into the analytics and data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of efforts to shift.
Now, people on social networks ask me how I got started in the data science field.</description>

      <content:encoded>  
        
        <![CDATA[  I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
But I didn’t like it and so I left that.
I made it my goal to move into the analytics and data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of efforts to shift.
Now, people on social networks ask me how I got started in the data science field. So I thought of giving a definitive answer.
 It is not really impossible to do this but it will take a lot of time and effort. Fortunately, I had an ample supply of both.
 Given below is the way that I took, and any aspiring person could choose to become a self-trained data scientist.
Some of the courses are not the same I did since some of them don’t exist and some have been merged into bigger specializations. But I have tried to keep it as similar to my experience as possible.
Also, I hope that you don’t lose hope after seeing the long list. You have to start with one or two courses. The rest will follow with time. Remember we have ample time.
Follow in order. I have tried to include everything that comes to my mind, including some post links which I think could be beneficial.
Introduction to Probability and Statistics Stat 110: The quintessential Probability and Statistics course you gotta take. All the lectures and notes are available on Youtube and his site for free.
If not for the content then for Prof. Joseph Blitzstein sense of humor. The above picture is a testament to that.
I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that.
Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.
This is a challenging class for a beginner but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving the proofs quickly. Sometimes the same proof was done in different ways to facilitate the learning of a concept.
One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts.
The inclusion of Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox, etc. made this course much much more exciting and enjoyable than any ordinary Statistics Course.
It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions.
He has also got a textbook based on this course, which is an excellent text and a must for any bookshelf.
 
Introduction to Python and Data Science:  Do first, understand later
 We need to get a taste of machine learning before understanding it fully. This segment is made up of three parts. These are not the exact courses I took to learn Python and getting an intro to data science. But they are quite similar and they serve the purpose.
a) Introduction to Data Science in Python This course is about learning to use Python and creating things on your own. You will learn about Python Libraries like Numpy, Pandas for data science.
You might also like my posts on Minimal Pandas for Data Scientists and small shorts on advanced python while going through this course.
Course description from Website:
 This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses.
 b) Applied Machine Learning in Python This course gives an intro to many modern machine learning methods that you should know about. Not a thorough grinding but you will get the tools to build your own models. You will learn scikit-learn, which is the python library to create all sorts of models.
The focus here is to start creating things as soon as possible. No one likes to wait too long to get something useful, and you will become useful after this course.
 This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial.
 c) Visualizations  A well made visualization is worth more than any PPT
 One thing you also need to learn about is Visualizations. This is an area which is constantly evolving with a lot of new libraries coming frequently. The libraries I use most are Seaborn and Plotly.
You could take a look at the below posts to get started with both basic and advanced visualizations.
Python’s One-Liner graph creation library with animations Hans Rosling Style
3 Awesome Visualization Techniques for every dataset
Machine Learning Fundamentals After doing these above courses, you will gain the status of what I would like to call a “Beginner.”
Congrats!!!. You know stuff; you know how to implement things.
Yet you do not fully understand all the math and grind that goes behind all these models.
You need to understand what goes behind the clf.fit
 If you don’t understand it you won’t be able to improve it
 Here comes the Game Changer Machine Learning course. Contains the maths behind many of the Machine Learning algorithms.
I will put this course as the one course you gotta take as this course motivated me into getting in this field, and Andrew Ng is a great instructor. Also, this was the first course that I took myself when I started.
This course has a little of everything — Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.
After this one, you are done with the three musketeers of the trade.
You know Python, you understand Statistics, and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D’artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that add a new freshness to our data science journey.
Here comes Big Data for you.
Big Data Analytics Using Spark  Big Data is omnipresent. Deal with it.
 The whole big data ecosystem has changed a lot since the time I learned Hadoop. And Spark was the new kid on the block at that time. Those days…
The courses I took are pretty redundant as of now so I would try to recommend something suitable for this era. The best course I could find that embodies most of what I learned through scattered sources is Big Data Analytics Using Spark.
From the course website, after doing this course, you will learn:
 Programming Spark using Pyspark Identifying the computational tradeoffs in a Spark application Performing data loading and cleaning using Spark and Parquet Modeling data through statistical and machine learning methods  You could also take a look at my recent post on Spark.
The Hitchhikers guide to handle Big Data using Spark
Understand Linux Shell Not a hard requirement but a good to have skill. Shell is a big friend of data scientists. It allows you to do simple data-related tasks in the terminal itself. I couldn’t emphasize how much time shell saves for me every day.
You can read the below post by me to know about this: Impress Onlookers with your newly acquired Shell Skills
If you would like to take a course, you can look at The UNIX workbench course on Coursera.
Congrats you are a “Hacker” now.
 You have got all the main tools in your belt to be a data scientist.
 On to more advanced topics. From here, it depends on you what you want to learn.
You may want to take a totally different approach than what I took going from here. There is no particular order. “All Roads Lead to Rome” as long as you are moving.
Learn Statistical Inference Mine Çetinkaya-Rundel teaches this course on Inferential Statistics. And it cannot get simpler than this one.
She is a great instructor and explains the fundamentals of Statistical inference nicely — a must-take course.
You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
Deep Learning  It is all about layers
 Intro — Making neural nets uncool again. This is a code-first class for neural nets. An excellent Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.
Advanced — You can try out this Deep Learning Specialization by Andrew Ng again. Pure Gold.
Advanced Math Book — A math-intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow
Take a look at below post if you want to learn Pytorch.
Moving from Keras to Pytorch
Learn NLP, Use Deep Learning with Text and create Chatbots  Reading is overrated. Let the machine do it.
 Natural Language Processing is something which captured my attention a while back.
I wrote a series of 6 posts on it. If you want, you can take a look.
NLP Learning Series — Towards Data Science
Algorithms, Graph Algorithms, and More  Algorithms. Yes, you need them.
 Apart from that if you want to learn about Python and the underlying intricacies of the language you can take the Computer Science Mini Specialization from RICE university too.
This is a series of 6 short but good courses.
I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn to program is by doing it.
The lectures are good, but the problems and assignments are awesome. If you work on this, you will learn Object-Oriented Programming, Graph algorithms, and creating games in Python. Pretty cool stuff.
You could also take a look at:
The 5 Feature Selection Algorithms every Data Scientist should know
The 5 Sampling Algorithms every Data Scientist need to know
Some Advanced Math Topics  Math — The power behind it all
 I am writing it last here but don’t underestimate the importance of Math in Data Science. You might want to look a little into these courses if you want to refresh your concepts.
Linear Algebra By Gilbert Strang— A Great Class by a great Teacher. I would definitely recommend this class to anyone who wants to learn Linear Algebra.
Multivariate Calculus — MIT Open Courseware
Convex Optimization — a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.
Conclusion The Machine learning field is evolving, and new advancements are made every day. That’s why I didn’t put the third tier.
 The maximum I can call myself is a “Hacker,” and my learning continues.
 Everyone has their own path, and here I provided mine to become a data scientist. And this is in no way perfect as obviously, a lot of things can be added to it.
Though I did not complete any professional training, I consider myself more of a Computer science engineer than a mechanical engineer now due to the above courses.
I hope they help you too.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Nation of a Billion Votes</title>
      <link>https://mlwhiz.com/blog/2019/05/19/election/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/05/19/election/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://cdn-images-1.medium.com/max/2400/1*8hmipT2UUqjI80pXHp8WFA.png"></media:content>
      

      
      <description>It is election month in India and a quote by Dr. Rahat Indori sums it up pretty well.
 “सरहदों पर बहुत तनाव है क्या , पता तो करो चुनाव है क्या !”  For English speakers, this means: Is there a lot of tension at the borders? just ask if the elections are on.
This election India has talked about a lot of issues. News channels have talked about Patriotism, Socialism, Religion as well as terrorism.</description>

      <content:encoded>  
        
        <![CDATA[  It is election month in India and a quote by Dr. Rahat Indori sums it up pretty well.
 “सरहदों पर बहुत तनाव है क्या , पता तो करो चुनाव है क्या !”  For English speakers, this means: Is there a lot of tension at the borders? just ask if the elections are on.
This election India has talked about a lot of issues. News channels have talked about Patriotism, Socialism, Religion as well as terrorism.
You might have heard of the tension brewing between India and Pakistan.
This election season has also been marred with a cacophony of insults. Politicians seem to have forgotten basic manners.
Calling the Prime Minister a Chai-Wala(Tea-Seller) and a Chowkidaar(Watchman). And the Prime Minister accepting both titles with grace and putting a Chowkidaar in front of his name on Twitter too. It was pure genius.
Today is the final phase of the 2019 Lok Sabha elections. I urge all those voting in this phase to vote in record numbers. Your one vote will shape India’s development trajectory in the years to come. I also hope first time voters vote enthusiastically.
&amp;mdash; Chowkidar Narendra Modi (@narendramodi) May 19, 2019 
Now finally the elections are over and the news channels are releasing the exit polls. This is pretty good data to look at.
And why waste good data?
So, here is an effort to showing some of the election data using some interactive visualizations using Flourish.
Recap of Politics in India To give a little perspective, in India, the center is held by BJP(Bhartiya Janta Party) and Narendra Modi is the current prime minister of India.
Most of India’s political history has been dominated by INC(Indian National Congress). This is the party of Mahatma Gandhi. INC’s head Rahul Gandhi, who is sometimes called the 50 years old youth, could be the perfect example of dynastic politics.
There are a lot of other local state parties, which I will call as OTH(Others)
For a perspective to the US audience(very broadly): BJP=Right Wing and INC=Left Wing.
India has 28 states and each state has a number of electoral seats. This is the seat distribution as per states.

Let us talk a little about the swing states.
The first state is Uttar Pradesh. A region of caste-based politics. Holding 8o seats out of 543 seats. A state that could swing the whole election. This state is the home of a large coalition named “MahaGatbandhan”(Literally a large coalition). This coalition is created by two heavy-weights — Akhilesh Yadav and Mayawati. Both have totally opposite views, yet they came together to move the BJP government out of the center.
Then there is the case of West Bengal having 42 seats, which is held by Mamata Banerjee. Sometimes people say that it might be a dictatorship and there have been a lot of violent incidents in this state over the past few days.She might also form a coalition with INC in the later stage to overthrow the current government.
All in all, INC, MahaGatbandhan and all others are out there to overthrow BJP(Modi government)
Exit Polls Now the elections are over after 7 phases of voting. And today is the exit polls in India.
That means all the news channels will come up with their individual surveys across the 542 Loksabha seats across the 28 states.
Let us not wait further and look at exit poll data of various news channels.
Here it is:

Seems like a sweeping victory for BJP and alliance.
And here is a racing bar chart of the same.

The only thing that doesn’t seem to move is BJP&#43; and seems like there will be a second term based on this visualization.

The parliament seems to look a little bit orange(BJP Color)though not as much as the last time.
And the swing states. I am going to look at poll data by TimesNow.

While it seems like a defeat for BJP&#43; in West Bengal, more than 10 seats in WB is actually a lot as per the state politics of left and should be categorized as a win.
In Uttar Pradesh too, BJP&#43; was able to handle a great opposition in form of Mahagatbandhan and that seems a win too.
Conclusion The 2019 Mandate seems to be favoring BJP and its allies.
And it is a sweeping answer to people who promote caste-based politics, dynastic rules, and minority appeasement politics.
Here is a tweet showing some frustration by the current CM of West Bengal.
I don’t trust Exit Poll gossip. The game plan is to manipulate or replace thousands of EVMs through this gossip. I appeal to all Opposition parties to be united, strong and bold. We will fight this battle together
&amp;mdash; Mamata Banerjee (@MamataOfficial) May 19, 2019 
While the data seems to show a lot of promise for BJP&#43;, we still need to see if the predictions by these news channels stand up to the actuals. The Election results will be published by 23 May.
I would also like to highlight Flourish here, which made creating the visualizations a breeze.
Visualizations that were Easy to make, Publish and share.
If you want to learn about best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out
I am going to be writing more visualization posts in the future too. Follow me up at Medium or Subscribe to my blog.
PS: I am a BJP Supporter but I have tried to be as objective as possible.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Python’s One Liner graph creation library with animations Hans Rosling Style</title>
      <link>https://mlwhiz.com/blog/2019/05/05/plotly_express/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/05/05/plotly_express/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/plotly_ex/visualization.png"></media:content>
      

      
      <description>I distinctly remember the time when Seaborn came. I was really so fed up with Matplotlib. To create even simple graphs I had to run through so many StackOverflow threads.
The time I could have spent in thinking good ideas for presenting my data was being spent in handling Matplotlib. And it was frustrating.
Seaborn is much better than Matplotlib, yet it also demands a lot of code for a simple “good looking” graph.</description>

      <content:encoded>  
        
        <![CDATA[    I distinctly remember the time when Seaborn came. I was really so fed up with Matplotlib. To create even simple graphs I had to run through so many StackOverflow threads.
The time I could have spent in thinking good ideas for presenting my data was being spent in handling Matplotlib. And it was frustrating.
Seaborn is much better than Matplotlib, yet it also demands a lot of code for a simple “good looking” graph.
When Plotly came it tried to solve that problem. And when added with Pandas, plotly is a great tool.
Just using the iplot function, you can do so much with Plotly.
But still, it is not very intuitive. At least not for me.
I still didn’t switch to Plotly just because I had spent enough time with Seaborn to do things “quickly” enough and I didn’t want to spend any more time learning a new visualization library. I had created my own functions in Seaborn to create the visualizations I most needed. Yet it was still a workaround. I had given up hope of having anything better.
Comes Plotly Express in the picture. And is it awesome?
According to the creators of Plotly Express (who also created Plotly obviously), Plotly Express is to Plotly what Seaborn is to Matplotlib. &amp;gt; # A terse, consistent, high-level wrapper around Plotly.py for rapid data exploration and figure generation.
I just had to try it out.
And have the creators made it easy to start experimenting with it?
One-liners to do everything you want? ✅
Standardized functions? Learn to create a scatterplot and you have pretty much learned this tool — ✅
Interactive graphs? ✅
Animations? Racing Bar plots, Scatter plots with time, Maps ✅
Free and Open Source? ✅
Just a sneak peek of what we will be able to create(and more) by the end of this post. Using a single line of code.
  Ok enough of the talk, let’s get to it.
First the Dataset — Interesting, Depressing and Inspiring all at once We will be working with the Suicide dataset I took from Kaggle. This dataset is compiled from data taken from the UN, World Bank and World Health Organization. The dataset was accumulated with the inspiration for Suicide Prevention. I am always up for such good use of data.
You can find all the code for this post and run it yourself in this Kaggle Kernel
First I will do some data Cleaning to add continent information and Country ISO codes as they will be helpful later:
import pandas as pd import numpy as np import plotly_express as px # Suicide Data suicides = pd.read_csv(&amp;#34;../input/suicide-rates-overview-1985-to-2016/master.csv&amp;#34;) del suicides[&amp;#39;HDI for year&amp;#39;] del suicides[&amp;#39;country-year&amp;#39;] # Country ISO Codes iso_country_map = pd.read_csv(&amp;#34;../input/countries-iso-codes/wikipedia-iso-country-codes.csv&amp;#34;) iso_country_map = iso_country_map.rename(columns = {&amp;#39;English short name lower case&amp;#39;:&amp;#34;country&amp;#34;}) # Load Country Continents file concap =pd.read_csv(&amp;#34;../input/country-to-continent/countryContinent.csv&amp;#34;, encoding=&amp;#39;iso-8859-1&amp;#39;)[[&amp;#39;code_3&amp;#39;, &amp;#39;continent&amp;#39;, &amp;#39;sub_region&amp;#39;]] concap = concap.rename(columns = {&amp;#39;code_3&amp;#39;:&amp;#34;Alpha-3 code&amp;#34;}) correct_names = {&amp;#39;Cabo Verde&amp;#39;: &amp;#39;Cape Verde&amp;#39;, &amp;#39;Macau&amp;#39;: &amp;#39;Macao&amp;#39;, &amp;#39;Republic of Korea&amp;#39;: &amp;#34;Korea, Democratic People&amp;#39;s Republic of&amp;#34; , &amp;#39;Russian Federation&amp;#39;: &amp;#39;Russia&amp;#39;, &amp;#39;Saint Vincent and Grenadines&amp;#39;:&amp;#39;Saint Vincent and the Grenadines&amp;#39; , &amp;#39;United States&amp;#39;: &amp;#39;United States Of America&amp;#39;} def correct_country(x): if x in correct_names: return correct_names[x] else: return x suicides[&amp;#39;country&amp;#39;] = suicides[&amp;#39;country&amp;#39;].apply(lambda x : correct_country(x)) suicides = pd.merge(suicides,iso_country_map,on=&amp;#39;country&amp;#39;,how=&amp;#39;left&amp;#39;) suicides = pd.merge(suicides,concap,on=&amp;#39;Alpha-3 code&amp;#39;,how=&amp;#39;left&amp;#39;) suicides[&amp;#39;gdp&amp;#39;] = suicides[&amp;#39;gdp_per_capita ($)&amp;#39;]*suicides[&amp;#39;population&amp;#39;] Let us look at the suicides data:
  I will also group the data by continents. Honestly, I am doing this only to show the power of the library as the main objective of this post will still be to create awesome visualizations.
suicides_gby_Continent = suicides.groupby([&amp;#39;continent&amp;#39;,&amp;#39;sex&amp;#39;,&amp;#39;year&amp;#39;]).aggregate(np.sum).reset_index() suicides_gby_Continent[&amp;#39;gdp_per_capita ($)&amp;#39;] = suicides_gby_Continent[&amp;#39;gdp&amp;#39;]/suicides_gby_Continent[&amp;#39;population&amp;#39;] suicides_gby_Continent[&amp;#39;suicides/100k pop&amp;#39;] = suicides_gby_Continent[&amp;#39;suicides_no&amp;#39;]*1000/suicides_gby_Continent[&amp;#39;population&amp;#39;] # 2016 data is not full suicides_gby_Continent=suicides_gby_Continent[suicides_gby_Continent[&amp;#39;year&amp;#39;]!=2016] suicides_gby_Continent.head() The final data we created:
  Simplicity of use We are ready to visualize our data. Comes Plotly Express time. I can install it just by a simple:
pip install plotly_express and import it as:
import plotly_express as px Now let us create a simple scatter plot with it.
suicides_gby_Continent_2007 = suicides_gby_Continent[suicides_gby_Continent[&amp;#39;year&amp;#39;]==2007] px.scatter(suicides_gby_Continent_2007,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;)   Not very inspiring. Right. Let us make it better step by step. Lets color the points by Continent.
px.scatter(suicides_gby_Continent_2007,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;,color=&amp;#39;continent&amp;#39;)   Better but not inspiring. YET.
The points look so small. Right. Let us increase the point size. How? What could the parameter be….
px.scatter(suicides_gby_Continent_2007,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;,color=&amp;#39;ContinentName&amp;#39;,size =&amp;#39;suicides/100k pop&amp;#39;)   Can you see there are two points for every continent? They are for male and female. Let me show that in the graph. We can show this distinction using a couple of ways. We can use different symbol or use different facets for male and female.
Let me show them both.
px.scatter(suicides_gby_Continent_2007,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;, size = &amp;#39;suicides/100k pop&amp;#39;, color=&amp;#39;ContinentName&amp;#39;,symbol=&amp;#39;sex&amp;#39;)   We could also create a faceted plot.
px.scatter(suicides_gby_Continent_2007,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;, size = &amp;#39;suicides/100k pop&amp;#39;, color=&amp;#39;continent&amp;#39;,facet_col=&amp;#39;sex&amp;#39;)   The triangles are for male and the circles are for females in the symbol chart. We are already starting to see some good info from the chart. For example:
 There is a significant difference between the suicide rates of Male vs Females at least in 2007 data.
 European Males were highly susceptible to Suicide in 2007?
 The income disparity doesn’t seem to play a big role in suicide rates. Asia has a lower GDP per capita and a lower suicide rate than Europe.
 There doesn’t seem to be income disparity amongst males and females.
  Still not inspiring? Umm. Let us add some animation. That shouldn’t have to be hard. I will just add some more parameters,
 animation_frame which specifies what will be our animation dimension.
 range of x and y values using range_y and range_x
 text which labels all points with continents. Helps in visualizing data better
  px.scatter(suicides_gby_Continent,x = &amp;#39;suicides/100k pop&amp;#39;, y = &amp;#39;gdp_per_capita ($)&amp;#39;,color=&amp;#39;continent&amp;#39;, size=&amp;#39;suicides/100k pop&amp;#39;,symbol=&amp;#39;sex&amp;#39;,animation_frame=&amp;#39;year&amp;#39;, animation_group=&amp;#39;continent&amp;#39;,range_x = [0,0.6], range_y = [0,70000],text=&amp;#39;continent&amp;#39;)   Wait for the gif plot to show.
In the Jupyter notebook, you will be able to stop the visualization, hover over the points, just look at a particular continent and do so much more with interactions.
So much information with a single command. We can see that:
 From 1991–2001 European Males had a pretty bad Suicide rate.
 Oceania even after having a pretty high GDP per capita, it is still susceptible to suicides.
 Africa has lower suicide rates as compared to other countries.
 For the Americas, the suicide rates have been increasing gradually.
  All of my above observations would warrant more analysis. But that is the point of having so much information on a single graph. It will help you to come up with a lot of hypotheses.
The above style of the plot is known as Hans Rosling plot named after its founder.
Here I would ask you to see this presentation from Hans Rosling where he uses Gapminder data to explain how income and lifespan emerged in the world through years. See it. It&amp;rsquo;s great.

Function Standardization So till now, we have learned about scatter plots. So much time to just learn a single class of charts. In the start of my post, I told you that this library has a sort of standardized functions.
Let us specifically look at European data as we saw that European males have a high Suicide rate.
european_suicide_data = suicides[suicides[&amp;#39;continent&amp;#39;] ==&amp;#39;Europe&amp;#39;] european_suicide_data_gby = european_suicide_data.groupby([&amp;#39;age&amp;#39;,&amp;#39;sex&amp;#39;,&amp;#39;year&amp;#39;]).aggregate(np.sum).reset_index() european_suicide_data_gby[&amp;#39;suicides/100k pop&amp;#39;] = european_suicide_data_gby[&amp;#39;suicides_no&amp;#39;]*1000/european_suicide_data_gby[&amp;#39;population&amp;#39;] # A single line to create an animated Bar chart too. px.bar(european_suicide_data_gby,x=&amp;#39;age&amp;#39;,y=&amp;#39;suicides/100k pop&amp;#39;,facet_col=&amp;#39;sex&amp;#39;,animation_frame=&amp;#39;year&amp;#39;, animation_group=&amp;#39;age&amp;#39;, category_orders={&amp;#39;age&amp;#39;:[&amp;#39;5-14 years&amp;#39;, &amp;#39;15-24 years&amp;#39;, &amp;#39;25-34 years&amp;#39;, &amp;#39;35-54 years&amp;#39;, &amp;#39;55-74 years&amp;#39;, &amp;#39;75&#43; years&amp;#39;]},range_y=[0,1]) Just like that, we have learned about animating our bar plots too. In the function above I provide a category_order for the axes to force the order of categories since they are ordinal. Rest all is still the same.
We can see that from 1991 to 2001 the suicide rate of 75&#43; males was very high. That might have increased the overall suicide rate for males.
Want to see how the suicide rates decrease in a country using a map? That is why we got the ISO-codes for the country in the data.
How many lines should that take? You guessed right. One.
suicides_map = suicides.groupby([&amp;#39;year&amp;#39;,&amp;#39;country&amp;#39;,&amp;#39;Alpha-3 code&amp;#39;]).aggregate(np.sum).reset_index()[[&amp;#39;country&amp;#39;,&amp;#39;Alpha-3 code&amp;#39;,&amp;#39;suicides_no&amp;#39;,&amp;#39;population&amp;#39;,&amp;#39;year&amp;#39;]] suicides_map[&amp;#34;suicides/100k pop&amp;#34;]=suicides_map[&amp;#34;suicides_no&amp;#34;]*1000/suicides_map[&amp;#34;population&amp;#34;] px.choropleth(suicides_map, locations=&amp;#34;Alpha-3 code&amp;#34;, color=&amp;#34;suicides/100k pop&amp;#34;, hover_name=&amp;#34;country&amp;#34;, animation_frame=&amp;#34;year&amp;#34;, color_continuous_scale=px.colors.sequential.Plasma) The plot above shows how suicide rates have changed over time in different countries and based on the info we get from the plot the coding effort required is minimal. We can see that:
 A lot of countries are missing
 Africa has very few countries in data
 Almost all of Asia is also missing.
  We can get quite a good understanding of our data just by seeing the above graphs.
Animations on the time axis also add up a lot of value as we are able to see all our data using a single graph.
This can help us in finding hidden patterns in the data. And you have to agree, it looks cool too.
Conclusion This was just a preview of Plotly Express. You can do a lot of other things using this library.
The main thing I liked about this library is the way it has tried to simplify graph creation. And how the graphs look cool out of the box.
Just think of the lengths one would have to go to to create the same graphs in Seaborn or Matplotlib or even Plotly. And you will be able to appreciate the power the library provides even more.
There is a bit of lack of documentation for this project by Plotly, but I found that the functions are pretty much well documented. On that note, you can see function definitions using Shift&#43;Tab in Jupyter.
Also as per its announcement article: “Plotly Express is totally free: with its permissive open-source MIT license, you can use it however you like (yes, even in commercial products!).”
So there is no excuse left now to put off that visual. Just get to it…
You can find all the code for this post and run it yourself in this Kaggle Kernel
If you want to learn about best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out
I am going to be writing more beginner friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>3 Awesome Visualization Techniques for every dataset</title>
      <link>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/visualizations/football.jpeg"></media:content>
      

      
      <description>Visualizations are awesome. However, a good visualization is annoyingly hard to make.
Moreover, it takes time and effort when it comes to present these visualizations to a bigger audience.
We all know how to make Bar-Plots, Scatter Plots, and Histograms, yet we don&amp;amp;rsquo;t pay much attention to beautify them.
This hurts us - our credibility with peers and managers. You won&amp;amp;rsquo;t feel it now, but it happens.</description>

      <content:encoded>  
        
        <![CDATA[    Visualizations are awesome. However, a good visualization is annoyingly hard to make.
Moreover, it takes time and effort when it comes to present these visualizations to a bigger audience.
We all know how to make Bar-Plots, Scatter Plots, and Histograms, yet we don&amp;rsquo;t pay much attention to beautify them.
This hurts us - our credibility with peers and managers. You won&amp;rsquo;t feel it now, but it happens.
Also, I find it essential to reuse my code. Every time I visit a new dataset do I need to start again? Some reusable ideas of graphs that can help us to find information about the data FAST.
In this post, I am also going to talk about 3 cool visual tools:
 Categorical Correlation with Graphs, Pairplots, Swarmplots and Graph Annotations using Seaborn.  In short, this post is about useful and presentable graphs.
I will be using data from FIFA 19 complete player dataset on kaggle - Detailed attributes for every player registered in the latest edition of FIFA 19 database.
Since the Dataset has many columns, we will only focus on a subset of categorical and continuous columns.
import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline # We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) player_df = pd.read_csv(&amp;#34;../input/data.csv&amp;#34;) numcols = [ &amp;#39;Overall&amp;#39;, &amp;#39;Potential&amp;#39;, &amp;#39;Crossing&amp;#39;,&amp;#39;Finishing&amp;#39;, &amp;#39;ShortPassing&amp;#39;, &amp;#39;Dribbling&amp;#39;,&amp;#39;LongPassing&amp;#39;, &amp;#39;BallControl&amp;#39;, &amp;#39;Acceleration&amp;#39;, &amp;#39;SprintSpeed&amp;#39;, &amp;#39;Agility&amp;#39;, &amp;#39;Stamina&amp;#39;, &amp;#39;Value&amp;#39;,&amp;#39;Wage&amp;#39;] catcols = [&amp;#39;Name&amp;#39;,&amp;#39;Club&amp;#39;,&amp;#39;Nationality&amp;#39;,&amp;#39;Preferred Foot&amp;#39;,&amp;#39;Position&amp;#39;,&amp;#39;Body Type&amp;#39;] # Subset the columns player_df = player_df[numcols&#43; catcols] # Few rows of data player_df.head(5)   Player Data    This is a nicely formatted data, yet we need to do some preprocessing to the Wage and Value columns(as they are in Euro and contain strings) to make them numeric for our subsequent analysis.
def wage_split(x): try: return int(x.split(&amp;#34;K&amp;#34;)[0][1:]) except: return 0 player_df[&amp;#39;Wage&amp;#39;] = player_df[&amp;#39;Wage&amp;#39;].apply(lambda x : wage_split(x)) def value_split(x): try: if &amp;#39;M&amp;#39; in x: return float(x.split(&amp;#34;M&amp;#34;)[0][1:]) elif &amp;#39;K&amp;#39; in x: return float(x.split(&amp;#34;K&amp;#34;)[0][1:])/1000 except: return 0 player_df[&amp;#39;Value&amp;#39;] = player_df[&amp;#39;Value&amp;#39;].apply(lambda x : value_split(x)) Categorical Correlation with Graphs: In Simple terms, Correlation is a measure of how two variables move together.
For example, In the real world, Income and Spend are positively correlated. If one increases the other also increases.
Academic Performance and Video Games Usage is negatively correlated. Increase in one predicts a decrease in another.
So if our predictor variable is positively or negatively correlated with our target variable, it is valuable.
I feel that Correlations among different variables are a pretty good thing to do when we try to understand our data.
We can create a pretty good correlation plot using Seaborn easily.
corr = player_df.corr() g = sns.heatmap(corr, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&amp;#34;shrink&amp;#34;: .5}, annot=True, fmt=&amp;#39;.2f&amp;#39;, cmap=&amp;#39;coolwarm&amp;#39;) sns.despine() g.figure.set_size_inches(14,10) plt.show()   Where did all the categorical variables go?    But do you notice any problem?
Yes, this graph only calculates Correlation between Numerical columns. What if my target variable is Club or Position?
I want to be able to get a correlation among three different cases, and we use the following metrics of correlation to calculate these:
1. Numerical Variables We already have this in the form of Pearson&amp;rsquo;s Correlation which is a measure of how two variables move together. This Ranges from [-1,1]
2. Categorical Variables We will use Cramer&amp;rsquo;s V for categorical-categorical cases. It is the intercorrelation of two discrete variables and used with variables having two or more levels. It is a symmetrical measure as in the order of variable does not matter. Cramer(A,B) == Cramer(B,A).
For Example: In our dataset, Club and Nationality must be somehow correlated.
Let us check this using a stacked graph which is an excellent way to understand distribution between categorical vs. categorical variables. Note that we use a subset of data since there are a lot of nationalities and club in this data.
We keep only the best teams(Kept FC Porto just for more diversity in the sample)and the most common nationalities.
  Note that Club preference says quite a bit about Nationality: knowing the former helps a lot in predicting the latter.
We can see that if a player belongs to England, it is more probable that he plays in Chelsea or Manchester United and not in FC Barcelona or Bayern Munchen or Porto.
So there is some information present here. Cramer&amp;rsquo;s V captures the same information.
If all clubs have the same proportion of players from every nationality, Cramer&amp;rsquo;s V is 0.
If Every club prefers a single nationality Cramer&amp;rsquo;s V ==1, for example, all England player play in Manchester United, All Germans in Bayern Munchen and so on.
In all other cases, it ranges from [0,1]
3. Numerical and Categorical variables We will use the Correlation Ratio for categorical-continuous cases.
Without getting into too much Maths, it is a measure of Dispersion.
 Given a number can we find out which category it belongs to?
 For Example:
Suppose we have two columns from our dataset: SprintSpeed and Position:
 GK: 58(De Gea),52(T. Courtois), 58(M. Neuer), 43(G. Buffon) CB: 68(D. Godin), 59(V. Kompany), 73(S. Umtiti), 75(M. Benatia) ST: 91(C.Ronaldo), 94(G. Bale), 80(S.Aguero), 76(R. Lewandowski)  As you can see these numbers are pretty predictive of the bucket they fall into and thus high Correlation Ratio.
If I know the sprint speed is more than 85, I can definitely say this player plays at ST.
This ratio also ranges from [0,1]
The code to do this is taken from the dython package. I won&amp;rsquo;t write too much into code which you can anyway find in my Kaggle Kernel. The final result looks something like:
player_df = player_df.fillna(0) results = associations(player_df,nominal_columns=catcols,return_results=True)   Categorical vs. Categorical, Categorical vs. Numeric, Numeric vs. Numeric. Much more interesting    Isn&amp;rsquo;t it Beautiful?
We can understand so much about Football just by looking at this data. For Example:
 The position of a player is highly correlated with dribbling ability. You won&amp;rsquo;t play Messi at the back. Right?
 Value is more highly correlated with passing and ball control than dribbling. The rule is to pass the ball always. Neymar I am looking at you.
 Club and Wage have high Correlation. To be expected.
 Body Type and Preferred Foot is correlated highly. Does that mean if you are Lean, you are most likely left-footed? Doesn&amp;rsquo;t make much sense. One can investigate further.
  Moreover, so much info we could find with this simple graph which was not visible in the typical correlation plot without Categorical Variables.
I leave it here at that. One can look more into the chart and find more meaningful results, but the point is that this makes life so much easier to find patterns.
Pairplots While I talked a lot about correlation, it is a fickle metric.
To understand what I mean let us see one example.
Anscombe&amp;rsquo;s quartet comprises four datasets that have nearly identical Correlation of 1, yet have very different distributions and appear very different when graphed.
  Anscombe Quartet - Correlations can be fickle.    Thus sometimes it becomes crucial to plot correlated data. And see the distributions individually.
Now we have many columns in our dataset. Graphing them all would be so much effort.
No, it is a single line of code.
filtered_player_df = player_df[(player_df[&amp;#39;Club&amp;#39;].isin([&amp;#39;FC Barcelona&amp;#39;, &amp;#39;Paris Saint-Germain&amp;#39;, &amp;#39;Manchester United&amp;#39;, &amp;#39;Manchester City&amp;#39;, &amp;#39;Chelsea&amp;#39;, &amp;#39;Real Madrid&amp;#39;,&amp;#39;FC Porto&amp;#39;,&amp;#39;FC Bayern München&amp;#39;])) &amp;amp; (player_df[&amp;#39;Nationality&amp;#39;].isin([&amp;#39;England&amp;#39;, &amp;#39;Brazil&amp;#39;, &amp;#39;Argentina&amp;#39;, &amp;#39;Brazil&amp;#39;, &amp;#39;Italy&amp;#39;,&amp;#39;Spain&amp;#39;,&amp;#39;Germany&amp;#39;])) ] # Single line to create pairplot g = sns.pairplot(filtered_player_df[[&amp;#39;Value&amp;#39;,&amp;#39;SprintSpeed&amp;#39;,&amp;#39;Potential&amp;#39;,&amp;#39;Wage&amp;#39;]])   Pretty Good. We can see so much in this graph.
 Wage and Value are highly correlated.
 Most of the other values are correlated too. However, the trend of potential vs. value is unusual. We can see how the value increases exponentially as we reach a particular potential threshold. This information can be helpful in modeling. Can use some transformation on Potential to make it more correlated?
  Caveat: No categorical columns.
Can we do better? We always can.
g = sns.pairplot(filtered_player_df[[&amp;#39;Value&amp;#39;,&amp;#39;SprintSpeed&amp;#39;,&amp;#39;Potential&amp;#39;,&amp;#39;Wage&amp;#39;,&amp;#39;Club&amp;#39;]],hue = &amp;#39;Club&amp;#39;)   So much more info. Just by adding the hue parameter as a categorical variable Club.
 Porto&amp;rsquo;s Wage distribution is too much towards the lower side. I don&amp;rsquo;t see that steep distribution in value of Porto players. Porto&amp;rsquo;s players would always be looking out for an opportunity. See how a lot of pink points(Chelsea) form sort of a cluster on Potential vs. wage graph. Chelsea has a lot of high potential players with lower wages. Needs more attention.  I already know some of the points on the Wage/Value Subplot.
The blue point for wage 500k is Messi. Also, the orange point having more value than Messi is Neymar.
Although this hack still doesn&amp;rsquo;t solve the Categorical problem, I have something cool to look into categorical variables distribution. Though individually.
SwarmPlots How to see the relationship between categorical and numerical data?
Enter into picture Swarmplots, just like their name. A swarm of points plotted for each category with a little dispersion on the y-axis to make them easier to see.
They are my current favorite for plotting such relationships.
g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(14,10) plt.show()   Swarmplot...    Why don&amp;rsquo;t I use Boxplots? Where are the median values? Can I plot that? Obviously. Overlay a bar plot on top, and we have a great looking graph.
g = sns.boxplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7,color = &amp;#39;black&amp;#39;) # remove the top and right line in graph sns.despine() g.figure.set_size_inches(12,8) plt.show()   Swarmplot&#43;Boxplot, Interesting    Pretty good. We can see the individual points on the graph, see some statistics and understand the wage difference categorically.
The far right point is Messi. However, I should not have to tell you that in a text below the chart. Right?
This graph is going to go in a presentation. Your boss says. I want to write Messi on this graph. Comes into picture annotations.
max_wage = filtered_player_df.Wage.max() max_wage_player = filtered_player_df[(player_df[&amp;#39;Wage&amp;#39;] == max_wage)][&amp;#39;Name&amp;#39;].values[0] g = sns.boxplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, whis=np.inf) g = sns.swarmplot(y = &amp;#34;Club&amp;#34;, x = &amp;#39;Wage&amp;#39;, data = filtered_player_df, # Decrease the size of the points to avoid crowding size = 7,color=&amp;#39;black&amp;#39;) # remove the top and right line in graph sns.despine() # Annotate. xy for coordinate. max_wage is x and 0 is y. In this plot y ranges from 0 to 7 for each level # xytext for coordinates of where I want to put my text plt.annotate(s = max_wage_player, xy = (max_wage,0), xytext = (500,1), # Shrink the arrow to avoid occlusion arrowprops = {&amp;#39;facecolor&amp;#39;:&amp;#39;gray&amp;#39;, &amp;#39;width&amp;#39;: 3, &amp;#39;shrink&amp;#39;: 0.03}, backgroundcolor = &amp;#39;white&amp;#39;) g.figure.set_size_inches(12,8) plt.show()   Annotated, Statistical Info and point swarm. To the presentation, I go.     See Porto Down there. Competing with the giants with such a small wage budget. So many Highly paid players in Real and Barcelona. Manchester City has the highest median Wage. Manchester United and Chelsea believes in equality. Many players clustered in around the same wage scale. I am happy that while Neymar is more valued than Messi, Messi and Neymar have a huge Wage difference.  A semblance of normalcy in this crazy world.
So to recap, in this post, we talked about calculating and reading correlations between different variable types, plotting correlations between numerical data and Plotting categorical data with Numerical data using Swarmplots. I love how we can overlay chart elements on top of each other in Seaborn.
Also if you want to learn more about Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out
If you liked this post, do look at my other post on Seaborn too where I have created some more straightforward reusable graphs. I am going to be writing more beginner friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
Code for this post in this kaggle kernel.
References:  The Search for Categorical Correlation Seaborn Swarmplot Documentation Seaborn Pairplot Documentation  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      
      

      
      <description>I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;amp;rsquo;t cease to amaze me.</description>

      <content:encoded>  
        
        <![CDATA[  I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?
But it is not magic. And this is a way an aspiring guy could take to become a self-trained data scientist. Follow in order. I have tried to include everything that comes to my mind. So here goes:
1. Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University The one stat course you gotta take. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.
This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.
One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.
It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.
He got a textbook out based on this course which is clearly a great text:
 2. Data Science CS109: - Again by Professor Blitzstein. Again an awesome course. Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts. You will learn about Python Libraries like Numpy,Pandas for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.  3. CS229: Andrew Ng After doing these two above courses you will gain the status of what I would like to call a &amp;ldquo;Beginner&amp;rdquo;. Congrats!!!. You know stuff, you know how to implement stuff. Yet you do not fully understand all the math and grind that goes behind all this.
Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms. I will put this course as the one course you gotta take as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.
Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website here.
You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D&amp;rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.
4. Intro to Hadoop &amp;amp; Mapreduce - Udacity Let us first focus on the literal elephant in the room - Hadoop. Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.
Once you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem. Read this amazing Blog Post from Michael Noll: Writing An Hadoop MapReduce Program In Python - Michael G. Noll. Just read the basic mapreduce codes. Don&amp;rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.
Now try to solve these two problems from the CS109 Harvard course from 2013:
A. First, grab the file word_list.txt from here. This contains a list of six-letter words. To keep things simple, all of the words consist of lower-case letters only.Write a mapreduce job that finds all anagrams in word_list.txt.
B. For the next problem, download the file baseball_friends.csv. Each row of this csv file contains the following:
 A person&amp;rsquo;s name The team that person is rooting for &amp;ndash; either &amp;ldquo;Cardinals&amp;rdquo; or &amp;ldquo;Red Sox&amp;rdquo; A list of that person&amp;rsquo;s friends, which could have arbitrary length  For example: The first line tells us that Aaden is a Red Sox friend and he has 65 friends, who are all listed here. For this problem, it&amp;rsquo;s safe to assume that all of the names are unique and that the friendship structure is symmetric (i.e. if Alannah shows up in Aaden&amp;rsquo;s friends list, then Aaden will show up in Alannah&amp;rsquo;s friends list). Write an mr job that lists each person&amp;rsquo;s name, their favorite team, the number of Red Sox fans they are friends with, and the number of Cardinals fans they are friends with.
Try to do this yourself. Don&amp;rsquo;t use the mrjob (pronounced Mr. Job) way that they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.
If you are done with these, you can safely call yourself as someone who could &amp;ldquo;think in Mapreduce&amp;rdquo; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:
Hadoop Mapreduce Streaming Tricks and Techniques
If you are someone who likes learning from a book you can get: 
5. Spark - In memory Big Data tool. Now comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn&amp;rsquo;t.
Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.
Data Science and Engineering with Apache® Spark™
I have written a little bit about Basic data processing with Spark here. Take a look: Learning Spark using Python: Basics and Applications
Also take a look at some of the projects I did as part of course at github
If you would like a book to read: 
If you don&amp;rsquo;t go through the courses, try solving the same two problems above that you solved by Hadoop using Spark too. Otherwise the problem sets in the courses are more than enough.
6. Understand Linux Shell: Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn&amp;rsquo;t emphasize how much time shell saves for me everyday.
Read these tutorials by me for doing that:
Shell Basics every Data Scientist Should know -Part I Shell Basics every Data Scientist Should know - Part II(AWK)
If you would like a course you can go for this course on edX.
If you want a book, go for:
 Congrats you are an &amp;ldquo;Hacker&amp;rdquo; now. You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &amp;ldquo;All Roads lead to Rome&amp;rdquo; as long as you are running.
7. Learn Statistical Inference and Bayesian Statistics I took the previous version of the specialization which was a single course taught by Mine Çetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data. You can also use these books:
  8. Deep Learning Intro - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.
Advanced - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.
Bonus - A free online book by Michael Nielsen.
Advanced Math Book - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow
9. Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More This course used to be there on Coursera but now only video links on youtube available. You can learn from this book too: 
Apart from that if you want to learn about Python and the basic intricacies of the language you can take the Computer Science Mini Specialization from RICE university too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.
10. Advanced Maths: Couldn&amp;rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.
Linear Algebra By Gilbert Strang - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.
Multivariate Calculus - MIT OCW
Convex Optimization - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.
The Machine learning field is evolving and new advancements are made every day. That&amp;rsquo;s why I didn&amp;rsquo;t put a third tier. The maximum I can call myself is a &amp;ldquo;Hacker&amp;rdquo; and my learning continues. Hope you do the same.
Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.
Till then. Ciao!!!
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title>
      <link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid>
      
      

      
      <description>When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph.</description>

      <content:encoded>  
        
        <![CDATA[  When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph. This led me to think that a Blog post to create common Graph types in Python is in order. But being the procrastinator that I am it always got pushed to the back of my head.
One thing that helped me in pursuit of my data visualization needs in Python was this awesome course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Highly Recommended.
But, yesterday I got introduced to Seaborn and I must say I am quite impressed with it. It makes beautiful graphs that are in my opinion better than R&amp;rsquo;s ggplot2. Gives you enough options to customize and the best part is that it is so easy to learn.
So I am finally writing this blog post with a basic purpose of creating a code base that provides me with ready to use codes which could be put into analysis in a fairly straight-forward manner.
Right. So here Goes.
We Start by importing the libraries that we will need to use.
import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes To create a use case for our graphs, we will be working with the Tips data that contains the following information.
tips = sns.load_dataset(&amp;#34;tips&amp;#34;) tips.head()   Scatterplot With Regression Line Now let us work on visualizing this data. We will use the regplot option in seaborn.
# We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) # Here we create a matplotlib axes object. The extra parameters we use # &amp;#34;ci&amp;#34; to remove confidence interval # &amp;#34;marker&amp;#34; to have a x as marker. # &amp;#34;scatter_kws&amp;#34; to provide style info for the points.[s for size] # &amp;#34;line_kws&amp;#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;, data=tips, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, line_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;alpha&amp;#34;:0.5,&amp;#34;lw&amp;#34;:4},marker=&amp;#34;x&amp;#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Tip&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Total Bill&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Now that required a bit of a code but i feel that it looks much better than what either Matplotlib or ggPlot2 could have rendered. We got a lot of customization without too much code.
But that is not really what actually made me like Seaborn. The plot type that actually got my attention was lmplot, which lets us use regplot in a faceted mode.
# So this function creates a faceted plot. The plot is parameterized by the following: # col : divides the data points into days and creates that many plots # palette: deep, muted, pastel, bright, dark, and colorblind. change the colors in graph. Experiment with these # col_wrap: we want 2 graphs in a row? Yes.We do # scatter_kws: attributes for points # hue: Colors on a particular column. # size: controls the size of graph g = sns.lmplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;,ci=None,data=tips, col=&amp;#34;day&amp;#34;, palette=&amp;#34;muted&amp;#34;,col_wrap=2,scatter_kws={&amp;#34;s&amp;#34;: 100,&amp;#34;alpha&amp;#34;:.5}, line_kws={&amp;#34;lw&amp;#34;:4,&amp;#34;alpha&amp;#34;:0.5},hue=&amp;#34;day&amp;#34;,x_jitter=1.0,y_jitter=1.0,size=6) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabels(&amp;#34;Tip&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabels(&amp;#34;Total Bill&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here titles = [&amp;#39;Thursday&amp;#39;,&amp;#39;Friday&amp;#39;,&amp;#39;Saturday&amp;#39;,&amp;#39;Sunday&amp;#39;] for ax,title in zip(g.axes.flat,titles): ax.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   A side Note on Palettes:
You can build your own color palettes using color_palette() function. color_palette() will accept the name of any seaborn palette or matplotlib colormap(except jet, which you should never use). It can also take a list of colors specified in any valid matplotlib format (RGB tuples, hex color codes, or HTML color names). The return value is always a list of RGB tuples. This allows you to use your own color palettes in graph.   Barplots sns.set(style=&amp;#34;ticks&amp;#34;) flatui = [&amp;#34;#9b59b6&amp;#34;, &amp;#34;#3498db&amp;#34;, &amp;#34;#95a5a6&amp;#34;, &amp;#34;#e74c3c&amp;#34;, &amp;#34;#34495e&amp;#34;, &amp;#34;#2ecc71&amp;#34;] # This Function takes as input a custom palette g = sns.barplot(x=&amp;#34;sex&amp;#34;, y=&amp;#34;tip&amp;#34;, hue=&amp;#34;day&amp;#34;, palette=sns.color_palette(flatui),data=tips,ci=None) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Do We tend to \nTip high on Weekends?&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Gender&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Mean Tips&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Histograms and Distribution Diagrams They form another part of my workflow. Lets plot the normal Histogram using seaborn. For this we will use the distplot function. This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() function. It can also fit scipy.stats distributions and plot the estimated PDF over the data.
# Create a list of 1000 Normal RVs x = np.random.normal(size=1000) sns.set_context(&amp;#34;poster&amp;#34;) sns.set_style(&amp;#34;ticks&amp;#34;) # This Function creates a normed Histogram by default. # If we use the parameter kde=False and norm_hist=False then # we will be using a count histogram g=sns.distplot(x, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Normal Simulation&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   import scipy.stats as stats a = 1.5 b = 1.5 x = np.arange(0.01, 1, 0.01) y = stats.beta.rvs(a,b,size=10000) y_act = stats.beta.pdf(x,a,b) g=sns.distplot(y,kde=False,norm_hist=True, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # Note that we plotted on the graph using plt matlabplot function plt.plot(x,y_act) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title((&amp;#34;Beta Simulation vs. Calculated Beta Density\nFor a=%s,b=%s&amp;#34;) %(a,b),fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   PairPlots You need to see how variables vary with one another. What is the distribution of variables in the dataset. This is the graph to use with the pairplot function. Very helpful And Seaborn males it a joy to use. We will use Iris Dataset here for this example.
iris = sns.load_dataset(&amp;#34;iris&amp;#34;) iris.head()   # Create a Pairplot g = sns.pairplot(iris,hue=&amp;#34;species&amp;#34;,palette=&amp;#34;muted&amp;#34;,size=5, vars=[&amp;#34;sepal_width&amp;#34;, &amp;#34;sepal_length&amp;#34;],kind=&amp;#39;reg&amp;#39;,markers=[&amp;#39;o&amp;#39;,&amp;#39;x&amp;#39;,&amp;#39;&#43;&amp;#39;]) # To change the size of the scatterpoints in graph g = g.map_offdiag(plt.scatter, s=35,alpha=0.5) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Relation between Sepal Width and Sepal Length&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3)   Hope you found this post useful and worth your time. You can find the iPython notebook at github
I tried to make this as simple as possible but You may always ask me or see the documentation for doubts.
If you have any more ideas on how to use Seaborn or which graphs should i add here, please suggest in the comments section.
I will definitely try to add to this post as I start using more visualizations and encounter other libraries as good as seaborn.
Also since this is my first visualization post on this blog, I would like to call out a good course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>