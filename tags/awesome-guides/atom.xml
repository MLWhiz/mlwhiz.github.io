<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Awesome Guides on MLWhiz</title>
    <link>https://mlwhiz.com/tags/awesome-guides/</link>
    <description>Recent content in Awesome Guides on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/awesome-guides/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>The Most Complete Guide to PyTorch for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pytorch_guide/main.png"></media:content>
      

      
      <description>PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.
I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch.</description>

      <content:encoded>  
        
        <![CDATA[  PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.
I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch. With its high customizability and pythonic syntax,PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.
So, in this PyTorch guide, I will try to ease some of the pain with PyTorch for starters and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.
But, that is not to say that this is aimed at beginners only as I will also talk about the high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.
So let’s get some coffee ☕ ️and start it up.
Tensors Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.
1. Create a Tensor We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of more things with tensors just like you can do with NumPy arrays.
# Using torch.Tensor t = torch.Tensor([[1,2,3],[3,4,5]]) print(f&amp;#34;Created Tensor Using torch.Tensor:\n{t}&amp;#34;) # Using torch.randn t = torch.randn(3, 5) print(f&amp;#34;Created Tensor Using torch.randn:\n{t}&amp;#34;) # using torch.[ones|zeros](*size) t = torch.ones(3, 5) print(f&amp;#34;Created Tensor Using torch.ones:\n{t}&amp;#34;) t = torch.zeros(3, 5) print(f&amp;#34;Created Tensor Using torch.zeros:\n{t}&amp;#34;) # using torch.randint - a tensor of size 4,5 with entries between 0 and 10(excluded) t = torch.randint(low = 0,high = 10,size = (4,5)) print(f&amp;#34;Created Tensor Using torch.randint:\n{t}&amp;#34;) # Using from_numpy to convert from Numpy Array to Tensor a = np.array([[1,2,3],[3,4,5]]) t = torch.from_numpy(a) print(f&amp;#34;Convert to Tensor From Numpy Array:\n{t}&amp;#34;) # Using .numpy() to convert from Tensor to Numpy array t = t.numpy() print(f&amp;#34;Convert to Numpy Array From Tensor:\n{t}&amp;#34;) 2. Tensor Operations Again, there are a lot of operations you can do on these tensors. The full list of functions can be found here.
A = torch.randn(3,4) W = torch.randn(4,2) # Multiply Matrix A and W t = A.mm(W) print(f&amp;#34;Created Tensor t by Multiplying A and W:\n{t}&amp;#34;) # Transpose Tensor t t = t.t() print(f&amp;#34;Transpose of Tensor t:\n{t}&amp;#34;) # Square each element of t t = t**2 print(f&amp;#34;Square each element of Tensor t:\n{t}&amp;#34;) # return the size of a tensor print(f&amp;#34;Size of Tensor t using .size():\n{t.size()}&amp;#34;) Note: What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is deprecated, and all methods for variables work with Tensors. So, if you don’t know about them, it’s fine as they re not needed, and if you know them, you can forget about them.
The nn.Module Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Moduleto define any model as a class. Every model class necessarily contains an__init__ procedure block and a block for the forward pass.
 In the __init__ part, the user can define all the layers the network is going to have but doesn&amp;rsquo;t yet define how those layers would be connected to each other.
 In the forward pass block, the user defines how data flows from one layer to another inside the network.
  So, put simply, any network we define will look like:
class myNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) x = self.lin2(x) return x Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do).
class myCrazyNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 784) self.lin3 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x_lin1 = self.lin1(x) x_lin2 = x &#43; self.lin2(x_lin1) x_lin2 = self.lin1(x_lin2) x = self.lin3(x_lin2) return x We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.
x = torch.randn((100,784)) model = myCrazyNeuralNet() model(x).size() -------------------------- torch.Size([100, 10]) A word about Layers Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using nn.Module. For example, rather than using the predefined Linear Layer nn.Linear from Pytorch above, we could have created our custom linear layer.
class myCustomLinearLayer(nn.Module): def __init__(self,in_size,out_size): super().__init__() self.weights = nn.Parameter(torch.randn(in_size, out_size)) self.bias = nn.Parameter(torch.zeros(out_size)) def forward(self, x): return x.mm(self.weights) &#43; self.bias You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch docs:
 Parameters are *Tensor* subclasses, that have a very special property when used with Module - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in parameters() iterator
 As you will later see, the model.parameters() iterator will be an input to the optimizer. But more on that later.
Right now, we can now use this custom layer in any PyTorch network, just like any other layer.
class myCustomNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = myCustomLinearLayer(784,10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) return x x = torch.randn((100,784)) model = myCustomNeuralNet() model(x).size() ------------------------------------------ torch.Size([100, 10]) But then again, Pytorch would not be so widely used if it didn’t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are: nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.BatchNorm2d, nn.Dropout, nn.Embedding, nn.GRU/nn.LSTM, nn.Softmax, nn.LogSoftmax, nn.MultiheadAttention, nn.TransformerEncoder, nn.TransformerDecoder
I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.
So, a Conv2d Layer needs as input an Image of height H and width W, with Cin channels. Now, for the first layer in a convnet, the number of in_channels would be 3(RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.
To check a new layer which I don’t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:
conv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1) And then pass some random input through it. Here 100 is the batch size.
x = torch.randn((100,3,24,24)) conv_layer(x).size() -------------------------------- torch.Size([100, 64, 24, 24]) So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.
Datasets and DataLoaders How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at torchvision.datasets and torchtext.datasets. But, to give a concrete example for datasets, let’s say we had to pass images to an Image Neural net using a folder which has images in this structure:
data train sailboat kayak . .  We can use torchvision.datasets.ImageFolder dataset to get an example image like below:
from torchvision import transforms from torchvision.datasets import ImageFolder traindir = &amp;#34;data/train/&amp;#34; t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = ImageFolder(root=traindir,transform=t) print(&amp;#34;Num Images in Dataset:&amp;#34;, len(train_dataset)) print(&amp;#34;Example Image and Label:&amp;#34;, train_dataset[2]) This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:
for i in range(0,len(train_dataset)): image ,label = train_dataset[i] pred = model(image) But that is not optimal. We want to do batching. We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) We can simply iterate with batches using:
for image_batch, label_batch in train_dataloader: print(image_batch.size(),label_batch.size()) break ------------------------------------------------- torch.Size([64, 3, 224, 224]) torch.Size([64]) So actually, the whole process of using datasets and Dataloaders becomes:
t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = torchvision.datasets.ImageFolder(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch) You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning here.
This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case.
Understanding Custom Datasets To write our custom datasets, we can make use of the abstract class torch.utils.data.Dataset provided by Pytorch. We need to inherit this Dataset class and need to define two methods to create a custom Dataset.
 __len__ : a function that returns the size of the dataset. This one is pretty simple to write in most cases.
 __getitem__: a function that takes as input an index i and returns the sample at index i.
  For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in __init__ part where we use glob.glob to get image names and do some general preprocessing.
from glob import glob from PIL import Image from torch.utils.data import Dataset class customImageFolderDataset(Dataset): &amp;#34;&amp;#34;&amp;#34;Custom Image Loader dataset.&amp;#34;&amp;#34;&amp;#34; def __init__(self, root, transform=None): &amp;#34;&amp;#34;&amp;#34; Args: root (string): Path to the images organized in a particular folder structure. transform: Any Pytorch transform to be applied &amp;#34;&amp;#34;&amp;#34; # Get all image paths from a directory self.image_paths = glob(f&amp;#34;{root}/*/*&amp;#34;) # Get the labels from the image paths self.labels = [x.split(&amp;#34;/&amp;#34;)[-2] for x in self.image_paths] # Create a dictionary mapping each label to a index from 0 to len(classes). self.label_to_idx = {x:i for i,x in enumerate(set(self.labels))} self.transform = transform def __len__(self): # return length of dataset return len(self.image_paths) def __getitem__(self, idx): # open and send one image and label img_name = self.image_paths[idx] label = self.labels[idx] image = Image.open(img_name) if self.transform: image = self.transform(image) return image,self.label_to_idx[label] Also, note that we open our images one at a time in the __getitem__ method and not while initializing. This is not done in __init__ because we don&amp;rsquo;t want to load all our images in the memory and just need to load the required ones.
We can now use this dataset with the utility Dataloader just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.
t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = customImageFolderDataset(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch) Understanding Custom DataLoaders This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.
So let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes.
class BiLSTM(nn.Module): def __init__(self): super().__init__() self.hidden_size = 64 drp = 0.1 max_features, embed_size = 10000,300 self.embedding = nn.Embedding(max_features, embed_size) self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True) self.linear = nn.Linear(self.hidden_size*4 , 64) self.relu = nn.ReLU() self.dropout = nn.Dropout(drp) self.out = nn.Linear(64, 1) def forward(self, x): h_embedding = self.embedding(x) h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)) h_lstm, _ = self.lstm(h_embedding) avg_pool = torch.mean(h_lstm, 1) max_pool, _ = torch.max(h_lstm, 1) conc = torch.cat(( avg_pool, max_pool), 1) conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) return out This network expects its input to be of shape (batch_size, seq_length) and works with any seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).
model = BiLSTM() input_batch_1 = torch.randint(low = 0,high = 10000, size = (100,**10**)) input_batch_2 = torch.randint(low = 0,high = 10000, size = (100,**25**)) print(model(input_batch_1).size()) print(model(input_batch_2).size()) ------------------------------------------------------------------ torch.Size([100, 1]) torch.Size([100, 1]) Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.
So, how do we do this? Let’s write a very simple custom dataset class first.
class CustomTextDataset(Dataset): &amp;#39;&amp;#39;&amp;#39; Simple Dataset initializes with X and y vectors We start by sorting our X and y vectors by sequence lengths &amp;#39;&amp;#39;&amp;#39; def __init__(self,X,y=None): self.data = list(zip(X,y)) # Sort by length of first element in tuple self.data = sorted(self.data, key=lambda x: len(x[0])) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] Also, let’s generate some random data which we will use with this custom Dataset.
import numpy as np train_data_size = 1024 sizes = np.random.randint(low=50,high=300,size=(train_data_size,)) X = [np.random.randint(0,10000, (sizes[i])) for i in range(train_data_size)] y = np.random.rand(train_data_size).round() #checking one example in dataset print((X[0],y[0])) Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.
We can use the custom dataset now using:
train_dataset = CustomTextDataset(X,y) If we now try to use the Dataloader on this dataset with batch_size&amp;gt;1, we will get an error. Why is that?
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10) for xb,yb in train_dataloader: print(xb.size(),yb.size()) This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn’t face this error.
So, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?
We can use collate_fn parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch, y_batch ) with padded sequence lengths based on max_sequence_length in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.
def collate_text(batch): # get text sequences in batch data = [item[0] for item in batch] # get labels in batch target = [item[1] for item in batch] # get max_seq_length in batch max_seq_len = max([len(x) for x in data]) # pad text sequences based on max_seq_len data = [np.pad(p, (0, max_seq_len - len(p)), &amp;#39;constant&amp;#39;) for p in data] # convert data and target to tensor data = torch.LongTensor(data) target = torch.LongTensor(target) return [data, target] We can now use this collate_fn with our Dataloader as:
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,collate_fn = collate_text) for xb,yb in train_dataloader: print(xb.size(),yb.size()) It will work this time as we have provided a custom collate_fn. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.
Training a Neural Network We know how to create a neural network using nn.Module. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:
num_epochs = 5 for epoch in range(num_epochs): # Set model to train mode model.train() for x_batch,y_batch in train_dataloader: # Clear gradients optimizer.zero_grad() # Forward pass - Predicted outputs pred = model(x_batch) # Find Loss and backpropagation of gradients loss = loss_criterion(pred, y_batch) loss.backward() # Update the parameters optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: pred = model(x_batch) val_loss = loss_criterion(pred, y_batch) In the above code, we are running five epochs and in each epoch:
 We iterate through the dataset using a data loader.
 In each iteration, we do a forward pass using model(x_batch)
 We calculate the Loss using a loss_criterion
 We back-propagate that loss using loss.backward() call. We don&amp;rsquo;t have to worry about the calculation of the gradients at all, as this simple call does it all for us.
 Take an optimizer step to change the weights in the whole network using optimizer.step(). This is where weights of the network get modified using the gradients calculated in loss.backward() call.
 We go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using model.eval().Please note we don&amp;rsquo;t back-propagate losses in eval mode.
  Till now, we have talked about how to use nn.Module to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let&amp;rsquo;s talk about the various options available for Loss Functions and Optimizers.
Loss functions Pytorch provides us with a variety of loss functions for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss , nn.NLLLoss , nn.KLDivLoss and nn.MSELoss. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of nn.NLLLoss
The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:
 the input expected needs to be of size (batch_size x Num_Classes ) — These are the predictions from the Neural Network we have created.
 We need to have the log-probabilities of each class in the input — To get log-probabilities from a Neural Network, we can add a LogSoftmax Layer as the last layer of our network.
 The target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.
  So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don&amp;rsquo;t want to use this LogSoftmax layer, you could have just used nn.CrossEntropyLoss
class myClassificationNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin = nn.Linear(784, 10) self.logsoftmax = nn.LogSoftmax(dim=1) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin(x) x = self.logsoftmax(x) return x Let’s define a random input to pass to our network to test it:
# some random input: X = torch.randn(100,784) y = torch.randint(low = 0,high = 10,size = (100,)) And pass it through the model to get predictions:
model = myClassificationNet() preds = model(X) We can now get the loss as:
criterion = nn.NLLLoss() loss = criterion(preds,y) loss ------------------------------------------ tensor(2.4852, grad_fn=&amp;lt;NllLossBackward&amp;gt;) Custom Loss Function Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss
def customMseLoss(output,target): loss = torch.mean((output - target)**2) return loss You can use this custom loss just like before. But note that we don’t instantiate the loss using criterion this time as we have defined it as a function.
output = model(x) loss = customMseLoss(output, target) loss.backward() If we wanted, we could have also written it as a class using nn.Module , and then we would have been able to use it as an object. Here is an NLLLoss custom example:
class CustomNLLLoss(nn.Module): def __init__(self): super().__init__() def forward(self, x, y): # x should be output from LogSoftmax Layer log_prob = -1.0 * x # Get log_prob based on y class_index as loss=-mean(ylogp) loss = log_prob.gather(1, y.unsqueeze(1)) loss = loss.mean() return loss criterion = CustomNLLLoss() loss = criterion(preds,y) Optimizers Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta , torch.optim.Adagrad , torch.optim.RMSprop and the most widely used torch.optim.Adam.
To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999)) And then use optimizer.zero_grad() and optimizer.step() while training the model.
I am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the pytorch-optimizer library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in PyTorch or pytorch-optimizers.
Other optimizers from pytorch-optimizer library
Using GPU/Multiple GPUs Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using model.to(&#39;cuda&#39;). Or if you want to use multiple GPUs, you can use nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using DataParallel if needed.
# Whether to train on a gpu train_on_gpu = torch.cuda.is_available() print(f&amp;#39;Train on gpu: {train_on_gpu}&amp;#39;)# Number of gpus if train_on_gpu: gpu_count = torch.cuda.device_count() print(f&amp;#39;{gpu_count} gpus detected.&amp;#39;) if gpu_count &amp;gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to(&amp;#39;cuda&amp;#39;) if multi_gpu: model = nn.DataParallel(model) The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It’s as simple as adding a few lines of code to our training loop.
num_epochs = 5 for epoch in range(num_epochs): model.train() for x_batch,y_batch in train_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() optimizer.zero_grad() pred = model(x_batch) loss = loss_criterion(pred, y_batch) loss.backward() optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() pred = model(x_batch) val_loss = loss_criterion(pred, y_batch) Conclusion Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.
You can find the code for this post here on my GitHub repo, where I keep codes for all my blogs.
If you want to learn more about Pytorch using a course based structure, take a look at the Deep Neural Networks with PyTorch course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Create an Awesome Development Setup for Data Science using Atom</title>
      <link>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/atom_for_data_science/main.png"></media:content>
      

      
      <description>Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of “Coding one code block at a time,” as I like to call it, and it helps me to think more clearly while helping me make my code more modular.
Yet, Jupyter is not suitable for some tasks in its present form.</description>

      <content:encoded>  
        
        <![CDATA[  Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of “Coding one code block at a time,” as I like to call it, and it helps me to think more clearly while helping me make my code more modular.
Yet, Jupyter is not suitable for some tasks in its present form. And the most prominent is when I have to work with .py files. And one will need to work with .py files whenever they want to push your code to production or change other people’s code. So, until now, I used sublime text to edit Python files, and I found it excellent. But recently, when I looked at the Atom editor, my loyalties seemed to shift when I saw the multiple out of the box options provided by it.
Now, the real power to Atom comes from the various packages you can install. In this post, I will talk about the packages that help make Atom just the most hackable and wholesome development environment ever.
Installing Atom and Some Starting Tweaks Before we even begin, we need to install Atom. You can do it from the main website here. The installation process is pretty simple, whatever your platform is. For Linux, I just downloaded the .deb file and double-clicked it. Once you have installed Atom, You can look at doing some tweaks:
 Open Core settings in Atom using Ctrl&#43;Shift&#43;P and typing settings therein. This Ctrl&#43;Shift&#43;P command is going to be one of the most important commands in Atom as it lets you navigate and run a lot of commands.  Accessing the Settings window using Ctrl&#43;Shift&#43;P
 Now go to the Editor menu and Uncheck “Soft Tabs”. This is done so that TAB key registers as a TAB and not two spaces. If you want you can also activate “Soft Wrap” which wraps the text if the text exceeds the window width.  My preferred settings for soft-wrap and soft-tabs.
Now, as we have Atom installed, we can look at some of the most awesome packages it provides. And the most important of them is GitHub.
1. Commit to Github without leaving Editor Are you fed up with leaving your text editor to use terminal every time you push a commit to Github? If your answer is yes, Atom solves this very problem by letting you push commits without you ever leaving the text editor window.
This is one of the main features that pushed me towards Atom from Sublime Text. I like how this functionality comes preloaded with Atom and it doesn’t take much time to set it up.
To start using it, click on the GitHub link in the right bottom of the Atom screen, and the Atom screen will prompt you to log in to your Github to provide access. It is a one-time setup, and once you log in and give the token generated to Atom, you will be able to push your commits from the Atom screen itself without navigating to the terminal window.
  ![]  ![]   The process to push a commit is:
 Change any file or multiple files.
 Click on Git on the bottom right corner.
 Stage the Changes
 Write a commit message.
 Click on Push in the bottom right corner.
 And we are done:)
  Below, I am pushing a very simple commit to Github, where I add a title to my Markdown file. Its a GIF file, so it might take some time to load.
Committing in Atom
2. Write Markdown with real-time preview I am always torn between the medium editor vs. Markdown whenever I write blog posts for my site. For one, I prefer using Markdown when I have to use Math symbols for my post or have to use custom HTML. But, I also like the Medium editor as it is WYSIWYG(What You See Is What You Get). And with Atom, I have finally found the perfect markdown editor for me, which provides me with Markdown as well as WYSIWYG. And it has now become a default option for me to create any README.md files for GitHub.
Using Markdown in Atom is again a piece of cake and is activated by default. To see a live preview with Markdown in Atom:
 Use Ctrl&#43;Shift&#43;M to open Markdown Preview Pane.
 Whatever changes you do in the document will reflect near real-time in the preview window.
  Markdown Split Screen editor
3. Minimap — A navigation map for Large code files Till now, we haven’t installed any new package to Atom, so let’s install an elementary package as our first package. This package is called minimap, and it is something that I like to have from my Sublime Text days. It lets you have a side panel where you can click and reach any part of the code. Pretty useful for large files.
To install a package, you can go to settings and click on Install Packages. Ctrl&#43;Shift&#43;P &amp;gt; Settings &amp;gt; &#43; Install &amp;gt; Minimap&amp;gt; Install
Installing Minimap or any package
Once you install the package, you can see the minimap on the side of your screen.
Sidebar to navigate large files with ease
4. Python Autocomplete with function definitions in Text Editor An editor is never really complete until it provides you with some autocomplete options for your favorite language. Atom integrates well with Kite, which tries to integrate AI and autocomplete.
So, to enable autocomplete with Kite, we can use the package named autocomplete-python in Atom. The install steps remain the same as before. i.e.
Ctrl&#43;Shift&#43;P &amp;gt; Settings &amp;gt; &#43; Install &amp;gt; autocomplete-python&amp;gt; Install
You will also see the option of using Kite along with it. I usually end up using Kite instead of Jedi(Another autocomplete option). This is how it looks when you work on a Python document with Kite autocompletion.
Autocomplete with Kite lets you see function definitions too.
5. Hydrogen — Run Python code in Jupyter environment Want to run Python also in your Atom Editor with any Jupyter Kernel? There is a way for that too. We just need to install “Hydrogen” using the same method as before. Once Hydrogen is installed you can use it by:
 Run the command on which your cursor is on using Ctrl&#43;Enter.
 Select any Kernel from the Kernel Selection Screen. I select pyt kernel from the list.
 Now I can continue working in pyt kernel.
  Runnin command using Ctrl&#43;Enter will ask you which environment to use.
Sometimes it might happen that you don’t see an environment/kernel in Atom. In such cases, you can install ipykernel to make that kernel visible to Jupyter as well as Atom.
Here is how to make a new kernel and make it visible in Jupyter/Atom:
conda create -n exampleenv python=3.7 conda activate exampleenv conda install -c anaconda ipykernel python -m ipykernel install --user --name=exampleenv  Once you run these commands, your kernel will be installed. You can now update the Atom’s kernel list by using:
Ctrl&#43;Shift&#43;P &amp;gt;Hydrogen: Update Kernels
And your kernel should now be available in your Atom editor.
6. Search Stack Overflow from your Text Editor Stack Overflow is an integral part of any developer’s life. But you know what the hassle is? To leave the coding environment and go to Chrome to search for every simple thing you need to do. And we end up doing it back and forth throughout the day. So, what if we can access Stack Overflow from Atom? You can do precisely that through the “ask-stack” package, which lets one search for questions on SO. We can access it using Ctrl&#43;Alt&#43;A
Access Stack Overflow in Atom using Ctrl&#43;Alt&#43;A.
Some other honorable mentions of packages you could use are:
 Teletype: Do Pair Coding.
 Linter: Checks code for Stylistic and Programmatic errors. To enable linting in Python, You can use “linter” and “python-linters”.
 Highlight Selected: Highlight all occurrences of a text by double-clicking or selecting the text with a cursor.
 Atom-File-Icons: Provides you with file icons in the left side tree view. Looks much better than before, right?
  Icons for files
Conclusion In this post, I talked about how I use Atom in my Python Development flow.
There are a plethora of other packages in Atom which you may like, and you can look at them to make your environment even more customizable. Or one can even write their own packages as well as Atom is called as the “Most Hackable Editor”.
If you want to learn about Python and not exactly a Python editor, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out. Also, here are my course recommendations to become a Data Scientist in 2020.
I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman’s Introduction to GANs for Data Scientists using PyTorch</title>
      <link>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pyt_gan/main.png"></media:content>
      

      
      <description>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects.</description>

      <content:encoded>  
        
        <![CDATA[  Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects. Using this approach, we could create realistic textures or characters on demand.
So in this post, we’re going to look at the generative adversarial networks behind AI-generated images, and help you to understand how to create and build your similar application with PyTorch. We’ll try to keep the post as intuitive as possible for those of you just starting out, but we’ll try not to dumb it down too much.
At the end of this article, you’ll have a solid understanding of how General Adversarial Networks (GANs) work, and how to build your own.
Task Overview In this post, we will create unique anime characters using the Anime Face Dataset. It is a dataset consisting of 63,632 high-quality anime faces in a number of styles. It’s a good starter dataset because it’s perfect for our goal.
We will be using Deep Convolutional Generative Adversarial Networks (DC-GANs) for our project. Though we’ll be using it to generate the faces of new anime characters, DC-GANs can also be used to create modern fashion styles, general content creation, and sometimes for data augmentation as well.
But before we get into the coding, let’s take a quick look at how GANs work.
INTUITION: Brief Intro to GANs for Generating Fake Images GANs typically employ two dueling neural networks to train a computer to learn the nature of a dataset well enough to generate convincing fakes. One of these Neural Networks generates fakes (the generator), and the other tries to classify which images are fake (the discriminator). These networks improve over time by competing against each other.
Perhaps imagine the generator as a robber and the discriminator as a police officer. The more the robber steals, the better he gets at stealing things. But at the same time, the police officer also gets better at catching the thief. Well, in an ideal world, anyway.
The losses in these neural networks are primarily a function of how the other network performs:
 Discriminator network loss is a function of generator network quality: Loss is high for the discriminator if it gets fooled by the generator’s fake images.
 Generator network loss is a function of discriminator network quality: Loss is high if the generator is not able to fool the discriminator.
  In the training phase, we train our discriminator and generator networks sequentially, intending to improve performance for both. The end goal is to end up with weights that help the generator to create realistic-looking images. In the end, we’ll use the generator neural network to generate high-quality fake images from random noise.
The Generator architecture One of the main problems we face when working with GANs is that the training is not very stable. So we have to come up with a generator architecture that solves our problem and also results in stable training. The diagram below is taken from the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, which explains the DC-GAN generator architecture.
Though it might look a little bit confusing, essentially you can think of a generator neural network as a black box which takes as input a 100 dimension normally generated vector of numbers and gives us an image:
So how do we create such an architecture? Below, we use a dense layer of size 4x4x1024 to create a dense vector out of the 100-d vector. We then reshape the dense vector in the shape of an image of 4×4 with 1024 filters, as shown in the following figure:
Note that we don’t have to worry about any weights right now as the network itself will learn those during training.
Once we have the 1024 4×4 maps, we do upsampling using a series of transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, however, we don’t halve the number of maps. We reduce the maps to 3 for each RGB channel since we need three channels for the output image.
Now, What are Transpose convolutions? Put simply, transposing convolutions provides us with a way to upsample images. In a convolution operation, we try to go from a 4×4 image to a 2×2 image. But when we transpose convolutions, we convolve from 2×2 to 4×4 as shown in the following figure:
Some of you may already know that unpooling is commonly used for upsampling input feature maps in convolutional neural networks (CNN). So why don’t we use unpooling here?
The reason comes down to the fact that unpooling does not involve any learning. However, transposed convolution is learnable, so it’s preferred. Later in the article, we’ll see how the parameters can be learned by the generator.
The Discriminator architecture Now that we’ve covered the generator architecture, let’s look at the discriminator as a black box. In practice, it contains a series of convolutional layers with a dense layer at the end to predict if an image is fake or not. You can see an example in the figure below:
Every image convolutional neural network works by taking an image as input, and predicting if it is real or fake using a sequence of convolutional layers.
Data preprocessing and visualization Before going any further with our training, we preprocess our images to a standard size of 64x64x3. We will also need to normalize the image pixels before we train our GAN. You can see the process in the code below, which I’ve commented on for clarity.
# Root directory for dataset dataroot = &amp;#34;anime_images/&amp;#34; # Number of workers for dataloader workers = 2 # Batch size during training batch_size = 128 # Spatial size of training images. All images will be resized to this size using a transformer. image_size = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # We can use an image folder dataset the way we have it setup. # Create the dataset dataset = datasets.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # Create the dataloader dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # Decide which device we want to run on device = torch.device(&amp;#34;cuda:0&amp;#34; if (torch.cuda.is_available() and ngpu &amp;gt; 0) else &amp;#34;cpu&amp;#34;) # Plot some training images real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(&amp;#34;off&amp;#34;) plt.title(&amp;#34;Training Images&amp;#34;) plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) The resultant output of the code is as follows:
So Many different Characters — Can our Generator understand the patterns?
Implementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.
Generating noise vector for Generator We need to generate the noise which we want to convert to an image using our generator architecture.
We use a normal distribution
to generate the noise vector:
nz = 100 noise = torch.randn(64, nz, 1, 1, device=device)  Generator architecture The generator is the most crucial part of the GAN. Here, we’ll create a generator by adding some transposed convolution layers to upsample the noise vector to an image. You’ll notice that this generator architecture is not the same as the one given in the DC-GAN paper I linked above.
In order to make it a better fit for our data, I had to make some architectural changes. I added a convolution layer in the middle and removed all dense layers from the generator architecture to make it fully convolutional.
I also used a lot of Batchnorm layers and leaky ReLU activation. The following code block is the function I will use to create the generator:
# Size of feature maps in generator ngf = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input noise) nz = 100 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is noise, going into a convolution # Transpose 2D conv layer 1. nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # Resulting state size - (ngf*8) x 4 x 4 i.e. if ngf= 64 the size is 512 maps of 4x4 # Transpose 2D conv layer 2. nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # Resulting state size -(ngf*4) x 8 x 8 i.e 8x8 maps # Transpose 2D conv layer 3. nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # Resulting state size. (ngf*2) x 16 x 16 # Transpose 2D conv layer 4. nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # Resulting state size. (ngf) x 32 x 32 # Final Transpose 2D conv layer 5 to generate final image. # nc is number of channels - 3 for 3 image channel nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), # Tanh activation to get final normalized image nn.Tanh() # Resulting state size. (nc) x 64 x 64 ) def forward(self, input): &amp;#39;&amp;#39;&amp;#39; This function takes as input the noise vector&amp;#39;&amp;#39;&amp;#39; return self.main(input) Now we can instantiate the model using the generator class. We are keeping the default weight initializer for PyTorch even though the paper says to initialize the weights using a mean of 0 and std dev of 0.2. The default weights initializer from Pytorch is more than good enough for our project.
# Create the generator netG = Generator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &amp;gt; 1): netG = nn.DataParallel(netG, list(range(ngpu))) # Print the model print(netG)  We can see the final generator model:
The Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.
# Number of channels in the training images. For color images this is 3 nc = 3 # Size of feature maps in discriminator ndf = 64 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) Now we can instantiate the discriminator exactly as we did the generator.
# Create the Discriminator netD = Discriminator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &amp;gt; 1): netD = nn.DataParallel(netD, list(range(ngpu))) # Print the model print(netD)  Here is the architecture of the discriminator:
Training Understanding how the training works in GAN is essential. It’s interesting, too; we can see how training the generator and discriminator together improves them both at the same time.
Now that we have our discriminator and generator models, next we need to initialize separate optimizers for them.
# Initialize BCELoss function criterion = nn.BCELoss() # Create batch of latent vectors that we will use to visualize # the progression of the generator fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Establish convention for real and fake labels during training real_label = 1. fake_label = 0. # Setup Adam optimizers for both G and D # Learning rate for optimizers lr = 0.0002 # Beta1 hyperparam for Adam optimizers beta1 = 0.5 optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) The Training Loop This is the main area where we need to understand how the blocks we’ve created will assemble and work together.
# Lists to keep track of progress/Losses img_list = [] G_losses = [] D_losses = [] iters = 0 # Number of training epochs num_epochs = 50 # Batch size during training batch_size = 128 print(&amp;#34;Starting Training Loop...&amp;#34;) # For each epoch for epoch in range(num_epochs): # For each batch in the dataloader for i, data in enumerate(dataloader, 0): ############################ # (1) Update D network: maximize log(D(x)) &#43; log(1 - D(G(z))) # Here we: # A. train the discriminator on real data # B. Create some fake images from Generator using Noise # C. train the discriminator on fake data ########################### # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images using generator # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D&amp;#39;s loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real &#43; errD_fake # Update D optimizerD.step() ############################ # (2) Update G network: maximize log(D(G(z))) # Here we: # A. Find the discriminator output on Fake images # B. Calculate Generators loss based on this output. Note that the label is 1 for generator. # C. Update Generator ########################### netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost # Since we just updated D, perform another forward pass of all-fake batch through D output = netD(fake).view(-1) # Calculate G&amp;#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() # Output training stats every 50th Iteration in an epoch if i % 1000 == 0: print(&amp;#39;[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f/ %.4f&amp;#39; % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # Save Losses for plotting later G_losses.append(errG.item()) D_losses.append(errD.item()) # Check how the generator is doing by saving G&amp;#39;s output on a fixed_noise vector if (iters % 250 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): #print(iters) with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters &#43;= 1 It may seem complicated, but I’ll break down the code above step by step in this section. The main steps in every training iteration are:
Step 1: Sample a batch of normalized images from the dataset
for i, data in enumerate(dataloader, 0):  Step 2: Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their labels.
 # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D&#39;s loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real &#43; errD_fake # Update D optimizerD.step()  Step 3: Backpropagate the errors through the generator by computing the loss gathered from discriminator output on fake images as the input and 1’s as the target while keeping the discriminator as untrainable — This ensures that the loss is higher when the generator is not able to fool the discriminator. You can check it yourself like so: if the discriminator gives 0 on the fake image, the loss will be high i.e., BCELoss(0,1).
 netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost output = netD(fake).view(-1) # Calculate G&#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step()  We repeat the steps using the for loop to end up with a good discriminator and generator.
Results The final output of our generator can be seen below. The GAN generates pretty good images for our content editor friends to work with.
The images might be a little crude, but still, this project was a starter for our GAN journey. The field is constantly advancing with better and more complex GAN architectures, so we’ll likely see further increases in image quality from these architectures. Also, keep in mind that these images are generated from a noise vector only: this means the input is some noise, and the output is an image. It’s quite incredible.
ALL THESE IMAGES ARE FAKE
1. Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average, and the variance is also decreasing as we do more steps. It’s possible that training for even more iterations would give us even better results.
plt.figure(figsize=(10,5)) plt.title(&amp;quot;Generator and Discriminator Loss During Training&amp;quot;) plt.plot(G_losses,label=&amp;quot;G&amp;quot;) plt.plot(D_losses,label=&amp;quot;D&amp;quot;) plt.xlabel(&amp;quot;iterations&amp;quot;) plt.ylabel(&amp;quot;Loss&amp;quot;) plt.legend() plt.show()  2. Image Animation at every 250th Iteration in Jupyter Notebook We can choose to see the output as an animation using the below code:
#%%capture fig = plt.figure(figsize=(8,8)) plt.axis(&amp;quot;off&amp;quot;) ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml())  You can choose to save an animation object as a gif as well if you want to send them to some friends.
ani.save(&#39;animation.gif&#39;, writer=&#39;imagemagick&#39;,fps=5) Image(url=&#39;animation.gif&#39;)  3. Image generated at every 200 Iter Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases, the images are getting better.
# create a list of 16 images to show every_nth_image = np.ceil(len(img_list)/16) ims = [np.transpose(img,(1,2,0)) for i,img in enumerate(img_list)if i%every_nth_image==0] print(&amp;quot;Displaying generated images&amp;quot;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(20,20)) gs1 = gridspec.GridSpec(4, 4) gs1.update(wspace=0, hspace=0) step = 0 for i,image in enumerate(ims): ax1 = plt.subplot(gs1[i]) ax1.set_aspect(&#39;equal&#39;) fig = plt.imshow(image) # you might need to change some params here fig = plt.text(7,30,&amp;quot;Step: &amp;quot;&#43;str(step),bbox=dict(facecolor=&#39;red&#39;, alpha=0.5),fontsize=12) plt.axis(&#39;off&#39;) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) step&#43;=int(250*every_nth_image) #plt.tight_layout() plt.savefig(&amp;quot;GENERATEDimage.png&amp;quot;,bbox_inches=&#39;tight&#39;,pad_inches=0) plt.show()  Given below is the result of the GAN at different time steps:
Conclusion In this post, we covered the basics of GANs for creating fairly believable fake images. We hope you now have an understanding of generator and discriminator architecture for DC-GANs, and how to build a simple DC-GAN to generate anime images from scratch.
Though this model is not the most perfect anime face generator, using it as a base helps us to understand the basics of generative adversarial networks, which in turn can be used as a stepping stone to more exciting and complex GANs as we move forward.
Look at it this way, as long as we have the training data at hand, we now have the ability to conjure up realistic textures or characters on demand. That is no small feat.
For a closer look at the code for this post, please visit my GitHub repository where you can find the code for this post as well as all my posts.
If you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew Ng. Andrew is a great instructor, and this course is excellent too.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
This post was first published here
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Creating my First Deep Learning &#43; Data Science Workstation</title>
      <link>https://mlwhiz.com/blog/2020/08/09/owndlrig/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/09/owndlrig/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/owndlrig/main.png"></media:content>
      

      
      <description>Creating my workstation has been a dream for me, if nothing else.
I knew the process involved, yet I somehow never got to it. It might have been time or money. Mostly Money.
But this time I just had to do it. I was just fed up with setting up a server on AWS for any small personal project and fiddling with all the installations. Or I had to work on Google Collab notebooks, which have a lot of limitations on running times and network connections.</description>

      <content:encoded>  
        
        <![CDATA[  Creating my workstation has been a dream for me, if nothing else.
I knew the process involved, yet I somehow never got to it. It might have been time or money. Mostly Money.
But this time I just had to do it. I was just fed up with setting up a server on AWS for any small personal project and fiddling with all the installations. Or I had to work on Google Collab notebooks, which have a lot of limitations on running times and network connections. So, I found out some time to create a Deep Learning Rig with some assistance from NVIDIA folks.
The whole process involved a lot of reading up and watching a lot of Youtube videos from Linus Tech Tips. And as it was the first time I was assembling a computer from scratch, it was sort of special too.
Building the DL rig as per your requirements takes up a lot of research. I researched on individual parts, their performance, reviews, and even the aesthetics.
Now, most of the workstation builds I researched were focussed on gaming, so I thought of putting down a Deep Learning Rig Spec as well.
I will try to put all the components I used along with the reasons why I went with those particular parts as well.
***Also, if you want to see how I set up the Deep Learning libraries after setting up the system to use Ubuntu 18.04, you can view ***this definitive guide for Setting up a Deep Learning Workstation.
So why the need for a workstation? The very first answer that comes to my mind is, why not?
I work a lot on deep learning and machine learning applications, and it always has been such a massive headache to churn up a new server and installing all the dependencies every time I start to work on a new project.
Also, it looks great, sits on your desk, is available all the time, and is open to significant customization as per your requirements.
Adding to this the financial aspects of using the GCP or AWS, and I was pretty much sold on the idea of building my rig.
My Build It took me a couple of weeks to come up with the final build.
I knew from the start that I want to have a lot of computing power and also something that would be upgradable in the coming years. Currently, my main priorities were to get a system that could support two NVIDIA RTX Titan cards with NVLink. That would allow me to have 48GB GPU memory at my disposal. Simply awesome.
PS:* The below build might not be the best build, and there may be cheaper alternatives present, but I know for sure that it is the build with the minimal future headache. So I went with it. I also contacted Nvidia to get a lot of suggestions about this particular build and only went forward after they approved of it.
1. Intel i9 9920x 3.5 GHz 12 core Processor Yes, I went with an Intel processor and not an AMD one. My reason for this (though people may differ with me on this) is because Intel has more compatible and related software like Intel’s MKL, which benefits most of the Python libraries I use.
Another and maybe a more important reason, at least for me, was that it was suggested by the people at NVIDIA to go for i9 if I wanted to have a dual RTX Titan configuration. Again zero headaches in the future.
So why this particular one from the Intel range?
I started with 9820X with its ten cores and 9980XE with 18 cores, but the latter stretched my budget a lot. I found that i9–9920X, with its 12 cores and 3.5 GHz processor, fit my budget just fine, and as it is always better to go for the mid-range solution, I went with it.
Now a CPU is the component that decides a lot of other components you are going to end up using.
For example, if you choose an i9 9900X range of CPU, you will have to select an X299 motherboard, or if you are going to use an AMD Threadripper CPU, you will need an X399 Motherboard. So be mindful of choosing the right CPU and motherboard.
2. MSI X299 SLI PLUS ATX LGA2066 Motherboard This was a particularly difficult choice. There are just too many options here. I wanted a Motherboard that could support at least 96GB RAM (again as per the specifications by the NVIDIA Folks for supporting 2 Titans). That meant that I had to have at least six slots if I were to use 16GB RAM Modules as 16x6=96. I got 8 in this one, so it is expandable till 128 GB RAM.
I also wanted to be able to have 2 TB NVMe SSD in my system(in the future), and that meant I needed 2 M.2 ports, which this board has. Or else I would have to go for a much expensive 2TB Single NVMe SSD.
I looked into a lot of options, and based on the ATX Form factor, 4 PCI-E x16 slots, and the reasonable pricing of the board, I ended up choosing this one.
3. Noctua NH-D15 chromax.BLACK 82.52 CFM CPU Cooler Liquid cooling is in rage right now. And initially, I also wanted to go for an AIO cooler, i.e., liquid cooling.
But after talking to a couple of people at NVIDIA as well as scrouging through the internet forums on the pro and cons of both options, I realized that Air cooling is better suited to my needs. So I went for the Noctua NH-D15, which is one of the best Air coolers in the market. So, I went with the best air cooling instead of a mediocre water cooling. And this cooler is SILENT. More on this later.
4. Phanteks Enthoo Pro Tempered Glass Case The next thing to think was a case that is going to be big enough to handle all these components and also be able to provide the required cooling. It was where I spent most of my time while researching.
I mean, we are going to keep 2 Titan RTX, 9920x CPU, 128 GB RAM. It’s going to be a hellish lot of heat in there.
Add to that the space requirements for the Noctua air cooler and the capability to add a lot of fans, and I was left with two options based on my poor aesthetic sense as well as the availability in my country. The options were — Corsair Air 540 ATX and the Phanteks Enthoo Pro Tempered Glass PH-ES614PTG_SWT.
Both of them are exceptional cases, but I went through with the Enthoo Pro as it is a more recently launched case and has a bigger form factor(Full Tower) offers options for more customizable build in the future too.
5. Dual Titan RTX with 3 Slot NVLink These 2 Titan RTX are by far the most important and expensive part of the whole build. These alone take up 80% of the cost, but aren’t they awesome?
I wanted to have a high-performance GPU in my build, and the good folks at NVIDIA were generous enough to send me two of these to test out.
I just love them. The design. The way they look in the build and the fact that they can be combined using a 3 Slot NVLink to provide 48 GB of GPU RAM effectively. Just awesome. If money is an issue, 2 x RTX 2080 Ti would also work fine as well. Only a problem will be that you might need smaller batch sizes training on RTX 2080 Ti, and in some cases, you might not be able to train large models as RTX2080Ti has 11GB RAM only. Also, you won’t be able to use NVLink, which combines the VRAM of multiple GPUs in Titans.
6. Samsung 970 Evo Plus 1 TB NVME Solid State Drive What about storage? NVMe SSD, of course, and the Samsung Evo Plus is the unanimous and most popular winner in this SSD race.
I bought 1 of them till now, but as I have 2 M.2 ports in my motherboard, I will get total storage of 2TB SSD in the future.
You can also get a couple of 2.5&amp;rdquo; SSD for more storage space.
7. Corsair Vengeance LPX 128GB (8x16GB) DDR4 3200 MHz I wanted to have a minimum of 96GB RAM, as suggested by the NVIDIA team. So I said what the heck and went with the full 128 GB RAM without cheaping out.
As you can see, these RAM sticks are not RGB lit, and that is a conscious decision as the Noctua Air Cooler doesn’t provide a lot of clearance for RAM Slots and the RGB ones had a slightly higher height. So keep that in mind. Also, I was never trying to go for an RGB Build anyway as I want to focus on those lit up Titans in my build.
8. Corsair 1200W Power Supply A 1200W power supply is a pretty big one, but that is needed realizing that the estimated wattage of our components at full wattage is going to be ~965W.
I had a couple of options for the power supply from other manufacturers also but went with this one because of Corsair’s name. I would have gone with HX1200i, but it was not available, and AX1200i was much more expensive than this one at my location. But both of them are excellent options apart from this one.
9. Even More Fans The Phanteks case comes up with three fans, but I was recommended to upgrade the intake, and exhaust fans of the case to BeQuiet BL071 PWM Fans as Dual Titans can put out a lot of heat. I have noticed that the temperature of my room is almost 2–3 degrees higher than the outside temperature, as I generally keep the machine on.
To get the best possible airflow, I bought 5 of these. I have put two at the top of the case along with a Phanteks case fan, 2 of them in the front, and one fan at the back of the case.
10. Peripherals The Essentials — A cup of tea and those speakers
This section is not necessary but wanted to put it in for completion.
Given all the power we have got, I didn’t want to cheap out on the peripherals. So I got myself an LG 27UK650 4k monitor for content creation, BenQ EX2780Q 1440p 144hz Gaming Monitor for a little bit of gaming, a Mechanical Cherry MX Red Corsair K68 Keyboard and a Corsair M65 Pro Mouse.
And my build is complete.
Pricing 💰💰💰 I will put the price as per the PCPartPicker site as I have gotten my components from different countries and sources. You can also check the part list at the PCPartPicker site: https://pcpartpicker.com/list/zLVjZf
As you can see, this is pretty expensive by any means (even after getting the GPUs from NVIDIA), but that is the price you pay for certain afflictions, I guess.
Finally In this post, I talked about all the parts you are going to need to assemble your deep learning rig and my reasons for getting these in particular.
You might try to look out for better components or a different design, but this one has been working pretty well for me for quite some time now, and is it fast.
If you want to see how I set up the Deep Learning libraries after setting up the system with these components, you can view this definitive guide for Setting up a Deep Learning Workstation with Ubuntu 18.04
Let me know what you think in the comments.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title>
      <link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dlrig/main.png"></media:content>
      

      
      <description>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.</description>

      <content:encoded>  
        
        <![CDATA[  Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.
Now, every time I create the whole deep learning setup from an installation viewpoint, I end up facing similar challenges. It’s like running around in circles with all these various dependencies and errors. This time also I had to try many things before the whole configuration came to life without errors.
So this time, I made it a point to document everything while installing all the requirements and their dependencies in my own system.
This post is about setting up your own Linux Ubuntu 18.04 system for deep learning with everything you might need.
If a pre-built deep learning system is preferred, I can recommend Exxact’s line of workstations and servers.
I assume that you have a fresh Ubuntu 18.04 installation. I am taking inspiration from Slav Ivanov’s excellent post in 2017 on creating a Deep Learning box. You can call it the 2020 version for the same post from a setup perspective, but a lot of the things have changed from then, and there are a lot of caveats with specific CUDA versions not supported by Tensorflow and Pytorch.
Starting up Before we do anything with our installation, we need to update our Linux system to the latest packages. We can do this simply by using:
sudo apt-get update sudo apt-get --assume-yes upgrade sudo apt-get --assume-yes install tmux build-essential gcc g&#43;&#43; make binutils sudo apt-get --assume-yes install software-properties-common sudo apt-get --assume-yes install git  The Process So now we have everything set up we want to install the following four things:
 GPU Drivers: Why is your PC not supporting high graphic resolutions? Or how would your graphics cards talk to your python interfaces?
 CUDA: A layer to provide access to the GPU’s instruction set and parallel computation units. In simple words, it allows us a way to write code for GPUs
 CuDNN: a library that provides Primitives for Deep Learning Network
 Pytorch, Tensorflow, and Rapids: higher-level APIs to code Deep Neural Networks
  1. GPU Drivers The first step is to add the latest NVIDIA drivers. You can choose the GPU product type, Linux 64 bit, and download Type as “Linux Long-Lived” for the 18.04 version.
Clicking on search will take you to a downloads page:
From where you can download the driver file NVIDIA-Linux-x86_64–440.44.run and run it using:
chmod &#43;x NVIDIA-Linux-x86_64–440.44.run sudo sh NVIDIA-Linux-x86_64–440.44.run  For you, the file may be named differently, depending on the latest version.
2. CUDA We will now need to install the CUDA toolkit. Somehow the CUDA toolkit 10.2 is still not supported by Pytorch and Tensorflow, so we will go with CUDA Toolkit 10.1, which is supported by both.
Also, the commands on the product page for CUDA 10.1 didn’t work for me and the commands I ended up using are:
sudo apt-key adv --fetch-keys [http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub](http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub) &amp;amp;&amp;amp; echo &amp;quot;deb [https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64) /&amp;quot; | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -o Dpkg::Options::=&amp;quot;--force-overwrite&amp;quot; install cuda-10-1 cuda-drivers  The next step is to create the LD_LIBRARY_PATH and append to the PATH variable the path where CUDA got installed. Just run this below command on your terminal.
echo &#39;export PATH=/usr/local/cuda-10.1/bin${PATH:&#43;:${PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:&#43;:${LD_LIBRARY_PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc &amp;amp;&amp;amp; sudo ldconfig  After this, one can check if CUDA is installed correctly by using:
nvcc --version  As you can see, the CUDA Version is 10.1 as we wanted. Also, check if you can use the command:
nvidia-smi  For me, it showed an error when I used it the first time, but a simple reboot solved the issue. And both my NVIDIA graphic cards show up in all their awesome glory. Don’t worry that the display says the CUDA version supported is 10.2. I was also confused, but it is just the maximum CUDA version supported by the graphics driver that is shown in nvidia-smi.
3.CuDNN What is the use of all these libraries if we are not going to train neural nets? CuDNN provides various primitives for Deep Learning, which are later used by PyTorch/TensorFlow.
But we first need to get a developer account first to install CuDNN. Once you fill-up the signup form, you will see the screen below. Select the cuDNN version that applies to your CUDA version. For me, the CUDA version is 10.1, so I select the second one.
Once you select the appropriate CuDNN version the screen expands:
For my use case, I needed to download three files for Ubuntu 18.04:
[cuDNN Runtime Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Developer Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-dev_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-doc_7.6.5.32-1%2Bcuda10.1_amd64.deb)  After downloading these files, you can install using these commands. You can also see the exact commands if anything changes in the future:
# Install the runtime library: sudo dpkg -i libcudnn7_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the developer library: sudo dpkg -i libcudnn7-dev_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the code samples and cuDNN User Guide(Optional): sudo dpkg -i libcudnn7-doc_7.6.5.32-1&#43;cuda10.1_amd64.deb  4. Anaconda, Pytorch, Tensorflow, and Rapids And finally, we reach the crux. We will install the software which we will interface with most of the times.
We need to install Python with virtual environments. I have downloaded python3 as it is the most stable version as of now, and it is time to say goodbye to Python 2.7. It was great while it lasted. And we will also install Pytorch and Tensorflow. I prefer them both for specific tasks as applicable.
You can go to the anaconda distribution page and download the package.
Once downloaded you can simply run the shell script:
sudo sh Anaconda3-2019.10-Linux-x86_64.sh  You will also need to run these commands on your shell to add some commands to your ~/.bashrc file, and update the conda distribution with the latest libraries versions.
cat &amp;gt;&amp;gt; ~/.bashrc &amp;lt;&amp;lt; &#39;EOF&#39; export PATH=$HOME/anaconda3/bin:${PATH} EOF source .bashrc conda upgrade -y --all  The next step is creating a new environment for your deep learning pursuits or using an existing one. I created a new Conda environment using:
conda create --name py37  Here py37 is the name we provide to this new conda environment. You can activate this conda environment using:
conda activate py37  You should now be able to see something like:
Notice the py37 at the start of command in terminal
We can now add all our required packages to this environment using pip or conda. The latest version 1.3, as seen from the pytorch site, is not yet available for CUDA 10.2, as I already mentioned, so we are in luck with CUDA 10.1. Also, we will need to specify the version of TensorFlow as 2.1.0, as this version was built using 10.1 CUDA.
I also install RAPIDS, which is a library to get your various data science workloads to GPUs. Why use GPUs only for deep learning and not for Data processing? You can get the command to install rapids from the rapids release selector:
sudo apt install python3-pip conda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.11 python=3.7 cudatoolkit=10.1 pip install torchvision  Since PyTorch installation interfered with TensorFlow, I installed TensorFlow in another environment.
conda create --name tf conda activate tf pip install --upgrade tensorflow  Now we can check if the TF and Pytorch installations are correctly done by using the below commands in their own environments:
# Should print True python3 -c &amp;quot;import tensorflow as tf; print(tf.test.is_gpu_available())&amp;quot; # should print cuda python3 -c &amp;quot;import torch; print(torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;))&amp;quot;  If the install is showing some errors for TensorFlow or the GPU test is failing, you might want to add these two additional lines at the end of your bashrc file and restart the terminal:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 export CUDA_HOME=/usr/local/cuda  You might also want to install jupyter lab or jupyter notebook. Thanks to the developers, the process is as easy as just running jupyter labor jupyter notebook in your terminal, whichever you do prefer. I personally like notebook better without all the unnecessary clutter.
Conclusion In this post, I talked about all the software you are going to need to install in your deep learning rig without hassle.
You might still need some help and face some problems for which my best advice would be to check out the different NVIDIA and Stack Overflow forums.
So we have got our deep learning rig setup, and its time for some tests now. In the next few posts, I am going to do some benchmarking on the GPUs and will try to write more on various deep Learning libraries one can include in their workflow. So stay tuned.
Continue Learning If you want to learn more about Deep Learning, here is an excellent course. You can start for free with the 7-day Free Trial.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>End to End Pipeline for setting up Multiclass Image Classification for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/multiclass_image_classification_pytorch/main.png"></media:content>
      

      
      <description>Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?
All of the above are examples of image classification in different settings. Multiclass image classification is a common task in computer vision, where we categorize an image into three or more classes.
In the past, I always used Keras for computer vision projects.</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?
All of the above are examples of image classification in different settings. Multiclass image classification is a common task in computer vision, where we categorize an image into three or more classes.
In the past, I always used Keras for computer vision projects. However, recently when the opportunity to work on multiclass image classification presented itself, I decided to use PyTorch. I have already moved from Keras to PyTorch for all NLP tasks, so why not vision, too?
 PyTorch is powerful, and I also like its more pythonic structure.
 In this post, we’ll create an end to end pipeline for image multiclass classification using Pytorch.This will include training the model, putting the model’s results in a form that can be shown to business partners, and functions to help deploy the model easily. As an added feature we will look at Test Time Augmentation using Pytorch also.
But before we learn how to do image classification, let’s first look at transfer learning, the most common method for dealing with such problems.
What is Transfer Learning? Transfer learning is the process of repurposing knowledge from one task to another. From a modelling perspective, this means using a model trained on one dataset and fine-tuning it for use with another. But why does it work?
Let’s start with some background. Every year the visual recognition community comes together for a very particular challenge: The Imagenet Challenge. The task in this challenge is to classify 1,000,000 images into 1,000 categories.
This challenge has already resulted in researchers training big convolutional deep learning models. The results have included great models like Resnet50 and Inception.
But, what does it mean to train a neural model? Essentially, it means the researchers have learned the weights for a neural network after training the model on a million images.
So, what if we could get those weights? We could then use them and load them into our own neural networks model to predict on the test dataset, right? Actually, we can go even further than that; we can add an extra layer on top of the neural network these researchers have prepared to classify our own dataset.
 While the exact workings of these complex models is still a mystery, we do know that the lower convolutional layers capture low-level image features like edges and gradients. In comparison, higher convolutional layers capture more and more intricate details, such as body parts, faces, and other compositional features.
 Source: Visualizing and Understanding Convolutional Networks. You can see how the first few layers capture basic shapes, and the shapes become more and more complex in the later layers.
In the example above from ZFNet (a variant of Alexnet), one of the first convolutional neural networks to achieve success on the Imagenet task, you can see how the lower layers capture lines and edges, and the later layers capture more complex features. The final fully-connected layers are generally assumed to capture information that is relevant for solving the respective task, e.g. ZFNet’s fully-connected layers indicate which features are relevant for classifying an image into one of 1,000 object categories.
For a new vision task, it is possible for us to simply use the off-the-shelf features of a state-of-the-art CNN pre-trained on ImageNet, and train a new model on these extracted features.
The intuition behind this idea is that a model trained to recognize animals might also be used to recognize cats vs dogs. In our case, &amp;gt; # a model that has been trained on 1000 different categories has seen a lot of real-world information, and we can use this information to create our own custom classifier.
So that’s the theory and intuition. How do we get it to actually work? Let’s look at some code. You can find the complete code for this post on Github.
Data Exploration We will start with the Boat Dataset from Kaggle to understand the multiclass image classification problem. This dataset contains about 1,500 pictures of boats of different types: buoys, cruise ships, ferry boats, freight boats, gondolas, inflatable boats, kayaks, paper boats, and sailboats. Our goal is to create a model that looks at a boat image and classifies it into the correct category.
Here’s a sample of images from the dataset:
And here are the category counts:
Since the categories “freight boats”, “inflatable boats” , and “boats” don’t have a lot of images; we will be removing these categories when we train our model.
Creating the required Directory Structure Before we can go through with training our deep learning models, we need to create the required directory structure for our images. Right now, our data directory structure looks like:
images sailboat kayak . .  We need our images to be contained in 3 folders train, val and test. We will then train on the images in train dataset, validate on the ones in the val dataset and finally test them on images in the test dataset.
data train sailboat kayak . . val sailboat kayak . . test sailboat kayak . .  You might have your data in a different format, but I have found that apart from the usual libraries, the glob.glob and os.system functions are very helpful. Here you can find the complete data preparation code. Now let’s take a quick look at some of the not-so-used libraries that I found useful while doing data prep.
What is glob.glob? Simply, glob lets you get names of files or folders in a directory using a regex. For example, you can do something like:
from glob import glob categories = glob(“images/*”) print(categories) ------------------------------------------------------------------ [&#39;images/kayak&#39;, &#39;images/boats&#39;, &#39;images/gondola&#39;, &#39;images/sailboat&#39;, &#39;images/inflatable boat&#39;, &#39;images/paper boat&#39;, &#39;images/buoy&#39;, &#39;images/cruise ship&#39;, &#39;images/freight boat&#39;, &#39;images/ferry boat&#39;]  What is os.system? os.system is a function in os library which lets you run any command-line function in python itself. I generally use it to run Linux functions, but it can also be used to run R scripts within python as shown here. For example, I use it in my data preparation to copy files from one directory to another after getting the information from a pandas data frame. I also use f string formatting.
import os for i,row in fulldf.iterrows(): # Boat category cat = row[&#39;category&#39;] # section is train,val or test section = row[&#39;type&#39;] # input filepath to copy ipath = row[&#39;filepath&#39;] # output filepath to paste opath = ipath.replace(f&amp;quot;images/&amp;quot;,f&amp;quot;data/{section}/&amp;quot;) # running the cp command os.system(f&amp;quot;cp &#39;{ipath}&#39; &#39;{opath}&#39;&amp;quot;)  Now since we have our data in the required folder structure, we can move on to more exciting parts.
Data Preprocessing Transforms: 1. Imagenet Preprocessing
In order to use our images with a network trained on the Imagenet dataset, we need to preprocess our images in the same way as the Imagenet network. For that, we need to rescale the images to 224×224 and normalize them as per Imagenet standards. We can use the torchvision transforms library to do that. Here we take a CenterCrop of 224×224 and normalize as per Imagenet standards. The operations defined below happen sequentially. You can find a list of all transforms provided by PyTorch here.
transforms.Compose([ transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])  2. Data Augmentations
We can do a lot more preprocessing for data augmentations. Neural networks work better with a lot of data. Data augmentation is a strategy which we use at training time to increase the amount of data we have.
For example, we can flip the image of a boat horizontally, and it will still be a boat. Or we can randomly crop images or add color jitters. Here is the image transforms dictionary I have used that applies to both the Imagenet preprocessing as well as augmentations. This dictionary contains the various transforms we have for the train, test and validation data as used in this great post. As you’d expect, we don’t apply the horizontal flips or other data augmentation transforms to the test data and validation data because we don’t want to get predictions on an augmented image.
# Image transformations image_transforms = { # Train uses data augmentation &#39;train&#39;: transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]), # Validation does not use augmentation &#39;valid&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), # Test does not use augmentation &#39;test&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), }  Here is an example of the train transforms applied to an image in the training dataset. Not only do we get a lot of different images from a single image, but it also helps our network become invariant to the object orientation.
ex_img = Image.open(&#39;/home/rahul/projects/compvisblog/data/train/cruise ship/cruise-ship-oasis-of-the-seas-boat-water-482183.jpg&#39;) t = image_transforms[&#39;train&#39;] plt.figure(figsize=(24, 24)) for i in range(16): ax = plt.subplot(4, 4, i &#43; 1) _ = imshow_tensor(t(ex_img), ax=ax) plt.tight_layout()  DataLoaders The next step is to provide the training, validation, and test dataset locations to PyTorch. We can do this by using the PyTorch datasets and DataLoader class. This part of the code will mostly remain the same if we have our data in the required directory structures.
# Datasets from folders traindir = &amp;quot;data/train&amp;quot; validdir = &amp;quot;data/val&amp;quot; testdir = &amp;quot;data/test&amp;quot; data = { &#39;train&#39;: datasets.ImageFolder(root=traindir, transform=image_transforms[&#39;train&#39;]), &#39;valid&#39;: datasets.ImageFolder(root=validdir, transform=image_transforms[&#39;valid&#39;]), &#39;test&#39;: datasets.ImageFolder(root=testdir, transform=image_transforms[&#39;test&#39;]) } # Dataloader iterators, make sure to shuffle dataloaders = { &#39;train&#39;: DataLoader(data[&#39;train&#39;], batch_size=batch_size, shuffle=True,num_workers=10), &#39;val&#39;: DataLoader(data[&#39;valid&#39;], batch_size=batch_size, shuffle=True,num_workers=10), &#39;test&#39;: DataLoader(data[&#39;test&#39;], batch_size=batch_size, shuffle=True,num_workers=10) }  These dataloaders help us to iterate through the dataset. For example, we will use the dataloader below in our model training. The data variable will contain data in the form (batch_size, color_channels, height, width) while the target is of shape (batch_size) and hold the label information.
train_loader = dataloaders[&#39;train&#39;] for ii, (data, target) in enumerate(train_loader):  Modeling 1. Create the model using a pre-trained model Right now these following pre-trained models are available to use in the torchvision library:
 AlexNet
 VGG
 ResNet
 SqueezeNet
 DenseNet
 Inception v3
 GoogLeNet
 ShuffleNet v2
 MobileNet v2
 ResNeXt
 Wide ResNet
 MNASNet
  Here I will be using resnet50 on our dataset, but you can effectively use any other model too as per your choice.
from torchvision import models model = models.resnet50(pretrained=True)  We start by freezing our model weights since we don’t want to change the weights for the renet50 models.
# Freeze model weights for param in model.parameters(): param.requires_grad = False  The next thing we need to do is to replace the linear classification layer in the model by our custom classifier. I have found that to do this, it is better first to see the model structure to determine what is the final linear layer. We can do this simply by printing the model object:
print(model) ------------------------------------------------------------------ ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) . . . . (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) **(fc): Linear(in_features=2048, out_features=1000, bias=True)** )  Here we find that the final linear layer that takes the input from the convolutional layers is named fc
We can now simply replace the fc layer using our custom neural network. This neural network takes input from the previous layer to fc and gives the log softmax output of shape (batch_size x n_classes).
n_inputs = model.fc.in_features model.fc = nn.Sequential( nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))  Please note that the new layers added now are fully trainable by default.
2. Load the model on GPU We can use a single GPU or multiple GPU(if we have them) using DataParallel from PyTorch. Here is what we can use to detect the GPU as well as the number of GPUs to load the model on GPU. Right now I am training my models on dual NVIDIA Titan RTX GPUs.
# Whether to train on a gpu train_on_gpu = cuda.is_available() print(f&#39;Train on gpu: {train_on_gpu}&#39;) # Number of gpus if train_on_gpu: gpu_count = cuda.device_count() print(f&#39;{gpu_count} gpus detected.&#39;) if gpu_count &amp;gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to(&#39;cuda&#39;) if multi_gpu: model = nn.DataParallel(model)  3. Define criterion and optimizers One of the most important things to notice when you are training any model is the choice of loss-function and the optimizer used. Here we want to use categorical cross-entropy as we have got a multiclass classification problem and the Adam optimizer, which is the most commonly used optimizer. But since we are applying a LogSoftmax operation on the output of our model, we will be using the NLL loss.
from torch import optim criteration = nn.NLLLoss() optimizer = optim.Adam(model.parameters())  4. Training the model Given below is the full code used to train the model. It might look pretty big on its own, but essentially what we are doing is as follows:
 Start running epochs. In each epoch-
 Set the model mode to train using model.train().
 Loop through the data using the train dataloader.
 Load your data to the GPU using the data, target = data.cuda(), target.cuda() command
 Set the existing gradients in the optimizer to zero using optimizer.zero_grad()
 Run the forward pass through the batch using output = model(data)
 Compute loss using loss = criterion(output, target)
 Backpropagate the losses through the network using loss.backward()
 Take an optimizer step to change the weights in the whole network using optimizer.step()
 All the other steps in the training loop are just to maintain the history and calculate accuracy.
 Set the model mode to eval using model.eval().
 Get predictions for the validation data using valid_loader and calculate valid_loss and valid_acc
 Print the validation loss and validation accuracy results every print_every epoch.
 Save the best model based on validation loss.
 Early Stopping: If the cross-validation loss doesn’t improve for max_epochs_stop stop the training and load the best available model with the minimum validation loss.
  def train(model, criterion, optimizer, train_loader, valid_loader, save_file_name, max_epochs_stop=3, n_epochs=20, print_every=1): &amp;#34;&amp;#34;&amp;#34;Train a PyTorch Model Params -------- model (PyTorch model): cnn to train criterion (PyTorch loss): objective to minimize optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters train_loader (PyTorch dataloader): training dataloader to iterate through valid_loader (PyTorch dataloader): validation dataloader used for early stopping save_file_name (str ending in &amp;#39;.pt&amp;#39;): file path to save the model state dict max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping n_epochs (int): maximum number of training epochs print_every (int): frequency of epochs to print training stats Returns -------- model (PyTorch model): trained cnn with best weights history (DataFrame): history of train and validation loss and accuracy &amp;#34;&amp;#34;&amp;#34; # Early stopping intialization epochs_no_improve = 0 valid_loss_min = np.Inf valid_max_acc = 0 history = [] # Number of epochs already trained (if using loaded in model weights) try: print(f&amp;#39;Model has been trained for: {model.epochs} epochs.\n&amp;#39;) except: model.epochs = 0 print(f&amp;#39;Starting Training from Scratch.\n&amp;#39;) overall_start = timer() # Main loop for epoch in range(n_epochs): # keep track of training and validation loss each epoch train_loss = 0.0 valid_loss = 0.0 train_acc = 0 valid_acc = 0 # Set to training model.train() start = timer() # Training loop for ii, (data, target) in enumerate(train_loader): # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Clear gradients optimizer.zero_grad() # Predicted outputs are log probabilities output = model(data) # Loss and backpropagation of gradients loss = criterion(output, target) loss.backward() # Update the parameters optimizer.step() # Track train loss by multiplying average loss by number of examples in batch train_loss &#43;= loss.item() * data.size(0) # Calculate accuracy by finding max log probability _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) # Need to convert correct tensor from int to float to average accuracy = torch.mean(correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples in batch train_acc &#43;= accuracy.item() * data.size(0) # Track training progress print( f&amp;#39;Epoch: {epoch}\t{100 * (ii &#43; 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.&amp;#39;, end=&amp;#39;\r&amp;#39;) # After training loops ends, start validation else: model.epochs &#43;= 1 # Don&amp;#39;t need to keep track of gradients with torch.no_grad(): # Set to evaluation mode model.eval() # Validation loop for data, target in valid_loader: # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Forward pass output = model(data) # Validation loss loss = criterion(output, target) # Multiply average loss times the number of examples in batch valid_loss &#43;= loss.item() * data.size(0) # Calculate validation accuracy _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) accuracy = torch.mean( correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples valid_acc &#43;= accuracy.item() * data.size(0) # Calculate average losses train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) # Calculate average accuracy train_acc = train_acc / len(train_loader.dataset) valid_acc = valid_acc / len(valid_loader.dataset) history.append([train_loss, valid_loss, train_acc, valid_acc]) # Print training and validation results if (epoch &#43; 1) % print_every == 0: print( f&amp;#39;\nEpoch: {epoch} \tTraining Loss: {train_loss:.4f} \tValidation Loss: {valid_loss:.4f}&amp;#39; ) print( f&amp;#39;\t\tTraining Accuracy: {100 * train_acc:.2f}%\tValidation Accuracy: {100 * valid_acc:.2f}%&amp;#39; ) # Save the model if validation loss decreases if valid_loss &amp;lt; valid_loss_min: # Save model torch.save(model.state_dict(), save_file_name) # Track improvement epochs_no_improve = 0 valid_loss_min = valid_loss valid_best_acc = valid_acc best_epoch = epoch # Otherwise increment count of epochs with no improvement else: epochs_no_improve &#43;= 1 # Trigger early stopping if epochs_no_improve &amp;gt;= max_epochs_stop: print( f&amp;#39;\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%&amp;#39; ) total_time = timer() - overall_start print( f&amp;#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch&#43;1):.2f} seconds per epoch.&amp;#39; ) # Load the best state dict model.load_state_dict(torch.load(save_file_name)) # Attach the optimizer model.optimizer = optimizer # Format history history = pd.DataFrame( history, columns=[ &amp;#39;train_loss&amp;#39;, &amp;#39;valid_loss&amp;#39;, &amp;#39;train_acc&amp;#39;, &amp;#39;valid_acc&amp;#39; ]) return model, history # Attach the optimizer model.optimizer = optimizer # Record overall time and print out stats total_time = timer() - overall_start print( f&amp;#39;\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%&amp;#39; ) print( f&amp;#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.&amp;#39; ) # Format history history = pd.DataFrame( history, columns=[&amp;#39;train_loss&amp;#39;, &amp;#39;valid_loss&amp;#39;, &amp;#39;train_acc&amp;#39;, &amp;#39;valid_acc&amp;#39;]) return model, history # Running the model model, history = train( model, criterion, optimizer, dataloaders[&amp;#39;train&amp;#39;], dataloaders[&amp;#39;val&amp;#39;], save_file_name=save_file_name, max_epochs_stop=3, n_epochs=100, print_every=1) Here is the output from running the above code. Just showing the last few epochs. The validation accuracy started at ~55% in the first epoch, and we ended up with a validation accuracy of ~90%.
And here are the training curves showing the loss and accuracy metrics:
Inference and Model Results We want our results in different ways to use our model. For one, we require test accuracies and confusion matrices. All of the code for creating these results is in the code notebook.
1. Test Results The overall accuracy of the test model is:
Overall Accuracy: 88.65 %  Here is the confusion matrix for results on the test dataset.
We can also look at the category wise accuracies. I have also added the train counts to see the results from a new perspective.
2. Visualizing Predictions for Single Image For deployment purposes, it helps to be able to get predictions for a single image. You can get the code from the notebook.
3. Visualizing Predictions for a Category We can also see the category wise results for debugging purposes and presentations.
4. Test results with Test Time Augmentation We can also do test time augmentation to increase our test accuracy. Here I am using a new test data loader and transforms:
# Image transformations tta_random_image_transforms = transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]) # Datasets from folders ttadata = { &#39;test&#39;: datasets.ImageFolder(root=testdir, transform=tta_random_image_transforms) } # Dataloader iterators ttadataloader = { &#39;test&#39;: DataLoader(ttadata[&#39;test&#39;], batch_size=512, shuffle=False,num_workers=10) }  We can then get the predictions on the test set using the below function:
def tta_preds_n_averaged(model, test_loader,n=5): &amp;#34;&amp;#34;&amp;#34;Returns the TTA preds from a trained PyTorch model Params -------- model (PyTorch model): trained cnn for inference test_loader (PyTorch DataLoader): test dataloader Returns -------- results (array): results for each category &amp;#34;&amp;#34;&amp;#34; # Hold results results = np.zeros((len(test_loader.dataset), n_classes)) bs = test_loader.batch_size model.eval() with torch.no_grad(): #aug loop: for _ in range(n): # Testing loop tmp_pred = np.zeros((len(test_loader.dataset), n_classes)) for i,(data, targets) in enumerate(tqdm.tqdm(test_loader)): # Tensors to gpu if train_on_gpu: data, targets = data.to(&amp;#39;cuda&amp;#39;), targets.to(&amp;#39;cuda&amp;#39;) # Raw model output out = model(data) tmp_pred[i*bs:(i&#43;1)*bs] = np.array(out.cpu()) results&#43;=tmp_pred return results/n In the function above, I am applying the tta_random_image_transforms to each image 5 times before getting its prediction. The final prediction is the average of all five predictions. When we use TTA over the whole test dataset, we noticed that the accuracy increased by around 1%
TTA Accuracy: 89.71%  Also, here is the results for TTA compared to normal results category wise:
In this small dataset, the TTA might not seem to add much value, but I have noticed that it adds value with big datasets.
Conclusion In this post, I talked about the end to end pipeline for working on a multiclass image classification project using PyTorch. We worked on creating some readymade code to train a model using transfer learning, visualize the results, use Test time augmentation, and got predictions for a single image so that we can deploy our model when needed using any tool like Streamlit.
You can find the complete code for this post on Github.
If you would like to learn more about Image Classification and Convolutional Neural Networks take a look at the Deep Learning Specialization from Andrew Ng. Also, to learn more about PyTorch and start from the basics, you can take a look at the Deep Neural Networks with PyTorch course offered by IBM.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
This post was first published here.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Most Complete Guide to pySpark DataFrames</title>
      <link>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/spark_df_complete_guide/main.png"></media:content>
      

      
      <description>Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.
Reason: Too much data is getting generated day by day
And that brings us to Spark which is one of the most used tools when it comes to working with Big Data.</description>

      <content:encoded>  
        
        <![CDATA[  Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.
Reason: Too much data is getting generated day by day
And that brings us to Spark which is one of the most used tools when it comes to working with Big Data.
While once upon a time Spark used to be heavily reliant on RDD manipulations, Spark has now provided a DataFrame API for us Data Scientists to work with. Here is the documentation for the adventurous folks. But while the documentation is good, it does not explain it from the perspective of a Data Scientist. Neither does it properly document the most common use cases for Data Science.
In this post, I will talk about installing Spark, standard Spark functionalities you will need to work with DataFrames, and finally some tips to handle the inevitable errors you will face.
This post is going to be quite long. Actually one of my longest posts on medium, so go on and pick up a Coffee.
Also here is the Table of Contents, if you want to skip to a specific section:
 Installation Data 1. Basic Functions  Read See a few rows in the file Change Column Names Select Columns Sort Cast Filter GroupBy Joins  2. Broadcast/Map Side Joins 3. Use SQL with DataFrames 4. Create New Columns  Using Spark Native Functions Using Spark UDFs Using RDDs Using Pandas UDF  5. Spark Window Functions  Ranking Lag Variables Rolling Aggregations  6. Pivot Dataframes 7. Unpivot/Stack Dataframes 8. Salting Some More Tips and Tricks  Caching Save and Load from an intermediate step Repartitioning Reading Parquet File in Local  Conclusion  Installation I am working on installing Spark on Ubuntu 18.04, but the steps should remain the same for MAC too. I am assuming that you already have Anaconda and Python3 installed. After that, you can just go through these steps:
 Download the Spark Binary from Apache Spark Website. And click on the Download Spark link to download Spark.  Once you have downloaded the above file, you can start with unzipping the file in your home directory. Just Open up the terminal and put these commands in.
cd ~ cp Downloads/spark-2.4.5-bin-hadoop2.7.tgz ~ tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz  Check your Java Version. As of version 2.4 Spark works with Java 8. You can check your Java Version using the command java -version on the terminal window.
I had Java 11 in my machine, so I had to run the following commands on my terminal to install and change default Java to Java 8:
sudo apt install openjdk-8-jdk sudo update-alternatives --config java  You will need to manually select the Java version 8 by typing the selection number.
Rechecking Java version should give something like:
Edit your ~/.bashrc file and add the following lines at the end of the file:
function pysparknb () { #Spark path SPARK_PATH=~/spark-2.4.5-bin-hadoop2.7 export PYSPARK_DRIVER_PYTHON=&amp;quot;jupyter&amp;quot; export PYSPARK_DRIVER_PYTHON_OPTS=&amp;quot;notebook&amp;quot; # For pyarrow 0.15 users, you have to add the line below or you will get an error while using pandas_udf export ARROW_PRE_0_15_IPC_FORMAT=1 **# Change the local[10] to local[numCores in your machine]** $SPARK_PATH/bin/pyspark --master **local[10]** }  Source ~/.bashrc
source ~/.bashrc  Run the pysparknb function in the terminal and you will be able to access the notebook. You will be able to open a new notebook as well as the sparkcontext will be loaded automatically.
pysparknb  Data With the installation out of the way, we can move to the more interesting part of this post. I will be working with the Data Science for COVID-19 in South Korea, which is one of the most detailed datasets on the internet for COVID.
Please note that I will be using this dataset to showcase some of the most useful functionalities of Spark, but this should not be in any way considered a data exploration exercise for this amazing dataset.
I will mainly work with the following three tables only in this post:
 Cases
 Region
 TimeProvince
  You can find all the code at the GitHub repository.
1. Basic Functions Read We can start by loading the files in our dataset using the spark.read.load command. This command reads parquet files, which is the default file format for spark, but you can add the parameter format to read .csv files using it.
cases = spark.read.load(&amp;quot;/home/rahul/projects/sparkdf/coronavirusdataset/Case.csv&amp;quot;,format=&amp;quot;csv&amp;quot;, sep=&amp;quot;,&amp;quot;, inferSchema=&amp;quot;true&amp;quot;, header=&amp;quot;true&amp;quot;)  See a few rows in the file cases.show()  This file contains the cases grouped by way of the infection spread. This might have helped in the rigorous tracking of Corona Cases in South Korea.
The way this file looks is great right now, but sometimes as we increase the number of columns, the formatting becomes not too great. I have noticed that the following trick helps in displaying in pandas format in my Jupyter Notebook. The .toPandas() function converts a spark dataframe into a pandas Dataframe which is easier to show.
cases.limit(10).toPandas()  Change Column Names Sometimes we would like to change the name of columns in our Spark Dataframes. We can do this simply using the below command to change a single column:
cases = cases.withColumnRenamed(&amp;quot;infection_case&amp;quot;,&amp;quot;infection_source&amp;quot;)  Or for all columns:
cases = cases.toDF(*[&#39;case_id&#39;, &#39;province&#39;, &#39;city&#39;, &#39;group&#39;, &#39;infection_case&#39;, &#39;confirmed&#39;, &#39;latitude&#39;, &#39;longitude&#39;])  Select Columns We can select a subset of columns using the select keyword.
cases = cases.select(&#39;province&#39;,&#39;city&#39;,&#39;infection_case&#39;,&#39;confirmed&#39;) cases.show()  Sort We can sort by the number of confirmed cases. Here note that the cases data frame will not change after performing this command as we don’t assign it to any variable.
cases.sort(&amp;quot;confirmed&amp;quot;).show()  But that is inverted. We want to see the most cases at the top. We can do this using the F.desc function:
# descending Sort from pyspark.sql import functions as F cases.sort(F.desc(&amp;quot;confirmed&amp;quot;)).show()  We can see the most cases in a logical area in South Korea originated from Shincheonji Church.
Cast Though we don’t face it in this dataset, there might be scenarios where Pyspark reads a double as integer or string, In such cases, you can use the cast function to convert types.
from pyspark.sql.types import DoubleType, IntegerType, StringType cases = cases.withColumn(&#39;confirmed&#39;, F.col(&#39;confirmed&#39;).cast(IntegerType())) cases = cases.withColumn(&#39;city&#39;, F.col(&#39;city&#39;).cast(StringType()))  Filter We can filter a data frame using multiple conditions using AND(&amp;amp;), OR(|) and NOT(~) conditions. For example, we may want to find out all the different infection_case in Daegu Province with more than 10 confirmed cases.
cases.filter((cases.confirmed&amp;gt;10) &amp;amp; (cases.province==&#39;Daegu&#39;)).show()  GroupBy We can use groupBy function with a spark DataFrame too. Pretty much same as the pandas groupBy with the exception that you will need to import pyspark.sql.functions. Here is the list of functions you can use with this function module.
from pyspark.sql import functions as F cases.groupBy([&amp;quot;province&amp;quot;,&amp;quot;city&amp;quot;]).agg(F.sum(&amp;quot;confirmed&amp;quot;) ,F.max(&amp;quot;confirmed&amp;quot;)).show()  If you don’t like the new column names, you can use the alias keyword to rename columns in the agg command itself.
cases.groupBy([&amp;quot;province&amp;quot;,&amp;quot;city&amp;quot;]).agg( F.sum(&amp;quot;confirmed&amp;quot;).alias(&amp;quot;TotalConfirmed&amp;quot;),\ F.max(&amp;quot;confirmed&amp;quot;).alias(&amp;quot;MaxFromOneConfirmedCase&amp;quot;)\ ).show()  Joins To Start with Joins we will need to introduce one more CSV file. We will go with the region file which contains region information such as elementary_school_count, elderly_population_ratio, etc.
regions = spark.read.load(&amp;quot;/home/rahul/projects/sparkdf/coronavirusdataset/Region.csv&amp;quot;,format=&amp;quot;csv&amp;quot;, sep=&amp;quot;,&amp;quot;, inferSchema=&amp;quot;true&amp;quot;, header=&amp;quot;true&amp;quot;) regions.limit(10).toPandas()  We want to get this information in our cases file by joining the two DataFrames. We can do this by using:
cases = cases.join(regions, [&#39;province&#39;,&#39;city&#39;],how=&#39;left&#39;) cases.limit(10).toPandas()  2. Broadcast/Map Side Joins Sometimes you might face a scenario where you need to join a very big table(~1B Rows) with a very small table(~100–200 rows). The scenario might also involve increasing the size of your database like in the example below.
Such sort of operations is aplenty in Spark where you might want to apply multiple operations to a particular key. But assuming that the data for each key in the Big table is large, it will involve a lot of data movement. And sometimes so much that the application itself breaks. A small optimization then you can do when joining on such big tables(assuming the other table is small) is to broadcast the small table to each machine/node when you perform a join. You can do this easily using the broadcast keyword. This has been a lifesaver many times with Spark when everything else fails.
from pyspark.sql.functions import broadcast cases = cases.join(broadcast(regions), [&#39;province&#39;,&#39;city&#39;],how=&#39;left&#39;)  3. Use SQL with DataFrames If you want, you can also use SQL with data frames. Let us try to run some SQL on the cases table.
We first register the cases dataframe to a temporary table cases_table on which we can run SQL operations. As you can see, the result of the SQL select statement is again a Spark Dataframe.
cases.registerTempTable(&#39;cases_table&#39;) newDF = sqlContext.sql(&#39;select * from cases_table where confirmed&amp;gt;100&#39;) newDF.show()  I have shown a minimal example above, but you can use pretty much complex SQL queries involving GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query.
4. Create New Columns There are many ways that you can use to create a column in a PySpark Dataframe. I will try to show the most usable of them.
Using Spark Native Functions The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.
We can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. Our first function, the F.col function gives us access to the column. So if we wanted to add 100 to a column, we could use F.col as:
import pyspark.sql.functions as F casesWithNewConfirmed = cases.withColumn(&amp;quot;NewConfirmed&amp;quot;, 100 &#43; F.col(&amp;quot;confirmed&amp;quot;)) casesWithNewConfirmed.show()  We can also use math functions like F.exp function:
casesWithExpConfirmed = cases.withColumn(&amp;quot;ExpConfirmed&amp;quot;, F.exp(&amp;quot;confirmed&amp;quot;)) casesWithExpConfirmed.show()  There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list here.
Using Spark UDFs Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I need more matured Python functionality.
To use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()
import pyspark.sql.functions as F from pyspark.sql.types import * def casesHighLow(confirmed): if confirmed &amp;lt; 50: return &#39;low&#39; else: return &#39;high&#39; #convert to a UDF Function by passing in the function and return type of function casesHighLowUDF = F.udf(casesHighLow, StringType()) CasesWithHighLow = cases.withColumn(&amp;quot;HighLow&amp;quot;, casesHighLowUDF(&amp;quot;confirmed&amp;quot;)) CasesWithHighLow.show()  Using RDDs This might seem a little odd, but sometimes both the spark UDFs and SQL functions are not enough for a particular use-case. I have observed the RDDs being much more performant in some use-cases in real life. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs.
Whatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem. Don’t worry much if you don’t understand it. It is just here for completion.
The process below makes use of the functionality to convert between Row and pythondict objects. We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again. This might come in handy in a lot of situations.
import math from pyspark.sql import Row def rowwise_function(row): # convert row to python dictionary: row_dict = row.asDict() # Add a new key in the dictionary with the new column name and value. # This might be a big complex function. row_dict[&#39;expConfirmed&#39;] = float(np.exp(row_dict[&#39;confirmed&#39;])) # convert dict to row back again: newrow = Row(**row_dict) # return new row return newrow # convert cases dataframe to RDD cases_rdd = cases.rdd # apply our function to RDD cases_rdd_new = cases_rdd.map(lambda row: rowwise_function(row)) # Convert RDD Back to DataFrame casesNewDf = sqlContext.createDataFrame(cases_rdd_new) casesNewDf.show()  Using Pandas UDF This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupBy operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions rather than Spark window functions which we will go through later in this post.
The way we use it is by using the F.pandas_udf decorator. We assume here that the input to the function will be a pandas data frame. And we need to return a pandas dataframe in turn from this function.
The only complexity here is that we have to provide a schema for the output Dataframe. We can use the original schema of a dataframe to create the outSchema.
cases.printSchema()  Here I am using Pandas UDF to get normalized confirmed cases grouped by infection_case. The main advantage here is that I get to work with pandas dataframes in Spark.
from pyspark.sql.types import IntegerType, StringType, DoubleType, BooleanType from pyspark.sql.types import StructType, StructField # Declare the schema for the output of our function outSchema = StructType([StructField(&#39;case_id&#39;,IntegerType(),True), StructField(&#39;province&#39;,StringType(),True), StructField(&#39;city&#39;,StringType(),True), StructField(&#39;group&#39;,BooleanType(),True), StructField(&#39;infection_case&#39;,StringType(),True), StructField(&#39;confirmed&#39;,IntegerType(),True), StructField(&#39;latitude&#39;,StringType(),True), StructField(&#39;longitude&#39;,StringType(),True), StructField(&#39;normalized_confirmed&#39;,DoubleType(),True) ]) # decorate our function with pandas_udf decorator @F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def subtract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.confirmed v = v - v.mean() pdf[&#39;normalized_confirmed&#39;] = v return pdf confirmed_groupwise_normalization = cases.groupby(&amp;quot;infection_case&amp;quot;).apply(subtract_mean) confirmed_groupwise_normalization.limit(10).toPandas()  5. Spark Window Functions Window functions may make a whole blog post in itself. Here I will talk about some of the most important window functions available in spark.
For this, I will also use one more data CSV, which has dates present as that will help with understanding Window functions much better. I will use the TimeProvince dataframe which contains daily case information for each province.
Ranking You can get rank as well as dense_rank on a group using this function. For example, you may want to have a column in your cases table that provides the rank of infection_case based on the number of infection_case in a province. We can do this by:
from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(F.desc(&#39;confirmed&#39;)) cases.withColumn(&amp;quot;rank&amp;quot;,F.rank().over(windowSpec)).show()  Lag Variables Sometimes our data science models may need lag based features. For example, a model might have variables like the price last week or sales quantity the previous day. We can create such features using the lag function with window functions. Here I am trying to get the confirmed cases 7 days before. I am filtering to show the results as the first few days of corona cases were zeros. You can see here that the lag_7 day feature is shifted by 7 days.
from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;date&#39;) timeprovinceWithLag = timeprovince.withColumn(&amp;quot;lag_7&amp;quot;,F.lag(&amp;quot;confirmed&amp;quot;, 7).over(windowSpec)) timeprovinceWithLag.filter(timeprovinceWithLag.date&amp;gt;&#39;2020-03-10&#39;).show()  Rolling Aggregations Sometimes it helps to provide rolling averages to our models. For example, we might want to have a rolling 7-day sales sum/mean as a feature for our sales regression model. Let us calculate the rolling mean of confirmed cases for the last 7 days here. This is what a lot of the people are already doing with this dataset to see the real trends.
from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;date&#39;).rowsBetween(-6,0) timeprovinceWithRoll = timeprovince.withColumn(&amp;quot;roll_7_confirmed&amp;quot;,F.mean(&amp;quot;confirmed&amp;quot;).over(windowSpec)) timeprovinceWithRoll.filter(timeprovinceWithLag.date&amp;gt;&#39;2020-03-10&#39;).show()  There are a few things here to understand. First is the rowsBetween(-6,0) function that we are using here. This function has a form of rowsBetween(start,end) with both start and end inclusive. Using this we only look at the past 7 days in a particular window including the current_day. Here 0 specifies the current_row and -6 specifies the seventh row previous to current_row. Remember we count starting from 0.
So to get roll_7_confirmed for date 2020–03–22 we look at the confirmed cases for dates 2020–03–22 to 2020–03–16 and take their mean.
If we had used rowsBetween(-7,-1) we would just have looked at past 7 days of data and not the current_day.
One could also find a use for rowsBetween(Window.unboundedPreceding, Window.currentRow) where we take the rows between the first row in a window and the current_row to get running totals. I am calculating cumulative_confirmed here.
from pyspark.sql.window import Window windowSpec = Window().partitionBy([&#39;province&#39;]).orderBy(&#39;date&#39;).rowsBetween(Window.unboundedPreceding,Window.currentRow) timeprovinceWithRoll = timeprovince.withColumn(&amp;quot;cumulative_confirmed&amp;quot;,F.sum(&amp;quot;confirmed&amp;quot;).over(windowSpec)) timeprovinceWithRoll.filter(timeprovinceWithLag.date&amp;gt;&#39;2020-03-10&#39;).show()  6. Pivot Dataframes Sometimes we may need to have the dataframe in flat format. This happens frequently in movie data where we may want to show genres as columns instead of rows. We can use pivot to do this. Here I am trying to get one row for each date and getting the province names as columns.
pivotedTimeprovince = timeprovince.groupBy(&#39;date&#39;).pivot(&#39;province&#39;).agg(F.sum(&#39;confirmed&#39;).alias(&#39;confirmed&#39;) , F.sum(&#39;released&#39;).alias(&#39;released&#39;)) pivotedTimeprovince.limit(10).toPandas()  One thing to note here is that we need to provide an aggregation always with the pivot function even if the data has a single row for a date.
7. Unpivot/Stack Dataframes This is just the opposite of the pivot. Given a pivoted dataframe like above, can we go back to the original?
Yes, we can. But the way is not that straightforward. For one we will need to replace - with _ in the column names as it interferes with what we are about to do. We can simply rename the columns:
newColnames = [x.replace(&amp;quot;-&amp;quot;,&amp;quot;_&amp;quot;) for x in pivotedTimeprovince.columns] pivotedTimeprovince = pivotedTimeprovince.toDF(*newColnames)  Now we will need to create an expression which looks like the below:
&amp;quot;stack(34, &#39;Busan_confirmed&#39; , Busan_confirmed,&#39;Busan_released&#39; , Busan_released,&#39;Chungcheongbuk_do_confirmed&#39; , . . . &#39;Seoul_released&#39; , Seoul_released,&#39;Ulsan_confirmed&#39; , Ulsan_confirmed,&#39;Ulsan_released&#39; , Ulsan_released) as (Type,Value)&amp;quot;  The general format is as follows:
&amp;quot;stack(&amp;lt;cnt of columns you want to put in one column&amp;gt;, &#39;firstcolname&#39;, firstcolname , &#39;secondcolname&#39; ,secondcolname ......) as (Type, Value)&amp;quot;  It may seem daunting, but we can create such an expression using our programming skills.
expression = &amp;quot;&amp;quot; cnt=0 for column in pivotedTimeprovince.columns: if column!=&#39;date&#39;: cnt &#43;=1 expression &#43;= f&amp;quot;&#39;{column}&#39; , {column},&amp;quot; expression = f&amp;quot;stack({cnt}, {expression[:-1]}) as (Type,Value)&amp;quot;  And we can unpivot using:
unpivotedTimeprovince = pivotedTimeprovince.select(&#39;date&#39;,F.expr(exprs))  And voila! we have got our dataframe in a vertical format. There are quite a few column creations, filters, and join operations needed to get exactly the same format as before, but I will not get into those.
8. Salting Sometimes it might happen that a lot of data goes to a single executor since the same key is assigned for a lot of rows in our data. Salting is another way that helps you to manage data skewness.
So assuming we want to do the sum operation when we have skewed keys. We can start by creating the Salted Key and then doing a double aggregation on that key as the sum of a sum still equals sum. To understand this assume we need the sum of confirmed infection_cases on the cases table and assume that the key infection_cases is skewed. We can do the required operation in two steps.
1. Create a Salting Key
We first create a salting key using a concatenation of infection_case column and a random_number between 0 to 9. In case your key is even more skewed, you can split it in even more than 10 parts.
cases = cases.withColumn(&amp;quot;salt_key&amp;quot;, F.concat(F.col(&amp;quot;infection_case&amp;quot;), F.lit(&amp;quot;_&amp;quot;), F.monotonically_increasing_id() % 10))  This is how the table looks after the operation:
2. First Groupby on salt key
cases_temp = cases.groupBy([&amp;quot;infection_case&amp;quot;,&amp;quot;salt_key&amp;quot;]).agg(F.sum(&amp;quot;confirmed&amp;quot;)).show()  3. Second Group On the original Key
Here we saw how the sum of sum can be used to get the final sum. You can also make use of facts like:
 min of min is min
 max of max is max
 sum of count is count
  You can think about ways in which salting as an idea could be applied to joins too.
Some More Tips and Tricks Caching Spark works on the lazy execution principle. What that means is that nothing really gets executed until you use an action function like the .count() on a dataframe. And if you do a .count function, it generally helps to cache at this step. So I have made it a point to cache() my dataframes whenever I do a .count() operation.
df.cache().count()  Save and Load from an intermediate step df.write.parquet(&amp;quot;data/df.parquet&amp;quot;) df.unpersist() spark.read.load(&amp;quot;data/df.parquet&amp;quot;)  When you work with Spark you will frequently run with memory and storage issues. While in some cases such issues might be resolved using techniques like broadcasting, salting or cache, sometimes just interrupting the workflow and saving and reloading the whole dataframe at a crucial step has helped me a lot. This helps spark to let go of a lot of memory that gets utilized for storing intermediate shuffle data and unused caches.
Repartitioning You might want to repartition your data if you feel your data has been skewed while working with all the transformations and joins. The simplest way to do it is by using:
df = df.repartition(1000)  Sometimes you might also want to repartition by a known scheme as this scheme might be used by a certain join or aggregation operation later on. You can use multiple columns to repartition using:
df = df.repartition(&#39;cola&#39;, &#39;colb&#39;,&#39;colc&#39;,&#39;cold&#39;)  You can get the number of partitions in a data frame using:
df.rdd.getNumPartitions()  You can also check out the distribution of records in a partition by using the glom function. This helps in understanding the skew in the data that happens while working with various transformations.
df.glom().map(len).collect()  Reading Parquet File in Local Sometimes you might want to read the parquet files in a system where Spark is not available. In such cases, I normally use the below code:
from glob import glob def load_df_from_parquet(parquet_directory): df = pd.DataFrame() for file in glob(f&amp;quot;{parquet_directory}/*&amp;quot;): df = pd.concat([df,pd.read_parquet(file)]) return df  Conclusion This was a big post and congratulations on you reaching the end. These are the most common functionalities I end up using in my day to day job.
Hopefully, I’ve covered the Dataframe basics well enough to pique your interest and help you get started with Spark. If you want to learn more about how Spark Started or RDD basics take a look at this post
You can find all the code at this GitHub repository where I keep code for all my posts.
Continue Learning Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on Big Data Essentials: HDFS, MapReduce and Spark RDD and Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames by Yandex on Coursera.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Stop Worrying and Create your Deep Learning Server in 30 minutes</title>
      <link>https://mlwhiz.com/blog/2020/05/25/dls/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/05/25/dls/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dls/main.png"></media:content>
      

      
      <description>I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating.</description>

      <content:encoded>  
        
        <![CDATA[  I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating. Going through all those Stack Overflow threads. Often wondering what has gone wrong.
So is there a way to do this more efficiently?
It turns out there is. In this blog, I will try to set up a deep learning server on EC2 with minimal effort so that I could focus on more important things.
This blog consists explicitly of two parts:
 Setting up an Amazon EC2 Machine with preinstalled deep learning libraries.
 Setting Up Jupyter Notebook using TMUX and SSH tunneling.
  Don’t worry; it’s not as difficult as it sounds. Just follow the steps and click Next.
Setting up Amazon EC2 Machine I am assuming that you have an AWS account, and you have access to the AWS Console. If not, you might need to sign up for an Amazon AWS account.
 First of all, we need to go to the Services tab to access the EC2 dashboard.   On the EC2 Dashboard, you can start by creating your instance.   Amazon provides Community AMIs(Amazon Machine Image) with Deep Learning software preinstalled. To access these AMIs, you need to look in the community AMIs and search for “Ubuntu Deep Learning” in the Search Tab. You can choose any other Linux flavor, but I have found Ubuntu to be most useful for my Deep Learning needs. In the present setup, I will use The Deep Learning AMI (Ubuntu 18.04) Version 27.0   Once you select an AMI, you can select the Instance Type. It is here you specify the number of CPUs, Memory, and GPUs you will require in your system. Amazon provides a lot of options to choose from based on one’s individual needs. You can filter for GPU instances using the “Filter by” filter.  In this tutorial, I have gone with p2.xlarge instance, which provides NVIDIA K80 GPU with 2,496 parallel processing cores and 12GiB of GPU memory. To know about different instance types, you can look at the documentation here and the pricing here.
 You can change the storage that is attached to the machine in the 4th step. It is okay if you don’t add storage upfront, as you can also do this later. I change the storage from 90 GB to 500 GB as most of the deep learning needs will require proper storage.   That’s all, and you can Launch the Instance after going to the Final Review instance settings Screen. Once you click on Launch, you will see this screen. Just type in any key name in the Key Pair Name and click on “Download key pair”. Your key will be downloaded to your machine by the name you provided. For me, it got saved as “aws_key.pem”. Once you do that, you can click on “Launch Instances”.  Keep this key pair safe as this will be required whenever you want to login to your instance.
 You can now click on “View Instances” on the next page to see your instance. This is how your instance will look like:   To connect to your instance, Just open a terminal window in your Local machine and browse to the folder where you have kept your key pair file and modify some permissions.
chmod 400 aws_key.pem
  Once you do that, you will be able to connect to your instance by SSHing. The SSH command will be of the form:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For me, the command was:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@ec2-54-202-223-197.us-west-2.compute.amazonaws.com  Also, keep in mind that the Public DNS might change once you shut down your instance.
 You have already got your machine up and ready. This machine contains different environments that have various libraries you might need. This particular machine has MXNet, Tensorflow, and Pytorch with different versions of python. And the best thing is that we get all this preinstalled, so it just works out of the box.  Setting Up Jupyter Notebook But there are still a few things you will require to use your machine fully. One of them being Jupyter Notebooks. To set up Jupyter Notebooks with your Machine, I recommend using TMUX and tunneling. Let us go through setting up the Jupyter notebook step by step.
1. Using TMUX to run Jupyter Notebook We will first use TMUX to run the Jupyter notebook on our instance. We mainly use this so that our notebook still runs even if the terminal connection gets lost.
To do this, you will need to create a new TMUX session using:
tmux new -s StreamSession  Once you do that, you will see a new screen with a green border at the bottom. You can start your Jupyter Notebook in this machine using the usual jupyter notebook command. You will see something like:
It will be beneficial to copy the login URL so that we will be able to get the token later when we try to login to our jupyter notebook later. In my case, it is:
[http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872](http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872&amp;amp;token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872)  The next step is to detach our TMUX session so that it continues running in the background even when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)You will come back to the initial screen with the message that you have detached from your TMUX session.
If you want, you can reattach to the session again using:
tmux attach -t StreamSession  2. SSH Tunneling to access the notebook on your Local Browser The second step is to tunnel into the Amazon instance to be able to get the Jupyter notebook on your Local Browser. As we can see, the Jupyter Notebook is actually running on the localhost on the Cloud instance. How do we access it? We use SSH tunneling. Worry not, it is straightforward fill in the blanks. Just use this command on your local machine terminal window:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L &amp;lt;Local Machine Port&amp;gt;:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For this case, I have used:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L 8001:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)ec2-54-202-223-197.us-west-2.compute.amazonaws.com  This means that I will be able to use the Jupyter Notebook If I open the localhost:8001 in my local machine browser. And I surely can. We can now just input the token that we already have saved in one of our previous steps to access the notebook. For me the token is 5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872
You can just login using your token and voila we get the notebook in all its glory.
You can now choose to work on a new project by selecting any of the different environments you want. You can come from Tensorflow or Pytorch or might be willing to get the best of both worlds. This notebook will not disappoint you.
Troubleshooting It might happen that once the machine is restarted, you face some problems with the NVIDIA graphics card. Specifically, in my case, the nvidia-smi command stopped working. If you encounter this problem, the solution is to download the graphics driver from the NVIDIA website.
Above are the settings for the particular AMI I selected. Once you click on Search you will be able to see the next page:
Just copy the download link by right-clicking and copying the link address. And run the following commands on your machine. You might need to change the link address and the file name in this.
# When nvidia-smi doesnt work: wget [https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla](https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla) sudo sh NVIDIA-Linux-x86_64-410.129-diagnostic.run --no-drm --disable-nouveau --dkms --silent --install-libglvnd modinfo nvidia | head -7 sudo modprobe nvidia  Stop Your Instance And that’s it. You have got and up and running Deep Learning machine at your disposal, and you can work with it as much as you want. Just keep in mind to stop the instance whenever you stop working, so you won’t need to pay Amazon when you are not working on your instance. You can do it on the instances page by right-clicking on your instance. Just note that when you need to log in again to this machine, you will need to get the Public DNS (IPv4) address from the instance page back as it might have changed.
Conclusion I have always found it a big chore to set up a deep learning environment.
In this blog, we set up a new Deep Learning server on EC2 in minimal time by using Deep Learning Community AMI, TMUX, and Tunneling for the Jupyter Notebooks. This server comes preinstalled with all the deep learning libraries you might need at your work, and it just works out of the box.
So what are you waiting for? Just get started with Deep Learning with your own server.
If you want to learn more about AWS and how to use it in production settings and deploying models, I would like to call out an excellent course on AWS. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How and Why to use f strings in Python3?</title>
      <link>https://mlwhiz.com/blog/2020/05/24/fstring/</link>
      <pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/05/24/fstring/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/fstring/main.png"></media:content>
      

      
      <description>Python provides us with many styles of coding.
And with time, Python has regularly come up with new coding standards and tools that adhere even more to the coding standards in the Zen of Python.
 Beautiful is better than ugly.
 In this series of posts named Python Shorts, I will explain some simple but very useful constructs provided by Python, some essential tips, and some use cases I come up with regularly in my Data Science work.</description>

      <content:encoded>  
        
        <![CDATA[  Python provides us with many styles of coding.
And with time, Python has regularly come up with new coding standards and tools that adhere even more to the coding standards in the Zen of Python.
 Beautiful is better than ugly.
 In this series of posts named Python Shorts, I will explain some simple but very useful constructs provided by Python, some essential tips, and some use cases I come up with regularly in my Data Science work.
This post is specifically about using f strings in Python that was introduced in Python 3.6.
3 Common Ways of Printing: Let me explain this with a simple example. Suppose you have some variables, and you want to print them within a statement.
name = &#39;Andy&#39; age = 20 print(?) ---------------------------------------------------------------- Output: I am Andy. I am 20 years old  You can do this in various ways:
a) Concatenate: A very naive way to do is to simply use &#43; for concatenation within the print function. But that is clumsy. We would need to convert our numeric variables to string and keep care of the spaces while concatenating. And it doesn’t look good as the code readability suffers a little when we use it.
name = &#39;Andy&#39; age = 20 print(&amp;quot;I am &amp;quot; &#43; name &#43; &amp;quot;. I am &amp;quot; &#43; str(age) &#43; &amp;quot; years old&amp;quot;) ---------------------------------------------------------------- I am Andy. I am 20 years old  b) % Format: The second option is to use % formatting. But it also has its problems. For one, it is not readable. You would need to look at the first %s and try to find the corresponding variable in the list at the end. And imagine if you have a long list of variables that you may want to print.
print(&amp;quot;I am %s. I am %s years old&amp;quot; % (name, age))  c) str.format(): Next comes the way that has been used in most Python 3 codes and has become the standard of printing in Python. Using str.format()
print(&amp;quot;I am {}. I am {} years old&amp;quot;.format(name, age))  Here we use {} to denote the placeholder of the object in the list. It still has the same problem of readability, but we can also use str.format :
print(&amp;quot;I am {name}. I am {age} years old&amp;quot;.format(name = name, age = age))  If this seems a little too repetitive, we can use dictionaries too:
data = {&#39;name&#39;:&#39;Andy&#39;,&#39;age&#39;:20} print(&amp;quot;I am {name}. I am {age} years old&amp;quot;.format(**data))  The Fourth Way with f Since Python 3.6, we have a new formatting option, which makes it even more trivial. We could simply use:
print(f&amp;quot;I am {name}. I am {age} years old&amp;quot;)  We just append f at the start of the string and use {} to include our variable name, and we get the required results.
An added functionality that f string provides is that we can put expressions in the {} brackets. For Example:
num1 = 4 num2 = 5 print(f&amp;quot;The sum of {num1} and {num2} is {num1&#43;num2}.&amp;quot;) --------------------------------------------------------------- The sum of 4 and 5 is 9.  This is quite useful as you can use any sort of expression inside these brackets. The expression can contain dictionaries or functions. A simple example:
def totalFruits(apples,oranges): return apples&#43;oranges data = {&#39;name&#39;:&#39;Andy&#39;,&#39;age&#39;:20} apples = 20 oranges = 30 print(f&amp;quot;{data[&#39;name&#39;]} has {totalFruits(apples,oranges)} fruits&amp;quot;) ---------------------------------------------------------------- Andy has 50 fruits  Also, you can use ’’’ to use multiline strings.
num1 = 4 num2 = 5 print(f&#39;&#39;&#39;The sum of {num1} and {num2} is {num1&#43;num2}.&#39;&#39;&#39;) --------------------------------------------------------------- The sum of 4 and 5 is 9.  An everyday use case while formatting strings is to format floats. You can do that using f string as following
numFloat = 10.23456678 print(f&#39;Printing Float with 2 decimals: {numFloat:.2f}&#39;) ----------------------------------------------------------------- Printing Float with 2 decimals: 10.23  Conclusion Until recently, I had been using Python 2 for all my work, and so was not able to check out this new feature.
But now, as I am shifting to Python 3, f strings has become my go-to syntax to format strings. It is easy to write and read with the ability to incorporate arbitrary expressions as well. In a way, this new function adheres to at least 3 PEP concepts —
 Beautiful is better than ugly, Simple is better than complex and Readability counts.
 If you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Newspaper for COVID-19 — The CoronaTimes</title>
      <link>https://mlwhiz.com/blog/2020/03/29/coronatimes/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/03/29/coronatimes/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/coronatimes/main.gif"></media:content>
      

      
      <description>It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea.</description>

      <content:encoded>  
        
        <![CDATA[  It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea. Why not create an international news dashboard for Corona? And here it is.
This post is about how I created the news dashboard using Streamlit and data from NewsApi and European CDC.
TLDR; Link to the App here.
Getting The Data The most important thing while creating this Dashboard was acquiring the data. I am using two data sources:
1. Data from the European Centre for Disease Prevention and Control. The downloadable data file is updated daily and contains the latest available public data on COVID-19. Here is a snapshot of this data.
def get_data(date): os.system(&amp;quot;rm cases.csv&amp;quot;) url = &amp;quot;[https://opendata.ecdc.europa.eu/covid19/casedistribution/csv](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)&amp;quot; filename = wget.download(url,&amp;quot;cases.csv&amp;quot;) casedata = pd.read_csv(filename, encoding=&#39;latin-1&#39;) return casedata  2. News API The second source of data comes from the News API, which lets me access articles from leading news outlets from various countries for free. The only caveat is that I could only hit the API 500 times a day, and there is a result limit of 100 results for a particular query for free accounts.
I tried to get around those limit barriers by using streamlit caching(So I don’t hit the API a lot). I also tried to get news data from last month using multiple filters to get a lot of data.
from newsapi import NewsApiClient newsapi = NewsApiClient(api_key=&#39;aedb6aa9bebb4011a4eb5447019dd592&#39;)  The primary way the API works is by giving us access to 3 functions.
a) A function to get Recent News from a country:
json_data = newsapi.get_top_headlines(q=q,language=&#39;en&#39;, country=&#39;us&#39;) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  b) A function to get “Everything” related to a query from the country. You can see the descriptions of API parameters here:
json_data = newsapi.get_everything(q=&#39;corona&#39;, language=&#39;en&#39;, from_param=str(date.today() -timedelta(days=29)), to= str(date.today()), sources = &#39;usa-today&#39;, page_size=100, page = 1, sort_by=&#39;relevancy&#39; ) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  c) A function to get a list of sources from a Country programmatically. We can then use these sources to pull data from the “everything” API
def get_sources(country): sources = newsapi.get_sources(country=country) sources = [x[&#39;id&#39;] for x in sources[&#39;sources&#39;]] return sources sources = get_sources(country=&#39;us&#39;) print(sources[:5]) ------------------------------------------------------------------- [&#39;abc-news&#39;, &#39;al-jazeera-english&#39;, &#39;ars-technica&#39;, &#39;associated-press&#39;, &#39;axios&#39;]  I used all the functions above to get data that refreshes at a particular cadence. You can see how I use these API functions in a loop to download the data by looking at my code at GitHub.
Creating the Dashboard I wanted to have a few important information in the Dashboard that I was interested in. So I started by creating various widgets.
1. Current World Snapshot: The first information was regarding the whole world situation. The Number of Cases and Deaths. The case and death curve in various countries? What are the fatality rates in various countries? Below is the current world situation on 28 Mar 2020.
Observations: We can see the deaths in Italy are still on the rise, while we are seeing the deaths shooting up in Spain, France, and the United States as well. The death rates in some countries are worrying with death rates of 10.56% in Italy and 8.7% in Iraq. I suspect that the death rate statistic of 2% in the starting days of CoronaVirus was misinformed if not wrong.
Technical Details — To create this part of the Dashboard, I used the ECDC data. I also used a lot of HTML hacks with Streamlit, where I used bootstrap widgets as well as custom HTML to get data in the way I wanted to display it. Here are a few of the hacks:
 Using Bootstrap Cards: You can use bootstrap or, in that case, any HTML element in Streamlit if you change the parameter unsafe_allow_html to True. Do note that I am also using python f string formatting here.  st.sidebar.markdown(f&amp;#39;&amp;#39;&amp;#39;&amp;lt;div class=&amp;#34;card text-white bg-info mb-3&amp;#34; style=&amp;#34;width: 18rem&amp;#34;&amp;gt; &amp;lt;div class=&amp;#34;card-body&amp;#34;&amp;gt; &amp;lt;h5 class=&amp;#34;card-title&amp;#34;&amp;gt;Total Cases&amp;lt;/h5&amp;gt; &amp;lt;p class=&amp;#34;card-text&amp;#34;&amp;gt;{sum(casedata[&amp;#39;cases&amp;#39;]):,d}&amp;lt;/p&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;&amp;#39;&amp;#39;&amp;#39;, unsafe_allow_html=True) The above code is behind the Dashboard styled cards in the streamlit app sidebar.
 Changed the width of the streamlit main page:  Again, there was no parameter given by streamlit to do this, and I was finding the page width a little too small for my use case. Adding the above code at the start of the app solved the issue.
st.markdown( f&amp;#34;&amp;#34;&amp;#34; &amp;lt;style&amp;gt; .reportview-container .main .block-container{{ max-width: 1000px; }} &amp;lt;/style&amp;gt; &amp;#34;&amp;#34;&amp;#34;, unsafe_allow_html=True, ) 2. Most Recent News from Country The primary purpose of creating this Dashboard was to get news from various outlets from top media outlets in the country.
Observations: As here you can see, here we have the top recent news from the United Kingdom concerning cases in Ireland and Boris Johnson’s corona woes.
Technical Details: As said before, I am using the News API to get this data. And here is how I am using a mashup of HTML and markdown to display the news results.
def create_most_recent_markdown(df,width=700): if len(df)&amp;gt;0: # img url img_path = df[&#39;urlToImage&#39;].iloc[0] if not img_path: images = [x for x in df.urlToImage.values if x is not None] if len(images)!=0: img_path = random.choice(images) else: img_path = &#39;[https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;](https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;) img_alt = df[&#39;title&#39;].iloc[0] df = df[:5] **markdown_str = f&amp;quot;&amp;lt;img src=&#39;{img_path}&#39; width=&#39;{width}&#39;/&amp;gt; &amp;lt;br&amp;gt; &amp;lt;br&amp;gt;&amp;quot;** for index, row in df.iterrows(): **markdown_str &#43;= f&amp;quot;[{row[&#39;title&#39;]}]({row[&#39;url&#39;]}) by {row[&#39;author&#39;]}&amp;lt;br&amp;gt; &amp;quot;** return markdown_str else: return &#39;&#39;  Few things to note here:
 The image width cannot be set using markdown so using custom HTML
 The usage of python f strings to create the article titles and URLs.
 If no image is found, we are defaulting to a custom image.
  3. News Sentiment Another thing that has been bothering me in these trying times is so much negativity everywhere. I wanted to see the news covered from a positive angle if it could be in any way. So I did some simple sentiment analysis using the custom sentiment analyzer from Textblob to do this.
I found out sentiments by news outlets as well as some of the most positive and negative news related to Coronavirus in the past 30 days. (Past 30 days because I cannot go more back with the free API).
Observations: As you can see that one of the most positive news is Trump changing his coronavirus stance on March 17th, and I agree. The second positive report seems to be regarding some sort of solution to the problem. While the first Negative news is regarding Cardi B slamming celebrities for sowing confusion about the Coronavirus. I won’t comment on this :)
Technical Details: To get the sentiment scores of an article I used TextBlob. Getting the sentiment scores that range from -1 to 1 is as simple as using the below function. I used a concatenation of title and description to find the sentiment as the content from the News API was truncated.
def textblob_sentiment(title,description): blob = TextBlob(str(title)&#43;&amp;quot; &amp;quot;&#43;str(description)) return blob.sentiment.polarity  The main difficulty here was to have a two-column layout to give both positive and negative news. For that again, I had to use a mashup of HTML and markdown. I used the HTML table to do this. Also, note how I used markdown to convert markdown to HTML using Python f strings.
import markdown md = markdown.Markdown() positive_results_markdown = create_most_recent_markdown(positivedata,400) negative_results_markdown = create_most_recent_markdown(negativedata,400) html = f&#39;&#39;&#39;&amp;lt;table style=&amp;quot;width:100%&amp;quot;&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Positive News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Negative News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(positive_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(negative_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/table&amp;gt;&#39;&#39;&#39; #print md.convert(&amp;quot;# sample heading text&amp;quot;) st.markdown(html,unsafe_allow_html=True)  4. News Source WordCloud A visualization dashboard that works with text is never really complete without a word cloud, so I thought of adding a word cloud to understand the word usage from a particular source.
Observations: We can see Vice news using words like “New” and “Tested” a lot of times. While Business Insider used “China” a lot.
      Technical Details:Here is what I used to create this masked word cloud:
import cv2 def create_mask(): mask = np.array(Image.open(&amp;quot;coronavirus.png&amp;quot;)) im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) _, mask = cv2.threshold(im_gray, thresh=20, maxval=255, type=cv2.THRESH_BINARY) mask = 255 - mask return mask mask = create_mask() def create_wc_by(source): data = fulldf[fulldf[&#39;source&#39;]==source] text = &amp;quot; &amp;quot;.join([x for x in data.content.values if x is not None]) stopwords = set(STOPWORDS) stopwords.add(&#39;chars&#39;) stopwords.add(&#39;coronavirus&#39;) stopwords.add(&#39;corona&#39;) stopwords.add(&#39;chars&#39;) wc = WordCloud(background_color=&amp;quot;white&amp;quot;, max_words=1000, mask=mask, stopwords=stopwords, max_font_size=90, random_state=42, contour_width=3, contour_color=&#39;steelblue&#39;) wc.generate(text) plt.figure(figsize=[30,30]) plt.imshow(wc, interpolation=&#39;bilinear&#39;) plt.axis(&amp;quot;off&amp;quot;) return plt st.pyplot(create_wc_by(source),use_container_width=True)  Other Technical Considerations 1. Advanced Caching: In new streamlit release notes for 0.57.0 which just came out yesterday, streamlit has made updates to st.cache. One notable change to this release is the “ability to set expiration options for cached functions by setting the max_entries and ttl arguments”. From the documentation:
 max_entries (int or None) — The maximum number of entries to keep in the cache, or None for an unbounded cache. (When a new entry is added to a full cache, the oldest cached entry will be removed.) The default is None.
 ttl (float or None) — The maximum number of seconds to keep an entry in the cache, or None if cache entries should not expire. The default is None.
  Two use cases where this might help would be:
 If you’re serving your app and don’t want the cache to grow forever.
 If you have a cached function that reads live data from a URL and should clear every few hours to fetch the latest data
  So this is what is being used in a lot of functions to avoid hitting APIs multiple times and to prevent them from getting stale at the same time.
For Example, Top results from a country are fetched at a period of 360 seconds i.e., 6 minutes.
st.cache(ttl=360,max_entries=20) def create_dataframe_top(queries,country): #Hits API Here  While full results from the everything API are fetched at a period of one day.
[@st](http://twitter.com/st).cache(ttl = 60*60*24,max_entries=20) def create_dataframe_last_30d(queries, sources): # hits API  2. Deployment: I used the amazon free ec2 instance to deploy this app at http://54.149.204.138:8501/. If you want to know the steps,read my post on How to Deploy a Streamlit App using an Amazon Free ec2 instance?
There are also a few caveats:
 Since it is a free server, it might not take too much load.
 I have not thoroughly tested the caching routine. I just hope that there are no memory errors with the limited memory on the server.
 The News API is also free. There might be rate limits that might kick in even after I have tried to handle that.
  3. Learning For folks who are lost, you might like to start with the basics first. Here is my introductory posts on Streamlit and Plotly express.
 How to write Web apps using simple Python for Data Scientists? Python’s One Liner graph creation library with animations Hans Rosling Style  Conclusion Here I have tried creating a dashboard for news on Coronavirus, but it is still in a nascent stage, and a lot needs to be done.
For one, it needs a large server. For another, a lot of time to improve the visualization and layouts. And also a lot of testing.
Also, we have done a few things in a roundabout way using HTML and few hacks. There are still a lot of things that I will love to have in Streamlit. I have been in talks with the Streamlit team over the new functionality that they are going to introduce, and I will try to keep you updated on the same. The good news is that Layout options are a part of the new functionality that Streamlit is working on.
You can find the full code for the final app here at my Github repo. And here is the full app on the web.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>5 Ways to add a new column in a PySpark Dataframe</title>
      <link>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/sparkcolumns/main.png"></media:content>
      

      
      <description>Too much data is getting generated day by day.
Although sometimes we can manage our big data using tools like Rapids or Parallelization, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.
In my last post on Spark, I explained how to work with PySpark RDDs and Dataframes.
Although this post explains a lot on how to work with RDDs and basic Dataframe operations, I missed quite a lot when it comes to working with PySpark Dataframes.</description>

      <content:encoded>  
        
        <![CDATA[  Too much data is getting generated day by day.
Although sometimes we can manage our big data using tools like Rapids or Parallelization, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.
In my last post on Spark, I explained how to work with PySpark RDDs and Dataframes.
Although this post explains a lot on how to work with RDDs and basic Dataframe operations, I missed quite a lot when it comes to working with PySpark Dataframes.
And it is only when I required more functionality that I read up and came up with multiple solutions to do one single thing.
How to create a new column in spark?
Now, this might sound trivial, but believe me, it isn’t. With so much you might want to do with your data, I am pretty sure you will end up using most of these column creation processes in your workflow. Sometimes to utilize Pandas functionality, or occasionally to use RDDs based partitioning or sometimes to make use of the mature python ecosystem.
This post is going to be about — “Multiple ways to create a new column in Pyspark Dataframe.”
If you have PySpark installed, you can skip the Getting Started section below.
Getting Started with Spark I know that a lot of you won’t have spark installed in your system to try and learn. But installing Spark is a headache of its own.
Since we want to understand how it works and work with it, I would suggest that you use Spark on Databricks here online with the community edition. Don’t worry, it is free, albeit fewer resources, but that works for us right now for learning purposes.
Once you register and login will be presented with the following screen.
You can start a new notebook here.
Select the Python notebook and give any name to your notebook.
Once you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.
The next step will be to check if the sparkcontext is present. To check if the sparkcontext is present, you have to run this command:
sc  This means that we are set up with a notebook where we can run Spark.
Data Here, I will work on the Movielens ml-100k.zip dataset. 100,000 ratings from 1000 users on 1700 movies. In this zipped folder, the file we will specifically work with is the rating file. This filename is kept as “u.data”
If you want to upload this data or any data, you can click on the Data tab in the left and then Add Data by using the GUI provided.
We can then load the data using the following commands:
ratings = spark.read.load(&amp;quot;/FileStore/tables/u.data&amp;quot;,format=&amp;quot;csv&amp;quot;, sep=&amp;quot;\t&amp;quot;, inferSchema=&amp;quot;true&amp;quot;, header=&amp;quot;false&amp;quot;) ratings = ratings.toDF(*[&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;unix_timestamp&#39;])  Here is how it looks:
ratings.show()  Ok, so now we are set up to begin the part we are interested in finally. How to create a new column in PySpark Dataframe?
1. Using Spark Native Functions The most pysparkish way to create a new column in a PySpark DataFrame is by using built-in functions. This is the most performant programmatical way to create a new column, so this is the first place I go whenever I want to do some column manipulation.
We can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. We can import spark functions as:
import pyspark.sql.functions as F  Our first function, the F.col function gives us access to the column. So if we wanted to multiply a column by 2, we could use F.col as:
ratings_with_scale10 = ratings.withColumn(&amp;quot;ScaledRating&amp;quot;, 2*F.col(&amp;quot;rating&amp;quot;)) ratings_with_scale10.show()  We can also use math functions like F.exp function:
ratings_with_exp = ratings.withColumn(&amp;quot;expRating&amp;quot;, 2*F.exp(&amp;quot;rating&amp;quot;)) ratings_with_exp.show()  There are a lot of other functions provided in this module, which are enough for most simple use cases. You can check out the functions list here.
2. Spark UDFs Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I want to use the more matured Python functionality.
To use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()
import pyspark.sql.functions as F from pyspark.sql.types import * def somefunc(value): if value &amp;lt; 3: return &#39;low&#39; else: return &#39;high&#39; #convert to a UDF Function by passing in the function and return type of function udfsomefunc = F.udf(somefunc, StringType()) ratings_with_high_low = ratings.withColumn(&amp;quot;high_low&amp;quot;, udfsomefunc(&amp;quot;rating&amp;quot;)) ratings_with_high_low.show()  3. Using RDDs Sometimes both the spark UDFs and SQL Functions are not enough for a particular use-case. You might want to utilize the better partitioning that you get with spark RDDs. Or you may want to use group functions in Spark RDDs. You can use this one, mainly when you need access to all the columns in the spark data frame inside a python function.
Whatever the case be, I find this way of using RDD to create new columns pretty useful for people who have experience working with RDDs that is the basic building block in the Spark ecosystem.
The process below makes use of the functionality to convert between Row and pythondict objects. We convert a row object to a dictionary. Work with the dictionary as we are used to and convert that dictionary back to row again.
import math from pyspark.sql import Row def rowwise_function(row): # convert row to dict: row_dict = row.asDict() # Add a new key in the dictionary with the new column name and value. row_dict[&#39;Newcol&#39;] = math.exp(row_dict[&#39;rating&#39;]) # convert dict to row: newrow = Row(**row_dict) # return new row return newrow # convert ratings dataframe to RDD ratings_rdd = ratings.rdd # apply our function to RDD ratings_rdd_new = ratings_rdd.map(lambda row: rowwise_function(row)) # Convert RDD Back to DataFrame ratings_new_df = sqlContext.createDataFrame(ratings_rdd_new) ratings_new_df.show()  4. Pandas UDF This functionality was introduced in the Spark version 2.3.1. And this allows you to use pandas functionality with Spark. I generally use it when I have to run a groupby operation on a Spark dataframe or whenever I need to create rolling features and want to use Pandas rolling functions/window functions.
The way we use it is by using the F.pandas_udf decorator. We assume here that the input to the function will be a pandas data frame. And we need to return a pandas dataframe in turn from this function.
The only complexity here is that we have to provide a schema for the output Dataframe. We can make that using the format below.
# Declare the schema for the output of our function outSchema = StructType([StructField(&#39;user_id&#39;,IntegerType(),True),StructField(&#39;movie_id&#39;,IntegerType(),True),StructField(&#39;rating&#39;,IntegerType(),True),StructField(&#39;unix_timestamp&#39;,IntegerType(),True),StructField(&#39;normalized_rating&#39;,DoubleType(),True)]) # decorate our function with pandas_udf decorator [@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def subtract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.rating v = v - v.mean() pdf[&#39;normalized_rating&#39;] =v return pdf rating_groupwise_normalization = ratings.groupby(&amp;quot;movie_id&amp;quot;).apply(subtract_mean) rating_groupwise_normalization.show()  We can also make use of this to train multiple individual models on each spark node. For that, we replicate our data and give each replication a key and some training params like max_depth, etc. Our function then takes the pandas Dataframe, runs the required model, and returns the result. The structure would look something like below.
# 0. Declare the schema for the output of our function outSchema = StructType([StructField(&#39;replication_id&#39;,IntegerType(),True),StructField(&#39;RMSE&#39;,DoubleType(),True)]) # decorate our function with pandas_udf decorator [@F](http://twitter.com/F).pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP) def run_model(pdf): # 1. Get hyperparam values num_trees = pdf.num_trees.values[0] depth = pdf.depth.values[0] replication_id = pdf.replication_id.values[0] # 2. Train test split Xtrain,Xcv,ytrain,ycv = train_test_split..... # 3. Create model using the pandas dataframe clf = RandomForestRegressor(max_depth = depth, num_trees=num_trees,....) clf.fit(Xtrain,ytrain) # 4. Evaluate the model rmse = RMSE(clf.predict(Xcv,ycv) # 5. return results as pandas DF res =pd.DataFrame({&#39;replication_id&#39;:replication_id,&#39;RMSE&#39;:rmse}) return res results = replicated_data.groupby(&amp;quot;replication_id&amp;quot;).apply(run_model)  Above is just an idea and not a working code. Though it should work with minor modifications.
5. Using SQL For people who like SQL, there is a way even to create columns using SQL. For this, we need to register a temporary SQL table and then use simple select queries with an additional column. One might also use it to do joins.
ratings.registerTempTable(&#39;ratings_table&#39;) newDF = sqlContext.sql(&#39;select *, 2*rating as newCol from ratings_table&#39;) newDF.show()  Conclusion And that is the end of this column(pun intended)
Hopefully, I’ve covered the column creation process well to help you with your Spark problems. If you need to learn more of spark basics, take a look at:
The Hitchhikers guide to handle Big Data using Spark
You can find all the code for this post at the GitHub repository or the published notebook on databricks.
Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on Big Data Essentials, which is part of the Big Data Specialization provided by Yandex.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to Deploy a Streamlit App using an Amazon Free ec2 instance?</title>
      <link>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlitec2/main.png"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
In my last post on Streamlit, I talked about how to write Web apps using simple Python for Data Scientists.
But still, a major complaint, if you would check out the comment section of that post, was regarding the inability to deploy Streamlit apps over the web.
And it was a valid complaint.
 A developer can’t show up with his laptop every time the client wanted to use the app. What is the use of such an app?
 So in this post, we will go one step further deploy our Streamlit app over the Web using an Amazon Free ec2 instance.
Setting up the Amazon Instance Before we start with using the amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a simple sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine”. Here we are trying to set up the machine where we will deploy our Streamlit app.
 In the first step, you need to choose the AMI template for the machine. I select the 18.04 Ubuntu Server since it is applicable for the Free Tier. And Ubuntu.
   In the second step, I select the t2.micro instance as again it is the one which is eligible for the free tier. As you can see t2.micro is just a single CPU instance with 512 MB RAM. You can opt for a bigger machine if you are dealing with a powerful model or are willing to pay.   Keep pressing Next until you reach the “6. Configure Security Group” tab. You will need to add a rule with Type: “Custom TCP Rule”, Port Range:8501, and Source: Anywhere. We use the port 8501 here since it is the custom port used by Streamlit.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch you might need to create a new key pair. Here I am creating a new key pair named streamlit and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state, it should be showing “Running”   Select your instance, and copy the Public DNS(IPv4) Address from the description. It should be something starting with ec2.
 Once you have that run the following commands in the folder you saved the streamlit.pem file. I have masked some of the information here.
   chmod 400 streamlit.pem ssh -i &amp;quot;streamlit.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Installing Required Libraries Whoa, that was a handful. After all the above steps you should be able to see the ubuntu prompt for the virtual machine. We will need to set up this machine to run our app. I am going to be using the same streamlit_football_demo app that I used in my previous post.
We start by installing miniconda and adding its path to the environment variable.
sudo apt-get update wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p ~/miniconda echo &amp;quot;PATH=$PATH:$HOME/miniconda/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc  We then install additional dependencies for our app to run. That means I install streamlit and plotly_express.
pip install streamlit pip install plotly_express  And our machine is now prepped and ready to run.
Running Streamlit on Amazon ec2 As I am set up with the instance, I can get the code for my demo app from Github. Or you can choose to create or copy another app as you wish.
git clone https://github.com/MLWhiz/streamlit_football_demo.git cd streamlit_football_demo streamlit run helloworld.py  Now you can go to a browser and type the external URL to access your app. In my case the address is http://35.167.158.251:8501. Here is the output. This app will be up right now if you want to play with it.
A Very Small Problem Though We are up and running with our app for the world to see. But whenever you are going to close the SSH terminal window the process will stop and so will your app.
So what do we do?
TMUX to the rescue. TMUX allows us to keep running our sessions even after we leave the terminal window. It also helps with a lot of other things but I will just go through the steps we need.
First, we stop our app using Ctrl&#43;C and install tmux
sudo apt-get install tmux  We start a new tmux session using the below command. We keep the name of our session as StreamSession. You could use any name here.
tmux new -s StreamSession  You can see that the session name is “StreamSession” at the bottom of the screen. You can now start running streamlit in the tmux session.
streamlit run helloworld.py  You will be able to see your app at the External URL. The next step is to detach our TMUX session so that it continues running in the background when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)
You can now close your SSH session and the app will continue running at the External URL.
And Voila! We are up and running.
Pro TMUX Tip: You can reattach to the same session by using the attach command below. The best part is that you can close your SSH shell and then maybe come back after some hours and reattach to a session and keep working from wherever you were when you closed the SSH shell.
tmux attach -t StreamSession  Simple Troubleshooting: If your app is not hosting at 8501, it means that an instance of streamlit app is already running on your system and you will need to stop that. You can do so by first finding the process ID
ps aux | grep streamlit  You will see something like:
ubuntu 20927 2.4 18.8 713780 189580 pts/3 Sl&#43; 19:55 0:26 /home/ubuntu/miniconda/bin/python /home/ubuntu/miniconda/bin/streamlit run helloworld.py  You will need to kill this process. You can do this simply by
kill -9 20947  Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more. If you want to learn more about how to create awesome web apps with Streamlit then read up my last post.
In this post, we deployed a simple web app on AWS using amazon ec2.
In the process of doing this, we created our own Amazon ec2 instance, logged into the SSH shell, installed miniconda and dependencies, ran our Streamlit application and learned about TMUX. Enough learning for a day?
So go and show on these Mad skills. To end on a lighter note, as Sten Sootla says in his satire piece which I thoroughly enjoyed:
 The secret: it’s not what you know, it’s what you show.
 If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Become a Data Scientist in 2020 with these 10 resources</title>
      <link>https://mlwhiz.com/blog/2020/02/21/ds2020/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/21/ds2020/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/ds2020/main.png"></media:content>
      

      
      <description>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
With those heavy steel enforced gumboots and that plastic helmet, venturing around big blast furnaces and rolling mills. Artificial safety measures, to say the least, as I knew that nothing would save me if something untoward happens. Maybe some running shoes would have helped. As for the helmet. I would just say that molten steel burns at 1370 degrees C.</description>

      <content:encoded>  
        
        <![CDATA[  I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
With those heavy steel enforced gumboots and that plastic helmet, venturing around big blast furnaces and rolling mills. Artificial safety measures, to say the least, as I knew that nothing would save me if something untoward happens. Maybe some running shoes would have helped. As for the helmet. I would just say that molten steel burns at 1370 degrees C.
As I realized based on my constant fear, that job was not for me, and so I made it my goal to move into the Analytics and Data Science space somewhere around in 2011. From that time, MOOCs have been my goto option for learning new things, and I ended up taking a lot of them. Good ones and bad ones.
Now in 2020, with the Data Science field changing so rapidly, there is no shortage of resources to learn data science. But that also often poses a problem for a beginner as to where to start learning and what to learn? There are a lot of great resources on the internet, but that means there are a lot of bad ones too.
 A lot of choices may often result in stagnation as anxiety is not good when it comes to learning.
 In his book, The Paradox of Choice — Why More Is Less, Schwartz argues that eliminating consumer choices can greatly reduce anxiety for shoppers. And the same remains true for Data Science courses as well.
This post is about providing recommendations to lost souls with a lot of choices on where to start their Data Science Journey.
1) Python 3 Programming Specialization  “GoodBye World” for Python 2.7!!!
 First, you need a programming language. This specialization from the University of Michigan is about learning to use Python and creating things on your own.
You will learn about programming fundamentals like variables, conditionals, and loops, and get to some intermediate material like keyword parameters, list comprehensions, lambda expressions, and class inheritance.
You might also like to go through my Python Shorts posts while going through this specialization.
Python Shorts Posts
2) Applied Data Science with Python  Do first, understand later
 We need to get a taste of Machine Learning before understanding it fully.
This specialization in Applied Data Science with Python gives an intro to many modern machine learning methods that you should know about. Not a thorough grinding, but you will get the tools to build your models.
 This skills-based specialization is intended for learners who have a basic python or programming background, and want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular python toolkits such as pandas, matplotlib, scikit-learn, nltk, and networkx to gain insight into their data.
 You might also like to go through a few of my posts while going through this specialization:
 Minimal Pandas Subset for Data Scientists
 Python’s One Liner graph creation library with animations Hans Rosling Style
 3 Awesome Visualization Techniques for every dataset
  3) Machine Learning Theory and Fundamentals After doing these above courses, you will gain the status of what I would like to call a “Beginner.”
Congrats!!!. You know stuff; you know how to implement things.
 You are Useful
 Yet, you do not fully understand all the math and grind that goes behind all these models.
You need to understand what goes behind the clf.fit. Its time to face the music. Nobody is going to take you seriously till you understand the Math behind your models.
 If you don’t understand it you won’t be able to improve it
 Here comes the Game Changer Machine Learning course. It contains the maths behind many of the Machine Learning algorithms.
I will put this course as the one course you have to take as this course motivated me into getting into this field, and Andrew Ng is a great instructor. Also, this was the first course that I took myself when I started.
This course has a little of everything — Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.
You might also want to go through a few of my posts while going through this course:
 The Hitchhiker’s Guide to Feature Extraction
 The 5 Classification Evaluation metrics every Data Scientist must know
 The 5 Feature Selection Algorithms every Data Scientist should know
 The Simple Math behind 3 Decision Tree Splitting criterions
  4) Learn Statistical Inference  “Facts are stubborn things, but statistics are pliable.”― Mark Twain
 Mine Çetinkaya-Rundel teaches this course on Inferential Statistics. And it cannot get simpler than this one.
She is a great instructor and explains the fundamentals of Statistical inference nicely — a must-take course.
You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
You might also want to go through a few of my posts while going through this specialization:
 P-value Explained Simply for Data Scientists Confidence Intervals Explained Simply for Data Scientists  5) Learn SQL Basics for Data Science  SQL is the heart of all data ETL
 While we feel much more accomplished by creating models and coming up with the different hypotheses, the role of data munging can’t be understated.
And with the ubiquitousness of SQL when it comes to ETL and data preparation tasks, everyone should know a little bit of it to at least be useful.
SQL has also become a de facto standard of working with Big Data Tools like Apache Spark. This SQL specialization from UC Davis will teach you about SQL as well as how to use SQL for distributed computing.
From the Course website:
 Through four progressively more difficult SQL projects with data science applications, you will cover topics such as SQL basics, data wrangling, SQL analysis, AB testing, distributed computing using Apache Spark, and more
 You might also want to go through a few of my posts while going through this specialization:
 Learning SQL the Hard Way The Hitchhikers guide to handle Big Data using Spark 5 Ways to add a new column in a PySpark Dataframe  6) Advanced Machine Learning  In the big leagues, there is no spoonfeeding.
 You might not agree to this, but till now, whatever we have done has been spoonfed learning. The material was structured, and the Math has been minimal. But that has prepared you for the next steps. This Advanced Machine Learning specialization by Top Kaggle machine learning practitioners and CERN scientists takes another approach to learning by going through a lot of difficult concepts and guiding you through how things worked in the past and the most recent advancements in the Machine Learning World. The description on the website says:
 This specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice.
 You might like to look at a few of my posts while trying to understand some of the material in this course.
 MCMC Intuition for Everyone NLP Learning Series  7) Deep Learning  Deep Learning is the Future
 Andrew NG is back again with his new Deep Learning Specialization. And this is Pure Gold.
Andrew Ng has achieved mastery in explaining difficult concepts in an easy to understand way. The nomenclature he follows is different from all other tutorials and courses on the net, and I hope it catches on as it is pretty helpful in understanding all the basic concepts.
From the specialization website:
 Learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing.
 You might like to look at a few of my posts while trying to understand some of the material in this course.
 An End to End Introduction to GANs Object Detection using Deep Learning Approaches: An End to End Theoretical Perspective  8) Pytorch  Python on Fire
 I usually never advocate to learn a tool, but here I do. The reason being that it is incredible and seriously, you will be able to read code in a lot of recent research papers if you understand Pytorch. Pytorch has become a default programming language for researchers working in Deep Learning, and it will only pay for us to learn it.
A structured way to learn Pytorch is by taking this course on Deep Neural Networks with Pytorch. From the course website:
 The course will start with Pytorch’s tensors and Automatic differentiation package. Then each section will cover different models starting off with fundamentals such as Linear Regression, and logistic/softmax regression. Followed by Feedforward deep neural networks, the role of different activation functions, normalization and dropout layers. Then Convolutional Neural Networks and Transfer learning will be covered. Finally, several other Deep learning methods will be covered.
 You might also look at this post of mine, where I try to explain how to work with PyTorch. - Moving from Keras to Pytorch
9) Getting Started with AWS for Machine Learning  The secret: it’s not what you know, it’s what you show.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we, as data scientists, only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.
Having a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.
 This course will teach you: 1. How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance. 2. How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.
 You might also look at this post of mine, where I try to talk about apps and explain how to plan for Production.
 How to write Web apps using simple Python for Data Scientists? How to Deploy a Streamlit App using an Amazon Free ec2 instance? Take your Machine Learning Models to Production with these 5 simple steps  10) Data Structures and Algorithms  Algorithms. Yes, you need them.
 Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are essential all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
They will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.
One of the best resources I found to learn algorithms is the Algorithm Specialization on Coursera by UCSanDiego. From the specialization website:
 You will learn algorithmic techniques for solving various computational problems and will implement about 100 algorithmic coding problems in a programming language of your choice. No other online course in Algorithms even comes close to offering you a wealth of programming challenges that you may face at your next job interview.
 You might also like to look at a few of my posts while trying to understand some of the material in this specialization.
 3 Programming concepts for Data Scientists A simple introduction to Linked Lists for Data Scientists Dynamic Programming for Data Scientists -Handling Trees in Data Science Algorithmic Interview  Continue Learning I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Learning SQL the Hard Way</title>
      <link>https://mlwhiz.com/blog/2020/02/21/sql/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/21/sql/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/sql/main.png"></media:content>
      

      
      <description>A Data Scientist who doesn’t know SQL is not worth his salt
And that seems correct to me in every sense of the world. While we feel much more accomplished creating models and coming up with the different hypotheses, the role of data munging can’t be understated.
And with the ubiquitousness of SQL when it comes to ETL and data preparation tasks, everyone should know a little bit of it to at least be useful.</description>

      <content:encoded>  
        
        <![CDATA[  A Data Scientist who doesn’t know SQL is not worth his salt
And that seems correct to me in every sense of the world. While we feel much more accomplished creating models and coming up with the different hypotheses, the role of data munging can’t be understated.
And with the ubiquitousness of SQL when it comes to ETL and data preparation tasks, everyone should know a little bit of it to at least be useful.
I still remember the first time I got my hands on SQL. It was the first language (if you can call it that) I learned. And it made an impact on me. I was able to automate things, and that was something I hadn’t thought of before.
Before SQL, I used to work with Excel — VLOOKUPs and pivots. I was creating reporting systems, doing the same work again and again. SQL made it all go away. Now I could write a big script, and everything would be automated — all the crosstabs and analysis generated on the fly.
That is the power of SQL. And though you could do anything that you do with SQL using Pandas, you still need to learn SQL to deal with systems like HIVE, Teradata and sometimes Spark too.
This post is about installing SQL, explaining SQL and running SQL.
Setting up the SQL Environment Now the best way to learn SQL is to get your hands dirty with it(Same I can say for any other thing you want to learn)
I will advise against using the web-based recipes like w3schools/tutorialspoint for SQL since you cannot use your data with those.
Also, I will advise you to go with learning the MySQL flavour of SQL as it is Open Source, easy to set up in your laptop and has a great client named MySQL Workbench to make your life easier.
As we have gotten these points out of the way, here is a step by step to get set up with MySQL:
 You can download MySQL for your particular system (MACOSX, Linux, Windows) from Download MySQL Community Server. In my case, I downloaded the DMG Archive. After that, double click and install the file. You might need to set up a password. Remember this password as it would be required to connect to the MySQL instance later.   Create a file named my.cnf and put the following in it. This is needed to give Local file read permissions to your SQL database.   [client] port= 3306 [mysqld] port= 3306 secure_file_priv=&#39;&#39; local-infile=1   Open up System Preferences&amp;gt;MySQL. Go to Configuration and browse to the my.cnf file using the select button.   Restart the server from Instances tab by clicking stop and start.   Once you get that server running, download and install the MySQL Workbench: Download MySQL Workbench. The workbench gives you an editor to write your SQL Queries and get the results in a structured way.   Open up the MySQL workbench now and connect to SQL through it. You will see something like below.   You can see that the Local Instance connection has been set up for you beforehand. Now, you just need to click on that connection and get started using the password that we set up before for the MySQL server(You can also create a connection to an existing SQL server that might not be on your machine if you have the address, port number, username and password).   And you get an editor to write your queries on the particular database.   Check the Schemas tab on the top left to see the tables that are present. There is just a sys schema present with the table sys_config. Not an interesting data source to learn SQL. So let’s install some data to practice.
 If you have your own data to work. Then good and fine. You can create a new schema(database) and upload it into tables using these following commands. (You can run the commands by using Cmd&#43;Enter or by clicking the ⚡️lightning button)
  In this tutorial, however, I am going to use the Sakila Movie database which you can install using the following steps:
 Go to MySQL Documentation and download the Sakila ZIP file.
 Unzip the file.
 Now go to MySQL Workbench and select File&amp;gt;Run SQL Script&amp;gt;select location sakila-db/sakila-schema.sql
 Go to MySQL Workbench and select File &amp;gt;Run SQL Script &amp;gt;select location sakila-db/sakila-data.sql
  Once you do that, you will see a new database added in the SCHEMA list.
Playing with Data Now we have some data with us. Finally.
Let’s start with writing some queries.
You can try to understand the Schema of the Sakila Database in detail using the Sakila Sample Database document.
Schema Diagram
Now the basic syntax of any SQL query is:
SELECT col1, SUM(col2) as col2sum, AVG(col3) as col3avg FROM table_name WHERE col4 = &#39;some_value&#39; GROUP BY col1 ORDER BY col2sum DESC;  There are four elements in this query:
 SELECT: Which Columns to select? Here we choose col1 and do SUM aggregation on col2 and AVG aggregation on col3. We also give a new name to SUM(col2) by using the as keyword. This is known as aliasing.
 FROM: From which table should we SELECT?
 WHERE: We can filter data using WHERE statements.
 GROUP BY: All selected columns that are not in aggregation should be in GROUP BY.
 ORDER BY: Sort on col2sum
  The above query will help you with most of the simple things you want to find in a database.
For example, we can find out how differently censored rated movies are timed differently using:
SELECT rating, avg(length) as length_avg FROM sakila.film group by rating order by length_avg desc;  Exercise: Frame a Question You should now come up with some questions of your own.
For Example, you can try to find out all the movies released in the year 2006. Or try to find all of the movies which have a rating of PG and length greater than 50 minutes.
You can do this by running the following on MySQL Workbench:
select * from sakila.film where release_year = 2006; select * from sakila.film where length&amp;gt;50 and rating=&amp;quot;PG&amp;quot;;  Joins in SQL Till now, we have learned how we can work with single tables. But in reality, we need to work with multiple tables.
So, the next thing we would want to learn is how to do joins.
Now joins are an integral and an essential part of a MySQL Database and understanding them is necessary. The below visual talks about most of the joins that exist in SQL. I usually end up using just the LEFT JOIN, and INNER JOIN, so I will start with LEFT JOIN.
The LEFT JOIN is used when you want to keep all the records in the left table(A) and merge B on the matching records. The records of A where B is not merged are kept as NULL in the resulting table. The MySQL Syntax is:
SELECT A.col1, A.col2, B.col3, B.col4 from A LEFT JOIN B on A.col2=B.col3  Here we select col1 and col2 from table A and col3 and col4 from table B. We also specify which common columns to join on using the ON statement.
The INNER JOIN is used when you want to merge A and B and only to keep the common records in A and B.
Example: To give you a use case lets go back to our Sakila database. Suppose we wanted to find out how many copies of each movie we do have in our inventory. You can get that by using:
SELECT film_id,count(film_id) as num_copies FROM sakila.inventory group by film_id order by num_copies desc;  Does this result look interesting? Not really. IDs don’t make sense to us humans, and if we can get the names of the movies, we would be able to process the information better. So we snoop around and see that the table film has got film_id as well as the title of the film.
So we have all the data, but how do we get it in a single view?
Come Joins to the rescue. We need to add the title to our inventory table information. We can do this using —
SELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id  This will add another column to your inventory table information. As you might notice some films are in the film table that we don’t have in the inventory. We used a left join since we wanted to keep whatever is in the inventory table and join it with its corresponding counterpart in the film table and not everything in the film table.
So now we have got the title as another field in the data. This is just what we wanted, but we haven’t solved the whole puzzle yet. We want title and num_copies of the title in the inventory.
But before we can go any further, we should understand the concept of inner queries first.
Inner Query: Now you have a query that can give you the above result. One thing you can do is create a new table using
create table sakila.temp_table as SELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id;  And then use a simple group by operation using:
select title, count(title) as num_copies from sakila.temp_table group by title order by num_copies desc;  But this is one step too many. And we have to create a temporary table that ends up taking space on the system.
SQL provides us with the concept of the inner query just for these sort of issues. You can instead write all this in a single query using:
select temp.title, count(temp.title) as num_copies from (SELECT A.*, B.title from sakila.inventory A left join sakila.film B on A.film_id = B.film_id) temp group by title order by num_copies desc;  What we did here was sandwich our first query in parenthesis and gave that table an alias temp. We then did the group by operations considering temp just as we would consider any table. It is because of the inner query concept that we can write SQL queries that span multiple pages at some times.
The HAVING Clause HAVING is yet another SQL construct that is useful to understand. So we have got the results, and now we want to get the films whose number of copies are less than or equal to 2.
We can do this by using the inner query concept and the WHERE clause. Here we nest one inner query inside another. Pretty neat.
Or, we can use the HAVING Clause.
The HAVING clause is used to filter on the final aggregated result. It is different from WHERE as where is used to filter the table that is used in the FROM statement. HAVING filters the final result after the GROUP BY happens.
There are a lot of ways to do the same thing with SQL as you have already seen in the above example. We need to try to come up with the least verbose and thus HAVING makes sense in many cases.
If you can follow this far, you already know more SQL than most people.
Next thing to do: Practice.
Try to come up with your questions on your dataset and try to find out the answers you have using SQL.
Some questions I could provide for a start:
 Which Actor has the most distinct films in our inventory?
 Which Genre films are the most rented in our inventory?
  Continue Learning This was just a simple tutorial on how to use SQL. If you want to learn more about SQL, I would like to call out an excellent course on SQL for Data Science from the University of California. Do check it out as it talks about other SQL concepts like UNION, String Manipulation, functions, Date Handling, etc.
I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 most useful Techniques to Handle Imbalanced datasets</title>
      <link>https://mlwhiz.com/blog/2020/01/28/imbal/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/28/imbal/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/imbal/main.png"></media:content>
      

      
      <description>Have you ever faced an issue where you have such a small sample for the positive class in your dataset that the model is unable to learn?
In such cases, you get a pretty high accuracy just by predicting the majority class, but you fail to capture the minority class, which is most often the point of creating the model in the first place.
Such datasets are a pretty common occurrence and are called as an imbalanced dataset.</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever faced an issue where you have such a small sample for the positive class in your dataset that the model is unable to learn?
In such cases, you get a pretty high accuracy just by predicting the majority class, but you fail to capture the minority class, which is most often the point of creating the model in the first place.
Such datasets are a pretty common occurrence and are called as an imbalanced dataset.
 Imbalanced datasets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class
 Imbalanced datasets can be found for different use cases in various domains:
 Finance: Fraud detection datasets commonly have a fraud rate of ~1–2%
 Ad Serving: Click prediction datasets also don’t have a high clickthrough rate.
 Transportation/Airline: Will Airplane failure occur?
 Medical: Does a patient has cancer?
 Content moderation: Does a post contain NSFW content?
  So how do we solve such problems?
This post is about explaining the various techniques you can use to handle imbalanced datasets.
1. Random Undersampling and Oversampling Source
A widely adopted and perhaps the most straightforward method for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).
Let us first create some example imbalanced data.
from sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[&amp;#39;target&amp;#39;] = y We can now do random oversampling and undersampling using:
num_0 = len(X[X[&amp;#39;target&amp;#39;]==0]) num_1 = len(X[X[&amp;#39;target&amp;#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0].sample(num_1) , X[X[&amp;#39;target&amp;#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0] , X[X[&amp;#39;target&amp;#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180  2. Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.
It provides a variety of methods to undersample and oversample.
a. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.
In this algorithm, we end up removing the majority element from the Tomek link, which provides a better decision boundary for a classifier.
Source
from imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=&amp;#39;majority&amp;#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.
Source
from imblearn.over_sampling import SMOTE smote = SMOTE(ratio=&amp;#39;minority&amp;#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.
3. Class weights in the models Most of the machine learning models provide a parameter called class_weights. For example, in a random forest classifier using, class_weights we can specify a higher weight for the minority class using a dictionary.
from sklearn.linear_model import LogisticRegression clf = LogisticRegression(class_weight={0:1,1:10}) But what happens exactly in the background?
In logistic Regression, we calculate loss per example using binary cross-entropy:
Loss = -ylog(p) - (1-y)log(1-p) In this particular form, we give equal weight to both the positive and the negative classes. When we set class_weight as class_weight = {0:1,1:20}, the classifier in the background tries to minimize:
NewLoss = -20ylog(p) - 1(1-y)log(1-p) So what happens exactly here?
 If our model gives a probability of 0.3 and we misclassify a positive example, the NewLoss acquires a value of -20log(0.3) = 10.45
 If our model gives a probability of 0.7 and we misclassify a negative example, the NewLoss acquires a value of -log(0.3) = 0.52
  That means we penalize our model around twenty times more when it misclassifies a positive minority example in this case.
How can we compute class_weights?
There is no one method to do this, and this should be constructed as a hyperparameter search problem for your particular problem.
But if you want to get class_weights using the distribution of the y variable, you can use the following nifty utility from sklearn.
from sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight(&amp;#39;balanced&amp;#39;, np.unique(y), y) 4. Change your Evaluation Metric Choosing the right evaluation metric is pretty essential whenever we work with imbalanced datasets. Generally, in such cases, the F1 Score is what I want as my evaluation metric.
The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.
So how does it help?
Let us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.
So we create a model that predicts “No” for the whole training set.
What is the accuracy(Normally the most used evaluation metric)?
It is more than 99%, and so according to accuracy, this model is pretty good, but it is worthless.
Now, what is the F1 score?
Our precision here is 0. What is the recall of our positive class? It is zero. And hence the F1 score is also 0.
And thus we get to know that the classifier that has an accuracy of 99% is worthless for our case. And hence it solves our problem.
Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low, and if the recall is low again, your F1 score is low.
 If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
 How to Use? You can calculate the F1 score for binary prediction problems using:
from sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred) This is one of the functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.
# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)&amp;gt;thresh) if score &amp;gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh 5. Miscellaneous Various other methods might work depending on your use case and the problem you are trying to solve:
a) Collect more Data This is a definite thing you should try if you can. Getting more data with more positive examples is going to help your models get a more varied perspective of both the majority and minority classes.
b) Treat the problem as anomaly detection You might want to treat your classification problem as an anomaly detection problem.
Anomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data
You can use Isolation forests or autoencoders for anomaly detection.
c) Model-based Some models are particularly suited for imbalanced datasets.
For example, in boosting models, we give more weights to the cases that get misclassified in each tree iteration.
Conclusion There is no one size fits all when working with imbalanced datasets. You will have to try multiple things based on your problem.
In this post, I talked about the usual suspects that come to my mind whenever I face such a problem.
A suggestion would be to try using all of the above approaches and try to see whatever works best for your use case.
If you want to learn more about imbalanced datasets and the problems they pose, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Using Gradient Boosting for Time Series prediction tasks</title>
      <link>https://mlwhiz.com/blog/2019/12/28/timeseries/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/28/timeseries/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/timeseries/main.png"></media:content>
      

      
      <description>Time series prediction problems are pretty frequent in the retail domain.
Companies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.
While there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.</description>

      <content:encoded>  
        
        <![CDATA[  Time series prediction problems are pretty frequent in the retail domain.
Companies like Walmart and Target need to keep track of how much product should be shipped from Distribution Centres to stores. Even a small improvement in such a demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.
While there are many techniques to solve this particular problem like ARIMA, Prophet, and LSTMs, we can also treat such a problem as a regression problem too and use trees to solve it.
In this post, we will try to solve the time series problem using XGBoost.
The main things I am going to focus on are the sort of features such a setup takes and how to create such features.
Dataset Kaggle master Kazanova along with some of his friends released a “How to win a data science competition” Coursera course. The Course involved a final project which itself was a time series prediction problem.
In this competition, we are given a challenging time-series dataset consisting of daily sales data, provided by one of the largest Russian software firms — 1C Company.
We have to predict total sales for every product and store in the next month.
Here is how the data looks like:
We are given the data at a daily level, and we want to build a model which predicts total sales for every product and store in the next month.
The variable date_block_num is a consecutive month number, used for convenience. January 2013 is 0, and October 2015 is 33. You can think of it as a proxy to month variable. I think all the other variables are self-explanatory.
So how do we approach this sort of a problem?
Data Preparation The main thing that I noticed is that the data preparation and feature generation aspect is by far the most important thing when we attempt to solve the time series problem using regression.
1. Do Basic EDA and remove outliers sales = sales[sales[&amp;#39;item_price&amp;#39;]&amp;lt;100000] sales = sales[sales[&amp;#39;item_cnt_day&amp;#39;]&amp;lt;=1000] 2. Group data at a level you want your predictions to be: We start with creating a dataframe of distinct date_block_num, store and item combinations.
This is important because in the months we don’t have a data for an item store combination, the machine learning algorithm needs to be told explicitly that the sales are zero.
from itertools import product # Create &amp;#34;grid&amp;#34; with columns index_cols = [&amp;#39;shop_id&amp;#39;, &amp;#39;item_id&amp;#39;, &amp;#39;date_block_num&amp;#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&amp;#39;date_block_num&amp;#39;].unique(): cur_shops = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;shop_id&amp;#39;].unique() cur_items = sales.loc[sales[&amp;#39;date_block_num&amp;#39;] == block_num, &amp;#39;item_id&amp;#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&amp;#39;int32&amp;#39;)) grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) grid.head() The grid dataFrame contains all the shop, items and month combinations.
We then merge the Grid with Sales to get the monthly sales DataFrame. We also replace all the NA’s with zero for months that didn’t have any sales.
sales_m = sales.groupby([&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]).agg({&amp;#39;item_cnt_day&amp;#39;: &amp;#39;sum&amp;#39;,&amp;#39;item_price&amp;#39;: np.mean}).reset_index() # Merging sales numbers with the grid dataframe sales_m = pd.merge(grid,sales_m,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;).fillna(0) # adding the category id too from the items table. sales_m = pd.merge(sales_m,items,on=[&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;) 3. Create Target Encodings To create target encodings, we group by a particular column and take the mean/min/sum etc. of the target column on it. These features are the first features we create in our model.
Please note that these features may induce a lot of leakage/overfitting in our system and thus we don’t use them directly in our models. We will use the lag based version of these features in our models which we will create next.
groupcollist = [&amp;#39;item_id&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_category_id&amp;#39;] aggregationlist = [(&amp;#39;item_price&amp;#39;,np.mean,&amp;#39;avg&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.sum,&amp;#39;sum&amp;#39;),(&amp;#39;item_cnt_day&amp;#39;,np.mean,&amp;#39;avg&amp;#39;)] for type_id in groupcollist: for column_id,aggregator,aggtype in aggregationlist: # get numbers from sales data and set column names mean_df = sales_m.groupby([type_id,&amp;#39;date_block_num&amp;#39;]).aggregate(aggregator).reset_index()[[column_id,type_id,&amp;#39;date_block_num&amp;#39;]] mean_df.columns = [type_id&#43;&amp;#39;_&amp;#39;&#43;aggtype&#43;&amp;#39;_&amp;#39;&#43;column_id,type_id,&amp;#39;date_block_num&amp;#39;] # merge new columns on sales_m data sales_m = pd.merge(sales_m,mean_df,on=[&amp;#39;date_block_num&amp;#39;,type_id],how=&amp;#39;left&amp;#39;) We group by item_id, shop_id, and item_category_id and aggregate on the item_price and item_cnt_day column to create the following new features:
We could also have used featuretools for this. Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.
4. Create Lag Features The next set of features our model needs are the lag based Features.
When we create regular classification models, we treat training examples as fairly independent of each other. But in case of time series problems, at any point in time, the model needs information on what happened in the past.
We can’t do this for all the past days, but we can provide the models with the most recent information nonetheless using our target encoded features.
lag_variables = [&amp;#39;item_id_avg_item_price&amp;#39;,&amp;#39;item_id_sum_item_cnt_day&amp;#39;,&amp;#39;item_id_avg_item_cnt_day&amp;#39;,&amp;#39;shop_id_avg_item_price&amp;#39;,&amp;#39;shop_id_sum_item_cnt_day&amp;#39;,&amp;#39;shop_id_avg_item_cnt_day&amp;#39;,&amp;#39;item_category_id_avg_item_price&amp;#39;,&amp;#39;item_category_id_sum_item_cnt_day&amp;#39;,&amp;#39;item_category_id_avg_item_cnt_day&amp;#39;,&amp;#39;item_cnt_day&amp;#39;] lags = [1 ,2 ,3 ,4, 5, 12] # we will keep the results in thsi dataframe sales_means = sales_m.copy() for lag in lags: sales_new_df = sales_m.copy() sales_new_df.date_block_num&#43;=lag # subset only the lag variables we want sales_new_df = sales_new_df[[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43;lag_variables] sales_new_df.columns = [&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;]&#43; [lag_feat&#43;&amp;#39;_lag_&amp;#39;&#43;str(lag) for lag_feat in lag_variables] # join with date_block_num,shop_id and item_id sales_means = pd.merge(sales_means, sales_new_df,on=[&amp;#39;date_block_num&amp;#39;,&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;] ,how=&amp;#39;left&amp;#39;) So we aim to add past information for a few features in our data. We do it for all the new features we created and the item_cnt_day feature.
We fill the NA’s with zeros once we have the lag features.
for feat in sales_means.columns: if &amp;#39;item_cnt&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(0) elif &amp;#39;item_price&amp;#39; in feat: sales_means[feat]=sales_means[feat].fillna(sales_means[feat].median()) We end up creating a lot of lag features with different lags:
&#39;item_id_avg_item_price_lag_1&#39;,&#39;item_id_sum_item_cnt_day_lag_1&#39;, &#39;item_id_avg_item_cnt_day_lag_1&#39;,&#39;shop_id_avg_item_price_lag_1&#39;, &#39;shop_id_sum_item_cnt_day_lag_1&#39;,&#39;shop_id_avg_item_cnt_day_lag_1&#39;,&#39;item_category_id_avg_item_price_lag_1&#39;,&#39;item_category_id_sum_item_cnt_day_lag_1&#39;,&#39;item_category_id_avg_item_cnt_day_lag_1&#39;, &#39;item_cnt_day_lag_1&#39;, &#39;item_id_avg_item_price_lag_2&#39;, &#39;item_id_sum_item_cnt_day_lag_2&#39;,&#39;item_id_avg_item_cnt_day_lag_2&#39;, &#39;shop_id_avg_item_price_lag_2&#39;,&#39;shop_id_sum_item_cnt_day_lag_2&#39;, &#39;shop_id_avg_item_cnt_day_lag_2&#39;,&#39;item_category_id_avg_item_price_lag_2&#39;,&#39;item_category_id_sum_item_cnt_day_lag_2&#39;,&#39;item_category_id_avg_item_cnt_day_lag_2&#39;, &#39;item_cnt_day_lag_2&#39;, ...  Modelling 1. Drop the unrequired columns As previously said, we are going to drop the target encoded features as they might induce a lot of overfitting in the model. We also lose the item_name and item_price feature.
cols_to_drop = lag_variables[:-1] &#43; [&amp;#39;item_name&amp;#39;,&amp;#39;item_price&amp;#39;] for col in cols_to_drop: del sales_means[col] 2. Take a recent bit of data only When we created the lag variables, we induced a lot of zeroes in the system. We used the maximum lag as 12. To counter that we remove the first 12 months indexes.
sales_means = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;gt;11] 3. Train and CV Split When we do a time series split, we usually don’t take a cross-sectional split as the data is time-dependent. We want to create a model that sees till now and can predict the next month well.
X_train = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]&amp;lt;33] X_cv = sales_means[sales_means[&amp;#39;date_block_num&amp;#39;]==33] Y_train = X_train[&amp;#39;item_cnt_day&amp;#39;] Y_cv = X_cv[&amp;#39;item_cnt_day&amp;#39;] del X_train[&amp;#39;item_cnt_day&amp;#39;] del X_cv[&amp;#39;item_cnt_day&amp;#39;] 4. Create Baseline Before we proceed with modelling steps, lets check the RMSE of a naive model, as we want to have an RMSE to compare to. We assume that we are going to predict the last month sales as current month sale for our baseline model. We can quantify the performance of our model using this baseline RMSE.
from sklearn.metrics import mean_squared_error sales_m_test = sales_m[sales_m[&amp;#39;date_block_num&amp;#39;]==33] preds = sales_m.copy() preds[&amp;#39;date_block_num&amp;#39;]=preds[&amp;#39;date_block_num&amp;#39;]&#43;1 preds = preds[preds[&amp;#39;date_block_num&amp;#39;]==33] preds = preds.rename(columns={&amp;#39;item_cnt_day&amp;#39;:&amp;#39;preds_item_cnt_day&amp;#39;}) preds = pd.merge(sales_m_test,preds,on = [&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;],how=&amp;#39;left&amp;#39;)[[&amp;#39;shop_id&amp;#39;,&amp;#39;item_id&amp;#39;,&amp;#39;preds_item_cnt_day&amp;#39;,&amp;#39;item_cnt_day&amp;#39;]].fillna(0) # We want our predictions clipped at (0,20). Competition Specific preds[&amp;#39;item_cnt_day&amp;#39;] = preds[&amp;#39;item_cnt_day&amp;#39;].clip(0,20) preds[&amp;#39;preds_item_cnt_day&amp;#39;] = preds[&amp;#39;preds_item_cnt_day&amp;#39;].clip(0,20) baseline_rmse = np.sqrt(mean_squared_error(preds[&amp;#39;item_cnt_day&amp;#39;],preds[&amp;#39;preds_item_cnt_day&amp;#39;])) print(baseline_rmse) 1.1358170090812756  5. Train XGB We use the XGBRegressor object from the xgboost scikit API to build our model. Parameters are taken from this kaggle kernel. If you have time, you can use hyperopt to automatically find out the hyperparameters yourself.
from xgboost import XGBRegressor model = XGBRegressor( max_depth=8, n_estimators=1000, min_child_weight=300, colsample_bytree=0.8, subsample=0.8, eta=0.3, seed=42) model.fit( X_train, Y_train, eval_metric=&amp;#34;rmse&amp;#34;, eval_set=[(X_train, Y_train), (X_cv, Y_cv)], verbose=True, early_stopping_rounds = 10) After running this, we can see RMSE in ranges of 0.93 on the CV set. And that is pretty impressive based on our baseline validation RMSE of 1.13. And so we work on deploying this model as part of our continuous integration effort.
5. Plot Feature Importance We can also see the important features that come from XGB.
feature_importances = pd.DataFrame({&amp;#39;col&amp;#39;: columns,&amp;#39;imp&amp;#39;:model.feature_importances_}) feature_importances = feature_importances.sort_values(by=&amp;#39;imp&amp;#39;,ascending=False) px.bar(feature_importances,x=&amp;#39;col&amp;#39;,y=&amp;#39;imp&amp;#39;) Conclusion In this post, we talked about how we can use trees for even time series modelling. The purpose was not to get perfect scores on the kaggle leaderboard but to gain an understanding of how such models work.
When I took part in this competition as part of the course, a couple of years back, using trees I reached near the top of the leaderboard.
Over time people have worked a lot on tweaking the model, hyperparameter tuning and creating even more informative features. But the basic approach has remained the same.
You can find the whole running code on GitHub.
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of ways to improve your models using feature engineering and hyperparameter tuning.
I am going to be writing more beginner-friendly posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Take your Machine Learning Models to Production with these 5 simple steps</title>
      <link>https://mlwhiz.com/blog/2019/12/25/prod/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/25/prod/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/prod/main.png"></media:content>
      

      
      <description>Creating a great machine learning system is an art.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.</description>

      <content:encoded>  
        
        <![CDATA[   Creating a great machine learning system is an art.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.
This post is about the process requirements for a successful ML project — One that goes to production.
1. Establish a Baseline at the onset You don’t really have to have a model to get the baseline results.
Let us say we will be using RMSE as an evaluation metric for our time series models. We evaluated the model on the test set, and the RMSE came out to be 3.64.
Is 3.64 a good RMSE? How do we know? We need a baseline RMSE.
This could come from a currently employed model for the same task. Or by using some very simple heuristic. For a Time series model, a baseline to defeat is last day prediction. i.e., predict the number on the previous day.
Or how about Image classification task. Take 1000 labelled samples and get them classified by humans. And Human accuracy can be your Baseline. If a human is not able to get a 70% prediction accuracy on the task, you can always think of automating a process if your models reach a similar level.
Learning: Try to be aware of the results you are going to get before you create your models. Setting some out of the world expectations is only going to disappoint you and your client.
2. Continuous Integration is the Way Forward You have created your model now. It performs better than the baseline/your current model on your local test dataset. Should we go forward?
We have two choices-
 Go into an endless loop in improving our model further.
 Test our model in production settings, get more insights about what could go wrong and then continue improving our model with continuous integration.
  I am a fan of the second approach. In his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization, Andrew Ng says —
 “Don’t start off trying to design and build the perfect system. Instead, build and train a basic system quickly — perhaps in just a few days. Even if the basic system is far from the “best” system you can build, it is valuable to examine how the basic system functions: you will quickly find clues that show you the most promising directions in which to invest your time.”
 Done is better than perfect.
Learning: If your new model is better than the current model in production or your new model is better than the baseline, it doesn’t make sense to wait to go to production.
3. Your model might break into Production Is your model better than the Baseline? It performed better on the local test dataset, but will it really work well on the whole?
To test the validity of your assumption that your model is better than the existing model, you can set up an A/B test. Some users(Test group)see predictions from your model while some users(Control) see the predictions from the previous model.
In fact, this is the right way to deploy your model. And you might find that indeed your model is not as good as it seems.
 Being wrong is not wrong really, what’s wrong is to not anticipate that we could be wrong.
 It is hard to point out the real reason behind why your model performs poorly in production settings, but some causes could be:
 You might see the data coming in real-time to be significantly different from the training data.
 Or you have not done the preprocessing pipeline correctly.
 Or you do not measure the performance correctly.
 Or maybe there is a bug in your implementation.
  Learning: Don’t go into production with a full scale. A/B test is always an excellent way to go forward. Have something ready to fall back upon(An older model perhaps). There might always be things that might break, which you couldn’t have anticipated.
4. Your model might not even go to Production I have created this impressive ML model, it gives 90% accuracy, but it takes around 10 seconds to fetch a prediction.
Is that acceptable? For some use-cases maybe, but really no.
In the past, there have been many Kaggle competitions whose winners ended up creating monster ensembles to take the top spots on the leaderboard. Below is a particular mindblowing example model which was used to win Otto classification challenge on Kaggle:
Another example is the Netflix Million dollar Recommendation Engine Challenge. The Netflix team ended up never using the wining solution due to the engineering costs involved.
So how do you make your models accurate yet easy on the machine?
Teacher — Student Model: Source
Here comes the concept of Teacher-Student models or Knowledge distillation. In Knowledge distillation, we train a smaller student model on a bigger already trained teacher model.
Here we use the soft labels/probabilities from the teacher model and use it as the training data for the Student model.
 The point is that the teacher is outputting class probabilities — “soft labels” rather than “hard labels”. For example, a fruit classifier might say “Apple 0.9, Pear 0.1” instead of “Apple 1.0, Pear 0.0” . Why bother? Because these “soft labels” are more informative than the original ones — telling the student that yes, a particular apple does very slightly resemble a pear. Student models can often come very close to teacher-level performance, even while using 1–2 orders of magnitude fewer parameters! — Source
 Learning: Sometimes, we don’t have a lot of compute available at prediction time, and so we want to have a lighter model. We can try to build simpler models or try using knowledge distillation for such use cases.
5. Maintainance and Feedback Loop  The world is not constant and so are your model weights
 The world around us is rapidly changing, and what might be applicable two months back might not be relevant now. In a way, the models we build are reflections of the world, and if the world is changing our models should be able to reflect this change.
Model performance deteriorates typically with time.
For this reason, we must think of ways to upgrade our models as part of the maintenance cycle at the onset itself.
The frequency of this cycle depends entirely on the business problem that you are trying to solve. In an Ad prediction system where the users tend to be fickle and buying patterns emerge continuously, the frequency needs to be pretty high. While in a review sentiment analysis system, the frequency need not be that high as language doesn’t change its structure quite so much.
Feedback Loop: Source
I would also like to acknowledge the importance of the feedback loop in a machine learning system. Let’s say that you predicted that a particular image is a dog with low probability in a dog vs cat classifier. Can we learn something from these low confidence examples? You can send it to manual review to check if it could be used to retrain the model or not. In this way, we train our classifier on instances it is unsure about.
Learning: When thinking of production, come up with a plan to maintain and improve the model using feedback as well.
Conclusion These are some of the things I find important before thinking of putting a model into production.
While this is not an exhaustive list of things that you need to think about and things that could go wrong, it might undoubtedly act as food for thought for the next time you create your machine learning system.
If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to write Web apps using simple Python for Data Scientists?</title>
      <link>https://mlwhiz.com/blog/2019/12/07/streamlit/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/07/streamlit/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlit/main.jpeg"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
So, are we doomed to learn web frameworks? Or to call our developer friend for silly doubts in the middle of the night?
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
 Zen of Python: Simple is better than complex and Streamlit makes it dead simple to create apps.
 This post is about understanding how to create apps that support data science projects using Streamlit.
To understand more about the architecture and the thought process that led to streamlit, have a look at this excellent post by one of the original developers/founder Adrien Treuille.
Installation Installation is as simple as running the command:
pip install streamlit
To see if our installation is successful, we can just run:
streamlit hello
This should show you a screen that says:
You can go to the local URL: localhost:8501 in your browser to see a Streamlit app in action. The developers have provided some cool demos that you can play with. Do take your time and feel the power of the tool before coming back.
Streamlit Hello World Streamlit aims to make app development easy using simple Python.
So let us write a simple app to see if it delivers on that promise.
Here I start with a simple app which we will call the Hello World of streamlit. Just paste the code given below in a file named helloworld.py
import streamlit as st x = st.slider(&amp;#39;x&amp;#39;) st.write(x, &amp;#39;squared is&amp;#39;, x * x) And, on the terminal run:
streamlit run helloworld.py And voila, you should be able to see a simple app in action in your browser at localhost:8501 that allows you to move a slider and gives the result.
It was pretty easy. In the above app, we used two features from Streamlit:
 the st.slider widget that we can slide to change the output of the web app.
 and the versatile st.write command. I am amazed at how it can write anything from charts, dataframes, and simple text. More on this later.
  Important: Remember that every time we change the widget value, the whole app runs from top to bottom.
Streamlit Widgets Widgets provide us a way to control our app. The best place to read about the widgets is the API reference documentation itself but I will describe some most prominent ones that you might end up using.
1. Slider streamlit.slider(label, min_value=None, max_value=None, value=None, step=None, format=None) We already saw st.slider in action above. It can be used with min_value,max_value, and step for getting inputs in a range.
2. Text Input The simplest way to get user input be it some URL input or some text input for sentiment analysis. It just needs a single label for naming the textbox.
import streamlit as st url = st.text_input(&amp;#39;Enter URL&amp;#39;) st.write(&amp;#39;The Entered URL is&amp;#39;, url) This is how the app looks:
Tip: You can just change the file helloworld.py and refresh the browser. The way I work is to open and change helloworld.py in sublime text and see the changes in the browser side by side.
3. Checkbox One use case for checkboxes is to hide or show/hide a specific section in an app. Another could be setting up a boolean value in the parameters for a function.st.checkbox() takes a single argument, which is the widget label. In this app, the checkbox is used to toggle a conditional statement.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) if st.checkbox(&amp;#39;Show dataframe&amp;#39;): st.write(df) 4. SelectBox We can use st.selectbox to choose from a series or a list. Normally a use case is to use it as a simple dropdown to select values from a list.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) option = st.selectbox( &amp;#39;Which Club do you like best?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, option) 5. MultiSelect We can also use multiple values from a dropdown. Here we use st.multiselect to get multiple values as a list in the variable options
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) options = st.multiselect( &amp;#39;What are your favorite clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, options) Creating Our Simple App Step by Step So much for understanding the important widgets. Now, we are going to create a simple app using multiple widgets at once.
To start simple, we will try to visualize our football data using streamlit. It is pretty much simple to do this with the help of the above widgets.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) # Filter dataframe new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] # write dataframe to screen st.write(new_df) Our simple app looks like:
That was easy. But it seems pretty basic right now. Can we add some charts?
Streamlit currently supports many libraries for plotting.Plotly, Bokeh, Matplotlib, Altair, and Vega charts being some of them. Plotly Express also works, although they didn’t specify it in the docs. It also has some inbuilt chart types that are “native” to Streamlit, like st.line_chart and st.area_chart.
We will work with plotly_express here. Here is the code for our simple app. We just used four calls to streamlit. Rest is all simple python.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # create figure using plotly express fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) Improvements In the start we said that each time we change any widget, the whole app runs from start to end. This is not feasible when we create apps that will serve deep learning models or complicated machine learning models. Streamlit covers us in this aspect by introducing Caching.
1. Caching In our simple app. We read the pandas dataframe again and again whenever a value changes. While it works for the small data we have, it will not work for big data or when we have to do a lot of processing on the data. Let us use caching using the st.cache decorator function in streamlit like below.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) Or for more complex and time taking functions that need to run only once(think loading big Deep Learning models), using:
@st.cache def complex_func(a,b): DO SOMETHING COMPLEX # Won&amp;#39;t run again and again. complex_func(a,b) When we mark a function with Streamlit’s cache decorator, whenever the function is called streamlit checks the input parameters that you called the function with.
If this is the first time Streamlit has seen these params, it runs the function and stores the result in a local cache.
When the function is called the next time, if those params have not changed, Streamlit knows it can skip executing the function altogether. It just uses the results from the cache.
2. Sidebar For a cleaner look based on your preference, you might want to move your widgets into a sidebar, something like Rshiny dashboards. This is pretty simple. Just add st.sidebar in your widget’s code.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) 3. Markdown? I love writing in Markdown. I find it less verbose than HTML and much more suited for data science work. So, can we use Markdown with the streamlit app?
Yes, we can. There are a couple of ways to do this. In my view, the best one is to use Magic commands. Magic commands allow you to write markdown as easily as comments. You could also have used the command st.markdown
import streamlit as st import pandas as pd import numpy as np import plotly_express as px &amp;#39;&amp;#39;&amp;#39; # Club and Nationality App This very simple webapp allows you to select and visualize players from certain clubs and certain nationalities. &amp;#39;&amp;#39;&amp;#39; df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) &amp;#39;&amp;#39;&amp;#39; ### Here is a simple chart between player age and overall &amp;#39;&amp;#39;&amp;#39; st.plotly_chart(fig) Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more.
In this post, we created a simple web app. But the possibilities are endless. To give an example here is face GAN from the streamlit site. And it works by just using the same guiding ideas of widgets and caching.
I love the default colors and styles that the developers have used, and I found it much more comfortable than using Dash, which I was using until now for my demos. You can also include audio and video in your streamlit apps.
On top of that, Streamlit is a free and open-source rather than a proprietary web app that just works out of the box.
In the past, I had to reach out to my developer friends for any single change in a demo or presentation; now it is relatively trivial to do that.
 I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
 I don’t have an idea if it will perform well in a production environment yet, but its a boon for the small proof of concept projects and demos. I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
You can find the full code for the final app here.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Demystifying Object Detection and Instance Segmentation for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/12/05/od/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/05/od/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/od/main.jpeg"></media:content>
      

      
      <description>I like deep learning a lot but Object Detection is something that doesn’t come easily to me.
And Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.
It is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.</description>

      <content:encoded>  
        
        <![CDATA[  I like deep learning a lot but Object Detection is something that doesn’t come easily to me.
And Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.
It is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.
This post is about distilling that history into an easy explanation and explaining the gory details for Object Detection and Instance Segmentation.
Introduction We all know about the image classification problem. Given an image can you find out the class the image belongs to?
We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets.
 ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.
 But there are lots of other interesting problems in the Image domain:
Source
These problems can be divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:
 Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class?
 Classification&#43;Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number of objects(commonly 1) in the image.
 Object Detection: A More general case of the Classification&#43;Localization problem. In a real-world setting, we don’t know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them?
 Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won’t be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.
  As you can see all the problems have something of a similar flavour but a little different than each other. In this post, I will focus mainly on Object Detection and Instance segmentation as they are the most interesting.I will go through the 4 most famous techniques for object detection and how they improved with time and new ideas.
Classification&#43;Localization So lets first try to understand how we can solve the problem when we have a single object in the image. How to solve the Classification&#43;Localization case.
 💡 Treat localization as a regression problem!
 Input Data Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting, we used to have data in the form (X,y) where X is the image and y used to be the class label.
In the Classification&#43;Localization setting, we will have data normally in the form (X,y), where X is still the image and y is an array containing (class_label, x,y,w,h) where,
x = bounding box top left corner x-coordinate
y = bounding box top left corner y-coordinate
w = width of the bounding box in pixels
h = height of the bounding box in pixels
Model So in this setting, we create a multi-output model which takes an image as the input and has (n_labels &#43; 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).
Loss Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).
 Loss = alpha*Softmax_Loss &#43; (1-alpha)*L2_Loss
 Since these two losses would be on a different scale, the alpha hyper-parameter is something that needs to be tuned.
There is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function.
And herein lies the essence of the whole Deep Learning framework — Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem. And that is what we are going to see a lot going forward.
Object Detection So how does this idea of localization using regression get mapped to Object Detection? It doesn’t.
We don’t have a fixed number of objects. So we can’t have 4 outputs denoting, the bounding box coordinates.
One naive idea could be to apply CNN to many different crops of the image. CNN classifies each crop as an object class or background class. This is intractable. There could be a lot of such crops that you can create.
Region Proposals: So, if just there was just a method(Normally called Region Proposal Network)which could find some smaller number of cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is the basic idea behind RCNN-The first major success in object detection.
And that is what selective search (Uijlings et al, “Selective Search for Object Recognition”, IJCV 2013) provided.
So what are Region Proposals?
 Find “blobby” image regions that are likely to contain objects
 Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU
  So, how exactly the region proposals are made?
Selective Search for Object Recognition: This paper finds regions in two steps.
First, we start with a set of some initial regions using Efficient GraphBased Image Segmentation.
 Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels.
 In this paper they take an approach:
 Each edge (vi , vj )∈ E has a corresponding weight w((vi , vj )), which is a non-negative measure of the similarity between neighboring elements vi and vj . In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ∈ S corresponds to a connected component in a graph.
 Put simply, they use graph-based methods to find connected components in an image and the edges are made on some measure of similarity between pixels.
As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. And that is the second step.
For that, the authors of Selective Search for Object Recognition apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm, they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill to provide us with much better region proposals.
1. R-CNN So now we have our region proposals. How do we exactly use them in R-CNN?
 Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.
 Along with this, the authors have also used a class-specific bounding box regressor, that takes:
Input : (Px, Py, Ph, Pw) — the location of the proposed region.
Target: (Gx, Gy, Gh, Gw) — Ground truth labels for the region.
The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G)
Training R-CNN What is the input to an RCNN?
So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. We train class-specific SVM’s
So every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class-specific data.
Test Time R-CNN At test time we predict detection boxes using class-specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Thus, non-maximum suppression is an integral part of the object detection pipeline.
First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed.
This process is recursively applied on all the remaining boxes until we are left with good bounding boxes only.
Problems with RCNN:  Training is slow.
 Inference (detection) is slow. 47s / image with VGG16 — Since the Convnet needs to be run many times.
  Need for speed. So Fast R-CNN.
2. Fast R-CNN  💡 So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM/classifier to classify those crops.
 Something like:
 From Paper: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.
 💡Idea So the basic idea is to have to run the convolution only once in the image rather than so many convolution networks in R-CNN. Then we can map the ROI proposals using some method and filter the last convolution layer and just run a final classifier on that.
This idea depends a little upon the architecture of the model that gets used too.
So the architecture that the authors have proposed is:
 We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net’s first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network’s last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K &#43; 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.
 Don’t worry if you don’t understand the above. This obviously is a little confusing, so let us break this down. But for that, we need to see VGG16 architecture first.
The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).
Now the location of ROI coordinates is in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512.
As we are using VGG, we have transformed the image (224 x 224 x 3) into (14 x 14 x 512) — i.e. the height and width are divided by 16. We can map ROIs coordinates onto the feature map just by dividing them by 16.
 In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the “things” it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.
 What is ROI pooling?
Remember that the final classifier runs for each crop. And so each crop needs to be of the same size. And that is what ROI Pooling does.
In the above image, our region proposal is (0,3,5,7) in x,y,w,h format.
We divide that area into 4 regions since we want to have an ROI pooling layer of 2x2. We divide the whole area into buckets by rounding 5&amp;frasl;2 and 7&amp;frasl;2 and then just do a max-pool.
How do you do ROI-Pooling on Areas smaller than the target size? if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.
After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K &#43; 1 categories(&#43;1 for Background) and category-specific bounding-box regressors.
Training Fast-RCNN What is the input to a Fast- RCNN?
Pretty much similar to R-CNN: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. This time we have a dense layer on top, and we use multi-task loss.
So every ROI becomes a training example. The main difference is that there is a concept of multi-task loss:
A Fast R-CNN network has two sibling output layers.
The first outputs a discrete probability distribution (per RoI), p = (p0, &amp;hellip; , pK), over K &#43; 1 categories. As usual, p is computed by a softmax over the K&#43;1 outputs of a fully connected layer.
The second sibling layer outputs bounding-box regression offsets, t= (tx, ty, tw, th), for each of the K object classes. Each training RoI is labelled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labelled RoI to jointly train for classification and bounding-box regression
Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class.
Problem: Region proposals are still taking up most of the time. Can we reduce the time taken for Region proposals?
3. Faster-RCNN The next question that got asked was: Can the network itself do region proposals?
 The intuition is that: With FastRCNN we’re already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes &#43; bbox coordinates?
 How does the Region Proposal Network work? One of the main ideas in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.
So, first of all, we define anchor centres on the image.
The anchor centres are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224&amp;frasl;14).
This is how anchors look like:
 So we start with some predefined regions we think our objects could be with Anchors.
 Our Region Proposal Network(RPN) classifies which regions have the object and the offset of the object bounding box. Training is done using the same logic. 1 if IOU for anchor with bounding box&amp;gt;0.5 0 otherwise.
 Non-Maximum suppression to reduce region proposals
 Fast RCNN detection network on top of proposals
  Faster-RCNN Loss The whole network is then jointly trained with 4 losses:
 RPN classify object / not object
 RPN regress box coordinates offset
 Final classification score (object classes)
 Final box coordinates offset
  Performance Instance Segmentation Now comes the most interesting part — Instance segmentation. Can we create masks for each individual object in the image? Specifically something like:
Mask-RCNN The same authors come to rescue again. The basic idea is to add another output layer that predicts the mask. And to use ROIAlign instead of ROIPooling.
Source: Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign
Mask R-CNN adopts the same two-stage procedure, with an identical first stage (RPN).
In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.
ROIAlign vs ROIPooling In ROI pooling we lose the exact location-based information. See how we arbitrarily divided our region into 4 different sized boxes. For a classification task, it works well.
But for providing masks on a pixel level, we don’t want to lose this information. And hence we don’t quantize the pooling layer and use bilinear interpolation to find out values that properly aligns the extracted features with the input. See how 0.8 differs from 0.88
Source
Training During training, we define a multi-task loss on each sampled RoI as
L = Lcls &#43; Lbox &#43; Lmask
The classification loss Lcls and bounding-box loss Lbox are identical as in Faster R-CNN. The mask branch has a K × m × m — dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes.
To this, we apply a per-pixel sigmoid and define Lmask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, Lmask is only defined on the kth mask (other mask outputs do not contribute to the loss).
Mask Prediction The mask layer is K × m × m dimensional where K is the number of classes. The m×m floating-number mask output is resized to the RoI size and binarized at a threshold of 0.5 to get final masks.
Conclusion Congrats for reaching the end. This post was a long one.
In this post, I talked about some of the most important advancements in the field of Object detection and Instance segmentation and tried to explain them as easily as I can.
This is my own understanding of these papers with inputs from many blogs and slides on the internet and I sincerely thank the creators. Let me know if you find something wrong with my understanding.
Object detection is a vast field and there are a lot of other methods that dominate this field. Some of them being U-net, SSD and YOLO.
There is no dearth of resources to learn them so I would encourage you to go and take a look at them. You have got a solid backing/understanding now.
In this post, I didn’t write about coding and implementation. So stay tuned for my next post in which we will train a Mask RCNN model for a custom dataset.
If you want to know more about various Object Detection techniques, motion estimation, object tracking in video etc., I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Classification Evaluation metrics every Data Scientist must know</title>
      <link>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/eval/main.jpeg"></media:content>
      

      
      <description>What do we want to optimize for? Most of the businesses fail to answer this simple question.
Every business problem is a little different, and it should be optimized differently.
We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?
What if we are predicting the number of asteroids that will hit the earth.</description>

      <content:encoded>  
        
        <![CDATA[  What do we want to optimize for? Most of the businesses fail to answer this simple question.
Every business problem is a little different, and it should be optimized differently.
We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?
What if we are predicting the number of asteroids that will hit the earth.
Just say zero all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable. What should we do in such cases?
 Designing a Data Science project is much more important than the modeling itself.
 This post is about various evaluation metrics and how and when to use them.
1. Accuracy, Precision, and Recall: A. Accuracy Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.
Accuracy = (TP&#43;TN)/(TP&#43;FP&#43;FN&#43;TN)
Accuracy is the proportion of true results among the total number of cases examined.
When to use? Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.
Caveats Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.
B. Precision Let’s start with precision, which answers the following question: what proportion of predicted Positives is truly Positive?
Precision = (TP)/(TP&#43;FP)
In the asteroid prediction problem, we never predicted a true positive.
And thus precision=0
When to use? Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.
Caveats Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money.
C. Recall Another very useful measure is recall, which answers a different question: what proportion of actual Positives is correctly classified?
Recall = (TP)/(TP&#43;FN)
In the asteroid prediction problem, we never predicted a true positive.
And thus recall is also equal to 0.
When to use? Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.
Caveats Recall is 1 if we predict 1 for all examples.
And thus comes the idea of utilizing tradeoff of precision vs. recall — F1 Score.
2. F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.
The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.
Let us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.
So if we say “No” for the whole training set. Our precision here is 0. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.
And hence the F1 score is also 0. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.
When to use? We want to have a model with both good precision and recall.
Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low. &amp;gt; # If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
How to Use? You can calculate the F1 score for binary prediction problems using:
from sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred) This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.
# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)&amp;gt;thresh) if score &amp;gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh Caveats The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.
To solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.
Here we give β times as much importance to recall as precision.
from sklearn.metrics import fbeta_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] fbeta_score(y_true, y_pred,beta=0.5) F1 Score can also be used for Multiclass problems. See this awesome blog post by Boaz Shmueli for details.
3. Log Loss/Binary Crossentropy Log loss is a pretty good evaluation metric for binary classifiers and it is sometimes the optimization objective as well in case of Logistic regression and Neural Networks.
Binary Log loss for an example is given by the below formula where p is the probability of predicting 1.
As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1.
When to Use? When the output of a classifier is prediction probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view of the performance of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.
How to Use? from sklearn.metrics import log_loss # where y_pred are probabilities and y_true are binary class labels log_loss(y_true, y_pred, eps=1e-15) Caveats It is susceptible in case of imbalanced datasets. You might have to introduce class weights to penalize minority errors more or you may use this after balancing your dataset.
4. Categorical Crossentropy The log loss also generalizes to the multiclass problem. The classifier in a multiclass setting must assign a probability to each class for all examples. If there are N samples belonging to M classes, then the Categorical Crossentropy is the summation of -ylogp values:
$y_{ij}$ is 1 if the sample i belongs to class j else 0
$p_{ij}$ is the probability our classifier predicts of sample i belonging to class j.
When to Use? When the output of a classifier is multiclass prediction probabilities. We generally use Categorical Crossentropy in case of Neural Nets. In general, minimizing Categorical cross-entropy gives greater accuracy for the classifier.
How to Use? from sklearn.metrics import log_loss # Where y_pred is a matrix of probabilities with shape ***= (n_samples, n_classes)*** and y_true is an array of class labels log_loss(y_true, y_pred, eps=1e-15) Caveats: It is susceptible in case of imbalanced datasets.
5. AUC AUC is the area under the ROC curve.
AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes
What is the ROC curve?
We have got the probabilities from our classifier. We can use various threshold values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we will have a ROC curve.
Where True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.
Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP&#43;FP)
and False positive rate or FPR is just the proportion of false we are capturing using our algorithm.
1- Specificity = FPR(False Positive Rate)= FP/(TN&#43;FP)
Here we can use the ROC curves to decide on a Threshold value. The choice of threshold value will also depend on how the classifier is intended to be used.
If it is a cancer classification application you don’t want your threshold to be as big as 0.5. Even if a patient has a 0.3 probability of having cancer you would classify him to be 1.
Otherwise, in an application for reducing the limits on the credit card, you don’t want your threshold to be as less as 0.5. You are here a little worried about the negative effect of decreasing limits on customer satisfaction.
When to Use? AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So, for example, if you as a marketer want to find a list of users who will respond to a marketing campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which you will create a list of users to send the marketing campaign.
Another benefit of using AUC is that it is classification-threshold-invariant like log loss. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen, unlike F1 score or accuracy which depend on the choice of threshold.
How to Use? import numpy as np from sklearn.metrics import roc_auc_score y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8]) print(roc_auc_score(y_true, y_scores)) Caveats Sometimes we will need well-calibrated probability outputs from our models and AUC doesn’t help with that.
Conclusion An important step while creating our machine learning pipeline is evaluating our different models against each other. A bad choice of an evaluation metric could wreak havoc to your whole system.
So, always be watchful of what you are predicting and how the choice of evaluation metric might affect/alter your final predictions.
Also, the choice of an evaluation metric should be well aligned with the business objective and hence it is a bit subjective. And you can come up with your own evaluation metric as well.
Continue Learning If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out. It talks about the pitfalls and a lot of basic ideas to improve your models.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Ultimate Guide to using the Python regex module</title>
      <link>https://mlwhiz.com/blog/2019/09/01/regex/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/09/01/regex/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/regex/1.png"></media:content>
      

      
      <description>One of the main tasks while working with text data is to create a lot of text-based features.
One could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text.
While it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python’s regex module.
For example, let&amp;amp;rsquo;s say you are tasked with finding the number of punctuations in a particular piece of text.</description>

      <content:encoded>  
        
        <![CDATA[  One of the main tasks while working with text data is to create a lot of text-based features.
One could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text.
While it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python’s regex module.
For example, let&amp;rsquo;s say you are tasked with finding the number of punctuations in a particular piece of text. Using text from Dickens here.
How do you normally go about it?
A simple enough way is to do something like:
target = [&amp;#39;;&amp;#39;,&amp;#39;.&amp;#39;,&amp;#39;,&amp;#39;,&amp;#39;–&amp;#39;] string = &amp;#34;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.**&amp;#34; num_puncts = 0 for punct in target: if punct in string: num_puncts&#43;=string.count(punct) print(num_puncts) 19  And that is all but fine if we didn’t have the re module at our disposal. With re it is simply 2 lines of code:
import re pattern = r&amp;#34;[;.,–]&amp;#34; print(len(re.findall(pattern,string)))  19  This post is about one of the most commonly used regex patterns and some regex functions I end up using regularly.
What is regex? In simpler terms, a regular expression(regex) is used to find patterns in a given string.
The pattern we want to find could be anything.
We can create patterns that resemble an email or a mobile number. We can create patterns that find out words that start with a and ends with z from a string.
In the above example:
import re pattern = r&amp;#39;[,;.,–]&amp;#39; print(len(re.findall(pattern,string))) The pattern we wanted to find out was r’[,;.,–]’. This pattern captures any of the 4 characters we wanted to capture. I find regex101 a great tool for testing patterns. This is how the pattern looks when applied to the target string.
As we can see we are able to find all the occurrences of ,;.,– in the target string as required.
I use the above tool whenever I need to test a regex. Much faster than running a python program again and again and much easier to debug.
So now we know that we can find patterns in a target string but how do we really create these patterns?
Creating Patterns The first thing we need to learn while using regex is how to create patterns.
I will go through some most commonly used patterns one by one.
As you would think, the simplest pattern is a simple string.
pattern = r&amp;#39;times&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; print(len(re.findall(pattern,string))) But that is not very useful. To help with creating complex patterns regex provides us with special characters/operators. Let us go through some of these operators one by one. Please wait for the gifs to load.
1. the [] operator This is the one we used in our first example. We want to find one instance of any character within these square brackets.
[abc]- will find all occurrences of a or b or c.
[a-z]- will find all occurrences of a to z.
[a-z0–9A-Z]- will find all occurrences of a to z, A to Z and 0 to 9.
We can easily use this pattern as below in Python:
pattern = r&amp;#39;[a-zA-Z]&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; print(len(re.findall(pattern,string))) There are other functionalities in regex apart from .findall but we will get to them a little bit later.
2. The dot Operator The dot operator(.) is used to match a single instance of any character except the newline character.
The best part about the operators is that we can use them in conjunction with one another.
For example, We want to find out the substrings in the string that start with small d or Capital D and end with e with a length of 6.
3. Some Meta Sequences There are some patterns that we end up using again and again while using regex. And so regex has created a few shortcuts for them. The most useful shortcuts are:
\w, Matches any letter, digit or underscore. Equivalent to [a-zA-Z0–9_]
\W, Matches anything other than a letter, digit or underscore.
\d, Matches any decimal digit. Equivalent to [0–9].
\D, Matches anything other than a decimal digit.
4. The Plus and Star operator The dot character is used to get a single instance of any character. What if we want to find more.
The Plus character &#43;, is used to signify 1 or more instance of the leftmost character.
The Star character *, is used to signify 0 or more instance of the leftmost character.
For example, if we want to find out all substrings that start with d and end with e, we can have zero characters or more characters between d and e. We can use: d\w*e
If we want to find out all substrings that start with d and end with e with at least one character between d and e, we can use: d\w&#43;e
We could also have used a more generic approach using {} \w{n} - Repeat \w exactly n number of times.
\w{n,} - Repeat \w at least n times or more.
\w{n1, n2} - Repeat \w at least n1 times but no more than n2 times.
5. ^ Caret Operator and $ Dollar operator. ^ Matches the start of a string, and $ Matches the end of the string.
6. Word Boundary This is an important concept.
Did you notice how I always matched substring and never a word in the above examples?
So, what if we want to find all words that start with d?
Can we use d\w* as the pattern? Let&amp;rsquo;s see using the web tool.
Regex Functions Till now we have only used the findall function from the re package, but it also supports a lot more functions. Let us look into the functions one by one.
1. findall We already have used findall. It is one of the regex functions I end up using most often. Let us understand it a little more formally.
Input: Pattern and test string
Output: List of strings.
#USAGE: pattern = r&amp;#39;[iI]t&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; matches = re.findall(pattern,string) for match in matches: print(match) It it  2. Search Input: Pattern and test string
Output: Location object for the first match.
#USAGE: pattern = r&amp;#39;[iI]t&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; location = re.search(pattern,string) print(location) &amp;lt;_sre.SRE_Match object; span=(0, 2), match=&#39;It&#39;&amp;gt;  We can get this location object’s data using
print(location.group()) &#39;It&#39;  3. Substitute This is another great functionality. When you work with NLP you sometimes need to substitute integers with X’s. Or you might need to redact some document. Just the basic find and replace in any of the text editors.
Input: search pattern, replacement pattern, and the target string
Output: Substituted string
string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; string = re.sub(r&amp;#39;times&amp;#39;, r&amp;#39;life&amp;#39;, string) print(string) It was the best of life, it was the worst of life.  Some Case Studies: Regex is used in many cases when validation is required. You might have seen prompts on websites like “This is not a valid email address”. While such a prompt could be written using multiple if and else conditions, regex is probably the best for such use cases.
1. PAN Numbers In India, we have got PAN Numbers for Tax identification rather than SSN numbers in the US. The basic validation criteria for PAN is that it must have all its letters in uppercase and characters in the following order:
&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;char&amp;gt;  So the question is:
Is ‘ABcDE1234L’ a valid PAN?
How would you normally attempt to solve this without regex? You will most probably write a for loop and keep an index going through the string. With regex it is as simple as below:
match=re.search(r&amp;#39;[A-Z]{5}[0–9]{4}[A-Z]&amp;#39;,&amp;#39;ABcDE1234L&amp;#39;) if match: print(True) else: print(False) False  2. Find Domain Names Sometimes we have got a large text document and we have got to find out instances of telephone numbers or email IDs or domain names from the big text document.
For example, Suppose you have this text:
&amp;lt;div class=&amp;#34;reflist&amp;#34; style=&amp;#34;list-style-type: decimal;&amp;#34;&amp;gt; &amp;lt;ol class=&amp;#34;references&amp;#34;&amp;gt; &amp;lt;li id=&amp;#34;cite_note-1&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [&amp;#34;Train (noun)&amp;#34;](http://www.askoxford.com/concise_oed/train?view=uk). &amp;lt;i&amp;gt;(definition – Compact OED)&amp;lt;/i&amp;gt;. Oxford University Press&amp;lt;span class=&amp;#34;reference-accessdate&amp;#34;&amp;gt;. Retrieved 2008-03-18&amp;lt;/span&amp;gt;.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.atitle=Train&#43;%28noun%29&amp;amp;rft.genre=article&amp;amp;rft_id=http%3A%2F%2Fwww.askoxford.com%2Fconcise_oed%2Ftrain%3Fview%3Duk&amp;amp;rft.jtitle=%28definition&#43;%E2%80%93&#43;Compact&#43;OED%29&amp;amp;rft.pub=Oxford&#43;University&#43;Press&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-2&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;#34;reference-text&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;citation book&amp;#34;&amp;gt;Atchison, Topeka and Santa Fe Railway (1948). &amp;lt;i&amp;gt;Rules: Operating Department&amp;lt;/i&amp;gt;. p. 7.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.au=Atchison%2C&#43;Topeka&#43;and&#43;Santa&#43;Fe&#43;Railway&amp;amp;rft.aulast=Atchison%2C&#43;Topeka&#43;and&#43;Santa&#43;Fe&#43;Railway&amp;amp;rft.btitle=Rules%3A&#43;Operating&#43;Department&amp;amp;rft.date=1948&amp;amp;rft.genre=book&amp;amp;rft.pages=7&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-3&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [Hydrogen trains](http://www.hydrogencarsnow.com/blog2/index.php/hydrogen-vehicles/i-hear-the-hydrogen-train-a-comin-its-rolling-round-the-bend/)&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-4&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [Vehicle Projects Inc. Fuel cell locomotive](http://www.bnsf.com/media/news/articles/2008/01/2008-01-09a.html)&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-5&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;#34;reference-text&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;citation book&amp;#34;&amp;gt;Central Japan Railway (2006). &amp;lt;i&amp;gt;Central Japan Railway Data Book 2006&amp;lt;/i&amp;gt;. p. 16.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.au=Central&#43;Japan&#43;Railway&amp;amp;rft.aulast=Central&#43;Japan&#43;Railway&amp;amp;rft.btitle=Central&#43;Japan&#43;Railway&#43;Data&#43;Book&#43;2006&amp;amp;rft.date=2006&amp;amp;rft.genre=book&amp;amp;rft.pages=16&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-6&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [&amp;#34;Overview Of the existing Mumbai Suburban Railway&amp;#34;](http://web.archive.org/web/20080620033027/http://www.mrvc.indianrail.gov.in/overview.htm). _Official webpage of Mumbai Railway Vikas Corporation_. Archived from [the original](http://www.mrvc.indianrail.gov.in/overview.htm) on 2008-06-20&amp;lt;span class=&amp;#34;reference-accessdate&amp;#34;&amp;gt;. Retrieved 2008-12-11&amp;lt;/span&amp;gt;.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.atitle=Overview&#43;Of&#43;the&#43;existing&#43;Mumbai&#43;Suburban&#43;Railway&amp;amp;rft.genre=article&amp;amp;rft_id=http%3A%2F%2Fwww.mrvc.indianrail.gov.in%2Foverview.htm&amp;amp;rft.jtitle=Official&#43;webpage&#43;of&#43;Mumbai&#43;Railway&#43;Vikas&#43;Corporation&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ol&amp;gt; &amp;lt;/div&amp;gt; And you need to find out all the primary domains from this text- askoxford.com;bnsf.com;hydrogencarsnow.com;mrvc.indianrail.gov.in;web.archive.org
How would you do this?
match=re.findall(r&amp;#39;http(s:|:)\/\/([www.|ww2.|)([0-9a-z.A-Z-]*\.\w{2,3})&amp;#39;,string)](http://www.|ww2.|)([0-9a-z.A-Z-]*\.\w{2,3})&amp;#39;,string))for elem in match:print(elem) (&#39;:&#39;, &#39;www.&#39;, &#39;askoxford.com&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;hydrogencarsnow.com&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;bnsf.com&#39;) (&#39;:&#39;, &#39;&#39;, &#39;web.archive.org&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;mrvc.indianrail.gov.in&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;mrvc.indianrail.gov.in&#39;)  | is the or operator here and match returns tuples where the pattern part inside () is kept.
3. Find Email Addresses: Below is a regex to find email addresses in a long text.
match=re.findall(r&amp;#39;([\w0-9-._]&#43;@[\w0-9-.]&#43;[\w0-9]{2,3})&amp;#39;,string) These are advanced examples but if you try to understand these examples for yourself you should be fine with the info provided.
Conclusion While it might look a little daunting at first, regex provides a great degree of flexibility when it comes to data manipulation, creating features and finding patterns.
I use it quite regularly when I work with text data and it can also be included while working on data validation tasks.
I am also a fan of the regex101 tool and use it frequently to check my regexes. I wonder if I would be using regexes as much if not for this awesome tool.
Also if you want to learn more about NLP here is an excellent course. You can start for free with the 7-day Free Trial.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Feature Selection Algorithms every Data Scientist should know</title>
      <link>https://mlwhiz.com/blog/2019/08/07/feature_selection/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/08/07/feature_selection/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/fs/1.png"></media:content>
      

      
      <description>Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.</description>

      <content:encoded>  
        
        <![CDATA[  Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.
We sometimes end up using correlation or tree-based methods to find out the important features.
Can we add some structure to it?
This post is about some of the most common feature selection techniques one can use while working with data.
Why Feature Selection? Before we proceed, we need to answer this question. Why don’t we give all the features to the ML algorithm and let it decide which feature is important?
So there are three reasons why we don’t:
1. Curse of dimensionality — Overfitting If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing.
2. Occam’s Razor: We want our models to be simple and explainable. We lose explainability when we have a lot of features.
3. Garbage In Garbage out: Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.
Also, a large number of features make a model bulky, time-taking, and harder to implement in production.
So What do we do? We select only useful features.
Fortunately, Scikit-learn has made it pretty much easy for us to make the feature selection. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major buckets
 Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.
 Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination
 Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.
  So enough of theory let us start with our five feature selection methods.
We will try to do this using a dataset to understand it better.
I am going to be using a football player dataset to find out what makes a good player great?
Don’t worry if you don’t understand football terminologies. I will try to keep it at a minimum.
Here is the Kaggle Kernel with the code to try out yourself.
Some Simple Data Preprocessing We have done some basic preprocessing such as removing Nulls and one hot encoding. And converting the problem to a classification problem using:
y = traindf[&#39;Overall&#39;]&amp;gt;=87  Here we use High Overall as a proxy for a great player.
Our dataset(X) looks like below and has 223 columns.
1. Pearson Correlation This is a filter-based method.
We check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.
def cor_selector(X, y,num_feats): cor_list = [] feature_name = X.columns.tolist() # calculate the correlation with y for each feature for i in X.columns.tolist(): cor = np.corrcoef(X[i], y)[0, 1] cor_list.append(cor) # replace NaN with 0 cor_list = [0 if np.isnan(i) else i for i in cor_list] # feature name cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist() # feature selection? 0 for not select, 1 for select cor_support = [True if i in cor_feature else False for i in feature_name] return cor_support, cor_feature cor_support, cor_feature = cor_selector(X, y,num_feats) print(str(len(cor_feature)), &amp;#39;selected features&amp;#39;) 2. Chi-Squared This is another filter-based method.
In this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.
Let us create a small example of how we calculate the chi-squared statistic for a sample.
So let’s say we have 75 Right-Forwards in our dataset and 25 Non-Right-Forwards. We observe that 40 of the Right-Forwards are good, and 35 are not good. Does this signify that the player being right forward affects the overall performance?
We calculate the chi-squared value:
To do this, we first find out the values we would expect to be falling in each bucket if there was indeed independence between the two categorical variables.
This is simple. We multiply the row sum and the column sum for each cell and divide it by total observations.
so Good and NotRightforward Bucket Expected value= 25(Row Sum)*60(Column Sum)/100(Total Observations)
Why is this expected? Since there are 25% notRightforwards in the data, we would expect 25% of the 60 good players we observed in that cell. Thus 15 players.
Then we could just use the below formula to sum over all the 4 cells:
I won’t show it here, but the chi-squared statistic also works in a hand-wavy way with non-negative numerical and categorical features.
We can get chi-squared features from our dataset as:
from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from sklearn.preprocessing import MinMaxScaler X_norm = MinMaxScaler().fit_transform(X) chi_selector = SelectKBest(chi2, k=num_feats) chi_selector.fit(X_norm, y) chi_support = chi_selector.get_support() chi_feature = X.loc[:,chi_support].columns.tolist() print(str(len(chi_feature)), &amp;#39;selected features&amp;#39;) 3. Recursive Feature Elimination This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.
From sklearn Documentation:
 The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
 As you would have guessed, we could use any estimator with the method. In this case, we use LogisticRegression, and the RFE observes the coef_ attribute of the LogisticRegression object
from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5) rfe_selector.fit(X_norm, y) rfe_support = rfe_selector.get_support() rfe_feature = X.loc[:,rfe_support].columns.tolist() print(str(len(rfe_feature)), &amp;#39;selected features&amp;#39;) 4. Lasso: SelectFromModel This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.
For example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.
Here we use Lasso to select variables.
from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=&amp;#34;l1&amp;#34;), max_features=num_feats) embeded_lr_selector.fit(X_norm, y) embeded_lr_support = embeded_lr_selector.get_support() embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist() print(str(len(embeded_lr_feature)), &amp;#39;selected features&amp;#39;) 5. Tree-based: SelectFromModel This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.
We can also use RandomForest to select features based on feature importance.
We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.
from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats) embeded_rf_selector.fit(X, y) embeded_rf_support = embeded_rf_selector.get_support() embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist() print(str(len(embeded_rf_feature)), &amp;#39;selected features&amp;#39;) We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.
from sklearn.feature_selection import SelectFromModel from lightgbm import LGBMClassifier lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40) embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats) embeded_lgb_selector.fit(X, y) embeded_lgb_support = embeded_lgb_selector.get_support() embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist() print(str(len(embeded_lgb_feature)), &amp;#39;selected features&amp;#39;) Bonus Why use one, when we can have all?
The answer is sometimes it won’t be possible with a lot of data and time crunch.
But whenever possible, why not do this?
# put all selection together feature_selection_df = pd.DataFrame({&amp;#39;Feature&amp;#39;:feature_name, &amp;#39;Pearson&amp;#39;:cor_support, &amp;#39;Chi-2&amp;#39;:chi_support, &amp;#39;RFE&amp;#39;:rfe_support, &amp;#39;Logistics&amp;#39;:embeded_lr_support, &amp;#39;Random Forest&amp;#39;:embeded_rf_support, &amp;#39;LightGBM&amp;#39;:embeded_lgb_support}) # count the selected times for each feature feature_selection_df[&amp;#39;Total&amp;#39;] = np.sum(feature_selection_df, axis=1) # display the top 100 feature_selection_df = feature_selection_df.sort_values([&amp;#39;Total&amp;#39;,&amp;#39;Feature&amp;#39;] , ascending=False) feature_selection_df.index = range(1, len(feature_selection_df)&#43;1) We check if we get a feature based on all the methods. In this case, as we can see Reactions and LongPassing are excellent attributes to have in a high rated player. And as expected Ballcontrol and Finishing occupy the top spot too.
Conclusion Feature engineering and feature selection are critical parts of any machine learning pipeline.
We strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.
In this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.
I also tried to provide some intuition into these methods, but you should probably try to see more into it and try to incorporate these methods into your work.
Do read my post on feature engineering too if you are interested.
If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Sampling Algorithms every Data Scientist need to know</title>
      <link>https://mlwhiz.com/blog/2019/07/30/sampling/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/30/sampling/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/sampling/1.jpg"></media:content>
      

      
      <description>Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.</description>

      <content:encoded>  
        
        <![CDATA[  Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.
Below we select 100 sample points from a dataset.
sample_df = df.sample(100) Stratified Sampling Assume that we need to estimate the average number of votes for each candidate in an election. Assume that the country has 3 towns:
Town A has 1 million factory workers,
Town B has 2 million workers, and
Town C has 3 million retirees.
We can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation.
Instead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of the sample.
You can do something like this pretty easily with Python:
from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25) Reservoir Sampling I love this problem statement:
Say you have a stream of items of large and unknown length that we can only iterate over once.
Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.
How can we do that?
Let us assume we have to sample 5 objects out of an infinite stream such that each element has an equal probability of getting selected.
import random def generator(max): number = 1 while number &amp;lt; max: number &#43;= 1 yield number # Create as stream generator stream = generator(10000) # Doing Reservoir Sampling from the stream k=5 reservoir = [] for i, element in enumerate(stream): if i&#43;1&amp;lt;= k: reservoir.append(element) else: probability = k/(i&#43;1) if random.random() &amp;lt; probability: # Select item in stream and remove one of the k items already selected reservoir[random.choice(range(0,k))] = element print(reservoir) [1369, 4108, 9986, 828, 5589]  It can be mathematically proved that in the sample each element has the same probability of getting selected from the stream.
How?
It always helps to think of a smaller problem when it comes to mathematics.
So, let us think of a stream of only 3 items and we have to keep 2 of them.
We see the first item, we hold it in the list as our reservoir has space. We see the second item, we hold it in the list as our reservoir has space.
We see the third item. Here is where things get interesting. We choose the third item to be in the list with probability 2&amp;frasl;3.
Let us now see the probability of first item getting selected:
The probability of removing the first item is the probability of element 3 getting selected multiplied by the probability of Element 1 getting randomly chosen as the replacement candidate from the 2 elements in the reservoir. That probability is:
2&amp;frasl;3*1&amp;frasl;2 = 1&amp;frasl;3
Thus the probability of 1 getting selected is:
1–1/3 = 2&amp;frasl;3
We can have the exact same argument for the Second Element and we can extend it for many elements.
Thus each item has the same probability of getting selected: 2&amp;frasl;3 or in general k/n
Random Undersampling and Oversampling It is too often that we encounter an imbalanced dataset.
A widely adopted technique for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).
Let us first create some example imbalanced data.
from sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[&amp;#39;target&amp;#39;] = y We can now do random oversampling and undersampling using:
num_0 = len(X[X[&amp;#39;target&amp;#39;]==0]) num_1 = len(X[X[&amp;#39;target&amp;#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0].sample(num_1) , X[X[&amp;#39;target&amp;#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0] , X[X[&amp;#39;target&amp;#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180  Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.
It provides a variety of methods to undersample and oversample.
a. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.
In this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier.
from imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=&amp;#39;majority&amp;#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.
from imblearn.over_sampling import SMOTE smote = SMOTE(ratio=&amp;#39;minority&amp;#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.
Conclusion Algorithms are the lifeblood of data science.
Sampling is an important topic in data science and we really don’t talk about it as much as we should.
A good sampling strategy sometimes could pull the whole project forward. A bad sampling strategy could give us incorrect results. So one should be careful while selecting a sampling strategy.
So use sampling, be it at work or at bars.
If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Minimal Pandas Subset for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pandas_subset/1.jpeg"></media:content>
      

      
      <description>Pandas is a vast library.
Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.
Still, I generally have some issues with it.
There are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.</description>

      <content:encoded>  
        
        <![CDATA[    Pandas is a vast library.
Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.
Still, I generally have some issues with it.
There are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.
This has inspired me to come up with a minimal subset of pandas functions I use while coding.
I have tried it all, and currently, I stick to a particular way. It is like a mind map.
Sometimes because it is fast and sometimes because it’s more readable and sometimes because I can do it with my current knowledge. And sometimes because I know that a particular way will be a headache in the long run(think multi-index)
This post is about handling most of the data manipulation cases in Python using a straightforward, simple, and matter of fact way.
With a sprinkling of some recommendations throughout.
I will be using a data set of 1,000 popular movies on IMDB in the last ten years. You can also follow along in the Kaggle Kernel.
Some Default Pandas Requirements As good as the Jupyter notebooks are, some things still need to be specified when working with Pandas.
***Sometimes your notebook won’t show you all the columns. Sometimes it will display all the rows if you print the dataframe. ***You can control this behavior by setting some defaults of your own while importing Pandas. You can automate it using this addition to your notebook.
For instance, this is the setting I use.
import pandas as pd # pandas defaults pd.options.display.max_columns = 500 pd.options.display.max_rows = 500 Reading Data with Pandas The first thing we do is reading the data source and so here is the code for that.
df = pd.read_csv(&amp;#34;IMDB-Movie-Data.csv&amp;#34;) Recommendation: I could also have used pd.read_table to read the file. The thing is that pd.read_csv has default separator as , and thus it saves me some code. I also genuinely don’t understand the use of pd.read_table
If your data is in some SQL Datasource, you could have used the following code. You get the results in the dataframe format.
# Reading from SQL Datasource import MySQLdb from pandas import DataFrame from pandas.io.sql import read_sql db = MySQLdb.connect(host=&amp;#34;localhost&amp;#34;, # your host, usually localhost user=&amp;#34;root&amp;#34;, # your username passwd=&amp;#34;password&amp;#34;, # your password db=&amp;#34;dbname&amp;#34;) # name of the data base query = &amp;#34;SELECT * FROM tablename&amp;#34; df = read_sql(query, db) Data Snapshot Always useful to see some of the data.
You can use simple head and tail commands with an option to specify the number of rows.
# top 5 rows df.head() # top 50 rows df.head(50) # last 5 rows df.tail() # last 50 rows df.tail(50) You can also see simple dataframe statistics with the following commands.
# To get statistics of numerical columns df.describe() # To get maximum value of a column. When you take a single column you can think of it as a list and apply functions you would apply to a list. You can also use min for instance. print(max(df[&amp;#39;rating&amp;#39;])) # no of rows in dataframe print(len(df)) # Shape of Dataframe print(df.shape) 9.0 1000 (1000,12)  Recommendation: Generally working with Jupyter notebook,I make it a point of having the first few cells in my notebook containing these snapshots of the data. This helps me see the structure of the data whenever I want to. If I don’t follow this practice, I notice that I end up repeating the .head() command a lot of times in my code.
Handling Columns in Dataframes a. Selecting a column For some reason Pandas lets you choose columns in two ways. Using the dot operator like df.Title and using square brackets like df[&#39;Title&#39;]
I prefer the second version, mostly. Why?
There are a couple of reasons you would be better off with the square bracket version in the longer run.
 If your column name contains spaces, then the dot version won’t work. For example, df.Revenue (Millions) won’t work while df[&#39;Revenue (Millions)&#39;] will.
 It also won’t work if your column name is count or mean or any of pandas predefined functions.
 Sometimes you might need to create a for loop over your column names in which your column name might be in a variable. In that case, the dot notation will not work. For Example, This works:
  colname = &amp;#39;height&amp;#39; df[colname] While this doesn’t:
colname = &amp;#39;height&amp;#39; df.colname Trust me. Saving a few characters is not worth it.
Recommendation: Stop using the dot operator. It is a construct that originated from a different language&amp;reg; and respectfully should be left there.
b. Getting Column Names in a list You might need a list of columns for some later processing.
columnnames = df.columns c. Specifying user-defined Column Names: Sometimes you want to change the column names as per your taste. I don’t like spaces in my column names, so I change them as such.
df.columns = [&amp;#39;Rank&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Genre&amp;#39;, &amp;#39;Description&amp;#39;, &amp;#39;Director&amp;#39;, &amp;#39;Actors&amp;#39;, &amp;#39;Year&amp;#39;, &amp;#39;Runtime_Minutes&amp;#39;, &amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Revenue_Millions&amp;#39;, &amp;#39;Metascore&amp;#39;] I could have used another way.
This is the one case where both of the versions are important. When I have to change a lot of column names, I go with the way above. When I have to change the name of just one or two columns I use:
df.rename(columns = {&amp;#39;Revenue (Millions)&amp;#39;:&amp;#39;Rev_M&amp;#39;,&amp;#39;Runtime (Minutes)&amp;#39;:&amp;#39;Runtime_min&amp;#39;},inplace=True) d. Subsetting specific columns: Sometimes you only need to work with particular columns in a dataframe. e.g., to separate numerical and categorical columns, or remove unnecessary columns. Let’s say in our example; we don’t need the description, director, and actor column.
df = df[[&amp;#39;Rank&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Genre&amp;#39;, &amp;#39;Year&amp;#39;,&amp;#39;Runtime_min&amp;#39;, &amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Rev_M&amp;#39;, &amp;#39;Metascore&amp;#39;]] e. Seeing column types: Very useful while debugging. If your code throws an error that you cannot add a str and int, you will like to run this command.
df.dtypes Applying Functions on DataFrame: Apply and Lambda apply and lambda are some of the best things I have learned to use with pandas.
I use apply and lambda anytime I get stuck while building a complex logic for a new column or filter.
a. Creating a Column You can create a new column in many ways.
If you want a column that is a sum or difference of columns, you can pretty much use simple basic arithmetic. Here I get the average rating based on IMDB and Normalized Metascore.
df[&amp;#39;AvgRating&amp;#39;] = (df[&amp;#39;Rating&amp;#39;] &#43; df[&amp;#39;Metascore&amp;#39;]/10)/2 But sometimes we may need to build complex logic around the creation of new columns.
To give you a convoluted example, let’s say that we want to build a custom movie score based on a variety of factors.
Say, If the movie is of the thriller genre, I want to add 1 to the IMDB rating subject to the condition that IMDB rating remains less than or equal to 10. And If a movie is a comedy I want to subtract one from the rating.
How do we do that?
Whenever I get a hold of such complex problems, I use apply/lambda. Let me first show you how I will do this.
def custom_rating(genre,rating): if &amp;#39;Thriller&amp;#39; in genre: return min(10,rating&#43;1) elif &amp;#39;Comedy&amp;#39; in genre: return max(0,rating-1) else: return rating df[&amp;#39;CustomRating&amp;#39;] = df.apply(lambda x: custom_rating(x[&amp;#39;Genre&amp;#39;],x[&amp;#39;Rating&amp;#39;]),axis=1) The general structure is:
 You define a function that will take the column values you want to play with to come up with your logic. Here the only two columns we end up using are genre and rating.
 You use an apply function with lambda along the row with axis=1. The general syntax is:
  df.apply(lambda x: func(x[&amp;#39;col1&amp;#39;],x[&amp;#39;col2&amp;#39;]),axis=1) You should be able to create pretty much any logic using apply/lambda since you just have to worry about the custom function.
b. Filtering a dataframe Pandas make filtering and subsetting dataframes pretty easy. You can filter and subset dataframes using normal operators and &amp;amp;,|,~ operators.
# Single condition: dataframe with all movies rated greater than 8 df_gt_8 = df[df[&amp;#39;Rating&amp;#39;]&amp;gt;8] # Multiple conditions: AND - dataframe with all movies rated greater than 8 and having more than 100000 votes And_df = df[(df[&amp;#39;Rating&amp;#39;]&amp;gt;8) &amp;amp; (df[&amp;#39;Votes&amp;#39;]&amp;gt;100000)] # Multiple conditions: OR - dataframe with all movies rated greater than 8 or having a metascore more than 90 Or_df = df[(df[&amp;#39;Rating&amp;#39;]&amp;gt;8) | (df[&amp;#39;Metascore&amp;#39;]&amp;gt;80)] # Multiple conditions: NOT - dataframe with all emovies rated greater than 8 or having a metascore more than 90 have to be excluded Not_df = df[~((df[&amp;#39;Rating&amp;#39;]&amp;gt;8) | (df[&amp;#39;Metascore&amp;#39;]&amp;gt;80))] Pretty simple stuff.
But sometimes we may need to do complex filtering operations.
And sometimes we need to do some operations which we won’t be able to do using just the above format.
For instance: Let us say we want to filter those rows where the number of words in the movie title is greater than or equal to than 4.
How would you do it?
Trying the below will give you an error. Apparently, you cannot do anything as simple as split with a series.
new_df = df[len(df[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;))&amp;gt;=4] AttributeError: &#39;Series&#39; object has no attribute &#39;split&#39;  One way is first to create a column which contains no of words in the title using apply and then filter on that column.
#create a new column df[&amp;#39;num_words_title&amp;#39;] = df.apply(lambda x : len(x[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;)),axis=1) #simple filter on new column new_df = df[df[&amp;#39;num_words_title&amp;#39;]&amp;gt;=4] And that is a perfectly fine way as long as you don’t have to create a lot of columns. But I prefer this:
new_df = df[df.apply(lambda x : len(x[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;))&amp;gt;=4,axis=1)] What I did here is that my apply function returns a boolean which can be used to filter.
Now once you understand that you just have to create a column of booleans to filter, you can use any function/logic in your apply statement to get however complex a logic you want to build.
Let us see another example. I will try to do something a little complex to show the structure.
We want to find movies for which the revenue is less than the average revenue for that particular year?
year_revenue_dict = df.groupby([&amp;#39;Year&amp;#39;]).agg({&amp;#39;Rev_M&amp;#39;:np.mean}).to_dict()[&amp;#39;Rev_M&amp;#39;] def bool_provider(revenue, year): return revenue&amp;lt;year_revenue_dict[year] new_df = df[df.apply(lambda x : bool_provider(x[&amp;#39;Rev_M&amp;#39;],x[&amp;#39;Year&amp;#39;]),axis=1)] We have a function here which we can use to write any logic. That provides a lot of power for advanced filtering as long as we can play with simple variables.
c. Change Column Types I even use apply to change the column types since I don’t want to remember the syntax for changing column type and also since it lets me do much more complicated things.
The usual syntax to change column type is astype in Pandas. So if I had a column named price in my data in an str format. I could do this:
df[&amp;#39;Price&amp;#39;] = newDf[&amp;#39;Price&amp;#39;].astype(&amp;#39;int&amp;#39;) But sometimes it won’t work as expected.
You might get the error: ValueError: invalid literal for long() with base 10: ‘13,000’. That is you cannot cast a string with “,” to an int. To do that we first have to get rid of the comma.
After facing this problem time and again, I have stopped using astype altogether now and just use apply to change column types.
df[&amp;#39;Price&amp;#39;] = df.apply(lambda x: int(x[&amp;#39;Price&amp;#39;].replace(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;)),axis=1) And lastly, there is progress_apply progress_apply is a function that comes with tqdm package.
And this has saved me a lot of time.
Sometimes when you have got a lot of rows in your data, or you end up writing a pretty complex apply function, you will see that apply might take a lot of time.
I have seen apply taking hours when working with Spacy. In such cases, you might like to see the progress bar with apply.
You can use tqdm for that.
After the initial imports at the top of your notebook, just replace apply with progress_apply and everything remains the same.
from tqdm import tqdm, tqdm_notebook tqdm_notebook().pandas() df.progress_apply(lambda x: custom_rating_function(x[&amp;#39;Genre&amp;#39;],x[&amp;#39;Rating&amp;#39;]),axis=1) And you get progress bars.
Recommendation:vWhenever you see that you have to create a column with custom complex logic, think of apply and lambda. Try using progress_apply too.
Aggregation on Dataframes: groupby groupby will come up a lot of times whenever you want to aggregate your data. Pandas lets you do this efficiently with the groupby function.
There are a lot of ways that you can use groupby. I have seen a lot of versions, but I prefer a particular style since I feel the version I use is easy, intuitive, and scalable for different use cases.
df.groupby(list of columns to groupby on).aggregate({&amp;#39;colname&amp;#39;:func1, &amp;#39;colname2&amp;#39;:func2}).reset_index() Now you see it is pretty simple. You just have to worry about supplying two primary pieces of information.
 List of columns to groupby on, and
 A dictionary of columns and functions you want to apply to those columns
  reset_index() is a function that resets the index of a dataframe. I apply this function ALWAYS whenever I do a groupby, and you might think of it as a default syntax for groupby operations.
Let us check out an example.
# Find out the sum of votes and revenue by year import numpy as np df.groupby([&amp;#39;Year&amp;#39;]).aggregate({&amp;#39;Votes&amp;#39;:np.sum, &amp;#39;Rev_M&amp;#39;:np.sum}).reset_index() You might also want to group by more than one column. It is fairly straightforward.
df.groupby([&amp;#39;Year&amp;#39;,&amp;#39;Genre&amp;#39;]).aggregate({&amp;#39;Votes&amp;#39;:np.sum, &amp;#39;Rev_M&amp;#39;:np.sum}).reset_index() Recommendation: Stick to one syntax for groupby. Pick your own if you don’t like mine but stick to one.
Dealing with Multiple Dataframes: Concat and Merge: a. concat Sometimes we get data from different sources. Or someone comes to you with multiple files with each file having data for a particular year.
How do we create a single dataframe from a single dataframe?
Here we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic filter operations we already know.
movies_2006 = df[df[&amp;#39;Year&amp;#39;]==2006] movies_2007 = df[df[&amp;#39;Year&amp;#39;]==2007] Here we start with two dataframes: movies_2006 containing info for movies released in 2006 and movies_2007 containing info for movies released in 2007. We want to create a single dataframe that includes movies from both 2006 and 2007
movies_06_07 = pd.concat([movies_2006,movies_2007]) b. merge Most of the data that you will encounter will never come in a single file. One of the files might contain ratings for a particular movie, and another might provide the number of votes for a movie.
In such a case we have two dataframes which need to be merged so that we can have all the information in a single view.
Here we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic column subset operations we already know.
rating_dataframe = df[[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;]] votes_dataframe = df[[&amp;#39;Title&amp;#39;,&amp;#39;Votes&amp;#39;]] We need to have all this information in a single dataframe. How do we do this?
rating_vote_df = pd.merge(rating_dataframe,votes_dataframe,on=&amp;#39;Title&amp;#39;,how=&amp;#39;left&amp;#39;) rating_vote_df.head() We provide this merge function with four attributes- 1st DF, 2nd DF, join on which column and the joining criteria:[&#39;left&#39;,&#39;right&#39;,&#39;inner&#39;,&#39;outer&#39;]
Recommendation: I usually always end up using left join. You will rarely need to join using outer or right. Actually whenever you need to do a right join you actually just really need a left join with the order of dataframes reversed in the merge function.
Reshaping Dataframes: Melt and pivot_table(reverseMelt) Most of the time, we don’t get data in the exact form we want.
For example, sometimes we might have data in columns which we might need in rows.
Let us create an artificial example again. You can look at the code below that I use to create the example, but really it doesn’t matter.
genre_set = set() for genre in df[&amp;#39;Genre&amp;#39;].unique(): for g in genre.split(&amp;#34;,&amp;#34;): genre_set.add(g) for genre in genre_set: df[genre] = df[&amp;#39;Genre&amp;#39;].apply(lambda x: 1 if genre in x else 0) working_df = df[[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Rev_M&amp;#39;]&#43;list(genre_set)] working_df.head() So we start from a working_df like this:
Now, this is not particularly a great structure to have data in. We might like it better if we had a dataframe with only one column Genre and we can have multiple rows repeated for the same movie. So the movie ‘Prometheus’ might be having three rows since it has three genres. How do we make that work?
We use melt:
reshaped_df = pd.melt(working_df,id_vars = [&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;,&amp;#39;Votes&amp;#39;,&amp;#39;Rev_M&amp;#39;],value_vars = list(genre_set),var_name = &amp;#39;Genre&amp;#39;, value_name =&amp;#39;Flag&amp;#39;) reshaped_df.head() So in this melt function, we provided five attributes:
 dataframe_name = working_df
 id_vars: List of vars we want in the current form only.
 value_vars: List of vars we want to melt/put in the same column
 var_name: name of the column for value_vars
 value_name: name of the column for value of value_vars
  There is still one thing remaining. For Prometheus, we see that it is a thriller and the flag is 0. The flag 0 is unnecessary data we can filter out, and we will have our results. We keep only the genres with flag 1
reshaped_df = reshaped_df[reshaped_df[&amp;#39;Flag&amp;#39;]==1] What if we want to go back?
We need the values in a column to become multiple columns. How? We use pivot_table
re_reshaped_df = reshaped_df.pivot_table(index=[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;,&amp;#39;Votes&amp;#39;,&amp;#39;Rev_M&amp;#39;], columns=&amp;#39;Genre&amp;#39;, values=&amp;#39;Flag&amp;#39;, aggfunc=&amp;#39;sum&amp;#39;).reset_index() re_reshaped_df.head() We provided four attributes to the pivot_table function.
 index: We don’t want to change these column structures
 columns: explode this column into multiple columns
 values: use this column to aggregate
 aggfunc: the aggregation function.
  We can then fill the missing values by 0 using fillna
re_reshaped_df=re_reshaped_df.fillna(0) Recommendation: Multiple columns to one column: melt and One column to multiple columns: pivot_table . There are other ways to do melt — stack and different ways to do pivot_table: pivot,unstack. Stay away from them and just use melt and pivot_table. There are some valid reasons for this like unstack and stack will create multi-index and we don’t want to deal with that, and pivot cannot take multiple columns as the index.
Conclusion  With Pandas, less choice is more
 Here I have tried to profile some of the most useful functions in pandas I end up using most often.
Pandas is a vast library with a lot of functionality and custom options. That makes it essential that you should have a mindmap where you stick to a particular syntax for a specific thing.
Here I have shared mine, and you can proceed with it and make it better as your understanding of the library grows.
I hope you found this post useful and worth your time. I tried to make this as simple as possible, but you may always ask me or see the documentation for doubts.
Whole code and data are posted in the Kaggle Kernel.
Also, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Hitchhikers guide to handle Big Data using Spark</title>
      <link>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/spark/spark.jpeg"></media:content>
      

      
      <description>Big Data has become synonymous with Data engineering.
But the line between Data Engineering and Data scientists is blurring day by day.
At this point in time, I think that Big Data must be in the repertoire of all data scientists.
Reason: Too much data is getting generated day by day
And that brings us to Spark.
Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.</description>

      <content:encoded>  
        
        <![CDATA[    Big Data has become synonymous with Data engineering.
But the line between Data Engineering and Data scientists is blurring day by day.
At this point in time, I think that Big Data must be in the repertoire of all data scientists.
Reason: Too much data is getting generated day by day
And that brings us to Spark.
Now most of the Spark documentation, while good, did not explain it from the perspective of a data scientist.
So I thought of giving it a shot.
This post is going to be about — “How to make Spark work?”
This post is going to be quite long. Actually my longest post on medium, so go pick up a Coffee.
How it all started?-MapReduce   Suppose you are tasked with cutting all the trees in the forest. Perhaps not a good business with all the global warming, but here it serves our purpose and we are talking hypothetically, so I will continue. You have two options:
 Get Batista with an electric powered chainsaw to do your work and make him cut each tree one by one.
 Get 500 normal guys with normal axes and make them work on different trees.
  Which would you prefer?
Although Option 1 is still the way some people would go, the need for option 2 led to the emergence of MapReduce.
In Bigdata speak, we call the Batista solution as scaling vertically/scaling-upas in we add/stuff a lot of RAM and hard disk in a single worker.
And the second solution is called scaling horizontally/scaling-sideways. As in you connect a lot of ordinary machines(with less RAM) together and use them in parallel.
Now, vertical scaling has certain benefits over Horizontal scaling:
 It is fast if the size of the problem is small: Think 2 trees. Batista would be through with both of them with his awesome chainsaw while our two guys would be still hacking with their axes.
 It is easy to understand. This is how we have always done things. We normally think about things in a sequential pattern and that is how our whole computer architecture and design has evolved.
  But, Horizontal Scaling is
 Less Expensive: Getting 50 normal guys itself is much cheaper than getting a single guy like Batista. Apart from that Batista needs a lot of care and maintenance to keep him cool and he is very sensitive to even small things just like machines with a high amount of RAM.
 Faster when the size of the problem is big: Now imagine 1000 trees and 1000 workers vs a single Batista. With Horizontal Scaling, if we face a very large problem we will just hire 100 or maybe 1000 more cheap workers. It doesn’t work like that with Batista. You have to increase RAM and that means more cooling infrastructure and more maintenance costs.
    MapReduce is what makes the second option possible by letting us use a cluster of computers for parallelization.
Now, MapReduce looks like a fairly technical term. But let us break it a little. MapReduce is made up of two terms:
Map: It is basically the apply/map function. We split our data into n chunks and send each chunk to a different worker(Mapper). If there is any function we would like to apply over the rows of Data our worker does that.
Reduce: Aggregate the data using some function based on a groupby key. It is basically a groupby.
Of course, there is a lot going in the background to make the system work as intended.
Don’t worry, if you don’t understand it yet. Just keep reading. Maybe you will understand it when we use MapReduce ourselves in the examples I am going to provide.
Why Spark?   Hadoop was the first open source system that introduced us to the MapReduce paradigm of programming and Spark is the system that made it faster, much much faster(100x).
There used to be a lot of data movement in Hadoop as it used to write intermediate results to the file system.
This affected the speed at which you could do analysis.
Spark provided us with an in-memory model, so Spark doesn’t write too much to the disk while working.
Simply, Spark is faster than Hadoop and a lot of people use Spark now.
So without further ado let us get started.
Getting Started with Spark Installing Spark is actually a headache of its own.
Since we want to understand how it works and really work with it, I would suggest that you use Sparks on Databricks here online with the community edition. Don’t worry it is free.
  Once you register and login will be presented with the following screen.
  You can start a new notebook here.
Select the Python notebook and give any name to your notebook.
Once you start a new notebook and try to execute any command, the notebook will ask you if you want to start a new cluster. Do it.
The next step will be to check if the sparkcontext is present. To check if the sparkcontext is present you just have to run this command:
sc   This means that we are set up with a notebook where we can run Spark.
Load Some Data The next step is to upload some data we will use to learn Spark. Just click on ‘Import and Explore Data’ on the home tab.
I will end up using multiple datasets by the end of this post but let us start with something very simple.
Let us add the file shakespeare.txt which you can download from here.
  You can see that the file is loaded to /FileStore/tables/shakespeare.txt location.
Our First Spark Program I like to learn by examples so let’s get done with the “Hello World” of Distributed computing: The WordCount Program.
# Distribute the data - Create a RDD lines = sc.textFile(&amp;#34;/FileStore/tables/shakespeare.txt&amp;#34;) # Create a list with all words, Create tuple (word,1), reduce by key i.e. the word counts = (lines.flatMap(lambda x: x.split(&amp;#39; &amp;#39;)) .map(lambda x: (x, 1)) .reduceByKey(lambda x,y : x &#43; y)) # get the output on local output = counts.take(10) # print output for (word, count) in output: print(&amp;#34;%s: %i&amp;#34; % (word, count))   So that is a small example which counts the number of words in the document and prints 10 of them.
And most of the work gets done in the second command.
Don’t worry if you are not able to follow this yet as I still need to tell you about the things that make Spark work.
But before we get into Spark basics, Let us refresh some of our Python Basics. Understanding Spark becomes a lot easier if you have used functional programming with Python.
For those of you who haven’t used it, below is a brief intro.
A functional approach to programming in Python   1. Map map is used to map a function to an array or a list. Say you want to apply some function to every element in a list.
You can do this by simply using a for loop but python lambda functions let you do this in a single line in Python.
my_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(lambda x:x**2,my_list) print(list(squared_list)) ------------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] In the above example, you could think of map as a function which takes two arguments — A function and a list.
It then applies the function to every element of the list.
What lambda allows you to do is write an inline function. In here the part lambda x:x**2 defines a function that takes x as input and returns x².
You could have also provided a proper function in place of lambda. For example:
def squared(x): return x**2 my_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want to square each term in my_list. squared_list = map(squared,my_list) print(list(squared_list)) ------------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] The same result, but the lambda expressions make the code compact and a lot more readable.
2. Filter The other function that is used extensively is the filter function. This function takes two arguments — A condition and the list to filter.
If you want to filter your list using some condition you use filter.
my_list = [1,2,3,4,5,6,7,8,9,10] # Lets say I want only the even numbers in my list. filtered_list = filter(lambda x:x%2==0,my_list) print(list(filtered_list)) --------------------------------------------------------------- [2, 4, 6, 8, 10] 3. Reduce The next function I want to talk about is the reduce function. This function will be the workhorse in Spark.
This function takes two arguments — a function to reduce that takes two arguments, and a list over which the reduce function is to be applied.
import functools my_list = [1,2,3,4,5] # Lets say I want to sum all elements in my list. sum_list = functools.reduce(lambda x,y:x&#43;y,my_list) print(sum_list) In python2 reduce used to be a part of Python, now we have to use reduce as a part of functools.
Here the lambda function takes in two values x, y and returns their sum. Intuitively you can think that the reduce function works as:
 Reduce function first sends 1,2 ; the lambda function returns 3 Reduce function then sends 3,3 ; the lambda function returns 6 Reduce function then sends 6,4 ; the lambda function returns 10 Reduce function finally sends 10,5 ; the lambda function returns 15  A condition on the lambda function we use in reduce is that it must be:
 commutative that is a &#43; b = b &#43; a and
 associative that is (a &#43; b) &#43; c == a &#43; (b &#43; c).
  In the above case, we used sum which is commutative as well as associative. Other functions that we could have used: max, min, * etc.
Moving Again to Spark As we have now got the fundamentals of Python Functional Programming out of the way, lets again head to Spark.
But first, let us delve a little bit into how spark works. Spark actually consists of two things a driver and workers.
Workers normally do all the work and the driver makes them do that work.
RDD An RDD(Resilient Distributed Dataset) is a parallelized data structure that gets distributed across the worker nodes. They are the basic units of Spark programming.
In our wordcount example, in the first line
lines = sc.textFile(&amp;quot;/FileStore/tables/shakespeare.txt&amp;quot;)  We took a text file and distributed it across worker nodes so that they can work on it in parallel. We could also parallelize lists using the function sc.parallelize
For example:
data = [1,2,3,4,5,6,7,8,9,10] new_rdd = sc.parallelize(data,4) new_rdd --------------------------------------------------------------- ParallelCollectionRDD[22] at parallelize at PythonRDD.scala:267 In Spark, we can do two different types of operations on RDD: Transformations and Actions.
 Transformations: Create new datasets from existing RDDs
 Actions: Mechanism to get results out of Spark
  Transformation Basics   So let us say you have got your data in the form of an RDD.
To requote your data is now accessible to the worker machines. You want to do some transformations on the data now.
You may want to filter, apply some function, etc.
In Spark, this is done using Transformation functions.
Spark provides many transformation functions. You can see a comprehensive list here. Some of the main ones that I use frequently are:
1. Map: Applies a given function to an RDD.
Note that the syntax is a little bit different from Python, but it necessarily does the same thing. Don’t worry about collect yet. For now, just think of it as a function that collects the data in squared_rdd back to a list.
data = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) squared_rdd = rdd.map(lambda x:x**2) squared_rdd.collect() ------------------------------------------------------ [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 2. Filter: Again no surprises here. Takes as input a condition and keeps only those elements that fulfill that condition.
data = [1,2,3,4,5,6,7,8,9,10] rdd = sc.parallelize(data,4) filtered_rdd = rdd.filter(lambda x:x%2==0) filtered_rdd.collect() ------------------------------------------------------ [2, 4, 6, 8, 10] 3. distinct: Returns only distinct elements in an RDD.
data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10] rdd = sc.parallelize(data,4) distinct_rdd = rdd.distinct() distinct_rdd.collect() ------------------------------------------------------ [8, 4, 1, 5, 9, 2, 10, 6, 3, 7]  4. flatmap: Similar to map, but each input item can be mapped to 0 or more output items.
data = [1,2,3,4] rdd = sc.parallelize(data,4) flat_rdd = rdd.flatMap(lambda x:[x,x**3]) flat_rdd.collect() ------------------------------------------------------ [1, 1, 2, 8, 3, 27, 4, 64] 5. Reduce By Key: The parallel to the reduce in Hadoop MapReduce.
Now Spark cannot provide the value if it just worked with Lists.
In Spark, there is a concept of pair RDDs that makes it a lot more flexible. Let&amp;rsquo;s assume we have a data in which we have a product, its category, and its selling price. We can still parallelize the data.
data = [(&amp;#39;Apple&amp;#39;,&amp;#39;Fruit&amp;#39;,200),(&amp;#39;Banana&amp;#39;,&amp;#39;Fruit&amp;#39;,24),(&amp;#39;Tomato&amp;#39;,&amp;#39;Fruit&amp;#39;,56),(&amp;#39;Potato&amp;#39;,&amp;#39;Vegetable&amp;#39;,103),(&amp;#39;Carrot&amp;#39;,&amp;#39;Vegetable&amp;#39;,34)] rdd = sc.parallelize(data,4) Right now our RDD rdd holds tuples.
Now we want to find out the total sum of revenue that we got from each category.
To do that we have to transform our rdd to a pair rdd so that it only contains key-value pairs/tuples.
category_price_rdd = rdd.map(lambda x: (x[1],x[2])) category_price_rdd.collect() ----------------------------------------------------------------- [(‘Fruit’, 200), (‘Fruit’, 24), (‘Fruit’, 56), (‘Vegetable’, 103), (‘Vegetable’, 34)] Here we used the map function to get it in the format we wanted. When working with textfile, the RDD that gets formed has got a lot of strings. We use map to convert it into a format that we want.
So now our category_price_rdd contains the product category and the price at which the product sold.
Now we want to reduce on the key category and sum the prices. We can do this by:
category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x&#43;y) category_total_price_rdd.collect() --------------------------------------------------------- [(‘Vegetable’, 137), (‘Fruit’, 280)] 6. Group By Key: Similar to reduceByKey but does not reduces just puts all the elements in an iterator. For example, if we wanted to keep as key the category and as the value all the products we would use this function.
Let us again use map to get data in the required form.
data = [(&amp;#39;Apple&amp;#39;,&amp;#39;Fruit&amp;#39;,200),(&amp;#39;Banana&amp;#39;,&amp;#39;Fruit&amp;#39;,24),(&amp;#39;Tomato&amp;#39;,&amp;#39;Fruit&amp;#39;,56),(&amp;#39;Potato&amp;#39;,&amp;#39;Vegetable&amp;#39;,103),(&amp;#39;Carrot&amp;#39;,&amp;#39;Vegetable&amp;#39;,34)] rdd = sc.parallelize(data,4) category_product_rdd = rdd.map(lambda x: (x[1],x[0])) category_product_rdd.collect() ------------------------------------------------------------ [(&amp;#39;Fruit&amp;#39;, &amp;#39;Apple&amp;#39;), (&amp;#39;Fruit&amp;#39;, &amp;#39;Banana&amp;#39;), (&amp;#39;Fruit&amp;#39;, &amp;#39;Tomato&amp;#39;), (&amp;#39;Vegetable&amp;#39;, &amp;#39;Potato&amp;#39;), (&amp;#39;Vegetable&amp;#39;, &amp;#39;Carrot&amp;#39;)] We then use groupByKey as:
grouped_products_by_category_rdd = category_product_rdd.groupByKey() findata = grouped_products_by_category_rdd.collect() for data in findata: print(data[0],list(data[1])) ------------------------------------------------------------ Vegetable [&amp;#39;Potato&amp;#39;, &amp;#39;Carrot&amp;#39;] Fruit [&amp;#39;Apple&amp;#39;, &amp;#39;Banana&amp;#39;, &amp;#39;Tomato&amp;#39;] Here the groupByKey function worked and it returned the category and the list of products in that category.
Action Basics   You have filtered your data, mapped some functions on it. Done your computation.
Now you want to get the data on your local machine or save it to a file or show the results in the form of some graphs in excel or any visualization tool.
You will need actions for that. A comprehensive list of actions is provided here.
Some of the most common actions that I tend to use are:
1. collect: We have already used this action many times. It takes the whole RDD and brings it back to the driver program.
2. reduce: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.
rdd = sc.parallelize([1,2,3,4,5]) rdd.reduce(lambda x,y : x&#43;y) --------------------------------- 15 3. take: Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. take returns a list with the first n elements of the RDD.
rdd = sc.parallelize([1,2,3,4,5]) rdd.take(3) --------------------------------- [1, 2, 3] 4. takeOrdered: takeOrdered returns the first n elements of the RDD using either their natural order or a custom comparator.
rdd = sc.parallelize([5,3,12,23]) # descending order rdd.takeOrdered(3,lambda s:-1*s) ---- [23, 12, 5] rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)]) # descending order rdd.takeOrdered(3,lambda s:-1*s[1]) --- [(12, 344), (3, 34), (23, 29)] We have our basics covered finally. Let us get back to our wordcount example
Understanding The WordCount Example   Now we sort of understand the transformations and the actions provided to us by Spark.
It should not be difficult to understand the wordcount program now. Let us go through the program line by line.
The first line creates an RDD and distributes it to the workers.
lines = sc.textFile(&amp;quot;/FileStore/tables/shakespeare.txt&amp;quot;)  This RDD lines contains a list of sentences in the file. You can see the rdd content using take
lines.take(5) -------------------------------------------- [&#39;The Project Gutenberg EBook of The Complete Works of William Shakespeare, by &#39;, &#39;William Shakespeare&#39;, &#39;&#39;, &#39;This eBook is for the use of anyone anywhere at no cost and with&#39;, &#39;almost no restrictions whatsoever. You may copy it, give it away or&#39;]  This RDD is of the form:
[&#39;word1 word2 word3&#39;,&#39;word4 word3 word2&#39;]  This next line is actually the workhorse function in the whole script.
counts = (lines.flatMap(lambda x: x.split(&#39; &#39;)) .map(lambda x: (x, 1)) .reduceByKey(lambda x,y : x &#43; y))  It contains a series of transformations that we do to the lines RDD. First of all, we do a flatmap transformation.
The flatmap transformation takes as input the lines and gives words as output. So after the flatmap transformation, the RDD is of the form:
[&#39;word1&#39;,&#39;word2&#39;,&#39;word3&#39;,&#39;word4&#39;,&#39;word3&#39;,&#39;word2&#39;]  Next, we do a map transformation on the flatmap output which converts the RDD to :
[(&#39;word1&#39;,1),(&#39;word2&#39;,1),(&#39;word3&#39;,1),(&#39;word4&#39;,1),(&#39;word3&#39;,1),(&#39;word2&#39;,1)]  Finally, we do a reduceByKey transformation which counts the number of time each word appeared.
After which the RDD approaches the final desirable form.
[(&#39;word1&#39;,1),(&#39;word2&#39;,2),(&#39;word3&#39;,2),(&#39;word4&#39;,1)]  This next line is an action that takes the first 10 elements of the resulting RDD locally.
output = counts.take(10)  This line just prints the output
for (word, count) in output: print(&amp;quot;%s: %i&amp;quot; % (word, count))  And that is it for the wordcount program. Hope you understand it now.
So till now, we talked about the Wordcount example and the basic transformations and actions that you could use in Spark. But we don’t do wordcount in real life.
We have to work on bigger problems which are much more complex. Worry not! Whatever we have learned till now will let us do that and more.
Spark in Action with Example   Let us work with a concrete example which takes care of some usual transformations.
We will work on Movielens ml-100k.zip dataset which is a stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies. Released 4&amp;frasl;1998.
The Movielens dataset contains a lot of files but we are going to be working with 3 files only:
1) Users: This file name is kept as “u.user”, The columns in this file are:
[&#39;user_id&#39;, &#39;age&#39;, &#39;sex&#39;, &#39;occupation&#39;, &#39;zip_code&#39;]  2) Ratings: This file name is kept as “u.data”, The columns in this file are:
[&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;unix_timestamp&#39;]  3) Movies: This file name is kept as “u.item”, The columns in this file are:
[&#39;movie_id&#39;, &#39;title&#39;, &#39;release_date&#39;, &#39;video_release_date&#39;, &#39;imdb_url&#39;, and 18 more columns.....]  Let us start by importing these 3 files into our spark instance using ‘Import and Explore Data’ on the home tab.
  Our business partner now comes to us and asks us to find out the 25 most rated movie titles from this data. How many times a movie has been rated?
Let us load the data in different RDDs and see what the data contains.
userRDD = sc.textFile(&amp;#34;/FileStore/tables/u.user&amp;#34;) ratingRDD = sc.textFile(&amp;#34;/FileStore/tables/u.data&amp;#34;) movieRDD = sc.textFile(&amp;#34;/FileStore/tables/u.item&amp;#34;) print(&amp;#34;userRDD:&amp;#34;,userRDD.take(1)) print(&amp;#34;ratingRDD:&amp;#34;,ratingRDD.take(1)) print(&amp;#34;movieRDD:&amp;#34;,movieRDD.take(1)) ----------------------------------------------------------- userRDD: [&amp;#39;1|24|M|technician|85711&amp;#39;] ratingRDD: [&amp;#39;196\t242\t3\t881250949&amp;#39;] movieRDD: [&amp;#39;1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0&amp;#39;] We note that to answer this question we will need to use the ratingRDD. But the ratingRDD does not have the movie name.
So we would have to merge movieRDD and ratingRDD using movie_id.
How we would do that in Spark?
Below is the code. We also use a new transformation leftOuterJoin. Do read the docs and comments in the below code.
# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating. RDD_movid_rating = ratingRDD.map(lambda x : (x.split(&amp;#34;\t&amp;#34;)[1],x.split(&amp;#34;\t&amp;#34;)[2])) print(&amp;#34;RDD_movid_rating:&amp;#34;,RDD_movid_rating.take(4)) # Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title. RDD_movid_title = movieRDD.map(lambda x : (x.split(&amp;#34;|&amp;#34;)[0],x.split(&amp;#34;|&amp;#34;)[1])) print(&amp;#34;RDD_movid_title:&amp;#34;,RDD_movid_title.take(2)) # merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin(). See the transformation document. rdd_movid_title_rating = RDD_movid_rating.leftOuterJoin(RDD_movid_title) print(&amp;#34;rdd_movid_title_rating:&amp;#34;,rdd_movid_title_rating.take(1)) # use the RDD in previous step to create (movie,1) tuple pair RDD rdd_title_rating = rdd_movid_title_rating.map(lambda x: (x[1][1],1 )) print(&amp;#34;rdd_title_rating:&amp;#34;,rdd_title_rating.take(2)) # Use the reduceByKey transformation to reduce on the basis of movie_title rdd_title_ratingcnt = rdd_title_rating.reduceByKey(lambda x,y: x&#43;y) print(&amp;#34;rdd_title_ratingcnt:&amp;#34;,rdd_title_ratingcnt.take(2)) # Get the final answer by using takeOrdered Transformation print &amp;#34;#####################################&amp;#34; print &amp;#34;25 most rated movies:&amp;#34;,rdd_title_ratingcnt.takeOrdered(25,lambda x:-x[1]) print &amp;#34;#####################################&amp;#34; OUTPUT: --------------------------------------------------------------------RDD_movid_rating: [(&#39;242&#39;, &#39;3&#39;), (&#39;302&#39;, &#39;3&#39;), (&#39;377&#39;, &#39;1&#39;), (&#39;51&#39;, &#39;2&#39;)] RDD_movid_title: [(&#39;1&#39;, &#39;Toy Story (1995)&#39;), (&#39;2&#39;, &#39;GoldenEye (1995)&#39;)] rdd_movid_title_rating: [(&#39;1440&#39;, (&#39;3&#39;, &#39;Above the Rim (1994)&#39;))] rdd_title_rating: [(&#39;Above the Rim (1994)&#39;, 1), (&#39;Above the Rim (1994)&#39;, 1)] rdd_title_ratingcnt: [(&#39;Mallrats (1995)&#39;, 54), (&#39;Michael Collins (1996)&#39;, 92)] ##################################### 25 most rated movies: [(&#39;Star Wars (1977)&#39;, 583), (&#39;Contact (1997)&#39;, 509), (&#39;Fargo (1996)&#39;, 508), (&#39;Return of the Jedi (1983)&#39;, 507), (&#39;Liar Liar (1997)&#39;, 485), (&#39;English Patient, The (1996)&#39;, 481), (&#39;Scream (1996)&#39;, 478), (&#39;Toy Story (1995)&#39;, 452), (&#39;Air Force One (1997)&#39;, 431), (&#39;Independence Day (ID4) (1996)&#39;, 429), (&#39;Raiders of the Lost Ark (1981)&#39;, 420), (&#39;Godfather, The (1972)&#39;, 413), (&#39;Pulp Fiction (1994)&#39;, 394), (&#39;Twelve Monkeys (1995)&#39;, 392), (&#39;Silence of the Lambs, The (1991)&#39;, 390), (&#39;Jerry Maguire (1996)&#39;, 384), (&#39;Chasing Amy (1997)&#39;, 379), (&#39;Rock, The (1996)&#39;, 378), (&#39;Empire Strikes Back, The (1980)&#39;, 367), (&#39;Star Trek: First Contact (1996)&#39;, 365), (&#39;Back to the Future (1985)&#39;, 350), (&#39;Titanic (1997)&#39;, 350), (&#39;Mission: Impossible (1996)&#39;, 344), (&#39;Fugitive, The (1993)&#39;, 336), (&#39;Indiana Jones and the Last Crusade (1989)&#39;, 331)] #####################################  Star Wars is the most rated movie in the Movielens Dataset.
Now we could have done all this in a single command using the below command but the code is a little messy now.
I did this to show that you can use chaining functions with Spark and you could bypass the process of variable creation.
print(((ratingRDD.map(lambda x : (x.split(&amp;#34;\t&amp;#34;)[1],x.split(&amp;#34;\t&amp;#34;)[2]))). leftOuterJoin(movieRDD.map(lambda x : (x.split(&amp;#34;|&amp;#34;)[0],x.split(&amp;#34;|&amp;#34;)[1])))). map(lambda x: (x[1][1],1)). reduceByKey(lambda x,y: x&#43;y). takeOrdered(25,lambda x:-x[1])) Let us do one more. For practice:
Now we want to find the most highly rated 25 movies using the same dataset. We actually want only those movies which have been rated at least 100 times.
# We already have the RDD rdd_movid_title_rating: [(u&amp;#39;429&amp;#39;, (u&amp;#39;5&amp;#39;, u&amp;#39;Day the Earth Stood Still, The (1951)&amp;#39;))] # We create an RDD that contains sum of all the ratings for a particular movie rdd_title_ratingsum = (rdd_movid_title_rating. map(lambda x: (x[1][1],int(x[1][0]))). reduceByKey(lambda x,y:x&#43;y)) print(&amp;#34;rdd_title_ratingsum:&amp;#34;,rdd_title_ratingsum.take(2)) # Merge this data with the RDD rdd_title_ratingcnt we created in the last step # And use Map function to divide ratingsum by rating count. rdd_title_ratingmean_rating_count = (rdd_title_ratingsum. leftOuterJoin(rdd_title_ratingcnt). map(lambda x:(x[0],(float(x[1][0])/x[1][1],x[1][1])))) print(&amp;#34;rdd_title_ratingmean_rating_count:&amp;#34;,rdd_title_ratingmean_rating_count.take(1)) # We could use take ordered here only but we want to only get the movies which have count # of ratings more than or equal to 100 so lets filter the data RDD. rdd_title_rating_rating_count_gt_100 = (rdd_title_ratingmean_rating_count. filter(lambda x: x[1][1]&amp;gt;=100)) print(&amp;#34;rdd_title_rating_rating_count_gt_100:&amp;#34;,rdd_title_rating_rating_count_gt_100.take(1)) # Get the final answer by using takeOrdered Transformation print(&amp;#34;#####################################&amp;#34;) print (&amp;#34;25 highly rated movies:&amp;#34;) print(rdd_title_rating_rating_count_gt_100.takeOrdered(25,lambda x:-x[1][0])) print(&amp;#34;#####################################&amp;#34;) OUTPUT: ------------------------------------------------------------ rdd_title_ratingsum: [(&#39;Mallrats (1995)&#39;, 186), (&#39;Michael Collins (1996)&#39;, 318)] rdd_title_ratingmean_rating_count: [(&#39;Mallrats (1995)&#39;, (3.4444444444444446, 54))] rdd_title_rating_rating_count_gt_100: [(&#39;Butch Cassidy and the Sundance Kid (1969)&#39;, (3.949074074074074, 216))] ##################################### 25 highly rated movies: [(&#39;Close Shave, A (1995)&#39;, (4.491071428571429, 112)), (&amp;quot;Schindler&#39;s List (1993)&amp;quot;, (4.466442953020135, 298)), (&#39;Wrong Trousers, The (1993)&#39;, (4.466101694915254, 118)), (&#39;Casablanca (1942)&#39;, (4.45679012345679, 243)), (&#39;Shawshank Redemption, The (1994)&#39;, (4.445229681978798, 283)), (&#39;Rear Window (1954)&#39;, (4.3875598086124405, 209)), (&#39;Usual Suspects, The (1995)&#39;, (4.385767790262173, 267)), (&#39;Star Wars (1977)&#39;, (4.3584905660377355, 583)), (&#39;12 Angry Men (1957)&#39;, (4.344, 125)), (&#39;Citizen Kane (1941)&#39;, (4.292929292929293, 198)), (&#39;To Kill a Mockingbird (1962)&#39;, (4.292237442922374, 219)), (&amp;quot;One Flew Over the Cuckoo&#39;s Nest (1975)&amp;quot;, (4.291666666666667, 264)), (&#39;Silence of the Lambs, The (1991)&#39;, (4.28974358974359, 390)), (&#39;North by Northwest (1959)&#39;, (4.284916201117318, 179)), (&#39;Godfather, The (1972)&#39;, (4.283292978208232, 413)), (&#39;Secrets &amp;amp; Lies (1996)&#39;, (4.265432098765432, 162)), (&#39;Good Will Hunting (1997)&#39;, (4.262626262626263, 198)), (&#39;Manchurian Candidate, The (1962)&#39;, (4.259541984732825, 131)), (&#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)&#39;, (4.252577319587629, 194)), (&#39;Raiders of the Lost Ark (1981)&#39;, (4.252380952380952, 420)), (&#39;Vertigo (1958)&#39;, (4.251396648044692, 179)), (&#39;Titanic (1997)&#39;, (4.2457142857142856, 350)), (&#39;Lawrence of Arabia (1962)&#39;, (4.23121387283237, 173)), (&#39;Maltese Falcon, The (1941)&#39;, (4.2101449275362315, 138)), (&#39;Empire Strikes Back, The (1980)&#39;, (4.204359673024523, 367))] #####################################  We have talked about RDDs till now as they are very powerful.
You can use RDDs to work with non-relational databases too.
They let you do a lot of things that you couldn’t do with SparkSQL?
Yes, you can use SQL with Spark too which I am going to talk about now.
Spark DataFrames   Spark has provided DataFrame API for us Data Scientists to work with relational data. Here is the documentation for the adventurous folks.
Remember that in the background it still is all RDDs and that is why the starting part of this post focussed on RDDs.
I will start with some common functionalities you will need to work with Spark DataFrames. Would look a lot like Pandas with some syntax changes.
1. Reading the File ratings = spark.read.load(&amp;#34;/FileStore/tables/u.data&amp;#34;,format=&amp;#34;csv&amp;#34;, sep=&amp;#34;\t&amp;#34;, inferSchema=&amp;#34;true&amp;#34;, header=&amp;#34;false&amp;#34;) 2. Show File We have two ways to show files using Spark Dataframes.
ratings.show()   display(ratings)   I prefer display as it looks a lot nicer and clean.
3. Change Column names Good functionality. Always required. Don’t forget the * in front of the list.
ratings = ratings.toDF(*[&amp;#39;user_id&amp;#39;, &amp;#39;movie_id&amp;#39;, &amp;#39;rating&amp;#39;, &amp;#39;unix_timestamp&amp;#39;]) display(ratings)   4. Some Basic Stats print(ratings.count()) #Row Count print(len(ratings.columns)) #Column Count --------------------------------------------------------- 100000 4 We can also see the dataframe statistics using:
display(ratings.describe())   5. Select a few columns display(ratings.select(&amp;#39;user_id&amp;#39;,&amp;#39;movie_id&amp;#39;))   6. Filter Filter a dataframe using multiple conditions:
display(ratings.filter((ratings.rating==5) &amp;amp; (ratings.user_id==253)))   7. Groupby We can use groupby function with a spark dataframe too. Pretty much same as a pandas groupby with the exception that you will need to import pyspark.sql.functions
from pyspark.sql import functions as F display(ratings.groupBy(&amp;#34;user_id&amp;#34;).agg(F.count(&amp;#34;user_id&amp;#34;),F.mean(&amp;#34;rating&amp;#34;))) Here we have found the count of ratings and average rating from each user_id
  8. Sort display(ratings.sort(&amp;#34;user_id&amp;#34;))   We can also do a descending sort using F.desc function as below.
# descending Sort from pyspark.sql import functions as F display(ratings.sort(F.desc(&amp;#34;user_id&amp;#34;)))   Joins/Merging with Spark Dataframes I was not able to find a pandas equivalent of merge with Spark DataFrames but we can use SQL with dataframes and thus we can merge dataframes using SQL.
Let us try to run some SQL on Ratings.
We first register the ratings df to a temporary table ratings_table on which we can run sql operations.
As you can see the result of the SQL select statement is again a Spark Dataframe.
ratings.registerTempTable(&amp;#39;ratings_table&amp;#39;) newDF = sqlContext.sql(&amp;#39;select * from ratings_table where rating&amp;gt;4&amp;#39;) display(newDF)   Let us now add one more Spark Dataframe to the mix to see if we can use join using the SQL queries:
#get one more dataframe to join movies = spark.read.load(&amp;#34;/FileStore/tables/u.item&amp;#34;,format=&amp;#34;csv&amp;#34;, sep=&amp;#34;|&amp;#34;, inferSchema=&amp;#34;true&amp;#34;, header=&amp;#34;false&amp;#34;) # change column names movies = movies.toDF(*[&amp;#34;movie_id&amp;#34;,&amp;#34;movie_title&amp;#34;,&amp;#34;release_date&amp;#34;,&amp;#34;video_release_date&amp;#34;,&amp;#34;IMDb_URL&amp;#34;,&amp;#34;unknown&amp;#34;,&amp;#34;Action&amp;#34;,&amp;#34;Adventure&amp;#34;,&amp;#34;Animation &amp;#34;,&amp;#34;Children&amp;#34;,&amp;#34;Comedy&amp;#34;,&amp;#34;Crime&amp;#34;,&amp;#34;Documentary&amp;#34;,&amp;#34;Drama&amp;#34;,&amp;#34;Fantasy&amp;#34;,&amp;#34;Film_Noir&amp;#34;,&amp;#34;Horror&amp;#34;,&amp;#34;Musical&amp;#34;,&amp;#34;Mystery&amp;#34;,&amp;#34;Romance&amp;#34;,&amp;#34;Sci_Fi&amp;#34;,&amp;#34;Thriller&amp;#34;,&amp;#34;War&amp;#34;,&amp;#34;Western&amp;#34;]) display(movies)   Now let us try joining the tables on movie_id to get the name of the movie in the ratings table.
movies.registerTempTable(&amp;#39;movies_table&amp;#39;) display(sqlContext.sql(&amp;#39;select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id&amp;#39;))   Let us try to do what we were doing earlier with the RDDs. Finding the top 25 most rated movies:
mostrateddf = sqlContext.sql(&amp;#39;select movie_id,movie_title, count(user_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title order by num_ratings desc &amp;#39;) display(mostrateddf)   And finding the top 25 highest rated movies having more than 100 votes:
highrateddf = sqlContext.sql(&amp;#39;select movie_id,movie_title, avg(rating) as avg_rating,count(movie_id) as num_ratings from (select ratings_table.*,movies_table.movie_title from ratings_table left join movies_table on movies_table.movie_id = ratings_table.movie_id)A group by movie_id,movie_title having num_ratings&amp;gt;100 order by avg_rating desc &amp;#39;) display(highrateddf)   I have used GROUP BY, HAVING, AND ORDER BY clauses as well as aliases in the above query. That shows that you can do pretty much complex stuff using sqlContext.sql
A Small Note About Display You can also use display command to display charts in your notebooks.
  You can see more options when you select Plot Options.
  Converting from Spark Dataframe to RDD and vice versa: Sometimes you may want to convert to RDD from a spark Dataframe or vice versa so that you can have the best of both worlds.
To convert from DF to RDD, you can simply do :
highratedrdd =highrateddf.rdd highratedrdd.take(2)   To go from an RDD to a dataframe:
from pyspark.sql import Row # creating a RDD first data = [(&amp;#39;A&amp;#39;,1),(&amp;#39;B&amp;#39;,2),(&amp;#39;C&amp;#39;,3),(&amp;#39;D&amp;#39;,4)] rdd = sc.parallelize(data) # map the schema using Row. rdd_new = rdd.map(lambda x: Row(key=x[0], value=int(x[1]))) # Convert the rdd to Dataframe rdd_as_df = sqlContext.createDataFrame(rdd_new) display(rdd_as_df)   RDD provides you with more control at the cost of time and coding effort. While Dataframes provide you with familiar coding platform. And now you can move back and forth between these two.
Conclusion   This was a big post and congratulations if you reached the end.
Spark has provided us with an interface where we could use transformations and actions on our data. Spark also has the Dataframe API to ease the transition of Data scientists to Big Data.
Hopefully, I’ve covered the basics well enough to pique your interest and help you get started with Spark.
You can find all the code at the GitHub repository.
Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out an excellent course on Big Data Essentials which is part of the Big Data Specialization provided by Yandex.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>An End to End Introduction to GANs</title>
      <link>https://mlwhiz.com/blog/2019/06/17/gans/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/06/17/gans/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/gans/faces.png"></media:content>
      

      
      <description>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.
In this post, I will help the reader to understand how they can create and build such applications on their own.</description>

      <content:encoded>  
        
        <![CDATA[    I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.
In this post, I will help the reader to understand how they can create and build such applications on their own.
I will try to keep this post as intuitive as possible for starters while not dumbing it down too much.
This post is about understanding how GANs work.
Task Overview I will work on creating our own anime characters using anime characters dataset.
The DC-GAN flavor of GANs which I will use here is widely applicable not only to generate Faces or new anime characters; it can also be used to create modern fashion styles, for general content creation and sometimes for data augmentation purposes as well.
As per my view, GANs will change the way video games and special effects are generated. The approach could create realistic textures or characters on demand.
You can find the full code for this chapter in the Github Repository. I have also uploaded the code to Google Colab so that you can try it yourself.
Using DCGAN architecture to generate anime images As always before we get into the coding, it helps to delve a little bit into the theory.
The main idea of DC-GAN’s stemmed from the paper UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS written in 2016 by Alec Radford, Luke Metz, and Soumith Chintala.
Although I am going to explain the paper in the next few sections, do take a look at it. It is an excellent paper.
INTUITION: Brief Intro to GANs for Generating Fake Images   Generator vs. Discriminator    Typically, GANs employ two dueling neural networks to train a computer to learn the nature of a data set well enough to generate convincing fakes.
We can think of this as two systems where one Neural Network works to generate fakes (Generator), and another neural network (Discriminator) tries to classify which image is a fake.
As both generator and discriminator networks do this repetitively, the networks eventually get better at their respective tasks.
Think of this as simple as swordplay. Two noobs start sparring with each other. After a while, both become better at swordplay.
Or you could think of this as a robber(generator) and a policeman(Discriminator). After a lot of thefts, the robber becomes better at thieving while the policeman gets better at catching the robber. In an ideal world.
The Losses in these neural networks are primarily a function of how the other network performs:
 Discriminator network loss is a function of generator network quality- Loss is high for the discriminator if it gets fooled by the generator’s fake images
 Generator network loss is a function of discriminator network quality — Loss is high if the generator is not able to fool the discriminator.
  In the training phase, we train our Discriminator and Generator networks sequentially intending to improve both the Discriminator and Generator performance.
The objective is to end up with weights that help Generators to generate realistic looking images. In the end, we can use the Generator Neural network to generate fake images from Random Noise.
Generator architecture One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.
     The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.
Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:
     How do we get such an architecture?
In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:
  tc   We don’t have to worry about any weights right now as the network itself will learn those while training.
Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don’t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.
Now, What are Transpose convolutions? In most simple terms, transpose convolutions provide us with a way to upsample images. While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:
     Q: We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don’t we use Un-pooling?
It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.
Discriminator architecture Now, as we have understood the generator architecture, here is the discriminator as a black box.
In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:
     Takes an image as input and predicts if it is real/fake. Every image conv net ever.
Data preprocessing and visualization The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:
filenames = glob.glob(&amp;#39;animeface-character-dataset/*/*.pn*&amp;#39;) plt.figure(figsize=(10, 8)) for i in range(5): img = plt.imread(filenames[i], 0) plt.subplot(4, 5, i&#43;1) plt.imshow(img) plt.title(img.shape) plt.xticks([]) plt.yticks([]) plt.tight_layout() plt.show() The resultant output is as follows:
     We get to see the sizes of the images and the images themselves.
We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.
We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.
# A function to normalize image pixels. def norm_img(img): &amp;#39;&amp;#39;&amp;#39;A function to Normalize Images. Input: img : Original image as numpy array. Output: Normailized Image as numpy array &amp;#39;&amp;#39;&amp;#39; img = (img / 127.5) - 1 return img def denorm_img(img): &amp;#39;&amp;#39;&amp;#39;A function to Denormailze, i.e. recreate image from normalized image Input: img : Normalized image as numpy array. Output: Original Image as numpy array &amp;#39;&amp;#39;&amp;#39; img = (img &#43; 1) * 127.5 return img.astype(np.uint8) def sample_from_dataset(batch_size, image_shape, data_dir=None): &amp;#39;&amp;#39;&amp;#39;Create a batch of image samples by sampling random images from a data directory. Resizes the image using image_shape and normalize the images. Input: batch_size : Sample size required image_size : Size that Image should be resized to data_dir : Path of directory where training images are placed. Output: sample : batch of processed images &amp;#39;&amp;#39;&amp;#39; sample_dim = (batch_size,) &#43; image_shape sample = np.empty(sample_dim, dtype=np.float32) all_data_dirlist = list(glob.glob(data_dir)) sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size) for index,img_filename in enumerate(sample_imgs_paths): image = Image.open(img_filename) image = image.resize(image_shape[:-1]) image = image.convert(&amp;#39;RGB&amp;#39;) image = np.asarray(image) image = norm_img(image) sample[index,...] = image return sample As you will see, we will be using the preceding defined functions in the training part of our code.
Implementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.
Generating noise vector for Generator   Kids: Normal Noise generators    The following code block is a helper function to create a noise vector of predefined length for a Generator. It will generate the noise which we want to convert to an image using our generator architecture.
We use a normal distribution
to generate the noise vector:
def gen_noise(batch_size, noise_shape): &amp;#39;&amp;#39;&amp;#39; Generates a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) Input: batch_size : size of batch noise_shape: shape of noise vector, normally kept as 100 Output:a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) &amp;#39;&amp;#39;&amp;#39; return np.random.normal(0, 1, size=(batch_size,)&#43;noise_shape) Generator architecture The Generator is the most crucial part of the GAN.
Here, I create a generator by adding some transposed convolution layers to upsample the noise vector to an image.
As you will notice, this generator architecture is not the same as given in the Original DC-GAN paper.
I needed to make some architectural changes to fit our data better, so I added a convolution layer in the middle and removed all dense layers from the generator architecture, making it fully convolutional.
I also use a lot of Batchnorm layers with a momentum of 0.5 and leaky ReLU activation. I use Adam optimizer with β=0.5. The following code block is the function I will use to create the generator:
def get_gen_normal(noise_shape): &amp;#39;&amp;#39;&amp;#39; This function takes as input shape of the noise vector and creates the Keras generator architecture. &amp;#39;&amp;#39;&amp;#39; kernel_init = &amp;#39;glorot_uniform&amp;#39; gen_input = Input(shape = noise_shape) # Transpose 2D conv layer 1. generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = &amp;#34;valid&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(gen_input) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 2. generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 3. generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 4. generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # conv 2D layer 1. generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) # Tanh activation to get final normalized image generator = Activation(&amp;#39;tanh&amp;#39;)(generator) # defining the optimizer and compiling the generator model. gen_opt = Adam(lr=0.00015, beta_1=0.5) generator_model = Model(input = gen_input, output = generator) generator_model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=gen_opt, metrics=[&amp;#39;accuracy&amp;#39;]) generator_model.summary() return generator_model You can plot the final generator model:
plot_model(generator, to_file=&amp;#39;gen_plot.png&amp;#39;, show_shapes=True, show_layer_names=True)   Generator Architecture    Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.
Here is the architecture of the discriminator:
def get_disc_normal(image_shape=(64,64,3)): dropout_prob = 0.4 kernel_init = &amp;#39;glorot_uniform&amp;#39; dis_input = Input(shape = image_shape) # Conv layer 1: discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(dis_input) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 2: discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 3:  discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 4: discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator)#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator) # Flatten discriminator = Flatten()(discriminator) # Dense Layer discriminator = Dense(1)(discriminator) # Sigmoid Activation discriminator = Activation(&amp;#39;sigmoid&amp;#39;)(discriminator) # Optimizer and Compiling model dis_opt = Adam(lr=0.0002, beta_1=0.5) discriminator_model = Model(input = dis_input, output = discriminator) discriminator_model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=dis_opt, metrics=[&amp;#39;accuracy&amp;#39;]) discriminator_model.summary() return discriminator_modelplot_model(discriminator, to_file=&amp;#39;dis_plot.png&amp;#39;, show_shapes=True, show_layer_names=True)   Discriminator Architecture    Training Understanding how the training works in GAN is essential. And maybe a little interesting too.
I start by creating our discriminator and generator using the functions defined in the previous section:
discriminator = get_disc_normal(image_shape) generator = get_gen_normal(noise_shape) The generator and discriminator are then combined to create the final GAN.
discriminator.trainable = False # Optimizer for the GAN opt = Adam(lr=0.00015, beta_1=0.5) #same as generator # Input to the generator gen_inp = Input(shape=noise_shape) GAN_inp = generator(gen_inp) GAN_opt = discriminator(GAN_inp) # Final GAN gan = Model(input = gen_inp, output = GAN_opt) gan.compile(loss = &amp;#39;binary_crossentropy&amp;#39;, optimizer = opt, metrics=[&amp;#39;accuracy&amp;#39;]) plot_model(gan, to_file=&amp;#39;gan_plot.png&amp;#39;, show_shapes=True, show_layer_names=True) This is the architecture of our whole GAN:
The Training Loop This is the main region where we need to understand how the blocks we have created until now assemble and work together to work as one.
# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(&amp;#34;Begin step: &amp;#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) # Genearate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) # Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 10th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir&#43;step_num&#43;&amp;#34;_image.png&amp;#34;) # Create the labels for real and fake data. We don&amp;#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(&amp;#34;Disc: real loss: %ffake loss: %f&amp;#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) # Train the generator using a random vector of noise and its labels (1&amp;#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(&amp;#34;GAN loss: %f&amp;#34; % (gan_metrics[0])) # Log results by opening a file in append mode text_file = open(log_dir&#43;&amp;#34;\\training_log.txt&amp;#34;, &amp;#34;a&amp;#34;) text_file.write(&amp;#34;Step: %dDisc: real loss: %ffake loss: %fGAN loss: %f\n&amp;#34; % (tot_step, dis_metrics_real[0], dis_metrics_fake[0],gan_metrics[0])) text_file.close() # save GAN loss to plot later avg_GAN_loss.append(gan_metrics[0]) end_time = time.time() diff_time = int(end_time - step_begin_time) print(&amp;#34;Step %dcompleted. Time took: %ssecs.&amp;#34; % (tot_step, diff_time)) # save model at every 500 steps if ((tot_step&#43;1) % 500) == 0: print(&amp;#34;-----------------------------------------------------------------&amp;#34;) print(&amp;#34;Average Disc_fake loss: %f&amp;#34; % (np.mean(avg_disc_fake_loss))) print(&amp;#34;Average Disc_real loss: %f&amp;#34; % (np.mean(avg_disc_real_loss))) print(&amp;#34;Average GAN loss: %f&amp;#34; % (np.mean(avg_GAN_loss))) print(&amp;#34;-----------------------------------------------------------------&amp;#34;) discriminator.trainable = False generator.trainable = False # predict on fixed_noise fixed_noise_generate = generator.predict(noise) step_num = str(tot_step).zfill(4) save_img_batch(fixed_noise_generate,img_save_dir&#43;step_num&#43;&amp;#34;fixed_image.png&amp;#34;) generator.save(save_model_dir&#43;str(tot_step)&#43;&amp;#34;_GENERATOR_weights_and_arch.hdf5&amp;#34;) discriminator.save(save_model_dir&#43;str(tot_step)&#43;&amp;#34;_DISCRIMINATOR_weights_and_arch.hdf5&amp;#34;) Don’t worry, I will try to break the above code step by step here. The main steps in every training iteration are:
Step 1: Sample a batch of normalized images from the dataset directory
# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(&amp;#34;Begin step: &amp;#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) Step2:Generate noise for input to the generator
# Generate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) Step3:Generate images using random noise using the generator.
# Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 100th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir&#43;step_num&#43;&amp;#34;_image.png&amp;#34;) Step 4:Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their noisy labels.
# Create the labels for real and fake data. We don&amp;#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(&amp;#34;Disc: real loss: %ffake loss: %f&amp;#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) Step 5:Train the GAN using noise as X and 1&amp;rsquo;s(noisy) as Y while keeping discriminator as untrainable.
# Train the generator using a random vector of noise and its labels (1&amp;#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(&amp;#34;GAN loss: %f&amp;#34; % (gan_metrics[0])) We repeat the steps using the for loop to end up with a good discriminator and generator.
Results The final output image looks like the following. As we can see, the GAN can generate pretty good images for our content editor friends to work with.
They might be a little crude for your liking, but still, this project was a starter for our GAN journey.
     Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average and the variance is decreasing too as we do more steps. One might want to train for even more iterations to get better results.
Image generated at every 1500 steps You can see the output and running code in Colab:
# Generating GIF from PNGs import imageio # create a list of PNGs generated_images = [img_save_dir&#43;str(x).zfill(4)&#43;&amp;#34;_image.png&amp;#34; for x in range(0,num_steps,100)] images = [] for filename in generated_images: images.append(imageio.imread(filename)) imageio.mimsave(img_save_dir&#43;&amp;#39;movie.gif&amp;#39;, images) from IPython.display import Image with open(img_save_dir&#43;&amp;#39;movie.gif&amp;#39;,&amp;#39;rb&amp;#39;) as f: display(Image(data=f.read(), format=&amp;#39;png&amp;#39;)) Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases the images are getting better.
# create a list of 20 PNGs to show generated_images = [img_save_dir&#43;str(x).zfill(4)&#43;&amp;#34;fixed_image.png&amp;#34; for x in range(0,num_steps,1500)] print(&amp;#34;Displaying generated images&amp;#34;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(16,20)) gs1 = gridspec.GridSpec(5, 4) gs1.update(wspace=0, hspace=0) for i,image in enumerate(generated_images): ax1 = plt.subplot(gs1[i]) ax1.set_aspect(&amp;#39;equal&amp;#39;) step = image.split(&amp;#34;fixed&amp;#34;)[0] image = Image.open(image) fig = plt.imshow(image) # you might need to change some params here fig = plt.text(20,47,&amp;#34;Step: &amp;#34;&#43;step,bbox=dict(facecolor=&amp;#39;red&amp;#39;, alpha=0.5),fontsize=12) plt.axis(&amp;#39;off&amp;#39;) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.tight_layout() plt.savefig(&amp;#34;GENERATEDimage.png&amp;#34;,bbox_inches=&amp;#39;tight&amp;#39;,pad_inches=0) plt.show() Given below is the result of the GAN at different time steps:
Conclusion In this post, we learned about the basics of GAN. We also learned about the Generator and Discriminator architecture for DC-GANs, and we built a simple DC-GAN to generate anime images from scratch.
This model is not very good at generating fake images, yet we get to understand the basics of GANs with this project, and we are fired up to build more exciting and complex GANs as we go forward.
The DC-GAN flavor of GANs is widely applicable not only to generate Faces or new anime characters, but it can also be used to generate new fashion styles, for general content creation and sometimes for data augmentation purposes as well.
We can now conjure up realistic textures or characters on demand if we have the training data at hand, and that is no small feat.
If you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew NG. Andrew is a great instructor, and this course is great too.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Hitchhiker’s Guide to Feature Extraction</title>
      <link>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/features/brain.png"></media:content>
      

      
      <description>Good Features are the backbone of any machine learning model.
And good feature creation often needs domain knowledge, creativity, and lots of time.
In this post, I am going to talk about:
 Various methods of feature creation- Both Automated and manual
 Different Ways to handle categorical features
 Longitude and Latitude features
 Some kaggle tricks
 And some other ideas to think about feature creation.</description>

      <content:encoded>  
        
        <![CDATA[    Good Features are the backbone of any machine learning model.
And good feature creation often needs domain knowledge, creativity, and lots of time.
In this post, I am going to talk about:
 Various methods of feature creation- Both Automated and manual
 Different Ways to handle categorical features
 Longitude and Latitude features
 Some kaggle tricks
 And some other ideas to think about feature creation.
  TLDR; this post is about useful feature engineering methods and tricks that I have learned and end up using often.
1. Automatic Feature Creation using featuretools:   Automation is the future    Have you read about featuretools yet? If not, then you are going to be delighted.
Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.
How? Let us work with a toy example to show you the power of featuretools.
Let us say that we have three tables in our database: Customers, Sessions, and Transactions.
  Datasets and relationships          This is a reasonably good toy dataset to work on since it has time-based columns as well as categorical and numerical columns.
If we were to create features on this data, we would need to do a lot of merging and aggregations using Pandas.
Featuretools makes it so easy for us. Though there are a few things, we will need to learn before our life gets easier.
Featuretools works with entitysets.
You can understand an entityset as a bucket for dataframes as well as relationships between them.
  Entityset = Bucket of dataframes and relationships    So without further ado, let us create an empty entityset. I just gave the name as customers. You can use any name here. It is just an empty bucket right now.
# Create new entityset es = ft.EntitySet(id = &amp;#39;customers&amp;#39;) Let us add our dataframes to it. The order of adding dataframes is not important. To add a dataframe to an existing entityset, we do the below operation.
# Create an entity from the customers dataframe es = es.entity_from_dataframe(entity_id = &amp;#39;customers&amp;#39;, dataframe = customers_df, index = &amp;#39;customer_id&amp;#39;, time_index = &amp;#39;join_date&amp;#39; ,variable_types = {&amp;#34;zip_code&amp;#34;: ft.variable_types.ZIPCode}) So here are a few things we did here to add our dataframe to the empty entityset bucket.
 Provided a entity_id: This is just a name. Put it as customers.
 dataframe name set as customers_df
 index : This argument takes as input the primary key in the table
 time_index : The time index is defined as the first time that any information from a row can be used. For customers, it is the joining date. For transactions, it will be the transaction time.
 variable_types: This is used to specify if a particular variable must be handled differently. In our Dataframe, we have the zip_code variable, and we want to treat it differently, so we use this. These are the different variable types we could use:
  [featuretools.variable_types.variable.Datetime, featuretools.variable_types.variable.Numeric, featuretools.variable_types.variable.Timedelta, featuretools.variable_types.variable.Categorical, featuretools.variable_types.variable.Text, featuretools.variable_types.variable.Ordinal, featuretools.variable_types.variable.Boolean, featuretools.variable_types.variable.LatLong, featuretools.variable_types.variable.ZIPCode, featuretools.variable_types.variable.IPAddress, featuretools.variable_types.variable.EmailAddress, featuretools.variable_types.variable.URL, featuretools.variable_types.variable.PhoneNumber, featuretools.variable_types.variable.DateOfBirth, featuretools.variable_types.variable.CountryCode, featuretools.variable_types.variable.SubRegionCode, featuretools.variable_types.variable.FilePath]  This is how our entityset bucket looks right now. It has just got one dataframe in it. And no relationships
  Let us add all our dataframes:
# adding the transactions_df es = es.entity_from_dataframe(entity_id=&amp;#34;transactions&amp;#34;, dataframe=transactions_df, index=&amp;#34;transaction_id&amp;#34;, time_index=&amp;#34;transaction_time&amp;#34;, variable_types={&amp;#34;product_id&amp;#34;: ft.variable_types.Categorical}) # adding sessions_df es = es.entity_from_dataframe(entity_id=&amp;#34;sessions&amp;#34;, dataframe=sessions_df, index=&amp;#34;session_id&amp;#34;, time_index = &amp;#39;session_start&amp;#39;) This is how our entityset buckets look now.
  All three dataframes but no relationships. By relationships, I mean that my bucket doesn’t know that customer_id in customers_df and session_df are the same columns.
We can provide this information to our entityset as:
# adding the customer_id relationship cust_relationship = ft.Relationship(es[&amp;#34;customers&amp;#34;][&amp;#34;customer_id&amp;#34;], es[&amp;#34;sessions&amp;#34;][&amp;#34;customer_id&amp;#34;]) # Add the relationship to the entity set es = es.add_relationship(cust_relationship) # adding the session_id relationship sess_relationship = ft.Relationship(es[&amp;#34;sessions&amp;#34;][&amp;#34;session_id&amp;#34;], es[&amp;#34;transactions&amp;#34;][&amp;#34;session_id&amp;#34;]) # Add the relationship to the entity set es = es.add_relationship(sess_relationship) After this our entityset looks like:
  We can see the datasets as well as the relationships. Most of our work here is done. We are ready to cook features.
  Cooking is no different from feature engineering. Think of features as ingredients.    Creating features is as simple as:
feature_matrix, feature_defs = ft.dfs(entityset=es, target_entity=&amp;#34;customers&amp;#34;,max_depth = 2) feature_matrix.head()   And we end up with 73 new features. You can see the feature names from feature_defs. Some of the features that we end up creating are:
[&amp;lt;Feature: NUM_UNIQUE(sessions.device)&amp;gt;, &amp;lt;Feature: MODE(sessions.device)&amp;gt;, &amp;lt;Feature: SUM(transactions.amount)&amp;gt;, &amp;lt;Feature: STD(transactions.amount)&amp;gt;, &amp;lt;Feature: MAX(transactions.amount)&amp;gt;, &amp;lt;Feature: SKEW(transactions.amount)&amp;gt;, &amp;lt;Feature: DAY(join_date)&amp;gt;, &amp;lt;Feature: YEAR(join_date)&amp;gt;, &amp;lt;Feature: MONTH(join_date)&amp;gt;, &amp;lt;Feature: WEEKDAY(join_date)&amp;gt;, &amp;lt;Feature: SUM(sessions.STD(transactions.amount))&amp;gt;, &amp;lt;Feature: SUM(sessions.MAX(transactions.amount))&amp;gt;, &amp;lt;Feature: SUM(sessions.SKEW(transactions.amount))&amp;gt;, &amp;lt;Feature: SUM(sessions.MIN(transactions.amount))&amp;gt;, &amp;lt;Feature: SUM(sessions.MEAN(transactions.amount))&amp;gt;, &amp;lt;Feature: SUM(sessions.NUM_UNIQUE(transactions.product_id))&amp;gt;, &amp;lt;Feature: STD(sessions.SUM(transactions.amount))&amp;gt;, &amp;lt;Feature: STD(sessions.MAX(transactions.amount))&amp;gt;, &amp;lt;Feature: STD(sessions.SKEW(transactions.amount))&amp;gt;, &amp;lt;Feature: STD(sessions.MIN(transactions.amount))&amp;gt;, &amp;lt;Feature: STD(sessions.MEAN(transactions.amount))&amp;gt;, &amp;lt;Feature: STD(sessions.COUNT(transactions))&amp;gt;, &amp;lt;Feature: STD(sessions.NUM_UNIQUE(transactions.product_id))&amp;gt;]  You can get features like the Sum of std of amount(SUM(sessions.STD(transactions.amount))) or Std of the sum of amount(STD(sessions.SUM(transactions.amount))) This is what max_depth parameter means in the function call. Here we specify it as 2 to get two level aggregations.
If we change max_depth to 3 we can get features like: MAX(sessions.NUM_UNIQUE(transactions.YEAR(transaction_time)))
Just think of how much time you would have to spend if you had to write code to get such features. Also, a caveat is that increasing the max_depth might take longer times.
2. Handling Categorical Features: Label/Binary/Hashing and Target/Mean Encoding Creating automated features has its perks. But why would we data scientists be required if a simple library could do all our work?
This is the section where I will talk about handling categorical features.
One hot encoding   One Hot Coffee    We can use One hot encoding to encode our categorical features. So if we have n levels in a category, we will get n-1 features.
In our sessions_df table, we have a column named device, which contains three levels — desktop, mobile, or tablet. We can get two columns from such a column using:
pd.get_dummies(sessions_df[&amp;#39;device&amp;#39;],drop_first=True)   This is the most natural thing that comes to mind when talking about categorical features and works well in many cases.
OrdinalEncoding Sometimes there is an order associated with categories. In such a case, I usually use a simple map/apply function in pandas to create a new ordinal column.
For example, if I had a dataframe containing temperature as three levels: high medium and low, I would encode that as:
map_dict = {&amp;#39;low&amp;#39;:0,&amp;#39;medium&amp;#39;:1,&amp;#39;high&amp;#39;:2} def map_values(x): return map_dict[x] df[&amp;#39;Temperature_oe&amp;#39;] = df[&amp;#39;Temperature&amp;#39;].apply(lambda x: map_values(x))   Using this I preserve the information that low &amp;lt; medium &amp;lt; high
LabelEncoder We could also have used LabelEncoder to encode our variable to numbers. What a label encoder essentially does is that it sees the first value in the column and converts it to 0, next value to 1 and so on. This approach works reasonably well with tree models, and I end up using it when I have a lot of levels in the categorical variable. We can use this as:
from sklearn.preprocessing import LabelEncoder # create a labelencoder object le = LabelEncoder() # fit and transform on the data sessions_df[&amp;#39;device_le&amp;#39;] = le.fit_transform(sessions_df[&amp;#39;device&amp;#39;]) sessions_df.head()   BinaryEncoder BinaryEncoder is another method that one can use to encode categorical variables. It is an excellent method to use if you have many levels in a column. While we can encode a column with 1024 levels using 1023 columns using One Hot Encoding, using Binary encoding we can do it by just using ten columns.
Let us say we have a column in our FIFA 19 player data that contains all club names. This column has 652 unique values. One Hot encoding means creating 651 columns that would mean a lot of memory usage and a lot of sparse columns.
If we use Binary encoder, we will only need ten columns as 2⁹&amp;lt;652 &amp;lt;2¹⁰.
We can binaryEncode this variable easily by using BinaryEncoder object from category_encoders:
from category_encoders.binary import BinaryEncoder # create a Binaryencoder object be = BinaryEncoder(cols = [&amp;#39;Club&amp;#39;]) # fit and transform on the data players = be.fit_transform(players)   HashingEncoder   One can think of Hashing Encoder as a black box function that converts a string to a number between 0 to some prespecified value.
It differs from binary encoding as in binary encoding two or more of the club parameters could have been 1 while in hashing only one value is 1.
We can use hashing as:
players = pd.read_csv(&amp;#34;../input/fifa19/data.csv&amp;#34;) from category_encoders.hashing import HashingEncoder # create a HashingEncoder object he = HashingEncoder(cols = [&amp;#39;Club&amp;#39;]) # fit and transform on the data players = he.fit_transform(players)   There are bound to be collisions(two clubs having the same encoding. For example, Juventus and PSG have the same encoding) but sometimes this technique works well.
Target/Mean Encoding   This is a technique that I found works pretty well in Kaggle competitions. If both training/test comes from the same dataset from the same time period(cross-sectional), we can get crafty with features.
For example: In the Titanic knowledge challenge, the test data is randomly sampled from the train data. In this case, we can use the target variable averaged over different categorical variable as a feature.
In Titanic, we can create a target encoded feature over the PassengerClass variable.
We have to be careful when using Target encoding as it might induce overfitting in our models. Thus we use k-fold target encoding when we use it.
# taken from https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b from sklearn import base from sklearn.model_selection import KFold class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin): def __init__(self,colnames,targetName, n_fold=5, verbosity=True, discardOriginal_col=False): self.colnames = colnames self.targetName = targetName self.n_fold = n_fold self.verbosity = verbosity self.discardOriginal_col = discardOriginal_col def fit(self, X, y=None): return self def transform(self,X): assert(type(self.targetName) == str) assert(type(self.colnames) == str) assert(self.colnames in X.columns) assert(self.targetName in X.columns) mean_of_target = X[self.targetName].mean() kf = KFold(n_splits = self.n_fold, shuffle = True, random_state=2019) col_mean_name = self.colnames &#43; &amp;#39;_&amp;#39; &#43; &amp;#39;Kfold_Target_Enc&amp;#39; X[col_mean_name] = np.nan for tr_ind, val_ind in kf.split(X): X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind] X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames) [self.targetName].mean()) X[col_mean_name].fillna(mean_of_target, inplace = True) if self.verbosity: encoded_feature = X[col_mean_name].values print(&amp;#39;Correlation between the new feature, {} and, {} is {}.&amp;#39;.format(col_mean_name,self.targetName, np.corrcoef(X[self.targetName].values, encoded_feature)[0][1])) if self.discardOriginal_col: X = X.drop(self.targetName, axis=1) return X We can then create a mean encoded feature as:
targetc = KFoldTargetEncoderTrain(&amp;#39;Pclass&amp;#39;,&amp;#39;Survived&amp;#39;,n_fold=5) new_train = targetc.fit_transform(train) new_train[[&amp;#39;Pclass_Kfold_Target_Enc&amp;#39;,&amp;#39;Pclass&amp;#39;]]   You can see how the passenger class 3 gets encoded as 0.261538 and 0.230570 based on which fold the average is taken from.
This feature is pretty helpful as it encodes the value of the target for the category. Just looking at this feature, we can say that the Passenger in class 1 has a high propensity of surviving compared with Class 3.
3. Some Kaggle Tricks: While not necessarily feature creation techniques, some postprocessing techniques that you may find useful.
log loss clipping Technique: Something that I learned in the Neural Network course by Jeremy Howard. It is based on an elementary Idea.
Log loss penalizes us a lot if we are very confident and wrong.
So in the case of Classification problems where we have to predict probabilities in Kaggle, it would be much better to clip our probabilities between 0.05–0.95 so that we are never very sure about our prediction. And in turn, get penalized less. Can be done by a simple np.clip
Kaggle submission in gzip format: A small piece of code that will help you save countless hours of uploading. Enjoy.
df.to_csv(‘submission.csv.gz’, index=False, compression=’gzip’) 4. Using Latitude and Longitude features: This part will tread upon how to use Latitude and Longitude features well.
For this task, I will be using Data from the Playground competition: New York City Taxi Trip Duration
The train data looks like:
  Most of the functions I am going to write here are inspired by a Kernel on Kaggle written by Beluga.
In this competition, we had to predict the trip duration. We were given many features in which Latitude and Longitude of pickup and Dropoff were also there. We created features like:
A. Haversine Distance Between the Two Lat/Lons:  The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes
 def haversine_array(lat1, lng1, lat2, lng2): lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) AVG_EARTH_RADIUS = 6371 # in km lat = lat2 - lat1 lng = lng2 - lng1 d = np.sin(lat * 0.5) ** 2 &#43; np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) return h We could then use the function as:
train[&amp;#39;haversine_distance&amp;#39;] = train.apply(lambda x: haversine_array(x[&amp;#39;pickup_latitude&amp;#39;], x[&amp;#39;pickup_longitude&amp;#39;], x[&amp;#39;dropoff_latitude&amp;#39;], x[&amp;#39;dropoff_longitude&amp;#39;]),axis=1) B. Manhattan Distance Between the two Lat/Lons:   Manhattan Skyline     The distance between two points measured along axes at right angles
def dummy_manhattan_distance(lat1, lng1, lat2, lng2): a = haversine_array(lat1, lng1, lat1, lng2) b = haversine_array(lat1, lng1, lat2, lng1) return a &#43; b We could then use the function as:
train[&amp;#39;manhattan_distance&amp;#39;] = train.apply(lambda x: dummy_manhattan_distance(x[&amp;#39;pickup_latitude&amp;#39;], x[&amp;#39;pickup_longitude&amp;#39;], x[&amp;#39;dropoff_latitude&amp;#39;], x[&amp;#39;dropoff_longitude&amp;#39;]),axis=1) C. Bearing Between the two Lat/Lons: A bearing is used to represent the direction of one point relative to another point.
def bearing_array(lat1, lng1, lat2, lng2): AVG_EARTH_RADIUS = 6371 # in km lng_delta_rad = np.radians(lng2 - lng1) lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) y = np.sin(lng_delta_rad) * np.cos(lat2) x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) return np.degrees(np.arctan2(y, x)) We could then use the function as:
train[&amp;#39;bearing&amp;#39;] = train.apply(lambda x: bearing_array(x[&amp;#39;pickup_latitude&amp;#39;], x[&amp;#39;pickup_longitude&amp;#39;], x[&amp;#39;dropoff_latitude&amp;#39;], x[&amp;#39;dropoff_longitude&amp;#39;]),axis=1) D. Center Latitude and Longitude between Pickup and Dropoff: train.loc[:, &amp;#39;center_latitude&amp;#39;] = (train[&amp;#39;pickup_latitude&amp;#39;].values &#43; train[&amp;#39;dropoff_latitude&amp;#39;].values) / 2 train.loc[:, &amp;#39;center_longitude&amp;#39;] = (train[&amp;#39;pickup_longitude&amp;#39;].values &#43; train[&amp;#39;dropoff_longitude&amp;#39;].values) / 2 These are the new columns that we create:
  5. AutoEncoders: Sometimes people use Autoencoders too for creating automatic features.
What are Autoencoders?
Encoders are deep learning functions which approximate a mapping from X to X, i.e. input=output. They first compress the input features into a lower-dimensional representation/code and then reconstruct the output from this representation.
  We can use this representation vector as a feature for our models.
6. Some Normal Things you can do with your features:  Scaling by Max-Min: This is good and often required preprocessing for Linear models, Neural Networks
 Normalization using Standard Deviation: This is good and often required preprocessing for Linear models, Neural Networks
 Log-based feature/Target: Use log based features or log-based target function. If one is using a Linear model which assumes that the features are normally distributed, a log transformation could make the feature normal. It is also handy in case of skewed variables like income.
  Or in our case trip duration. Below is the graph of trip duration without log transformation.
  And with log transformation:
train[&amp;#39;log_trip_duration&amp;#39;] = train[&amp;#39;trip_duration&amp;#39;].apply(lambda x: np.log(1&#43;x))   A log transformation on trip duration is much less skewed and thus much more helpful for a model.
7. Some Additional Features based on Intuition: Date time Features: One could create additional Date time features based on domain knowledge and intuition. For example, Time-based Features like “Evening,” “Noon,” “Night,” “Purchases_last_month,” “Purchases_last_week,” etc. could work for a particular application.
Domain Specific Features:   Style matters    Suppose you have got some shopping cart data and you want to categorize the TripType. It was the exact problem in Walmart Recruiting: Trip Type Classification on Kaggle.
Some examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes.
To solve this problem, you could think of creating a feature like “Stylish” where you create this variable by adding together the number of items that belong to category Men’s Fashion, Women’s Fashion, Teens Fashion.
Or you could create a feature like “Rare” which is created by tagging some items as rare, based on the data we have and then counting the number of those rare items in the shopping cart.
Such features might work or might not work. From what I have observed, they usually provide a lot of value.
I feel this is the way that Target’s “Pregnant Teen model” was made. They would have had a variable in which they kept all the items that a pregnant teen could buy and put them into a classification algorithm.
Interaction Features: If you have features A and B, you can create features A*B, A&#43;B, A/B, A-B, etc.
For example, to predict the price of a house, if we have two features length and breadth, a better idea would be to create an area(length x breadth) feature.
Or in some case, a ratio might be more valuable than having two features alone. Example: Credit Card utilization ratio is more valuable than having the Credit limit and limit utilized variables.
Conclusion   Creativity is vital!!!    These were just some of the methods I use for creating features.
But there is surely no limit when it comes to feature engineering, and it is only your imagination that limits you.
On that note, I always think about feature engineering while keeping what model I am going to use in mind. Features that work in a random forest may not work well with Logistic Regression.
Feature creation is the territory of trial and error. You won’t be able to know what transformation works or what encoding works best before trying it. It is always a trade-off between time and utility.
Sometimes the feature creation process might take a lot of time. In such cases, you might want to parallelize your Pandas function.
While I have tried to keep this post as exhaustive as possible, I might have missed some of the useful methods. Let me know about them in the comments.
You can find all the code for this post and run it yourself in this Kaggle Kernel
Take a look at the How to Win a Data Science Competition: Learn from Top Kagglers course in the Advanced machine learning specialization by Kazanova. This course talks about a lot of intuitive ways to improve your model. Definitely recommended.
I am going to be writing more beginner friendly posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman guide to moving from Keras to Pytorch</title>
      <link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/artificial-neural-network.png"></media:content>
      

      
      <description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds.</description>

      <content:encoded>  
        
        <![CDATA[    Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: Sentiment Analysis, summarization, dialogue state tracking, to name a few.
Also take a look at my other post: Text Preprocessing Methods for Deep Learning, which talks about different preprocessing techniques you can use for your NLP task and What Kagglers are using for Text Classification, which talks about various deep learning models in use in NLP.
Ok back to the task at hand. While Keras is great to start with deep learning, with time you are going to resent some of its limitations. I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole session.run commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.
Pytorch helps in that since it seems like the python way to do things. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:
I&amp;#39;ve been using PyTorch a few months now and I&amp;#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.
&amp;mdash; Andrej Karpathy (@karpathy) May 26, 2017 
So without further ado let me translate Keras to Pytorch for you.
The Classy way to write your network?   Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch take any network you have and try porting it to Pytorch. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post here.
def get_model(features,clipvalue=1.,num_filters=40,dropout=0.1,embed_size=501): features_input = Input(shape=(features.shape[1],)) inp = Input(shape=(maxlen, )) # Layer 1: Word2Vec Embeddings. x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) # Layer 2: SpatialDropout1D(0.1) x = SpatialDropout1D(dropout)(x) # Layer 3: Bidirectional CuDNNLSTM x = Bidirectional(LSTM(num_filters, return_sequences=True))(x) # Layer 4: Bidirectional CuDNNGRU x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x) # Layer 5: some pooling operations avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features x = concatenate([avg_pool, x_h, max_pool,features_input]) # Layer 7: A dense layer x = Dense(16, activation=&amp;#34;relu&amp;#34;)(x) # Layer 8: A dropout layer x = Dropout(0.1)(x) # Layer 9: Output dense layer with one output for our Binary Classification problem. outp = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(x) # Some keras model creation and compiling model = Model(inputs=[inp,features_input], outputs=outp) adam = optimizers.adam(clipvalue=clipvalue) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=adam, metrics=[&amp;#39;accuracy&amp;#39;]) return model So a model in pytorch is defined as a class(therefore a little more classy) which inherits from nn.module . Every class necessarily contains an __init__ procedure block and a block for the forward pass.
 In the __init__ part the user defines all the layers the network is going to have but doesn&amp;rsquo;t yet define how those layers would be connected to each other
 In the forward pass block, the user defines how data flows from one layer to another inside the network.
  Why is this Classy? Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:
1) It gives you a lot of control on how your network is built.
2) You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)
3) Easy to debug networks. Any time you find any problem with the network just use something like print(&amp;quot;avg_pool&amp;quot;, avg_pool.size()) in the forward pass to check the sizes of the layer and you will debug the network easily
4) You can return multiple outputs from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.
5) Pytorch tensors work in a very similar manner to numpy arrays. For example, I could have used Pytorch Maxpool function to write the maxpool layer but max_pool, _ = torch.max(h_gru, 1) will also work.
6) You can set up different layers with different initialization schemes. Something you won&amp;rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.
7) Wait until you see the training loop in Pytorch You will be amazed at the sort of control it provides.
Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.
class Alex_NeuralNet_Meta(nn.Module): def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix): super(Alex_NeuralNet_Meta, self).__init__() # Initialize some parameters for your model self.hidden_size = hidden_size drp = 0.1 # Layer 1: Word2Vec Embeddings. self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False # Layer 2: Dropout1D(0.1) self.embedding_dropout = nn.Dropout2d(0.1) # Layer 3: Bidirectional CuDNNLSTM self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True) for name, param in self.lstm.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 4: Bidirectional CuDNNGRU self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True) for name, param in self.gru.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 7: A dense layer self.linear = nn.Linear(hidden_size*6 &#43; features.shape[1], lin_size) self.relu = nn.ReLU() # Layer 8: A dropout layer self.dropout = nn.Dropout(drp) # Layer 9: Output dense layer with one output for our Binary Classification problem. self.out = nn.Linear(lin_size, 1) def forward(self, x): &amp;#39;&amp;#39;&amp;#39; here x[0] represents the first element of the input that is going to be passed. We are going to pass a tuple where first one contains the sequences(x[0]) and the second one is a additional feature vector(x[1]) &amp;#39;&amp;#39;&amp;#39; h_embedding = self.embedding(x[0]) # Based on comment by Ivank to integrate spatial dropout. embeddings = h_embedding.unsqueeze(2) # (N, T, 1, K) embeddings = embeddings.permute(0, 3, 2, 1) # (N, K, 1, T) embeddings = self.embedding_dropout(embeddings) # (N, K, 1, T), some features are masked embeddings = embeddings.permute(0, 3, 2, 1) # (N, T, 1, K) h_embedding = embeddings.squeeze(2) # (N, T, K) #h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0))) #print(&amp;#34;emb&amp;#34;, h_embedding.size()) h_lstm, _ = self.lstm(h_embedding) #print(&amp;#34;lst&amp;#34;,h_lstm.size()) h_gru, hh_gru = self.gru(h_lstm) hh_gru = hh_gru.view(-1, 2*self.hidden_size ) #print(&amp;#34;gru&amp;#34;, h_gru.size()) #print(&amp;#34;h_gru&amp;#34;, hh_gru.size()) # Layer 5: is defined dynamically as an operation on tensors. avg_pool = torch.mean(h_gru, 1) max_pool, _ = torch.max(h_gru, 1) #print(&amp;#34;avg_pool&amp;#34;, avg_pool.size()) #print(&amp;#34;max_pool&amp;#34;, max_pool.size()) # the extra features you want to give to the model f = torch.tensor(x[1], dtype=torch.float).cuda() #print(&amp;#34;f&amp;#34;, f.size()) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features conc = torch.cat(( hh_gru, avg_pool, max_pool,f), 1) #print(&amp;#34;conc&amp;#34;, conc.size()) # passing conc through linear and relu ops conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) # return the final output return out Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.
Tailored or Readymade: The Best Fit with a highly customizable Training Loop   In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.
This training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.
So a brief summary of this loop are as follows:
 Create stratified splits using train data Loop through the splits.  Convert your train and CV data to tensor and load your data to the GPU using the X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() command Load the model onto the GPU using the model.cuda() command Define Loss function, Scheduler and Optimizer create train_loader and valid_loader` to iterate through batches. Start running epochs. In each epoch  Set the model mode to train using model.train(). Go through the batches in train_loader and run the forward pass Run a scheduler step to change the learning rate Compute loss Set the existing gradients in the optimizer to zero Backpropagate the losses through the network Clip the gradients Take an optimizer step to change the weights in the whole network Set the model mode to eval using model.eval(). Get predictions for the validation data using valid_loader and store in variable valid_preds_fold Calculate Loss and print  After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final test_preds Get Out-of-fold(OOF) predictions for train set using train_preds[valid_idx] = valid_preds_fold These OOF predictions can then be used to calculate the Local CV score for your model.   def pytorch_model_run_cv(x_train,y_train,features,x_test, model_obj, feats = False,clip = True): seed_everything() avg_losses_f = [] avg_val_losses_f = [] # matrix for the out-of-fold predictions train_preds = np.zeros((len(x_train))) # matrix for the predictions on the test set test_preds = np.zeros((len(x_test))) splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train)) for i, (train_idx, valid_idx) in enumerate(splits): seed_everything(i*1000&#43;i) x_train = np.array(x_train) y_train = np.array(y_train) if feats: features = np.array(features) x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() if feats: kfold_X_features = features[train_idx.astype(int)] kfold_X_valid_features = features[valid_idx.astype(int)] x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda() y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() model = copy.deepcopy(model_obj) model.cuda() loss_fn = torch.nn.BCEWithLogitsLoss(reduction=&amp;#39;sum&amp;#39;) step_size = 300 base_lr, max_lr = 0.001, 0.003 optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr) ################################################################################################ scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode=&amp;#39;exp_range&amp;#39;, gamma=0.99994) ############################################################################################### train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold)) valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold)) train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False) print(f&amp;#39;Fold {i &#43; 1}&amp;#39;) for epoch in range(n_epochs): start_time = time.time() model.train() avg_loss = 0. for i, (x_batch, y_batch, index) in enumerate(train_loader): if feats: f = kfold_X_features[index] y_pred = model([x_batch,f]) else: y_pred = model(x_batch) if scheduler: scheduler.batch_step() # Compute and print loss. loss = loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() if clip: nn.utils.clip_grad_norm_(model.parameters(),1) optimizer.step() avg_loss &#43;= loss.item() / len(train_loader) model.eval() valid_preds_fold = np.zeros((x_val_fold.size(0))) test_preds_fold = np.zeros((len(x_test))) avg_val_loss = 0. for i, (x_batch, y_batch,index) in enumerate(valid_loader): if feats: f = kfold_X_valid_features[index] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() avg_val_loss &#43;= loss_fn(y_pred, y_batch).item() / len(valid_loader) valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0] elapsed_time = time.time() - start_time print(&amp;#39;Epoch {}/{} \tloss={:.4f} \tval_loss={:.4f} \ttime={:.2f}s&amp;#39;.format( epoch &#43; 1, n_epochs, avg_loss, avg_val_loss, elapsed_time)) avg_losses_f.append(avg_loss) avg_val_losses_f.append(avg_val_loss) # predict all samples in the test set batch per batch for i, (x_batch,) in enumerate(test_loader): if feats: f = test_features[i * batch_size:(i&#43;1) * batch_size] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() test_preds_fold[i * batch_size:(i&#43;1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0] train_preds[valid_idx] = valid_preds_fold test_preds &#43;= test_preds_fold / len(splits) print(&amp;#39;All \tloss={:.4f} \tval_loss={:.4f} \t&amp;#39;.format(np.average(avg_losses_f),np.average(avg_val_losses_f))) return train_preds, test_preds But Why? Why so much code? Okay. I get it. That was probably a handful. What you could have done with a simple.fit in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:
 While in Keras you have prespecified schedulers like ReduceLROnPlateau (and it is a task to write them), in Pytorch you can experiment like crazy. If you know how to write Python you are going to get along just fine Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly. And much more. It is only your imagination that will stop you.  Wanna Run it Yourself?   So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the seed_everything, MyDataset and CyclicLR (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a Kaggle Kernel with the whole running code. You can see the code here and include it in your projects.
If you liked this post, please don&amp;rsquo;t forget to upvote the Kernel too. I will be obliged.
Endnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.
 Discussion on 3rd Place winner model in Toxic comment 3rd Place model in Keras by Larry Freeman Pytorch starter Capsule model How to: Preprocessing when using embeddings Improve your Score with some Text Preprocessing Pytorch baseline Pytorch starter  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know - Part II(AWK)</title>
      <link>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</link>
      <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</guid>
      
      

      
      <description>Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.
And it is easy to learn.</description>

      <content:encoded>  
        
        <![CDATA[  Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.
And it is easy to learn.
In this post, I will try to give you a brief intro about how you could add awk to your daily work-flow.
Please see my previous post if you want some background or some basic to intermediate understanding of shell commands.
Basics/ Fundamentals So let me start with an example first. Say you wanted to sum a column in a comma delimited file. How would you do that in shell?
Here is the command. The great thing about awk is that it took me nearly 5 sec to write this command. I did not have to open any text editor to write a python script.
It lets you do adhoc work quickly.
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum }&amp;#39; data.txt 44662539172  See the command one more time. There is a basic structure to the awk command
BEGIN {action} pattern {action} pattern {action} . . pattern { action} END {action}   An awk program consists of:
 An optional BEGIN segment : In the begin part we initialize our variables before we even start reading from the file or the standard input.
 pattern - action pairs: In the middle part we Process the input data. You put multiple pattern action pairs when you want to do multiple things with the same line.
 An optional END segment: In the end part we do something we want to do when we have reached the end of file.
  An awk command is called on a file using:
awk &amp;#39;BEGIN{SOMETHING HERE} {SOMETHING HERE: could put Multiple Blocks Like this} END {SOMETHING HERE}&amp;#39; file.txt You also need to know about these preinitialized variables that awk keeps track of.:
 FS : field separator. Default is whitespace (1 or more spaces or tabs). If you are using any other seperator in the file you should specify it in the Begin Part. RS : record separator. Default record separator is newline. Can be changed in BEGIN action. NR : NR is the variable whose value is the number of the current record. You normally use it in the action blocks in the middle. NF : The Number of Fields after the single line has been split up using FS. Dollar variables : awk splits up the line which is coming to it by using the given FS and keeps the split parts in the $ variables. For example column 1 is in $1, column 2 is in $2. $0 is the string representation of the whole line. Note that if you want to access last column you don&amp;rsquo;t have to count. You can just use $NF. For second last column you can use $(NF-1). Pretty handy. Right.  So If you are with me till here, the hard part is done. Now the fun part starts. Lets look at the first awk command again and try to understand it.
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum }&amp;#39; data.txt So there is a begin block. Remember before we read any line. We initialize sum to 0 and FS to &amp;ldquo;,&amp;rdquo;.
Now as awk reads its input line by line it increments sum by the value in column 5(as specified by $5).
Note that there is no pattern specified here so awk will do the action for every line.
When awk has completed reading the file it prints out the sum.
What if you wanted mean?
We could create a cnt Variable:
awk &amp;#39;BEGIN{ sum=0;cnt=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5; cnt&#43;=1 } END { print sum/cnt }&amp;#39; data.txt 1.86436e&#43;06  or better yet, use our friend NR which bash is already keeping track of:
awk &amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum &#43;= $5 } END { print sum/NR }&amp;#39; data.txt 1.86436e&#43;06  Filter a file In the mean and sum awk commands we did not put any pattern in our middle commands. Let us use a simple pattern now. Suppose we have a file Salaries.csv which contains:
head salaries.txt yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000 1985,BAL,AL,lacyle01,725000 1985,BAL,AL,flanami01,641667 1985,BAL,AL,boddimi01,625000 1985,BAL,AL,stewasa01,581250 1985,BAL,AL,martide01,560000 1985,BAL,AL,roeniga01,558333  I want to filter records for players who who earn more than 22 M in 2013 just because I want to. You just do:
awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} $5&amp;gt;=22000000 &amp;amp;&amp;amp; $1==2013{print $0}&amp;#39; Salaries.csv 2013,DET,AL,fieldpr01,23000000 2013,MIN,AL,mauerjo01,23000000 2013,NYA,AL,rodrial01,29000000 2013,NYA,AL,wellsve01,24642857 2013,NYA,AL,sabatcc01,24285714 2013,NYA,AL,teixema01,23125000 2013,PHI,NL,leecl02,25000000 2013,SFN,NL,linceti01,22250000  Cool right. Now let me explain it a little bit. The part in the command &amp;ldquo;$5&amp;gt;=22000000 &amp;amp;&amp;amp; $1==2013&amp;rdquo; is called a pattern. It says that print this line($0) if and only if the Salary($5) is more than 22M and(&amp;amp;&amp;amp;) year($1) is equal to 2013. If the incoming record(line) does not satisfy this pattern it never reaches the inner block.
So Now you could do basic Select SQL at the command line only if you had:
The logic Operators:
 == equality operator; returns TRUE is both sides are equal
 != inverse equality operator
 &amp;amp;&amp;amp; logical AND
 || logical OR
 ! logical NOT
 &amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;= relational operators
  Normal Arithmetic Operators: &#43;, -, /, *, %, ^
Some String Functions: length, substr, split
GroupBy Now you will say: &amp;ldquo;Hey Dude SQL without groupby is incomplete&amp;rdquo;. You are right and for that we can use the associative array. Lets just see the command first and then I will explain. So lets create another useless use case(or may be something useful to someone :)) We want to find out the number of records for each year in the file. i.e we want to find the distribution of years in the file. Here is the command:
awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} {my_array[$1]=my_array[$1]&#43;1} END{ for (k in my_array){if(k!=&amp;#34;yearID&amp;#34;)print k&amp;#34;|&amp;#34;my_array[k]}; }&amp;#39; Salaries.csv 1990|867 1991|685 1996|931 1997|925 ...  Now I would like to tell you a secret. You don&amp;rsquo;t really need to declare the variables you want to use in awk. So you did not really needed to define sum, cnt variables before. I only did that because it is good practice. If you don&amp;rsquo;t declare a user defined variable in awk, awk assumes it to be null or zero depending on the context. So in the command above we don&amp;rsquo;t declare our myarray in the begin block and that is fine.
Associative Array: The variable myarray is actually an associative array. i.e. It stores data in a key value format.(Python dictionaries anyone). The same array could keep integer keys and String keys. For example, I can do this in a single code.
myarray[1]=&#34;key&#34; myarray[&#39;mlwhiz&#39;] = 1   For Loop for associative arrays: I could use a for loop to read associative array
for (k in array) { DO SOMETHING } # Assigns to k each Key of array (unordered) # Element is array[k]   If Statement:Uses a syntax like C for the if statement. the else block is optional:
if (n  0){ DO SOMETHING } else{ DO SOMETHING }   So lets dissect the above command now.
I set the File separator to &amp;ldquo;,&amp;rdquo; in the beginning. I use the first column as the key of myarray. If the key exists I increment the value by 1.
At the end, I loop through all the keys and print out key value pairs separated by &amp;ldquo;|&amp;rdquo;
I know that the header line in my file contains &amp;ldquo;yearID&amp;rdquo; in column 1 and I don&amp;rsquo;t want &amp;lsquo;yearID|1&amp;rsquo; in the output. So I only print when Key is not equal to &amp;lsquo;yearID&amp;rsquo;.
GroupBy with case statement: cat Salaries.csv | awk &amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} $5&amp;lt;100000{array5[&amp;#34;[0-100000)&amp;#34;]&#43;=1} $5&amp;gt;=100000&amp;amp;&amp;amp;$5&amp;lt;250000{array5[&amp;#34;[100000,250000)&amp;#34;]=array5[&amp;#34;[100000,250000)&amp;#34;]&#43;1} $5&amp;gt;=250000&amp;amp;&amp;amp;$5&amp;lt;500000{array5[&amp;#34;[250000-500000)&amp;#34;]=array5[&amp;#34;[250000-500000)&amp;#34;]&#43;1} $5&amp;gt;=500000&amp;amp;&amp;amp;$5&amp;lt;1000000{array5[&amp;#34;[500000-1000000)&amp;#34;]=array5[&amp;#34;[500000-1000000)&amp;#34;]&#43;1} $5&amp;gt;=1000000{array5[&amp;#34;[1000000)&amp;#34;]=array5[&amp;#34;[1000000)&amp;#34;]&#43;1} END{ print &amp;#34;VAR Distrib:&amp;#34;; for (v in array5){print v&amp;#34;|&amp;#34;array5[v]} }&amp;#39; VAR Distrib: [250000-500000)|8326 [0-100000)|2 [1000000)|23661 [100000,250000)|9480  Here we used multiple pattern-action blocks to create a case statement.
For The Brave: This is a awk code that I wrote to calculate the Mean,Median,min,max and sum of a column simultaneously. Try to go through the code and understand it.I have added comments too. Think of this as an exercise. Try to run this code and play with it. You may learn some new tricks in the process. If you don&amp;rsquo;t understand it do not worry. Just get started writing your own awk codes, you will be able to understand it in very little time.
# Create a New file named A.txt to keep only the salary column. cat Salaries.csv | cut -d &amp;#34;,&amp;#34; -f 5 &amp;gt; A.txt FILENAME=&amp;#34;A.txt&amp;#34; # The first awk counts the number of lines which are numeric. We use a regex here to check if the column is numeric or not. # &amp;#39;;&amp;#39; stands for Synchronous execution i.e sort only runs after the awk is over. # The output of both commands are given to awk command which does the whole work. # So Now the first line going to the second awk is the number of lines in the file which are numeric. # and from the second to the end line the file is sorted. (awk &amp;#39;BEGIN {c=0} $1 ~ /^[-0-9]*(\.[0-9]*)?$/ {c=c&#43;1;} END {print c;}&amp;#39; &amp;#34;$FILENAME&amp;#34;; \  sort -n &amp;#34;$FILENAME&amp;#34;) | awk &amp;#39; BEGIN { c = 0; sum = 0; med1_loc = 0; med2_loc = 0; med1_val = 0; med2_val = 0; min = 0; max = 0; } NR==1 { LINES = $1 # We check whether numlines is even or odd so that we keep only # the locations in the array where the median might be. if (LINES%2==0) {med1_loc = LINES/2-1; med2_loc = med1_loc&#43;1;} if (LINES%2!=0) {med1_loc = med2_loc = (LINES-1)/2;} } $1 ~ /^[-0-9]*(\.[0-9]*)?$/ &amp;amp;&amp;amp; NR!=1 { # setting min value if (c==0) {min = $1;} # middle two values in array if (c==med1_loc) {med1_val = $1;} if (c==med2_loc) {med2_val = $1;} c&#43;&#43; sum &#43;= $1 max = $1 } END { ave = sum / c median = (med1_val &#43; med2_val ) / 2 print &amp;#34;sum:&amp;#34; sum print &amp;#34;count:&amp;#34; c print &amp;#34;mean:&amp;#34; ave print &amp;#34;median:&amp;#34; median print &amp;#34;min:&amp;#34; min print &amp;#34;max:&amp;#34; max } &amp;#39; &amp;lt;pre style=&amp;#34;font-size:50%; padding:7px; margin:0em; background-color:#FFF112&amp;#34;&amp;gt;sum:44662539172 count:23956 mean:1.86436e&#43;06 median:507950 min:0 max:33000000 &amp;lt;/pre&amp;gt; Endnote: awk is an awesome tool and there are a lot of use-cases where it can make your life simple. There is a sort of a learning curve, but I think that it would be worth it in the long term. I have tried to give you a taste of awk and I have covered a lot of ground here in this post. To tell you a bit more there, awk is a full programming language. There are for loops, while loops, conditionals, booleans, functions and everything else that you would expect from a programming language. So you could look more still.
To learn more about awk you can use this book. This book is a free resource and you could learn more about awk and use cases.
Or if you like to have your book binded and in paper like me you can buy this book, which is a gem:
Do leave comments in case you find more use-cases for awk or if you want me to write on new use-cases. Or just comment weather you liked it or not and how I could improve as I am also new and trying to learn more of this.
Till then Ciao !!!
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Shell Basics every Data Scientist Should know -Part I</title>
      <link>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</guid>
      
      

      
      <description>Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).
Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.</description>

      <content:encoded>  
        
        <![CDATA[  Shell Commands are powerful. And life would be like hell without shell is how I like to say it(And that is probably the reason that I dislike windows).
Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.
Probably whatever you do it won&amp;rsquo;t be simpler/less time consuming than this
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 1 | sort | uniq | wc -l 30  And this will run way faster than whatever you do with perl/python script.
Now this command says
 Use the cat command to print/stream the contents of the file to stdout. Pipe the streaming contents from our cat command to the next command cut. The cut commands specifies the delimiter by the argument -d and the column by the argument -f and streams the output to stdout. Pipe the streaming content to the sort command which sorts the input and streams only the distinct values to the stdout. It takes the argument -u that specifies that we only need unique values. Pipe the output to the wc -l command which counts the number of lines in the input.  There is a lot going on here and I will try my best to ensure that you will be able to understand most of it by the end of this Blog post.Although I will also try to explain more advanced concepts than the above command in this post.
Now, I use shell commands extensively at my job. I will try to explain the usage of each of the commands based on use cases that I counter nearly daily at may day job as a data scientist.
Some Basic Commands in Shell: There are a lot of times when you just need to know a little bit about the data. You just want to see may be a couple of lines to inspect a file. One way of doing this is opening the txt/csv file in the notepad. And that is probably the best way for small files. But you could also do it in the shell using:
1. cat cat data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  Now the cat command prints the whole file in the terminal window for you.I have not shown the whole file here.
But sometimes the files will be so big that you wont be able to open them up in notepad&#43;&#43; or any other software utility and there the cat command will shine.
2. Head and Tail Now you might ask me why would you print the whole file in the terminal itself? Generally I won&amp;rsquo;t. But I just wanted to tell you about the cat command. For the use case when you want only the top/bottom n lines of your data you will generally use the head/tail commands. You can use them as below.
head data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  head -n 3 data.txt yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000  tail data.txt 2013|WAS|NL|bernaro01|1212500 2013|WAS|NL|tracych01|1000000 2013|WAS|NL|stammcr01|875000 2013|WAS|NL|dukeza01|700000 2013|WAS|NL|espinda01|526250 2013|WAS|NL|matthry01|504500 2013|WAS|NL|lombast02|501250 2013|WAS|NL|ramoswi01|501250 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  tail -n 2 data.txt 2013|WAS|NL|rodrihe03|501000 2013|WAS|NL|moorety01|493000  Notice the structure of the shell command here.
CommandName [-arg1name] [arg1value] [-arg2name] [arg2value] filename   3. Piping Now we could have also written the same command as:
cat data.txt | head yearID|teamID|lgID|playerID|salary 1985|BAL|AL|murraed02|1472819 1985|BAL|AL|lynnfr01|1090000 1985|BAL|AL|ripkeca01|800000 1985|BAL|AL|lacyle01|725000 1985|BAL|AL|flanami01|641667 1985|BAL|AL|boddimi01|625000 1985|BAL|AL|stewasa01|581250 1985|BAL|AL|martide01|560000 1985|BAL|AL|roeniga01|558333  This brings me to one of the most important concepts of Shell usage - piping. You won&amp;rsquo;t be able to utilize the full power the shell provides without using this concept. And the concept is actually simple.
Just read the &amp;ldquo;|&amp;rdquo; in the command as &amp;ldquo;pass the data on to&amp;rdquo;
So I would read the above command as:
cat(print) the whole data to stream, pass the data on to head so that it can just give me the first few lines only.
So did you understood what piping did? It is providing us a way to use our basic commands in a consecutive manner. There are a lot of commands that are fairly basic and it lets us use these basic commands in sequence to do some fairly non trivial things.
Now let me tell you about a couple of more commands before I show you how we can chain them to do fairly advanced tasks.
4. wc wc is a fairly useful shell utility/command that lets us count the number of lines(-l), words(-w) or characters(-c) in a given file
wc -l data.txt 23957 data.txt  5. grep You may want to print all the lines in your file which have a particular word. Or as a Data case you might like to see the salaries for the team BAL in 2000. In this case we have printed all the lines in the file which contain &amp;ldquo;2000|BAL&amp;rdquo;. grep is your friend.
grep &amp;#34;2000|BAL&amp;#34; data.txt | head 2000|BAL|AL|belleal01|12868670 2000|BAL|AL|anderbr01|7127199 2000|BAL|AL|mussimi01|6786032 2000|BAL|AL|ericksc01|6620921 2000|BAL|AL|ripkeca01|6300000 2000|BAL|AL|clarkwi02|6000000 2000|BAL|AL|johnsch04|4600000 2000|BAL|AL|timlimi01|4250000 2000|BAL|AL|deshide01|4209324 2000|BAL|AL|surhobj01|4146789  you could also use regular expressions with grep.
6. sort You may want to sort your dataset on a particular column.Sort is your friend. Say you want to find out the top 10 maximum salaries given to any player in your dataset.
sort -t &amp;#34;|&amp;#34; -k 5 -r -n data.txt | head -10 2010|NYA|AL|rodrial01|33000000 2009|NYA|AL|rodrial01|33000000 2011|NYA|AL|rodrial01|32000000 2012|NYA|AL|rodrial01|30000000 2013|NYA|AL|rodrial01|29000000 2008|NYA|AL|rodrial01|28000000 2011|LAA|AL|wellsve01|26187500 2005|NYA|AL|rodrial01|26000000 2013|PHI|NL|leecl02|25000000 2013|NYA|AL|wellsve01|24642857  So there are certainly a lot of options in this command. Lets go through them one by one.
 -t: Which delimiter to use? -k: Which column to sort on? -n: If you want Numerical Sorting. Dont use this option if you want Lexographical sorting. -r: I want to sort Descending. Sorts Ascending by Default.  7. cut This command lets you select certain columns from your data. Sometimes you may want to look at just some of the columns in your data. As in you may want to look only at the year, team and salary and not the other columns. cut is the command to use.
cut -d &amp;#34;|&amp;#34; -f 1,2,5 data.txt | head yearID|teamID|salary 1985|BAL|1472819 1985|BAL|1090000 1985|BAL|800000 1985|BAL|725000 1985|BAL|641667 1985|BAL|625000 1985|BAL|581250 1985|BAL|560000 1985|BAL|558333  The options are:
 -d: Which delimiter to use? -f: Which column/columns to cut?  8. uniq uniq is a little bit tricky as in you will want to use this command in sequence with sort. This command removes sequential duplicates. So in conjunction with sort it can be used to get the distinct values in the data. For example if I wanted to find out 10 distinct teamIDs in data, I would use:
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 2 | sort | uniq | head ANA ARI ATL BAL BOS CAL CHA CHN CIN CLE  This command could be used with argument -c to count the occurrence of these distinct values. Something akin to count distinct.
cat data.txt | cut -d &amp;#34;|&amp;#34; -f 2 | sort | uniq -c | head 247 ANA 458 ARI 838 ATL 855 BAL 852 BOS 368 CAL 812 CHA 821 CHN 46 CIN 867 CLE  Some Other Utility Commands for Other Operations Some Other command line tools that you could use without going in the specifics as the specifics are pretty hard.
1. Change delimiter in a file Find and Replace Magic.: You may want to replace certain characters in file with something else using the tr command.
cat data.txt | tr &amp;#39;|&amp;#39; &amp;#39;,&amp;#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  or the sed command
cat data.txt | sed -e &amp;#39;s/|/,/g&amp;#39; | head -4 yearID,teamID,lgID,playerID,salary 1985,BAL,AL,murraed02,1472819 1985,BAL,AL,lynnfr01,1090000 1985,BAL,AL,ripkeca01,800000  2. Sum of a column in a file Using the awk command you could find the sum of column in file. Divide it by the number of lines and you can get the mean.
cat data.txt | awk -F &amp;#34;|&amp;#34; &amp;#39;{ sum &#43;= $5 } END { printf sum }&amp;#39; 44662539172  awk is a powerful command which is sort of a whole language in itself. Do see the wiki page for awk for a lot of great usecases of awk. I also wrote a post on awk as a second part in this series. Check it HERE
3. Find the files in a directory that satisfy a certain condition You can do this by using the find command. Lets say you want to find all the .txt files in the current working dir that start with lowercase h.
find . -name &amp;#34;h*.txt&amp;#34; ./hamlet.txt  To find all .txt files starting with h regarless of case we could use regex.
find . -name &amp;#34;[Hh]*.txt&amp;#34; ./hamlet.txt ./Hamlet1.txt  4. Passing file list as Argument. xargs was suggested by Gaurav in the comments, so I read about it and it is actually a very nice command which you could use in a variety of use cases.
So if you just use a pipe, any command/utility receives data on STDIN (the standard input stream) as a raw pile of data that it can sort through one line at a time. However some programs don&amp;rsquo;t accept their commands on standard in. For example the rm command(which is used to remove files), touch command(used to create file with a given name) or a certain python script you wrote(which takes command line arguments). They expect it to be spelled out in the arguments to the command.
For example: rm takes a file name as a parameter on the command line like so: rm file1.txt. If I wanted to delete all &amp;lsquo;.txt&amp;rsquo; files starting with &amp;ldquo;h/H&amp;rdquo; from my working directory, the below command won&amp;rsquo;t work because rm expects a file as an input.
find . -name &amp;#34;[hH]*.txt&amp;#34; | rm usage: rm [-f | -i] [-dPRrvW] file ... unlink file  To get around it we can use the xargs command which reads the STDIN stream data and converts each line into space separated arguments to the command.
find . -name &amp;#34;[hH]*.txt&amp;#34; | xargs ./hamlet.txt ./Hamlet1.txt  Now you could use rm to remove all .txt files that start with h/H. A word of advice: Always see the output of xargs first before using rm.
find . -name &amp;#34;[hH]*.txt&amp;#34; | xargs rm Another usage of xargs could be in conjunction with grep to find all files that contain a given string.
find . -name &amp;#34;*.txt&amp;#34; | xargs grep &amp;#39;honest soldier&amp;#39; ./Data1.txt:O, farewell, honest soldier; ./Data2.txt:O, farewell, honest soldier; ./Data3.txt:O, farewell, honest soldier;  Hopefully You could come up with varied uses building up on these examples. One other use case could be to use this for passing arguments to a python script.
Other Cool Tricks Sometimes you want your data that you got by some command line utility(Shell commands/ Python scripts) not to be shown on stdout but stored in a textfile. You can use the &amp;rdquo;&amp;gt;&amp;rdquo; operator for that. For Example: You could have stored the file after replacing the delimiters in the previous example into anther file called newdata.txt as follows:
cat data.txt | tr &amp;#39;|&amp;#39; &amp;#39;,&amp;#39; &amp;gt; newdata.txt I really got confused between &amp;rdquo;|&amp;rdquo; (piping) and &amp;rdquo;&amp;gt;&amp;rdquo; (to_file) operations a lot in the beginning. One way to remember is that you should only use &amp;rdquo;&amp;gt;&amp;rdquo; when you want to write something to a file. &amp;rdquo;|&amp;rdquo; cannot be used to write to a file. Another operation you should know about is the &amp;rdquo;&amp;gt;&amp;gt;&amp;rdquo; operation. It is analogous to &amp;rdquo;&amp;gt;&amp;rdquo; but it appends to an existing file rather that replacing the file and writing over.
If you would like to know more about commandline, which I guess you would, here are some books that I would recommend for a beginner:
The first book is more of a fun read at leisure type of book. THe second book is a little more serious. Whatever suits you.
So, this is just the tip of the iceberg. Although I am not an expert in shell usage, these commands reduced my workload to a large extent. If there are some shell commands you use on a regular basis or some shell command that are cool, do tell in the comments. I would love to include it in the blogpost.
I wrote a blogpost on awk as a second part of this post. Check it Here
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title>
      <link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid>
      
      

      
      <description>When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph.</description>

      <content:encoded>  
        
        <![CDATA[  When it comes to data preparation and getting acquainted with data, the one step we normally skip is the data visualization. While a part of it could be attributed to the lack of good visualization tools for the platforms we use, most of us also get lazy at times.
Now as we know of it Python never had any good Visualization library. For most of our plotting needs, I would read up blogs, hack up with StackOverflow solutions and haggle with Matplotlib documentation each and every time I needed to make a simple graph. This led me to think that a Blog post to create common Graph types in Python is in order. But being the procrastinator that I am it always got pushed to the back of my head.
One thing that helped me in pursuit of my data visualization needs in Python was this awesome course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Highly Recommended.
But, yesterday I got introduced to Seaborn and I must say I am quite impressed with it. It makes beautiful graphs that are in my opinion better than R&amp;rsquo;s ggplot2. Gives you enough options to customize and the best part is that it is so easy to learn.
So I am finally writing this blog post with a basic purpose of creating a code base that provides me with ready to use codes which could be put into analysis in a fairly straight-forward manner.
Right. So here Goes.
We Start by importing the libraries that we will need to use.
import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes To create a use case for our graphs, we will be working with the Tips data that contains the following information.
tips = sns.load_dataset(&amp;#34;tips&amp;#34;) tips.head()   Scatterplot With Regression Line Now let us work on visualizing this data. We will use the regplot option in seaborn.
# We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) # Here we create a matplotlib axes object. The extra parameters we use # &amp;#34;ci&amp;#34; to remove confidence interval # &amp;#34;marker&amp;#34; to have a x as marker. # &amp;#34;scatter_kws&amp;#34; to provide style info for the points.[s for size] # &amp;#34;line_kws&amp;#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;, data=tips, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, line_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;alpha&amp;#34;:0.5,&amp;#34;lw&amp;#34;:4},marker=&amp;#34;x&amp;#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Tip&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Total Bill&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Now that required a bit of a code but i feel that it looks much better than what either Matplotlib or ggPlot2 could have rendered. We got a lot of customization without too much code.
But that is not really what actually made me like Seaborn. The plot type that actually got my attention was lmplot, which lets us use regplot in a faceted mode.
# So this function creates a faceted plot. The plot is parameterized by the following: # col : divides the data points into days and creates that many plots # palette: deep, muted, pastel, bright, dark, and colorblind. change the colors in graph. Experiment with these # col_wrap: we want 2 graphs in a row? Yes.We do # scatter_kws: attributes for points # hue: Colors on a particular column. # size: controls the size of graph g = sns.lmplot(x=&amp;#34;tip&amp;#34;, y=&amp;#34;total_bill&amp;#34;,ci=None,data=tips, col=&amp;#34;day&amp;#34;, palette=&amp;#34;muted&amp;#34;,col_wrap=2,scatter_kws={&amp;#34;s&amp;#34;: 100,&amp;#34;alpha&amp;#34;:.5}, line_kws={&amp;#34;lw&amp;#34;:4,&amp;#34;alpha&amp;#34;:0.5},hue=&amp;#34;day&amp;#34;,x_jitter=1.0,y_jitter=1.0,size=6) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Total Bill vs. Tip&amp;#39;, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabels(&amp;#34;Tip&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabels(&amp;#34;Total Bill&amp;#34;,size = 50,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here titles = [&amp;#39;Thursday&amp;#39;,&amp;#39;Friday&amp;#39;,&amp;#39;Saturday&amp;#39;,&amp;#39;Sunday&amp;#39;] for ax,title in zip(g.axes.flat,titles): ax.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   A side Note on Palettes:
You can build your own color palettes using color_palette() function. color_palette() will accept the name of any seaborn palette or matplotlib colormap(except jet, which you should never use). It can also take a list of colors specified in any valid matplotlib format (RGB tuples, hex color codes, or HTML color names). The return value is always a list of RGB tuples. This allows you to use your own color palettes in graph.   Barplots sns.set(style=&amp;#34;ticks&amp;#34;) flatui = [&amp;#34;#9b59b6&amp;#34;, &amp;#34;#3498db&amp;#34;, &amp;#34;#95a5a6&amp;#34;, &amp;#34;#e74c3c&amp;#34;, &amp;#34;#34495e&amp;#34;, &amp;#34;#2ecc71&amp;#34;] # This Function takes as input a custom palette g = sns.barplot(x=&amp;#34;sex&amp;#34;, y=&amp;#34;tip&amp;#34;, hue=&amp;#34;day&amp;#34;, palette=sns.color_palette(flatui),data=tips,ci=None) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Do We tend to \nTip high on Weekends?&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;Gender&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Mean Tips&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   Histograms and Distribution Diagrams They form another part of my workflow. Lets plot the normal Histogram using seaborn. For this we will use the distplot function. This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() function. It can also fit scipy.stats distributions and plot the estimated PDF over the data.
# Create a list of 1000 Normal RVs x = np.random.normal(size=1000) sns.set_context(&amp;#34;poster&amp;#34;) sns.set_style(&amp;#34;ticks&amp;#34;) # This Function creates a normed Histogram by default. # If we use the parameter kde=False and norm_hist=False then # we will be using a count histogram g=sns.distplot(x, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title(&amp;#39;Normal Simulation&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   import scipy.stats as stats a = 1.5 b = 1.5 x = np.arange(0.01, 1, 0.01) y = stats.beta.rvs(a,b,size=10000) y_act = stats.beta.pdf(x,a,b) g=sns.distplot(y,kde=False,norm_hist=True, kde_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;lw&amp;#34;:4,&amp;#34;label&amp;#34;:&amp;#34;KDE Estim&amp;#34;,&amp;#34;alpha&amp;#34;:0.5}, hist_kws={&amp;#34;color&amp;#34;:&amp;#34;r&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;label&amp;#34;:&amp;#34;Freq&amp;#34;}) # Note that we plotted on the graph using plt matlabplot function plt.plot(x,y_act) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,7) # Set the Title of the graph from here g.axes.set_title((&amp;#34;Beta Simulation vs. Calculated Beta Density\nFor a=%s,b=%s&amp;#34;) %(a,b),fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;X&amp;#34;,size = 67,color=&amp;#34;g&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;Density&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   PairPlots You need to see how variables vary with one another. What is the distribution of variables in the dataset. This is the graph to use with the pairplot function. Very helpful And Seaborn males it a joy to use. We will use Iris Dataset here for this example.
iris = sns.load_dataset(&amp;#34;iris&amp;#34;) iris.head()   # Create a Pairplot g = sns.pairplot(iris,hue=&amp;#34;species&amp;#34;,palette=&amp;#34;muted&amp;#34;,size=5, vars=[&amp;#34;sepal_width&amp;#34;, &amp;#34;sepal_length&amp;#34;],kind=&amp;#39;reg&amp;#39;,markers=[&amp;#39;o&amp;#39;,&amp;#39;x&amp;#39;,&amp;#39;&#43;&amp;#39;]) # To change the size of the scatterpoints in graph g = g.map_offdiag(plt.scatter, s=35,alpha=0.5) # remove the top and right line in graph sns.despine() # Additional line to adjust some appearance issue plt.subplots_adjust(top=0.9) # Set the Title of the graph from here g.fig.suptitle(&amp;#39;Relation between Sepal Width and Sepal Length&amp;#39;, fontsize=34,color=&amp;#34;b&amp;#34;,alpha=0.3)   Hope you found this post useful and worth your time. You can find the iPython notebook at github
I tried to make this as simple as possible but You may always ask me or see the documentation for doubts.
If you have any more ideas on how to use Seaborn or which graphs should i add here, please suggest in the comments section.
I will definitely try to add to this post as I start using more visualizations and encounter other libraries as good as seaborn.
Also since this is my first visualization post on this blog, I would like to call out a good course about Data Visualization and applied plotting from University of Michigan which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>