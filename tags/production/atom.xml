<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Production on MLWhiz</title>
    <link>https://mlwhiz.com/tags/production/</link>
    <description>Recent content in Production on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/production/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Become an ML Engineer with these courses from Amazon and Google</title>
      <link>https://mlwhiz.com/blog/2020/08/27/mlengineercourses/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/27/mlengineercourses/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/mlengineercourses/main.png"></media:content>
      

      
      <description>With ML Engineer job roles in all the vogue and a lot of people preparing for them, I get asked a lot of times by my readers to recommend courses for the ML engineer roles particularly and not for the Data Science roles.
Now, while both ML and Data Science pretty much have a high degree of overlap and I could very well argue that ML engineers do need to know many of the Data Science skills, there is a special place in hell reserved for ML engineers and that is the production and deployment part of the Data Science modeling process.</description>

      <content:encoded>  
        
        <![CDATA[  With ML Engineer job roles in all the vogue and a lot of people preparing for them, I get asked a lot of times by my readers to recommend courses for the ML engineer roles particularly and not for the Data Science roles.
Now, while both ML and Data Science pretty much have a high degree of overlap and I could very well argue that ML engineers do need to know many of the Data Science skills, there is a special place in hell reserved for ML engineers and that is the production and deployment part of the Data Science modeling process.
So, it doesn’t hurt to look at what two of the world’s biggest companies are looking at when it comes to ML engineering. These courses taught by Google and Amazon are rather popular and typically beginner level. So these are the best bet for people who are looking to start their journey to become an ML Engineer.
My main criteria for selecting these particular courses is the practical utility they provide as well as the pedigree they bring. Also, note that you don’t need to take these courses in any order or even take all of them. Just focus on one or two of them based on your particular requirements and you should be fine.
Google 1. Google IT Automation with Python Professional Certificate
Have you ever been asked to implement an API for your machine learning model, write some bash scripts to run a CRON job or some auto-scheduler and felt lost? This beginner-level professional certificate looks at Python from an engineering and production point of view rather than just Data Science. Something that we Data Scientists learn only after a few years of experience in the field and what is essentially a day to day thing for an MLE role.
The professional certificate from Google essentially talks about Python, Regex, Bash Scripting, automation testing, Github, debugging, Scaling up, and Programming Interfaces(APIs).
Most of these might not look so beginner-friendly right now, but they are some super cool skills to have in your portfolio and actually not that hard once you start understanding about the whole coding ecosystem.
2. Cloud Engineering with Google Cloud Professional Certificate
A lot of companies have started using Google Cloud Platform nowadays. If yours is such a company, this particular professional certificate might provide a lot of value. This specialization particularly focusses on the GCP platform and its various services like Google App Engine, Google Compute Engine, Google Kubernetes Engine, Google Cloud Storage, and BigQuery. You will also learn about other engineering concepts like load balancing, autoscaling, infrastructure automation, and managed services.
All of these services are pretty much becoming a standard for cloud computing at a lot of companies and it helps to learn about those if you are using the GCP infrastructure for building and deploying your models.
Amazon: 1. AWS Fundamentals
For people who aim to work at Amazon or the companies that use Amazon Web Services (AWS), this specialization teaches the AWS fundamentals and provides an overview of the features, benefits, and capabilities of AWS.
The main services you learn in this specialization are AWS Lambda(serverless compute), Amazon API Gateway(create, publish, maintain, monitor, and secure APIs at any scale), Amazon DynamoDB(Fast, flexible and scalable NoSQL database service), and Amazon Lex(Conversational AI for Chatbots), along with taking your application to the cloud.
This particular Amazon specialization is engineering heavy and by the end, you would understand how to build and deploy serverless applications with AWS.
2. Getting Started with AWS Machine Learning
There are a lot of things to consider while building a great machine learning system. We, as data scientists, only worry about the data and modeling part of the project but do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset. Having a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.
 This course will teach you:
1. How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance.
2. How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.
— Source
 3. AWS Computer Vision: Getting Started with GluonCV
This course also provides an overview of Machine Learning with Amazon Web Services but with a specific emphasis on Computer Vision applications.
In this course, you will learn how to build and train a CV model using the Apache MXNet and GluonCV toolkit. The instructors start by discussing artificial neural networks and other deep learning concepts and then walk through how to combine neural network building blocks into complete computer vision models and train them efficiently.
This course covers AWS services and frameworks including Amazon Rekognition, Amazon SageMaker, Amazon SageMaker GroundTruth, Amazon SageMaker Neo, AWS Deep Learning AMIs via Amazon EC2, AWS Deep Learning Containers, and Apache MXNet on AWS.
This course will provide a lot of value to learners who want to train a deep learning vision model from scratch and deploy it efficiently using the AWS computing platform.
Conclusion In this post, I talked about the best courses I would recommend for people who are trying to get into ML engineering in this not so much return to school session.
Also, here are my course recommendations to become a Data Scientist in 2020.
I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps</title>
      <link>https://mlwhiz.com/blog/2020/08/04/spark_dataproc/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/04/spark_dataproc/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/spark_dataproc/main.png"></media:content>
      

      
      <description>Data Exploration is a key part of Data Science. And does it take long? Ahh. Don’t even ask. Preparing a data set for ML not only requires understanding the data set, cleaning, and creating new features, it also involves doing these steps repeatedly until we have a fine-tuned system.
As we moved towards bigger datasets, Apache Spark came as a ray of hope. It gave us a scalable and distributed in-memory system to work with Big Data.</description>

      <content:encoded>  
        
        <![CDATA[  Data Exploration is a key part of Data Science. And does it take long? Ahh. Don’t even ask. Preparing a data set for ML not only requires understanding the data set, cleaning, and creating new features, it also involves doing these steps repeatedly until we have a fine-tuned system.
As we moved towards bigger datasets, Apache Spark came as a ray of hope. It gave us a scalable and distributed in-memory system to work with Big Data. By the by, we also saw frameworks like Pytorch and Tensorflow that inherently parallelized matrix computations using thousands of GPU cores.
But never did we see these two systems working in tandem in the past. We continued to use Spark for Big Data ETL tasks and GPUs for matrix intensive problems in Deep Learning.
And that is where Spark 3.0 comes. It provides us with a way to add NVIDIA GPUs to our Spark cluster nodes. The work done by these nodes can now be parallelized using both the CPU&#43;GPU using the software platform for GPU computing, RAPIDS.
 Spark &#43; GPU &#43; RAPIDS = Spark 3.0  As per NVIDIA, the early adopters of Spark 3.0 already see a significantly faster performance with their current data loads. Such reductions in processing times can allow Data Scientists to perform more iterations on much bigger datasets, allowing Retailers to improve their forecasting, finance companies to enhance their credit models, and ad tech firms to improve their ability to predict click-through rates.
Excited yet. So how can you start using Spark 3.0? Luckily, Google Cloud, Spark, and NVIDIA have come together and simplified the cluster creation process for us. With Dataproc on Google Cloud, we can have a fully-managed Apache Spark cluster with GPUs in a few minutes.
This post is about setting up your own Dataproc Spark Cluster with NVIDIA GPUs on Google Cloud.
1. Create a New GCP Project After the initial signup on the Google Cloud Platform, we can start a new project. Here I begin by creating a new project namedSparkDataProc.
Create a New Project
2. Enable the APIs in the GCP Project Once we add this project, we can go to our new project and start a Cloud Shell instance by clicking the “Activate Cloud Shell” button at the top right corner. Doing so will open up a terminal window at the bottom of our screen where we can run our next commands to set up a data proc cluster:
After this, we will need to run some commands to set up our project in the cloud shell. We start by enabling dataproc services within your project. Enable the Compute and Dataproc APIs to access Dataproc, and enable the Storage API as you’ll need a Google Cloud Storage bucket to house your data. We also set our default region. This may take several minutes:
gcloud services enable compute.googleapis.com gcloud services enable dataproc.googleapis.com gcloud services enable storage-api.googleapis.com gcloud config set dataproc/region us-central1  3. Create and Put some data in GCS Bucket Once done, we can create a new Google Cloud Storage Bucket, where we will keep all our data in the Cloud Shell:
#You might need to change this name as this needs to be unique across all the users export BUCKET_NAME=rahulsparktest #Create the Bucket gsutil mb gs://${BUCKET_NAME}  We can also put some data in the bucket for later run purposes when we are running our spark cluster.
# Get data in cloudshell terminal git clone https://github.com/caroljmcdonald/spark3-book mkdir -p ~/data/cal_housing tar -xzf spark3-book/data/cal_housing.tgz -C ~/data # Put data into Bucket using gsutil gsutil cp ~/data/CaliforniaHousing/cal_housing.data gs://${BUCKET_NAME}/data/cal_housing/cal_housing.csv  4. Setup the DataProc Rapids Cluster To create a DataProc RAPIDS cluster that uses NVIDIA T4 GPUs, we need to get some initialization scripts that are used to instantiate our cluster. These scripts will install the GPU drivers(install_gpu_driver.sh) and create the Rapids conda environment(rapids.sh) automatically for us. Since these scripts are in the development phase, the best way is to get the scripts from the GitHub source. We can do this using the below commands in our cloud shell in which we get the initialization scripts and copy them into our GS Bucket:
wget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/rapids/rapids.sh wget https://raw.githubusercontent.com/GoogleCloudDataproc/initialization-actions/master/gpu/install_gpu_driver.sh gsutil cp rapids.sh gs://$BUCKET_NAME gsutil cp install_gpu_driver.sh gs://$BUCKET_NAME  We can now create our cluster using the below command in the Cloud Shell. In the below command, we are using a predefined image version(2.0.0-RC2-ubuntu18) which has Spark 3.0 and python 3.7 to create our dataproc cluster. I am using a previous version of this image since the newest version has some issues with running Jupyter and Jupyter Lab. You can get a list of all versions here.
CLUSTER_NAME=sparktestcluster REGION=us-central1 gcloud beta dataproc clusters create ${CLUSTER_NAME} \ --image-version 2.0.0-RC2-ubuntu18 \ --master-machine-type n1-standard-8 \ --worker-machine-type n1-highmem-32 \ --worker-accelerator type=nvidia-tesla-t4,count=2 \ --optional-components ANACONDA,JUPYTER,ZEPPELIN \ --initialization-actions gs://$BUCKET_NAME/install_gpu_driver.sh,gs://$BUCKET_NAME/rapids.sh \ --metadata rapids-runtime=SPARK \ --metadata gpu-driver-provider=NVIDIA \ --bucket ${BUCKET_NAME} \ --subnet default \ --enable-component-gateway \ --properties=&amp;quot;^#^spark:spark.task.resource.gpu.amount=0.125#spark:spark.executor. cores=8#spark:spark.task.cpus=1#spark:spark.yarn.unmanagedAM.enabled=false&amp;quot;  Our resulting Dataproc cluster has:
 One 8-core master node and two 32-core worker nodes
 Two NVIDIA T4 GPUs attached to each worker node
 Anaconda, Jupyter, and Zeppelin enabled
 Component gateway enabled for accessing Web UIs hosted on the cluster
 Extra Spark config tuning suitable for a notebook environment set using the properties flag. Specifically, we set spark.executor.cores=8 for improved parallelization and spark.yarn.unmanagedAM.enabled=false since it currently breaks SparkUI.
  Troubleshooting: If you get errors regarding limits after this command, you might want to change some of the quotas in your default Google Console Quotas Page. The limits I ended up changing were:
 GPUs (all regions) to 12 (Minimum:4)
 CPUs (all regions) to 164 (Minimum:72)
 NVIDIA T4 GPUs in us-central1 to 12 (Minimum:4)
 CPUs in us-central1 to 164 (Minimum:72)
  I actually requested more limits than I required as the limit increase process might take a little longer and I will spin up some larger clusters later.
5. Run JupyterLab on DataProc Rapids Cluster Once your command succeeds(It might take 10–15 mins) you will be able to see your Dataproc cluster at https://console.cloud.google.com/dataproc/clusters. Or you can go to the Google Cloud Platform console on your browser and search for “Dataproc” and click on the “Dataproc” icon(It looks like three connected circles). This will navigate you to the Dataproc clusters page.
Now, you would be able to open a web interface(Jupyter/JupyterLab/Zeppelin) if you click on the sparktestcluster and then “Web Interfaces”.
After opening up your Jupyter Pyspark Notebook, here is some example code for you to run if you are following along with this tutorial. In this code, we load a small dataset, and we see that the df.count() function ran in 252ms which is indeed fast for Spark, but I would do a much detailed benchmarking post later so keep tuned.
file = &amp;quot;gs://rahulsparktest/data/cal_housing/cal_housing.csv&amp;quot; df = spark.read.load(file,format=&amp;quot;csv&amp;quot;, sep=&amp;quot;,&amp;quot;, inferSchema=&amp;quot;true&amp;quot;, header=&amp;quot;false&amp;quot;) colnames = [&amp;quot;longitude&amp;quot;,&amp;quot;latitude&amp;quot;,&amp;quot;medage&amp;quot;,&amp;quot;totalrooms&amp;quot;,&amp;quot;totalbdrms&amp;quot;,&amp;quot;population&amp;quot;,&amp;quot;houshlds&amp;quot;,&amp;quot;medincome&amp;quot;,&amp;quot;medhvalue&amp;quot;] df = df.toDF(*colnames) df.count()  6. Access the Spark UI That is all well and done, but one major problem I faced was that I was not able to access the Spark UI using the link provided in the notebook. I found out that there were two ways to access the Spark UI for debugging purposes:
A. Using the Web Interface option:
We can access Spark UI by clicking first on Yarn Resource Manager Link on the Web Interface and then on Application Master on the corresponding page:
And, you will arrive at the Spark UI Page:
B. Using the SSH Tunneling option:
Another option to access the Spark UI is using Tunneling. To do this, you need to go to the Web Interface Page and click on “Create an SSH tunnel to connect to a web interface”.
This will give you two commands that you want to run on your local machine and not on Cloud shell. But before running them, you need to install google cloud SDK to your machine and set it up for your current project:
sudo snap install google-cloud-sdk --classic # This Below command will open the browser where you can authenticate by selecting your own google account. gcloud auth login # Set up the project as sparkdataproc (project ID) gcloud config set project sparkdataproc  Once done with this, we can simply run the first command:
gcloud compute ssh sparktestcluster-m --project=sparkdataproc --zone=us-central1-b -- -D 1080 -N  And then the second one in another tab/window. This command will open up a new chrome window where you can access the Spark UI by clicking on Application Master the same as before.
/usr/bin/google-chrome --proxy-server=&amp;quot;socks5://localhost:1080&amp;quot; --user-data-dir=&amp;quot;/tmp/sparktestcluster-m&amp;quot; [http://sparktestcluster-m:8088](http://sparktestcluster-m:8088)  And that is it for setting up a Spark3.0 Cluster accelerated by GPUs.
It took me around 30 mins to go through all these steps if I don’t count the debugging time and the quota increase requests.
I am totally amazed by the concept of using a GPU on Spark and the different streams of experiments it opens up. Will be working on a lot of these in the coming weeks not only to benchmark but also because it is fun. So stay tuned.
Continue Learning Also, if you want to learn more about Spark and Spark DataFrames, I would like to call out these excellent courses on Big Data Essentials: HDFS, MapReduce, and Spark RDD and Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames by Yandex on Coursera.
I am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</title>
      <link>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/deployment_fastapi/main.png"></media:content>
      

      
      <description>Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS.</description>

      <content:encoded>  
        
        <![CDATA[  Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS. None of those seemed particularly great or complete to me.
So, I tried to do this myself using some help from FastAPI documentation. In this post, we will look at predominantly four things:
 Setting Up an Amazon Instance
 Creating a FastAPI API for Object Detection
 Deploying FastAPI using Docker
 An End to End App with UI
  So, without further ado, let’s get started.
You can skip any part you feel you are versed with though I would expect you to go through the whole post, long as it may be, as there’s a lot of interconnection between concepts.
1. Setting Up Amazon Instance Before we start with using the Amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a single sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine.” Here we are trying to set up the machine where we will deploy our FastAPI API.
 In the first step, you need to choose the AMI template for the machine. I am selecting the 18.04 Ubuntu Server since Ubuntu.
   In the second step, I select the t2.xlarge machine, which has 4 CPUs and 16GB RAM rather than the free tier since I want to use an Object Detection model and will need some resources.   Keep pressing Next until you reach the “6. Configure Security Group” tab. This is the most crucial step here. You will need to add a rule with Type: “HTTP” and Port Range:80.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch, you might need to create a new key pair. Here I am creating a new key pair named fastapi and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state; it should be showing “Running.”   Also, to note here are the Public DNS(IPv4) address and the IPv4 public IP. We will need it to connect to this machine. For me, they are:  Public DNS (IPv4): ec2-18-237-28-174.us-west-2.compute.amazonaws.com IPv4 Public IP: 18.237.28.174   Once you have that run the following commands in the folder, you saved the fastapi.pem file. If the file is named fastapi.txt you might need to rename it to fastapi.pem.  # run fist command if fastapi.txt gets downloaded. # mv fastapi.txt fastapi.pem chmod 400 fastapi.pem ssh -i &amp;quot;fastapi.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Now we have got our Amazon instance up and running. We can move on here to the real part of the post.
2. Creating a FastAPI API for Object Detection Before we deploy an API, we need to have an API with us, right? In one of my last posts, I had written a simple tutorial to understand FastAPI and API basics. Do read the post if you want to understand FastAPI basics.
So, here I will try to create an Image detection API. As for how to pass the Image data to the API? The idea is — What is an image but a string? An image is just made up of bytes, and we can encode these bytes as a string. We will use the base64 string representation, which is a popular way to get binary data to ASCII characters. And, we will pass this string representation to give an image to our API.
A. Some Image Basics: What is Image, But a String? So, let us first see how we can convert an Image to a String. We read the binary data from an image file using the ‘rb’ flag and turn it into a base64 encoded data representation using the base64.b64encode function. We then use the decode to utf-8 function to get the base encoded data into human-readable characters. Don’t worry if it doesn’t make a lot of sense right now. Just understand that any data is binary, and we can convert binary data to its string representation using a series of steps.
As a simple example, if I have a simple image like below, we can convert it to a string using:
import base64 with open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;, &amp;#34;rb&amp;#34;) as image_file: base64str = base64.b64encode(image_file.read()).decode(&amp;#34;utf-8&amp;#34;) Here I have got a string representation of a file named dog_with_ball.png on my laptop.
Great, we now have a string representation of an image. And, we can send this string representation to our FastAPI. But we also need to have a way to read an image back from its string representation. After all, our image detection API using PyTorch and any other package needs to have an image object that they can predict, and those methods don’t work on a string.
So here is a way to create a PIL image back from an image’s base64 string. Mostly we just do the reverse steps in the same order. We encode in ‘utf-8’ using .encode. We then use base64.b64decode to decode to bytes. We use these bytes to create a bytes object using io.BytesIO and use Image.open to open this bytes IO object as a PIL image, which can easily be used as an input to my PyTorch prediction code.*** Again simply, it is just a way to convert base64 image string to an actual image.***
import base64 import io from PIL import Image def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img So does this function work? Let’s see for ourselves. We can use just the string to get back the image.
And we have our happy dog back again. Looks better than the string.
B. Writing the Actual FastAPI code So, as now we understand that our API can get an image as a string from our user, let’s create an object detection API that makes use of this image as a string and outputs the bounding boxes for the object with the object classes as well.
Here, I will be using a Pytorch pre-trained fasterrcnn_resnet50_fpn detection model from the torchvision.models for object detection, which is trained on the COCO dataset to keep the code simple, but one can use any model. You can look at these posts if you want to train your custom image classification or image detection model using Pytorch.
Below is the full code for the FastAPI. Although it may look long, we already know all the parts. In this code, we essentially do the following steps:
 Create our fast API app using the FastAPI() constructor.
 Load our model and the classes it was trained on. I got the list of classes from the PyTorch docs.
 We also defined a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user. Here the end-user gives the base64str and some score threshold for object detection prediction.
 We add a function called base64str_to_PILImage which does just what it is named.
 And we write a predict function called get_predictionbase64 which returns a dict of bounding boxes and classes using a base64 string representation of an image and a threshold as an input. We also add @app.put(“/predict”) on top of this function to define our endpoint. If you need to understand put and endpoint refer to my previous post on FastAPI.
  from fastapi import FastAPI from pydantic import BaseModel import torchvision from torchvision import transforms import torch from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from PIL import Image import numpy as np import cv2 import io, json import base64 app = FastAPI() # load a pre-trained Model and convert it to eval mode. # This model loads just once when we start the API. model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) COCO_INSTANCE_CATEGORY_NAMES = [ &amp;#39;__background__&amp;#39;, &amp;#39;person&amp;#39;, &amp;#39;bicycle&amp;#39;, &amp;#39;car&amp;#39;, &amp;#39;motorcycle&amp;#39;, &amp;#39;airplane&amp;#39;, &amp;#39;bus&amp;#39;, &amp;#39;train&amp;#39;, &amp;#39;truck&amp;#39;, &amp;#39;boat&amp;#39;, &amp;#39;traffic light&amp;#39;, &amp;#39;fire hydrant&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;stop sign&amp;#39;, &amp;#39;parking meter&amp;#39;, &amp;#39;bench&amp;#39;, &amp;#39;bird&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;horse&amp;#39;, &amp;#39;sheep&amp;#39;, &amp;#39;cow&amp;#39;, &amp;#39;elephant&amp;#39;, &amp;#39;bear&amp;#39;, &amp;#39;zebra&amp;#39;, &amp;#39;giraffe&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;backpack&amp;#39;, &amp;#39;umbrella&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;handbag&amp;#39;, &amp;#39;tie&amp;#39;, &amp;#39;suitcase&amp;#39;, &amp;#39;frisbee&amp;#39;, &amp;#39;skis&amp;#39;, &amp;#39;snowboard&amp;#39;, &amp;#39;sports ball&amp;#39;, &amp;#39;kite&amp;#39;, &amp;#39;baseball bat&amp;#39;, &amp;#39;baseball glove&amp;#39;, &amp;#39;skateboard&amp;#39;, &amp;#39;surfboard&amp;#39;, &amp;#39;tennis racket&amp;#39;, &amp;#39;bottle&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;wine glass&amp;#39;, &amp;#39;cup&amp;#39;, &amp;#39;fork&amp;#39;, &amp;#39;knife&amp;#39;, &amp;#39;spoon&amp;#39;, &amp;#39;bowl&amp;#39;, &amp;#39;banana&amp;#39;, &amp;#39;apple&amp;#39;, &amp;#39;sandwich&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;broccoli&amp;#39;, &amp;#39;carrot&amp;#39;, &amp;#39;hot dog&amp;#39;, &amp;#39;pizza&amp;#39;, &amp;#39;donut&amp;#39;, &amp;#39;cake&amp;#39;, &amp;#39;chair&amp;#39;, &amp;#39;couch&amp;#39;, &amp;#39;potted plant&amp;#39;, &amp;#39;bed&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;dining table&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;toilet&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;tv&amp;#39;, &amp;#39;laptop&amp;#39;, &amp;#39;mouse&amp;#39;, &amp;#39;remote&amp;#39;, &amp;#39;keyboard&amp;#39;, &amp;#39;cell phone&amp;#39;, &amp;#39;microwave&amp;#39;, &amp;#39;oven&amp;#39;, &amp;#39;toaster&amp;#39;, &amp;#39;sink&amp;#39;, &amp;#39;refrigerator&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;book&amp;#39;, &amp;#39;clock&amp;#39;, &amp;#39;vase&amp;#39;, &amp;#39;scissors&amp;#39;, &amp;#39;teddy bear&amp;#39;, &amp;#39;hair drier&amp;#39;, &amp;#39;toothbrush&amp;#39; ] model.eval() # define the Input class class Input(BaseModel): base64str : str threshold : float def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img @app.put(&amp;#34;/predict&amp;#34;) def get_predictionbase64(d:Input): &amp;#39;&amp;#39;&amp;#39; FastAPI API will take a base 64 image as input and return a json object &amp;#39;&amp;#39;&amp;#39; # Load the image img = base64str_to_PILImage(d.base64str) # Convert image to tensor transform = transforms.Compose([transforms.ToTensor()]) img = transform(img) # get prediction on image pred = model([img]) pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][&amp;#39;labels&amp;#39;].numpy())] pred_boxes = [[(float(i[0]), float(i[1])), (float(i[2]), float(i[3]))] for i in list(pred[0][&amp;#39;boxes&amp;#39;].detach().numpy())] pred_score = list(pred[0][&amp;#39;scores&amp;#39;].detach().numpy()) pred_t = [pred_score.index(x) for x in pred_score if x &amp;gt; d.threshold][-1] pred_boxes = pred_boxes[:pred_t&#43;1] pred_class = pred_class[:pred_t&#43;1] return {&amp;#39;boxes&amp;#39;: pred_boxes, &amp;#39;classes&amp;#39; : pred_class} C. Local Before Global: Test the FastAPI code locally Before we move on to AWS, let us check if the code works on our local machine. We can start the API on our laptop using:
uvicorn fastapiapp:app --reload  The above means that your API is now running on your local server, and the &amp;ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &amp;ndash;reload flag when you put the API in production.
You should see something like:
You can now try to access this API and see if it works using the requests module:
import requests,json payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&amp;#34;,data = payload) data_dict = response.json() And so we get our results using the API. This image contains a dog and a sports ball. We also have corner 1 (x1,y1) and corner 2 (x2,y2) coordinates of our bounding boxes.
D. Lets Visualize Although not strictly necessary, we can visualize how the results look in our Jupyter notebook:
from PIL import Image import numpy as np import cv2 import matplotlib.pyplot as plt def PILImage_to_cv2(img): return np.asarray(img) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) # Write the prediction class plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() img = Image.open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) Here is the output:
Here you will note that I got the image from the local file system, and that sort of can be considered as cheating as we don’t want to save every file that the user sends to us through a web UI. We should have been able to use the same base64string object that we also had to create this image. Right?
Not to worry, we could do that too. Remember our base64str_to_PILImage function? We could have used that also.
img = base64str_to_PILImage(base64str) drawboundingbox(img, data_dict[&#39;boxes&#39;], data_dict[&#39;classes&#39;])  That looks great. We have our working FastAPI, and we also have our amazon instance. We can now move on to Deployment.
3. Deployment on Amazon ec2 Till now, we have created an AWS instance and, we have also created a FastAPI that takes as input a base64 string representation of an image and returns bounding boxes and the associated class. But all the FastAPI code still resides in our local machine. How do we put it on the ec2 server? And run predictions on the cloud.
A. Install Docker We will deploy our app using docker, as is suggested by the fastAPI creator himself. I will try to explain how docker works as we go. The below part may look daunting but it just is a series of commands and steps. So stay with me.
We can start by installing docker using:
sudo apt-get update sudo apt install docker.io  We then start the docker service using:
sudo service docker start  B. Creating the folder structure for docker └── dockerfastapi ├── Dockerfile ├── app │ └── main.py └── requirements.txt  Here dockerfastapi is our project’s main folder. And here are the different files in this folder:
i. requirements.txt: Docker needs a file, which tells it which all libraries are required for our app to run. Here I have listed all the libraries I used in my Fastapi API.
numpy opencv-python matplotlib torchvision torch fastapi pydantic  ii. Dockerfile: The second file is Dockerfile.
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app COPY requirements.txt . RUN pip --no-cache-dir install -r requirements.txt  How Docker works?: You can skip this section, but it will help to get some understanding of how docker works.
The dockerfile can be thought of something like a sh file,which contains commands to create a docker image that can be run in a container. One can think of a docker image as an environment where everything like Python and Python libraries is installed. A container is a unit which is just an isolated box in our system that uses a dockerimage. The advantage of using docker is that we can create multiple docker images and use them in multiple containers. For example, one image might contain python36, and another can contain python37. And we can spawn multiple containers in a single Linux server.
Our Dockerfile contains a few things:
 FROM command: Here the first line FROM specifies that we start with tiangolo’s (FastAPI creator) Docker image. As per his site: “This image has an “auto-tuning” mechanism included so that you can just add your code and get that same high performance automatically. And without making sacrifices”. What we are doing is just starting from an image that installs python3.7 for us along with some added configurations for uvicorn and gunicorn ASGI servers and a start.sh file for ASGI servers automatically. For adventurous souls, particularly commandset1 and commandset2 get executed through a sort of a daisy-chaining of commands.
 COPY command: We can think of a docker image also as a folder that contains files and such. Here we copy our app folder and the requirements.txt file, which we created earlier to our docker image.
 RUN Command: We run pip install command to install all our python dependencies using the requirements.txt file that is now on the docker image.
  iii. main.py: This file contains the fastapiapp.py code we created earlier. Remember to keep the name of the file main.py only.
C. Docker Build We have got all our files in the required structure, but we haven’t yet used any docker command. We will first need to build an image containing all dependencies using Dockerfile.
We can do this simply by:
sudo docker build -t myimage .  This downloads, copies and installs some files and libraries from tiangolo’s image and creates an image called myimage. This myimage has python37 and some python packages as specified by requirements.txt file.
We will then just need to start a container that runs this image. We can do this using:
sudo docker run -d --name mycontainer -p 80:80 myimage  This will create a container named mycontainer which runs our docker image myimage. The part 80:80 connects our docker container port 80 to our Linux machine port 80.
And actually that’s it. At this point, you should be able to open the below URL in your browser.
# &amp;lt;IPV4 public IP&amp;gt;/docs URL: 18.237.28.174/docs  And we can check our app programmatically using:
payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://18.237.28.174/predict](http://18.237.28.174/predict)&amp;#34;,data = payload) data_dict = response.json() print(data_dict) &amp;gt; # Yup, finally our API is deployed.
D. Troubleshooting as the real world is not perfect All the above was good and will just work out of the box if you follow the exact instructions, but the real world doesn’t work like that. You will surely get some errors along the way and would need to debug your code. So to help you with that, some docker commands may come handy:
 Logs: When we ran our container using sudo docker run we don’t get a lot of info, and that is a big problem when you are debugging. You can see the real-time logs using the below command. If you see an error here, you will need to change your code and build the image again.   sudo docker logs -f mycontainer   Starting and Stopping Docker: Sometimes, it might help just to restart your docker. In that case, you can use:   sudo service docker stop sudo service docker start   Listing images and containers: Working with docker, you will end up creating images and containers, but you won’t be able to see them in the working directory. You can list your images and containers using:   sudo docker container ls sudo docker image ls   Deleting unused docker images or containers: You might need to remove some images or containers as these take up a lot of space on the system. Here is how you do that.   # the prune command removes the unused containers and images sudo docker system prune # delete a particular container sudo docker rm mycontainer # remove myimage sudo docker image rm myimage # remove all images sudo docker image prune — all   Checking localhost:The Linux server doesn’t have a browser, but we can still see the browser output though it’s a little ugly:   curl localhost   Develop without reloading image again and again: For development, it’s useful to be able just to change the contents of the code on our machine and test it live, without having to build the image every time. In that case, it’s also useful to run the server with live auto-reload automatically at every code change. Here, we use our app directory on our Linux machine, and we replace the default (/start.sh) with the development alternative /start-reload.sh during development. After everything looks fine, we can build our image again run it inside the container.   sudo docker run -d -p 80:80 -v $(pwd):/app myimage /start-reload.sh  If this doesn’t seem sufficient, adding here a docker cheat sheet containing useful docker commands:
4. An End to End App with UI We are done here with our API creation, but we can also create a UI based app using Streamlit using our FastAPI API. This is not how you will do it in a production setting (where you might have developers making apps using react, node.js or javascript)but is mostly here to check the end-to-end flow of how to use an image API. I will host this barebones Streamlit app on local rather than the ec2 server, and it will get the bounding box info and classes from the FastAPI API hosted on ec2.
If you need to learn more about how streamlit works, you can check out this post. Also, if you would want to deploy this streamlit app also to ec2, here is a tutorial again.
Here is the flow of the whole app with UI and FastAPI API on ec2:
Project Architecture
The most important problems we need to solve in our streamlit app are:
How to get an image file from the user using Streamlit? A. Using File uploader: We can use the file uploader using:
bytesObj = st.file_uploader(“Choose an image file”) The next problem is, what is this bytesObj we get from the streamlit file uploader? In streamlit, we will get a bytesIO object from the file_uploader and we will need to convert it to base64str for our FastAPI app input. This can be done using:
def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) base64str = bytesioObj_to_base64str(bytesObj) B. Using URL: We can also get an image URL from the user using text_input.
url = st.text_input(‘Enter URL’) We can then get image from URL in base64 string format using the requests module and base64 encode and utf-8 decode:
def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) base64str = ImgURL_to_base64str(url) And here is the complete code of our Streamlit app. You have seen most of the code in this post already.
import streamlit as st import base64 import io import requests,json from PIL import Image import cv2 import numpy as np import matplotlib.pyplot as plt import requests import random # use file uploader object to recieve image # Remember that this bytes object can be used only once def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) # Image conversion functions def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img def PILImage_to_cv2(img): return np.asarray(img) def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() st.markdown(&amp;#34;&amp;lt;h1&amp;gt;Our Object Detector App using FastAPI&amp;lt;/h1&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) bytesObj = st.file_uploader(&amp;#34;Choose an image file&amp;#34;) st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h2&amp;gt;or&amp;lt;/h2&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) url = st.text_input(&amp;#39;Enter URL&amp;#39;) if bytesObj or url: # In streamlit we will get a bytesIO object from the file_uploader # and we convert it to base64str for our FastAPI if bytesObj: base64str = bytesioObj_to_base64str(bytesObj) elif url: base64str = ImgURL_to_base64str(url) # We will also create the image in PIL Image format using this base64 str # Will use this image to show in matplotlib in streamlit img = base64str_to_PILImage(base64str) # Run FastAPI payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;http://18.237.28.174/predict&amp;#34;,data = payload) data_dict = response.json() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;App Result&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) st.pyplot() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;FastAPI Response&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) st.write(data_dict) We can run this streamlit app in local using:
streamlit run streamlitapp.py  And we can see our app running on our localhost:8501. Works well with user-uploaded images as well as URL based images. Here is a cat image for some of you cat enthusiasts as well.
 So that’s it. We have created a whole workflow here to deploy image detection models through FastAPI on ec2 and utilizing those results in Streamlit. I hope this helps your woes around deploying models in production. You can find the code for this post as well as all my posts at my GitHub repository.
Let me know if you like this post and if you would like to include Docker or FastAPI or Streamlit in your day to day deployment needs. I am also looking to create a much detailed post on Docker so follow me up to stay tuned with my writing as well. Details below.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman’s Guide for Data Scientists to create APIs in minutes</title>
      <link>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/fastapi_for_data_scientists/main.png"></media:content>
      

      
      <description>Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.
As Data Science and Web developers try to collaborate, API’s become an essential piece of the puzzle to make codes as well as skills more modular.</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.
As Data Science and Web developers try to collaborate, API’s become an essential piece of the puzzle to make codes as well as skills more modular. In fact, in the same way, that a data scientist can’t be expected to know much about Javascript or nodeJS, a frontend developer should be able to get by without knowing any Data Science Language. And APIs do play a considerable role in this abstraction.
But, APIs are confusing. I myself have been confused a lot while creating and sharing them with my development teams who talk in their API terminology like GET request, PUT request, endpoint, Payloads, etc.
This post will be about simplifying and understanding how APIs work, explaining some of the above terms, and creating an API using the excellent API building framework called FastAPI, which makes creating APIs a breeze.
What is an API? Before we go any further, we need to understand what an API is. According to Wikipedia:
 An application programming interface (API) is a computing interface which defines interactions between multiple software intermediaries. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow, etc.
 The way I like to understand an API is that it’s an “online function,” a function that I can call online.
For example:
I can have a movie API, which returns me names of drama movies when I pass the “animation” genre as input.
The advantage of using such a sort of mechanism is that the API user doesn’t get access to the whole dataset or source code and yet they can get all the information they need. This is how many services on the internet like Amazon Rekognition, which is an image and video API, or Google Natural Language API, which is an NLP API works. They provide us access to some great functions without letting us have the source code, which is often valuable and kept hidden. For example, I can send an image to Amazon Rekognition API, and it can provide me with Face detection and Analysis.
For example, here is a free API floated by Open Movie DB, which lets us search for movies using parameters:
[http://www.omdbapi.com/?i=tt3896198&amp;amp;apikey=9ea43e94](http://www.omdbapi.com/?i=tt3896198&amp;amp;apikey=9ea43e94)  Here I provided the IMDB id for the movie Guardians of the Galaxy 2, using the i parameter for the API. If you open this link in your browser, you will see the whole information of the movie as per the Open Movie Database
Output from OMDB
But before we go any further, let’s understand some terms:
 Endpoint: In the above API call, the endpoint is : http://www.omdbapi.com/ . Simply this is the location of where the function code is running.
 API Access Key: Most of the public APIs will have some access key, which you can request. For OMDB API, I had to register and get the API key which is 9ea43e94.
 ? Operator:This operator is used to specify the parameters we want to send to the API or our “online function.” Here we give two params to our API i.e., IMDB movie ID and API Access Key using the ? operator. Since there are multiple inputs, we use &amp;amp; operator also.
  Why FastAPI? “If you’re looking to learn one modern framework for building REST APIs, check out FastAPI […] It’s fast, easy to use and easy to learn […]” — spaCy creators
While Python has many frameworks to build APIs, the most common being Flask and Tornado, FastAPI is much better than available alternatives in its ease of usage as it seems much more pythonic in comparison with Flask.
Also, FastAPI is fast. As the Github docs say, “Very high performance, on par with NodeJS and Go.” We can also check the latency benchmarks for ourselves.
That is around a speedup by a factor of 2 when compared to Flask and that too without a lot of code change. This means a huge deal when it comes to building an API that can serve millions of customers as it can reduce production efforts and also use less expensive hardware to serve.
So enough of comparison and talk, let’s try to use FastAPI to create our API.
How to write an API with FastAPI? One of the most common use cases for Data Science is how to create an API for getting a model’s prediction? Let us assume that we have a Titanic Survival model in place that predicts if a person will survive or not. And, it needs a person’s age and sex as input params to predict. We will create this API using FastAPI in two ways: GET and PUT. Don’t worry; I will explain each as we go.
What is GET? — In a GET request, we usually try to retrieve data using query parameters that are embedded in the query string itself. For example, in the OMDB API, we use the GET request to specify the movie id and access key as part of the query itself.
What is PUT? — An alternative to the GET request is the PUT request, where we send parameters using a payload, as we will see in the second method. The payload is not part of the query string, and thus PUT is more secure. It will become more clear when you see the second part.
But before we go any further, we need to install FastAPI and uvicorn ASGI server with:
pip install fastapi pip install uvicorn  1. The GET Way: A simple FastAPI method to writing a GET API for our titanic model use case is as follows:
from fastapi import FastAPI app = FastAPI() @app.get(&amp;quot;/predict&amp;quot;) def predict_complex_model(age: int,sex:str): # Assume a big and complex model here. For this test I am using a simple rule based model if age&amp;lt;10 or sex==&#39;F&#39;: return {&#39;survived&#39;:1} else: return {&#39;survived&#39;:0}  Save the above code in a file named fastapiapp.py and then you can run it using the below command on terminal.
$ uvicorn fastapiapp:app --reload  The above means that your API is now running on your server, and the &amp;ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &amp;ndash;reload flag when you put the API in production. Now you can visit the below path in your browser, and you will get the prediction results:
[http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M](http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M)  What happens is as you hit the command in your browser, it calls the http://127.0.0.1:8000/predict endpoint which in turn calls the associated method predict_complex_model with the with params age=10 and sex=&amp;rsquo;M&amp;rsquo;
So, it allows us to use our function from a browser, but that’s still not very helpful. Your developer friend needs to use your predict function to show output on a frontend website. How can you provide him with access to this function?
It is pretty simple. If your developer friend also uses Python, for example, he can use the requests module like below:
import requests age = 15 sex = &amp;quot;F&amp;quot; response = requests.get(f&amp;quot;[http://127.0.0.1:8000/predict?age={age}&amp;amp;sex={](http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M)sex}&amp;quot;) output = response.json()  So we can get the output from the running API(on the server) into our Python Program. A Javascript user would use Javascript Request Library, and a nodeJS developer will use something similar to do this in nodeJS. We will just need to provide them with the endpoint and parameters required.
To test your API, you could also go to the:
[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  Where you will find a GUI way to test your API.
But as we said earlier, THIS IS NOT SECURE as GET parameters are passed via URL. This means that parameters get stored in server logs and browser history. This is not intended. Further, this toy example just had two input parameters, so we were able to do it this way, think of a case where we need to provide many parameters to our predict function.
In such a case or I dare say in most of the cases, we use the PUT API.
2. The PUT Way Using the PUT API, we can call any function by providing a payload to the function. A payload is nothing but a JSON dictionary of input parameters that doesn’t get appended to the query string and is thus much more secure than GET.
Here is the minimal example where we do that same thing as before using PUT. We just change the content of fastapiapp.py to:
from fastapi import FastAPI from pydantic import BaseModel class Input(BaseModel): age : int sex : str app = FastAPI() [@app](http://twitter.com/app).put(&amp;quot;/predict&amp;quot;) def predict_complex_model(d:Input): if d.age&amp;lt;10 or d.sex==&#39;F&#39;: return {&#39;survived&#39;:1} else: return {&#39;survived&#39;:0}  note that we use app.put here in place of app.get previously. We also needed to provide a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user while previously in GET, we validated the inputs using the function parameter list. Also, this time you won’t be able to see your content using a URL on the web. For example, using the browser to point to the endpoint location gives:
So, we can check using the programmatic way using requests in Python again:
import requests,json payload = json.dumps({ &amp;quot;age&amp;quot;: 10, &amp;quot;sex&amp;quot;: &amp;quot;F&amp;quot; }) response = requests.put(&amp;quot;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&amp;quot;,data = payload) response.json()  Notice that we use requests.put here and we provide the payload using the data param in the requests.put function and we also make use of json library to convert our payload to JSON from a dict object.
We could also have used the GUI way as before using:
[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  And, we are done with creating our API. It was simple for a change.
FastAPI makes the API creation, which used to be one of the dreaded parts of the Data Science process, much more intuitive, easy, and Fast.
You can find the code for this post as well as all my posts at my GitHub repository.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Newspaper for COVID-19 — The CoronaTimes</title>
      <link>https://mlwhiz.com/blog/2020/03/29/coronatimes/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/03/29/coronatimes/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/coronatimes/main.gif"></media:content>
      

      
      <description>It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea.</description>

      <content:encoded>  
        
        <![CDATA[  It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.
To do this, I generally visit a lot of news media sites in various countries to read up on the news. This gave me an idea. Why not create an international news dashboard for Corona? And here it is.
This post is about how I created the news dashboard using Streamlit and data from NewsApi and European CDC.
TLDR; Link to the App here.
Getting The Data The most important thing while creating this Dashboard was acquiring the data. I am using two data sources:
1. Data from the European Centre for Disease Prevention and Control. The downloadable data file is updated daily and contains the latest available public data on COVID-19. Here is a snapshot of this data.
def get_data(date): os.system(&amp;quot;rm cases.csv&amp;quot;) url = &amp;quot;[https://opendata.ecdc.europa.eu/covid19/casedistribution/csv](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)&amp;quot; filename = wget.download(url,&amp;quot;cases.csv&amp;quot;) casedata = pd.read_csv(filename, encoding=&#39;latin-1&#39;) return casedata  2. News API The second source of data comes from the News API, which lets me access articles from leading news outlets from various countries for free. The only caveat is that I could only hit the API 500 times a day, and there is a result limit of 100 results for a particular query for free accounts.
I tried to get around those limit barriers by using streamlit caching(So I don’t hit the API a lot). I also tried to get news data from last month using multiple filters to get a lot of data.
from newsapi import NewsApiClient newsapi = NewsApiClient(api_key=&#39;aedb6aa9bebb4011a4eb5447019dd592&#39;)  The primary way the API works is by giving us access to 3 functions.
a) A function to get Recent News from a country:
json_data = newsapi.get_top_headlines(q=q,language=&#39;en&#39;, country=&#39;us&#39;) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  b) A function to get “Everything” related to a query from the country. You can see the descriptions of API parameters here:
json_data = newsapi.get_everything(q=&#39;corona&#39;, language=&#39;en&#39;, from_param=str(date.today() -timedelta(days=29)), to= str(date.today()), sources = &#39;usa-today&#39;, page_size=100, page = 1, sort_by=&#39;relevancy&#39; ) data = pd.DataFrame(json_data[&#39;articles&#39;]) data.head()  c) A function to get a list of sources from a Country programmatically. We can then use these sources to pull data from the “everything” API
def get_sources(country): sources = newsapi.get_sources(country=country) sources = [x[&#39;id&#39;] for x in sources[&#39;sources&#39;]] return sources sources = get_sources(country=&#39;us&#39;) print(sources[:5]) ------------------------------------------------------------------- [&#39;abc-news&#39;, &#39;al-jazeera-english&#39;, &#39;ars-technica&#39;, &#39;associated-press&#39;, &#39;axios&#39;]  I used all the functions above to get data that refreshes at a particular cadence. You can see how I use these API functions in a loop to download the data by looking at my code at GitHub.
Creating the Dashboard I wanted to have a few important information in the Dashboard that I was interested in. So I started by creating various widgets.
1. Current World Snapshot: The first information was regarding the whole world situation. The Number of Cases and Deaths. The case and death curve in various countries? What are the fatality rates in various countries? Below is the current world situation on 28 Mar 2020.
Observations: We can see the deaths in Italy are still on the rise, while we are seeing the deaths shooting up in Spain, France, and the United States as well. The death rates in some countries are worrying with death rates of 10.56% in Italy and 8.7% in Iraq. I suspect that the death rate statistic of 2% in the starting days of CoronaVirus was misinformed if not wrong.
Technical Details — To create this part of the Dashboard, I used the ECDC data. I also used a lot of HTML hacks with Streamlit, where I used bootstrap widgets as well as custom HTML to get data in the way I wanted to display it. Here are a few of the hacks:
 Using Bootstrap Cards: You can use bootstrap or, in that case, any HTML element in Streamlit if you change the parameter unsafe_allow_html to True. Do note that I am also using python f string formatting here.  st.sidebar.markdown(f&amp;#39;&amp;#39;&amp;#39;&amp;lt;div class=&amp;#34;card text-white bg-info mb-3&amp;#34; style=&amp;#34;width: 18rem&amp;#34;&amp;gt; &amp;lt;div class=&amp;#34;card-body&amp;#34;&amp;gt; &amp;lt;h5 class=&amp;#34;card-title&amp;#34;&amp;gt;Total Cases&amp;lt;/h5&amp;gt; &amp;lt;p class=&amp;#34;card-text&amp;#34;&amp;gt;{sum(casedata[&amp;#39;cases&amp;#39;]):,d}&amp;lt;/p&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;&amp;#39;&amp;#39;&amp;#39;, unsafe_allow_html=True) The above code is behind the Dashboard styled cards in the streamlit app sidebar.
 Changed the width of the streamlit main page:  Again, there was no parameter given by streamlit to do this, and I was finding the page width a little too small for my use case. Adding the above code at the start of the app solved the issue.
st.markdown( f&amp;#34;&amp;#34;&amp;#34; &amp;lt;style&amp;gt; .reportview-container .main .block-container{{ max-width: 1000px; }} &amp;lt;/style&amp;gt; &amp;#34;&amp;#34;&amp;#34;, unsafe_allow_html=True, ) 2. Most Recent News from Country The primary purpose of creating this Dashboard was to get news from various outlets from top media outlets in the country.
Observations: As here you can see, here we have the top recent news from the United Kingdom concerning cases in Ireland and Boris Johnson’s corona woes.
Technical Details: As said before, I am using the News API to get this data. And here is how I am using a mashup of HTML and markdown to display the news results.
def create_most_recent_markdown(df,width=700): if len(df)&amp;gt;0: # img url img_path = df[&#39;urlToImage&#39;].iloc[0] if not img_path: images = [x for x in df.urlToImage.values if x is not None] if len(images)!=0: img_path = random.choice(images) else: img_path = &#39;[https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;](https://www.nfid.org/wp-content/uploads/2020/02/Coronavirus-400x267.png&#39;) img_alt = df[&#39;title&#39;].iloc[0] df = df[:5] **markdown_str = f&amp;quot;&amp;lt;img src=&#39;{img_path}&#39; width=&#39;{width}&#39;/&amp;gt; &amp;lt;br&amp;gt; &amp;lt;br&amp;gt;&amp;quot;** for index, row in df.iterrows(): **markdown_str &#43;= f&amp;quot;[{row[&#39;title&#39;]}]({row[&#39;url&#39;]}) by {row[&#39;author&#39;]}&amp;lt;br&amp;gt; &amp;quot;** return markdown_str else: return &#39;&#39;  Few things to note here:
 The image width cannot be set using markdown so using custom HTML
 The usage of python f strings to create the article titles and URLs.
 If no image is found, we are defaulting to a custom image.
  3. News Sentiment Another thing that has been bothering me in these trying times is so much negativity everywhere. I wanted to see the news covered from a positive angle if it could be in any way. So I did some simple sentiment analysis using the custom sentiment analyzer from Textblob to do this.
I found out sentiments by news outlets as well as some of the most positive and negative news related to Coronavirus in the past 30 days. (Past 30 days because I cannot go more back with the free API).
Observations: As you can see that one of the most positive news is Trump changing his coronavirus stance on March 17th, and I agree. The second positive report seems to be regarding some sort of solution to the problem. While the first Negative news is regarding Cardi B slamming celebrities for sowing confusion about the Coronavirus. I won’t comment on this :)
Technical Details: To get the sentiment scores of an article I used TextBlob. Getting the sentiment scores that range from -1 to 1 is as simple as using the below function. I used a concatenation of title and description to find the sentiment as the content from the News API was truncated.
def textblob_sentiment(title,description): blob = TextBlob(str(title)&#43;&amp;quot; &amp;quot;&#43;str(description)) return blob.sentiment.polarity  The main difficulty here was to have a two-column layout to give both positive and negative news. For that again, I had to use a mashup of HTML and markdown. I used the HTML table to do this. Also, note how I used markdown to convert markdown to HTML using Python f strings.
import markdown md = markdown.Markdown() positive_results_markdown = create_most_recent_markdown(positivedata,400) negative_results_markdown = create_most_recent_markdown(negativedata,400) html = f&#39;&#39;&#39;&amp;lt;table style=&amp;quot;width:100%&amp;quot;&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Positive News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;th&amp;gt;&amp;lt;center&amp;gt;Most Negative News&amp;lt;/center&amp;gt;&amp;lt;/th&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;tr&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(positive_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;td&amp;gt;&amp;lt;center&amp;gt;**{md.convert(negative_results_markdown)}**&amp;lt;/center&amp;gt;&amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;/table&amp;gt;&#39;&#39;&#39; #print md.convert(&amp;quot;# sample heading text&amp;quot;) st.markdown(html,unsafe_allow_html=True)  4. News Source WordCloud A visualization dashboard that works with text is never really complete without a word cloud, so I thought of adding a word cloud to understand the word usage from a particular source.
Observations: We can see Vice news using words like “New” and “Tested” a lot of times. While Business Insider used “China” a lot.
      Technical Details:Here is what I used to create this masked word cloud:
import cv2 def create_mask(): mask = np.array(Image.open(&amp;quot;coronavirus.png&amp;quot;)) im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) _, mask = cv2.threshold(im_gray, thresh=20, maxval=255, type=cv2.THRESH_BINARY) mask = 255 - mask return mask mask = create_mask() def create_wc_by(source): data = fulldf[fulldf[&#39;source&#39;]==source] text = &amp;quot; &amp;quot;.join([x for x in data.content.values if x is not None]) stopwords = set(STOPWORDS) stopwords.add(&#39;chars&#39;) stopwords.add(&#39;coronavirus&#39;) stopwords.add(&#39;corona&#39;) stopwords.add(&#39;chars&#39;) wc = WordCloud(background_color=&amp;quot;white&amp;quot;, max_words=1000, mask=mask, stopwords=stopwords, max_font_size=90, random_state=42, contour_width=3, contour_color=&#39;steelblue&#39;) wc.generate(text) plt.figure(figsize=[30,30]) plt.imshow(wc, interpolation=&#39;bilinear&#39;) plt.axis(&amp;quot;off&amp;quot;) return plt st.pyplot(create_wc_by(source),use_container_width=True)  Other Technical Considerations 1. Advanced Caching: In new streamlit release notes for 0.57.0 which just came out yesterday, streamlit has made updates to st.cache. One notable change to this release is the “ability to set expiration options for cached functions by setting the max_entries and ttl arguments”. From the documentation:
 max_entries (int or None) — The maximum number of entries to keep in the cache, or None for an unbounded cache. (When a new entry is added to a full cache, the oldest cached entry will be removed.) The default is None.
 ttl (float or None) — The maximum number of seconds to keep an entry in the cache, or None if cache entries should not expire. The default is None.
  Two use cases where this might help would be:
 If you’re serving your app and don’t want the cache to grow forever.
 If you have a cached function that reads live data from a URL and should clear every few hours to fetch the latest data
  So this is what is being used in a lot of functions to avoid hitting APIs multiple times and to prevent them from getting stale at the same time.
For Example, Top results from a country are fetched at a period of 360 seconds i.e., 6 minutes.
st.cache(ttl=360,max_entries=20) def create_dataframe_top(queries,country): #Hits API Here  While full results from the everything API are fetched at a period of one day.
[@st](http://twitter.com/st).cache(ttl = 60*60*24,max_entries=20) def create_dataframe_last_30d(queries, sources): # hits API  2. Deployment: I used the amazon free ec2 instance to deploy this app at http://54.149.204.138:8501/. If you want to know the steps,read my post on How to Deploy a Streamlit App using an Amazon Free ec2 instance?
There are also a few caveats:
 Since it is a free server, it might not take too much load.
 I have not thoroughly tested the caching routine. I just hope that there are no memory errors with the limited memory on the server.
 The News API is also free. There might be rate limits that might kick in even after I have tried to handle that.
  3. Learning For folks who are lost, you might like to start with the basics first. Here is my introductory posts on Streamlit and Plotly express.
 How to write Web apps using simple Python for Data Scientists? Python’s One Liner graph creation library with animations Hans Rosling Style  Conclusion Here I have tried creating a dashboard for news on Coronavirus, but it is still in a nascent stage, and a lot needs to be done.
For one, it needs a large server. For another, a lot of time to improve the visualization and layouts. And also a lot of testing.
Also, we have done a few things in a roundabout way using HTML and few hacks. There are still a lot of things that I will love to have in Streamlit. I have been in talks with the Streamlit team over the new functionality that they are going to introduce, and I will try to keep you updated on the same. The good news is that Layout options are a part of the new functionality that Streamlit is working on.
You can find the full code for the final app here at my Github repo. And here is the full app on the web.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Handling Trees in Data Science Algorithmic Interview</title>
      <link>https://mlwhiz.com/blog/2020/01/29/altr/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/29/altr/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/altr/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are crucial all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are crucial all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.
This post is about fast-tracking this study and explaining tree concepts for the data scientists so that you breeze through the next time you get asked these in an interview.
But First, Why are Trees important for Data Science? To data scientists, Trees mean a different thing than they mean for a Software Engineer.
For a software engineer, a tree is just a simple Data Structure they can use to manage hierarchical relationships while for a Data Scientists trees form the basis of some of the most useful classification and regression algorithms.
So where do these two meet?
They are necessarily the same thing. Don’t be surprised. Below is how data scientists and software engineer’s look at trees.
The only difference is that Data science tree nodes keep much more information that helps us in identifying how to traverse the tree. For example, in the case of Data science tree for prediction, we will look at the feature in the node and determine which way we want to move based on the split value.
If you want to write your decision tree from scratch, you might need to understand how trees work from a software engineering perspective too.
Types of Trees: In this post, I will only be talking about two kinds of trees that get asked a lot in Data Science interview questions. Binary Trees(BT) and an extension of Binary Trees called Binary Search Trees(BST).
1. Binary Trees: A binary tree is simply a tree in which each node has up to two children. A decision tree is an example we see in our day to day lives.
2. Binary Search Tree(BST): A binary search tree is a binary tree in which:
 All left descendants of a node are less than or equal to the node, and
 All right descendants of the node are greater than the node.
  There are variations to this definition when it comes to equalities. Sometimes the equalities are on the right-hand side or either side. Sometimes only distinct values are allowed in the tree.
Source
8 is greater than all the elements in the left subtree and smaller than all elements in the right subtree. The same could be said for any node in the tree.
Creating a Simple Tree: So How do we construct a simple tree?
By definition, a tree is made up of nodes. So we start by defining the node class which we will use to create nodes. Our node class is pretty simple as it holds value for a node, the location of the left child and the location of the right child.
class node: def __init__(self,val): self.val = val self.left = None self.right = None We can create a simple tree now as:
root = node(1) root.left = node(2) root.right = node(3) Now I have noticed that we cannot really get the hang of Tree-based questions without doing some coding ourselves.
So let us get a little deeper into the coding part with some problems I found most interesting when it comes to trees.
Inorder Tree Traversal: There are a variety of ways to traverse a tree, but I find the inorder traversal to be most intuitive.
When we do an inorder traversal on the root node on a Binary Search tree, it visits/prints the node in ascending order.
def inorder(node): if node: inorder(node.left) print(node.val) inorder(node.right) This above method is pretty important as it allows us to visit all the nodes.
So if we want to search for a node in any binary tree, we might try to use inorder tree traversal.
Creating a Binary Search Tree from a Sorted array What kind of coders will we be if we need to create a tree piece by piece manually as we did above?
So can we create a BST from a sorted array of unique elements?
def create_bst(array,min_index,max_index): if max_index&amp;lt;min_index: return None mid = int((min_index&#43;max_index)/2) root = node(array[mid]) leftbst = create_bst(array,min_index,mid-1) rightbst = create_bst(array,mid&#43;1,max_index) root.left = leftbst root.right = rightbst return root a = [2,4,5,6,7] root = create_bst(a,0,len(a)-1) Trees are inherently recursive, and so we use recursion here. We take the mid element of the array and assign it as the node. We then apply the create_bst function to the left part of the array and assign it to node.left and do the same with the right part of the array.
And we get our BST.
Have we done it right? We can check it by creating the BST and then doing an inorder traversal.
inorder(root) 2 4 5 6 7  Seems Right!
Let’s check if our tree is a Valid BST But again what sort of coders are we if we need to print all the elements and check manually for the BST property being satisfied?
Here is a simple code to check if our BST is valid or not. We assume strict inequality in our Binary Search Tree.
def isValidBST(node, minval, maxval): if node: # Base case if node.val&amp;lt;=minval or node.val&amp;gt;=maxval: return False # Check the subtrees changing the min and max values return isValidBST(node.left,minval,node.val) &amp;amp; isValidBST(node.right,node.val,maxval) return True isValidBST(root,-float(&amp;#39;inf&amp;#39;),float(&amp;#39;inf&amp;#39;)) True  We check the subtrees recursively if they satisfy the Binary Search tree property or not. At each recursive call, we change the minval or maxval for the call to provide the function with the range of allowed values for the subtree.
Conclusion In this post, I talked about Trees from a software engineering perspective. If you want to see trees from a data science perspective, you might take a look at this post. The Simple Math behind 3 Decision Tree Splitting criterions
Trees form the basis of some of the most asked questions in Data Science algorithmic interviews. I used to despair such tree-based questions in the past, but now I have grown to like the mental exercise involved in them. And I love the recursive structure involved in such problems.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Here is a small notebook for you where I have put all these small concepts for you to try and run.
Take a look at my other posts in the Algorithmic Interviews Series, if you want to learn about Recursion, Dynamic Programming or Linked Lists.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A simple introduction to Linked Lists for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/01/28/ll/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/28/ll/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/ll/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure questions.
This post is about fast-tracking this study and explaining linked list concepts for the data scientists in an easy to understand way.
What are Linked Lists? The linked list is just a very simple data structure that represents a sequence of nodes.
Each node is just an object that contains a value and a pointer to the next node. For example, In the example here we have a node that contains the data 12 and points to the next node 99. Then 99 points to node 37 and so on until we encounter a NULL Node.
There are also doubly linked lists in which each node contains the address of the next as well as the previous node.
But why would we ever need Linked Lists? We all have worked with Lists in Python.But have you ever thought of the insertion time for the list data structure?
Lets say we need to insert an element at the start of a list. Inserting or removing elements at the start in a python list requires an O(n) copy operation.
What if we are faced with the problem in which there are a lot of such inserts and we need a data structure that actually does inserts in constant O(1) time?
There are many practical applications of a linked list that you can think about. One can use a doubly-linked list to implement a system where only the location of previous and next nodes are needed. For example, the previous page and next page functionality in the chrome browser. Or the previous and next photo in a photo editor.
Another benefit of using a linked list is that we don’t need to have contiguous space requirements for a linked list i.e. the nodes can reside anywhere in the memory while for a data structure like an array the nodes need to be allocated a sequence of memory.
How do we create a Linked list in Python? We first start by defining a class that can be used to create a single node.
class Node: def __init__(self,val): self.val = val self.next = None We then use this class object to create multiple nodes and stitch them together to form a linked list.
head = Node(12) a = Node(99) b = Node(37) head.next = a a.next = b And we have our linked list, starting at head. In most cases, we only keep the variable head to define our linked list as it contains all the information we require to access the whole list.
Common Operations or Interview Questions with the Linked Lists 1. Insert a new Node In the start, we said that we can insert an element at the start of the linked list in a constant O(1) time. Let’s see how we can do that.
def insert(head,val): new_head = Node(val) new_head.next = head return new_head So given the head of the node, we just create a new_head object and set its pointer to the previous head of the linked list. We just create a new node and just update a pointer.
2. Print/Traverse the linked list Printing the elements of a linked list is pretty simple. We just go through the linked list in an iterative fashion till we encounter the None node(or the end).
def print(head): while head: print(head.val) head = head.next 3. Reverse a singly linked list This is more of a very common interview question on linked lists. If you are given a linked list, can you reverse that linked list in O(n) time?
For Example: Input: 1-&amp;gt;2-&amp;gt;3-&amp;gt;4-&amp;gt;5-&amp;gt;NULL Output: 5-&amp;gt;4-&amp;gt;3-&amp;gt;2-&amp;gt;1-&amp;gt;NULL  So how do we deal with this?
We start by iterating through the linked list and reversing the pointer direction while moving the head to the next node until there is a next node.
def reverseList(head): newhead = None while head: tmp = head.next head.next = newhead newhead = head head = tmp return newhead Conclusion In this post, I talked about Linked List and its implementation.
Linked lists form the basis of some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Here is a small notebook for you where I have put all these small concepts.
I will leave you with solving this problem by yourself — Implement a function to check if a linked list is a palindrome.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Dynamic Programming for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/01/28/dp/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/01/28/dp/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dp/main.png"></media:content>
      

      
      <description>Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.
This post is about fast-tracking this study and explaining Dynamic Programming concepts for the data scientists in an easy to understand way.
How Dynamic Programming Works? Let’s say that we need to find the nth Fibonacci Number.
Fibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:
def fib(n): if n&amp;lt;=1: return 1 return fib(n-1) &#43; fib(n-2) This problem relates well to a recursive approach. But can you spot the problem here?
If you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.
Source
Now, Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.
In Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.
def fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n&#43;1): dp_sols[i] = dp_sols[i-1] &#43; dp_sols[i-2] return dp_sols[n] Why Dynamic Programming is Hard? Recursion is a mathematical concept and it comes naturally to us. We try to find a solution to a bigger problem by breaking it into smaller ones.
Now Dynamic Programming entails exactly the same idea but in the case of Dynamic programming, we precompute all the subproblems that might need to be calculated in a bottom-up manner.
We human beings are essentially hard-wired to work in a top-down manner. Be it our learning where most people try to go into the breadth of things before going in-depth. Or be it the way we think.
So how does one start thinking in a bottom-up way?
I found out that solving the below problem gives a lot of intuition in how DP works. I myself got highly comfortable with DP once I was able to solve this one and hope it helps you too.
Basically the idea is if you can derive/solve a bigger subproblem if you know the solution to a smaller one?
Maximum Path Sum Given a m x n grid filled with gold, find a path from top left to bottom right which maximizes the sum of gold along its path. We can only move down or right starting from (0,0)
Now there can be decidedly many paths. We can go all the way to the right and then the bottom. Or we can take a zigzag path?
But only one/few paths are going to make you rich.
So how do you even start thinking about such a problem?
When we think of Dynamic Programming questions, we take a bottom-up approach. So we start by thinking about the simplest of problems. In our case, the simplest of problems to solve is the base case. What is the maximum value of Gold we can acquire if we just had to reach cell (0,0)?
And the answer to that is pretty simple — It is the cell value itself.
So we move on to a little harder problem.
What about cell (0,1) and cell (1,0)?
These are also pretty simple. We can reach (0,1)and (1,0) through only (0,0) and hence the maximum gold we can obtain is the value in cell (0,1)/(1,0) plus the maximum gold we can have when we reach cell(0,0)
What about cell(0,2)? Again only one path. So if we know the solution to (0,1) we can just add the value of cell (0,2) to get the solution for (0,2)
Let’s now try to do the same for an arbitrary cell. We want to derive a relation here.
So in the case of an arbitrary cell, we can reach it from the top or from the left.If we know the solutions to the top and left of the cell, we can definitely compute the solution to the arbitrary current target cell.
Coding Once we have the intuition the coding exercise is pretty straightforward. We start by calculating the solutions for the first row and first column. And then we continue to calculate the other values in the grid using the relation we got previously.
def maxPathSum(grid): m = len(grid) n = len(grid[0]) # sol keeps the solutions for each point in the grid. sol = list(grid) # we start by calculating solutions for the first row for i in range(1,n): sol[0][i] &#43;= sol[0][i-1] # we then calculate solutions for the first column for i in range(1,m): sol[i][0] &#43;= sol[i-1][0] # we then calculate all the solutions in the grid for i in range(1,m): for j in range(1,n): sol[i][j] &#43;= max(sol[i-1][j],sol[i][j-1]) # return the last element return sol[-1][-1] Conclusion In this post, I talked about how I think about Dynamic Programming questions.
I start by asking myself the simplest problem I could solve and if I can solve the bigger problem by using the solutions to the simpler problem.
Dynamic Programming forms the basis of some of the most asked questions in Data Science/Machine Learning job interviews, and a good understanding of these might help you land your dream job.
So go out there and do some problems with Leetcode/HackerRank. The problems are surely interesting.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms and Data structures, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Take your Machine Learning Models to Production with these 5 simple steps</title>
      <link>https://mlwhiz.com/blog/2019/12/25/prod/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/25/prod/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/prod/main.png"></media:content>
      

      
      <description>Creating a great machine learning system is an art.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.</description>

      <content:encoded>  
        
        <![CDATA[   Creating a great machine learning system is an art.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.
This post is about the process requirements for a successful ML project — One that goes to production.
1. Establish a Baseline at the onset You don’t really have to have a model to get the baseline results.
Let us say we will be using RMSE as an evaluation metric for our time series models. We evaluated the model on the test set, and the RMSE came out to be 3.64.
Is 3.64 a good RMSE? How do we know? We need a baseline RMSE.
This could come from a currently employed model for the same task. Or by using some very simple heuristic. For a Time series model, a baseline to defeat is last day prediction. i.e., predict the number on the previous day.
Or how about Image classification task. Take 1000 labelled samples and get them classified by humans. And Human accuracy can be your Baseline. If a human is not able to get a 70% prediction accuracy on the task, you can always think of automating a process if your models reach a similar level.
Learning: Try to be aware of the results you are going to get before you create your models. Setting some out of the world expectations is only going to disappoint you and your client.
2. Continuous Integration is the Way Forward You have created your model now. It performs better than the baseline/your current model on your local test dataset. Should we go forward?
We have two choices-
 Go into an endless loop in improving our model further.
 Test our model in production settings, get more insights about what could go wrong and then continue improving our model with continuous integration.
  I am a fan of the second approach. In his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization, Andrew Ng says —
 “Don’t start off trying to design and build the perfect system. Instead, build and train a basic system quickly — perhaps in just a few days. Even if the basic system is far from the “best” system you can build, it is valuable to examine how the basic system functions: you will quickly find clues that show you the most promising directions in which to invest your time.”
 Done is better than perfect.
Learning: If your new model is better than the current model in production or your new model is better than the baseline, it doesn’t make sense to wait to go to production.
3. Your model might break into Production Is your model better than the Baseline? It performed better on the local test dataset, but will it really work well on the whole?
To test the validity of your assumption that your model is better than the existing model, you can set up an A/B test. Some users(Test group)see predictions from your model while some users(Control) see the predictions from the previous model.
In fact, this is the right way to deploy your model. And you might find that indeed your model is not as good as it seems.
 Being wrong is not wrong really, what’s wrong is to not anticipate that we could be wrong.
 It is hard to point out the real reason behind why your model performs poorly in production settings, but some causes could be:
 You might see the data coming in real-time to be significantly different from the training data.
 Or you have not done the preprocessing pipeline correctly.
 Or you do not measure the performance correctly.
 Or maybe there is a bug in your implementation.
  Learning: Don’t go into production with a full scale. A/B test is always an excellent way to go forward. Have something ready to fall back upon(An older model perhaps). There might always be things that might break, which you couldn’t have anticipated.
4. Your model might not even go to Production I have created this impressive ML model, it gives 90% accuracy, but it takes around 10 seconds to fetch a prediction.
Is that acceptable? For some use-cases maybe, but really no.
In the past, there have been many Kaggle competitions whose winners ended up creating monster ensembles to take the top spots on the leaderboard. Below is a particular mindblowing example model which was used to win Otto classification challenge on Kaggle:
Another example is the Netflix Million dollar Recommendation Engine Challenge. The Netflix team ended up never using the wining solution due to the engineering costs involved.
So how do you make your models accurate yet easy on the machine?
Teacher — Student Model: Source
Here comes the concept of Teacher-Student models or Knowledge distillation. In Knowledge distillation, we train a smaller student model on a bigger already trained teacher model.
Here we use the soft labels/probabilities from the teacher model and use it as the training data for the Student model.
 The point is that the teacher is outputting class probabilities — “soft labels” rather than “hard labels”. For example, a fruit classifier might say “Apple 0.9, Pear 0.1” instead of “Apple 1.0, Pear 0.0” . Why bother? Because these “soft labels” are more informative than the original ones — telling the student that yes, a particular apple does very slightly resemble a pear. Student models can often come very close to teacher-level performance, even while using 1–2 orders of magnitude fewer parameters! — Source
 Learning: Sometimes, we don’t have a lot of compute available at prediction time, and so we want to have a lighter model. We can try to build simpler models or try using knowledge distillation for such use cases.
5. Maintainance and Feedback Loop  The world is not constant and so are your model weights
 The world around us is rapidly changing, and what might be applicable two months back might not be relevant now. In a way, the models we build are reflections of the world, and if the world is changing our models should be able to reflect this change.
Model performance deteriorates typically with time.
For this reason, we must think of ways to upgrade our models as part of the maintenance cycle at the onset itself.
The frequency of this cycle depends entirely on the business problem that you are trying to solve. In an Ad prediction system where the users tend to be fickle and buying patterns emerge continuously, the frequency needs to be pretty high. While in a review sentiment analysis system, the frequency need not be that high as language doesn’t change its structure quite so much.
Feedback Loop: Source
I would also like to acknowledge the importance of the feedback loop in a machine learning system. Let’s say that you predicted that a particular image is a dog with low probability in a dog vs cat classifier. Can we learn something from these low confidence examples? You can send it to manual review to check if it could be used to retrain the model or not. In this way, we train our classifier on instances it is unsure about.
Learning: When thinking of production, come up with a plan to maintain and improve the model using feedback as well.
Conclusion These are some of the things I find important before thinking of putting a model into production.
While this is not an exhaustive list of things that you need to think about and things that could go wrong, it might undoubtedly act as food for thought for the next time you create your machine learning system.
If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>3 Programming concepts for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/12/09/pc/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/09/pc/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pc/main.png"></media:content>
      

      
      <description>Algorithms are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.</description>

      <content:encoded>  
        
        <![CDATA[  Algorithms are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
Now the question that many people ask here is what is the use of asking a data scientist such questions. The way I like to describe it is that a data structure question may be thought of as a coding aptitude test.
We all have given aptitude tests at various stages of our life, and while they are not a perfect proxy to judge someone, almost nothing ever really is. So, why not a standard algorithm test to judge people’s coding ability.
But let’s not kid ourselves, they will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms.
This post is about fast-tracking that study and panning some essential algorithms concepts for the data scientists in an easy to understand way.
1. Recursion/Memoization Recursion is where a function being defined is applied within its own definition. Put simply; recursion is when a function calls itself. Google does something pretty interesting when you search for recursion there.
Hope you get the joke. While recursion may seem a little bit daunting to someone just starting, it is pretty simple to understand. And it is a beautiful concept once you know it.
The best example I find for explaining recursion is to calculate the factorial of a number.
def factorial(n): if n==0: return 1 return n*factorial(n-1) We can see how factorial is a recursive function quite easily.
Factorial(n) = n*Factorial(n-1)
So how does it translates to programming?
A function for a recursive call generally consists of two things:
 A base case — The case where the recursion ends.
 A recursive formulation- a formulaic way to move towards the base case.
  A lot of problems you end up solving are recursive. It applies to data science, as well.
For example, A decision tree is just a binary tree, and tree algorithms are generally recursive. Or, we do use sort in a lot of times. The algorithm responsible for that is called mergesort, which in itself is a recursive algorithm. Another one is binary search, which includes finding an element in an array.
Now we have got a basic hang of recursion, let us try to find the nth Fibonacci Number. Fibonacci series is a series of numbers in which each number ( Fibonacci number ) is the sum of the two preceding numbers. The simplest is the series 1, 1, 2, 3, 5, 8, etc. The answer is:
def fib(n): if n&amp;lt;=1: return 1 return fib(n-1) &#43; fib(n-2) But do you spot the problem here?
If you try to calculate fib(n=7) it runs fib(5) twice, fib(4) thrice, fib(3) five times. As n becomes larger, a lot of calls are made for the same number, and our recursive function calculates it again and again.
Can we do better? Yes, we can. We can change our implementation a little bit an add a dictionary to add some storage to our method. Now, this memo dictionary gets updated any time a number has been calculated. If that number appears again, we don’t calculate it again but give results from the memo dictionary. This addition of storage is called Memoization.
memo = {} def fib_memo(n): if n in memo: return memo[n] if n&amp;lt;=1: memo[n]=1 return 1 memo[n] = fib_memo(n-1) &#43; fib_memo(n-2) return memo[n] Usually, I like to write the recursive function first, and if it is making a lot of calls to the same parameters again and again, I add a dictionary to memorize solutions.
How much does it help?
This is the run time comparison for different values of n. We can see the runtime for Fibonacci without Memoization increases exponentially, while for the memoized function, the time is linear.
2. Dynamic programming Recursion is essentially a top-down approach. As in when calculating Fibonacci number n we start from n and then do recursive calls for n-2 and n-1 and so on.
In Dynamic programming, we take a bottom-up approach. It is essentially a way to write recursion iteratively. We start by calculating fib(0) and fib(1) and then use previous results to generate new results.
def fib_dp(n): dp_sols = {0:1,1:1} for i in range(2,n&#43;1): dp_sols[i] = dp_sols[i-1] &#43; dp_sols[i-2] return dp_sols[n] Above is the comparison of runtimes for DP vs. Memoization. We can see that they are both linear, but DP still is a little bit faster.
Why? Because Dynamic Programming made only one call exactly to each subproblem in this case.
There is an excellent story on how Bellman who developed Dynamic Programming framed the term:
 Where did the name, dynamic programming, come from? The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word research. What title, what name, could I choose? In the first place, I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.
 3. Binary Search Let us say we have a sorted array of numbers, and we want to find out a number from this array. We can go the linear route that checks every number one by one and stops if it finds the number. The problem is that it takes too long if the array contains millions of elements. Here we can use a Binary search.
  Source:mathwarehouse.com|Finding 37 — There are 3.7 trillion fish in the ocean, they’re looking for one    # Returns index of target in nums array if present, else -1 def binary_search(nums, left, right, target): # Base case if right &amp;gt;= left: mid = int((left &#43; right)/2) # If target is present at the mid, return if nums[mid] == target: return mid # Target is smaller than mid search the elements in left elif nums[mid] &amp;gt; target: return binary_search(nums, left, mid-1, target) # Target is larger than mid, search the elements in right else: return binary_search(nums, mid&#43;1, right, target) else: # Target is not in nums return -1 nums = [1,2,3,4,5,6,7,8,9] print(binary_search(nums, 0, len(nums)-1,7)) This is an advanced case of a recursion based algorithm where we make use of the fact that the array is sorted. Here we recursively look at the middle element and see if we want to search in the left or right of the middle element. This makes our searching space go down by a factor of 2 every step.
And thus the run time of this algorithm is O(logn) as opposed to O(n) for linear search.
How much does that matter? Below is a comparison in run times. We can see that the Binary search is pretty fast compared to Linear search.
Conclusion In this post, I talked about some of the most exciting algorithms that form the basis for programming.
These algorithms are behind some of the most asked questions in Data Science interviews, and a good understanding of these might help you land your dream job.
And while you can go a fair bit in data science without learning them, you can learn them just for a little bit of fun and maybe to improve your programming skills.
Also take a look at my other posts in the series, if you want to learn about algorithms and Data structures.
Continue Learning If you want to read up more on Algorithms, here is an Algorithm Specialization on Coursera by UCSanDiego, which I highly recommend to learn the basics of algorithms.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      
      

      
      <description>I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;amp;rsquo;t cease to amaze me.</description>

      <content:encoded>  
        
        <![CDATA[  I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?
But it is not magic. And this is a way an aspiring guy could take to become a self-trained data scientist. Follow in order. I have tried to include everything that comes to my mind. So here goes:
1. Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University The one stat course you gotta take. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.
This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.
One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.
It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.
He got a textbook out based on this course which is clearly a great text:
 2. Data Science CS109: - Again by Professor Blitzstein. Again an awesome course. Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts. You will learn about Python Libraries like Numpy,Pandas for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.  3. CS229: Andrew Ng After doing these two above courses you will gain the status of what I would like to call a &amp;ldquo;Beginner&amp;rdquo;. Congrats!!!. You know stuff, you know how to implement stuff. Yet you do not fully understand all the math and grind that goes behind all this.
Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms. I will put this course as the one course you gotta take as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.
Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website here.
You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D&amp;rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.
4. Intro to Hadoop &amp;amp; Mapreduce - Udacity Let us first focus on the literal elephant in the room - Hadoop. Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.
Once you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem. Read this amazing Blog Post from Michael Noll: Writing An Hadoop MapReduce Program In Python - Michael G. Noll. Just read the basic mapreduce codes. Don&amp;rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.
Now try to solve these two problems from the CS109 Harvard course from 2013:
A. First, grab the file word_list.txt from here. This contains a list of six-letter words. To keep things simple, all of the words consist of lower-case letters only.Write a mapreduce job that finds all anagrams in word_list.txt.
B. For the next problem, download the file baseball_friends.csv. Each row of this csv file contains the following:
 A person&amp;rsquo;s name The team that person is rooting for &amp;ndash; either &amp;ldquo;Cardinals&amp;rdquo; or &amp;ldquo;Red Sox&amp;rdquo; A list of that person&amp;rsquo;s friends, which could have arbitrary length  For example: The first line tells us that Aaden is a Red Sox friend and he has 65 friends, who are all listed here. For this problem, it&amp;rsquo;s safe to assume that all of the names are unique and that the friendship structure is symmetric (i.e. if Alannah shows up in Aaden&amp;rsquo;s friends list, then Aaden will show up in Alannah&amp;rsquo;s friends list). Write an mr job that lists each person&amp;rsquo;s name, their favorite team, the number of Red Sox fans they are friends with, and the number of Cardinals fans they are friends with.
Try to do this yourself. Don&amp;rsquo;t use the mrjob (pronounced Mr. Job) way that they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.
If you are done with these, you can safely call yourself as someone who could &amp;ldquo;think in Mapreduce&amp;rdquo; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:
Hadoop Mapreduce Streaming Tricks and Techniques
If you are someone who likes learning from a book you can get: 
5. Spark - In memory Big Data tool. Now comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn&amp;rsquo;t.
Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.
Data Science and Engineering with Apache® Spark™
I have written a little bit about Basic data processing with Spark here. Take a look: Learning Spark using Python: Basics and Applications
Also take a look at some of the projects I did as part of course at github
If you would like a book to read: 
If you don&amp;rsquo;t go through the courses, try solving the same two problems above that you solved by Hadoop using Spark too. Otherwise the problem sets in the courses are more than enough.
6. Understand Linux Shell: Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn&amp;rsquo;t emphasize how much time shell saves for me everyday.
Read these tutorials by me for doing that:
Shell Basics every Data Scientist Should know -Part I Shell Basics every Data Scientist Should know - Part II(AWK)
If you would like a course you can go for this course on edX.
If you want a book, go for:
 Congrats you are an &amp;ldquo;Hacker&amp;rdquo; now. You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &amp;ldquo;All Roads lead to Rome&amp;rdquo; as long as you are running.
7. Learn Statistical Inference and Bayesian Statistics I took the previous version of the specialization which was a single course taught by Mine Çetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data. You can also use these books:
  8. Deep Learning Intro - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.
Advanced - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.
Bonus - A free online book by Michael Nielsen.
Advanced Math Book - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow
9. Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More This course used to be there on Coursera but now only video links on youtube available. You can learn from this book too: 
Apart from that if you want to learn about Python and the basic intricacies of the language you can take the Computer Science Mini Specialization from RICE university too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.
10. Advanced Maths: Couldn&amp;rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.
Linear Algebra By Gilbert Strang - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.
Multivariate Calculus - MIT OCW
Convex Optimization - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.
The Machine learning field is evolving and new advancements are made every day. That&amp;rsquo;s why I didn&amp;rsquo;t put a third tier. The maximum I can call myself is a &amp;ldquo;Hacker&amp;rdquo; and my learning continues. Hope you do the same.
Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.
Till then. Ciao!!!
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Deploying ML Apps using Python and Flask- Learning about Flask</title>
      <link>https://mlwhiz.com/blog/2016/01/10/deploying_ml_apps_using_python_flask/</link>
      <pubDate>Sun, 10 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/01/10/deploying_ml_apps_using_python_flask/</guid>
      
      

      
      <description>It has been a long time since I wrote anything on my blog. So thought about giving everyone a treat this time. Or so I think it is.
Recently I was thinking about a way to deploy all these machine learning models I create in python. I searched through the web but couldn&amp;amp;rsquo;t find anything nice and easy. Then I fell upon this book by Sebastian Rashcka and I knew that it was what I was looking for.</description>

      <content:encoded>  
        
        <![CDATA[  It has been a long time since I wrote anything on my blog. So thought about giving everyone a treat this time. Or so I think it is.
Recently I was thinking about a way to deploy all these machine learning models I create in python. I searched through the web but couldn&amp;rsquo;t find anything nice and easy. Then I fell upon this book by Sebastian Rashcka and I knew that it was what I was looking for. To tell you the truth I did had some experience in Flask earlier but this book made it a whole lot easier to deploy a machine learning model in flask.
So today I am going to give a brief intro about Flask Apps and how to deploy them using a service called Openshift.
So What is flask? Flask is a Python Web Framework that makes it easier to create webapps from python.
And Openshift? Openshift is a free service(if we only use 1 small instance) which lets us use their services to deploy our flask web-apps.
So that we don&amp;rsquo;t get lost, let me tell you the flow of this post.
 First of all we will learn about the installation* of Openshift and Flask. We will create a Hello World application using Flask. We will work on creating a very simple calculator App that operates on two numbers provided by the user. This will help us in understanding how user forms work with Flask by implementing a barebones app.  Installation:  Create your FREE OpenShift account Here Very simple sign-up email &#43; password only Install the OpenShift Client Tools. Use these directions for your particular Operating System these tools have a command line interface and allow more control over your app. The OpenShift tool requires an installation of Ruby.  Now once you do this you have installed Openshift Client tools on your system.
Helloworld So now I am going to do a lot of things in this post. But don&amp;rsquo;t get bothered much it is just code and HTML quirks. I will try to provide enough details on which parts are necessary. First of all, you will need to create a domain on Openshift platform. This can be done by using:
rhc domain create -n DomainName -l EmailAddress -p password For this example I created:
rhc domain create -n mlwhiz -l MyEmailAddress -p Mypassword In the free version for Openshift you can run 3 web-apps with a single domain. For example I can create a maximum of 3 webapps whose web address would be:
 myappname1-mlwhiz.rhcloud.com myappname2-mlwhiz.rhcloud.com myappname3-mlwhiz.rhcloud.com  Once we create a domain we need to create a webapp:
rhc app create HelloWorld python-2.7 This creates the app named helloworld for us. The app currently resides at this address on web: http://helloworld-mlwhiz.rhcloud.com/ This command also creates a folder where our app resides. cd into this folder.
cd helloworld Now get a basic template to work upon in this directory. You can think of this as a starter code for flask. We can do this by pulling and merging from Github using the following commands.
git remote add upstream -m master git://github.com/openshift/flask-example.git git pull -s recursive -X theirs upstream master Use Virtualenv to isolate Python development environments. It’s a tool that allows you setup an isolated, self-contained Python environment in a folder on your dev box. This way you can experiment with various versions of Python without affecting your system wide configurations:
brew install python-virtualenv cd helloworld/wsgi/ virtualenv venv --python=python2.7 #Activate the virtual environment . venv/bin/activate # Install all these into your virtual python environment. pip install flask flask-wtf flask-babel markdown flup Now Change the name of flaskapp.py in wsgi to run.py
put this code in run.py
import os from flask import Flask app = Flask(__name__) @app.route(&amp;#39;/&amp;#39;) def home(): &amp;#34;&amp;#34;&amp;#34;Render website&amp;#39;s home page.&amp;#34;&amp;#34;&amp;#34; return &amp;#39;Hello World!&amp;#39; if __name__ == &amp;#39;__main__&amp;#39;: app.run(debug=&amp;#34;True&amp;#34;) Also change the file named application to:
#!/usr/bin/python import os import sys sys.path.insert(0, os.path.dirname(__file__) or &amp;#39;.&amp;#39;) PY_DIR = os.path.join(os.environ[&amp;#39;OPENSHIFT_HOMEDIR&amp;#39;], &amp;#34;python&amp;#34;) virtenv = PY_DIR &#43; &amp;#39;/virtenv/&amp;#39; PY_CACHE = os.path.join(virtenv, &amp;#39;lib&amp;#39;, os.environ[&amp;#39;OPENSHIFT_PYTHON_VERSION&amp;#39;], &amp;#39;site-packages&amp;#39;) os.environ[&amp;#39;PYTHON_EGG_CACHE&amp;#39;] = os.path.join(PY_CACHE) virtualenv = os.path.join(virtenv, &amp;#39;bin/activate_this.py&amp;#39;) try: exec(open(virtualenv).read(), dict(__file__=virtualenv)) except IOError: pass from run import app as application Run this to host your app:
cd helloworld/wsgi python run.py  You should be able to see your app on: http://127.0.0.1:5000/ You can deploy this webapp to Openshift using:
cd helloworld git add . git commit -a -m &amp;#34;Initial deployment of this app to the web&amp;#34; git push Open http://helloworld-mlwhiz.rhcloud.com/ in your browser. You would see Hello World! there. Now we have got a very basic structure complete.
Our Simple Calculator App: We will now work on creating a app that operates on two numbers provided by the user. The functions possible are &#43;,- and *. You can see this web app in action here before moving on. This app will help us in understanding how user forms work with Flask and how to manage user inputs in Flask. First of all change the code in run.py to
import os from flask import Flask,render_template, request from wtforms import Form, TextAreaField, validators,SelectField app = Flask(__name__) # Code to create a WTForm with three fields. 2 text fields and 1 dropdown menu. class OutputForm(Form): myChoices=[(&amp;#39;&#43;&amp;#39;, &amp;#39;&#43;&amp;#39;), (&amp;#39;-&amp;#39;, &amp;#39;-&amp;#39;), (&amp;#39;*&amp;#39;, &amp;#39;*&amp;#39;)] num1 = TextAreaField(&amp;#39;&amp;#39;,[validators.DataRequired()]) num2 = TextAreaField(&amp;#39;&amp;#39;,[validators.DataRequired()]) Operator = SelectField(u&amp;#39;&amp;#39;, choices = myChoices, validators = [validators.DataRequired()]) # This uses the render_template method in flask to use a template first_app.html. # This html contains placeholders for the form that is provided in the kwargs argument to the function call. @app.route(&amp;#39;/&amp;#39;) def index(): #return &amp;#39;Hello World!&amp;#39; form = OutputForm(request.form) return render_template(&amp;#39;first_app.html&amp;#39;,form = form) # This is the output that is displayed. It checks if the form is validated and POST request is made. # If true it renders the output.html else renders the main index page. # Most of the work is done here. Gets the user inputs using the request.form method. @app.route(&amp;#39;/output&amp;#39;, methods=[&amp;#39;POST&amp;#39;]) def output(): form = OutputForm(request.form) if request.method == &amp;#39;POST&amp;#39; and form.validate(): num1 = request.form[&amp;#39;num1&amp;#39;] num2 = request.form[&amp;#39;num2&amp;#39;] op = request.form[&amp;#39;Operator&amp;#39;] if op==&amp;#34;&#43;&amp;#34;: name=str(int(num1)&#43;int(num2)) elif op==&amp;#34;-&amp;#34;: name=str(int(num1)-int(num2)) elif op==&amp;#34;*&amp;#34;: name=str(int(num1)*int(num2)) return render_template(&amp;#39;output.html&amp;#39;, name=name) return render_template(&amp;#39;first_app.html&amp;#39;, form=form) if __name__ == &amp;#39;__main__&amp;#39;: app.run(debug=&amp;#34;True&amp;#34;) We use WTF forms here to create a form object. We pass this form object to the HTML render_template method. We have accessed these again in the output function so that we can show them in output.html where all the major work is done for creating the app.
Now Create a folder named template in helloworld/wsgi and create a file named _formhelpers.html with this content. You really don&amp;rsquo;t need to see the content in this file.
{% macro render_field(field) %} &amp;lt;dt&amp;gt;{{ field.label }} &amp;lt;dd&amp;gt;{{ field(**kwargs)|safe }} {% if field.errors %} &amp;lt;ul class=errors&amp;gt; {% for error in field.errors %} &amp;lt;li&amp;gt;{{ error }}&amp;lt;/li&amp;gt; {% endfor %} &amp;lt;/ul&amp;gt; {% endif %} &amp;lt;/dd&amp;gt; {% endmacro %} Also add another file named first_app.html with this content. Notice how we access the wtform here.
&amp;lt;!doctype html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;First app&amp;lt;/title&amp;gt; &amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;{{ url_for(&amp;#39;static&amp;#39;,filename=&amp;#39;style.css&amp;#39;) }}&amp;#34;&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; {% from &amp;#34;_formhelpers.html&amp;#34; import render_field %} &amp;lt;div&amp;gt;Calculator: Please enter two numbers and a function you want to apply&amp;lt;/div&amp;gt; &amp;lt;form method=post action=&amp;#34;/output&amp;#34;&amp;gt; {{ render_field(form.num1) }}{{ render_field(form.Operator) }}{{ render_field(form.num2) }} &amp;lt;input type=submit value=&amp;#39;Result&amp;#39; name=&amp;#39;submit_btn&amp;#39;&amp;gt; &amp;lt;/form&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; Create a file named output.html where the final output will be shown.
&amp;lt;!doctype html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;First app&amp;lt;/title&amp;gt; &amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;{{ url_for(&amp;#39;static&amp;#39;,filename=&amp;#39;style.css&amp;#39;) }}&amp;#34;&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;div&amp;gt;The output is: {{ name }}&amp;lt;/div&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; Also add a style.css file in the static folder. You can put this in it for right now or any other thing you want.
h1 { color: blue; font-family: verdana; font-size: 300%; } p { color: red; font-family: courier; font-size: 160%; } And we are mostly done. Run run.py in the wsgi directory and you would be able to access the app at : http://127.0.0.1:5000/. Again deploy this webapp to Openshift using:
cd helloworld git add . git commit -a -m &amp;#34;Initial deployment of this app to the web&amp;#34; git push Endnotes So here we took inputs from the user and show the output using the flask App. The final app is hosted at http://helloworld-mlwhiz.rhcloud.com/ for you to see. This code provides us with a code skeletn which will be valuable when we will deploy a whole ML model, which is the main motive of this series.
References  Most of the code here is taken from this awesome book by Sebastian Raschka: Python Machine Learning https://blog.openshift.com/beginners-guide-to-writing-flask-apps-on-openshift/  ]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>