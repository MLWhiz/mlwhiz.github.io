<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on MLWhiz</title>
    <link>https://mlwhiz.com/tags/data-science/</link>
    <description>Recent content in Data Science on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Apr 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/data-science/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Maths Beats Intuition probably every damn time</title>
      <link>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</guid>
      <description>

&lt;p&gt;Newton once said that &lt;strong&gt;&amp;ldquo;God does not play dice with the universe&amp;rdquo;&lt;/strong&gt;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.&lt;/p&gt;

&lt;p&gt;One of such ways intuition plays with us is with the Birthday problem.&lt;/p&gt;

&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement:&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pigeonhole_principle&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;pigeonhole principle&lt;/a&gt;, the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;mathematical-proof&#34;&gt;Mathematical Proof:&lt;/h2&gt;

&lt;p&gt;Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event.
We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement)
Since there are k ways in which birthdays can be chosen with replacement.&lt;/p&gt;

&lt;p&gt;$P(birthday Match) = 1 - \dfrac{(365).364&amp;hellip;(365âˆ’k+1)}{365^k}$&lt;/p&gt;

&lt;h2 id=&#34;simulation&#34;&gt;Simulation:&lt;/h2&gt;

&lt;p&gt;Lets try to build around this result some more by trying to simulate this result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt  &lt;span style=&#34;color:#75715e&#34;&gt;#sets up plotting under plt&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns           &lt;span style=&#34;color:#75715e&#34;&gt;#sets up styles and gives us more plotting options&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd             &lt;span style=&#34;color:#75715e&#34;&gt;#lets us handle data as dataframes&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sim_bithday_problem&lt;/span&gt;(num_people_room, trials &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39;This function takes as input the number of people in the room.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Runs 1000 trials by default and returns
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    (number of times same brthday found)/(no of trials)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    same_birthdays_found &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(trials):
        &lt;span style=&#34;color:#75715e&#34;&gt;# randomly sample from the birthday space which could be any of a number from 1 to 365&lt;/span&gt;
        birthdays &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_people_room)]
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(birthdays) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; len(set(birthdays))&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
            same_birthdays_found&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; same_birthdays_found&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;float(trials)

num_people &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
probs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [sim_bithday_problem(i) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; num_people]
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame()
data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_peeps&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; num_people
data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;probs&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; probs
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;)

g &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;regplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;num_peeps&amp;#34;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;probs&amp;#34;&lt;/span&gt;, data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data, ci &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False,
    scatter_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;color&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;darkred&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;alpha&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;},
    marker&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;,fit_reg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)

sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;despine()
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_size_inches(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;As the Number of people in room reaches 23 the probability reaches ~0.5&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;At more than 50 people the probability is reaching 1&amp;#39;&lt;/span&gt;, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;g&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;# of people in room&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tick_params(labelsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;,labelcolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/bithdayproblem.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can see from the &lt;a href=&#34;https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/&#34;&gt;graph&lt;/a&gt; that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.&lt;/p&gt;

&lt;h2 id=&#34;intuition&#34;&gt;Intuition:&lt;/h2&gt;

&lt;p&gt;To understand it we need to think of this problem in terms of pairs. There are ${{23}\choose{2}} = 253$ pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at ${{70}\choose{2}} = 2450$ pairs.&lt;/p&gt;

&lt;p&gt;So thats it for now. To learn more about this go to &lt;a href=&#34;https://en.wikipedia.org/wiki/Birthday_problem&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt; which has an awesome page on this topic.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://amzn.to/2nIUkxq&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Introduction to Probability by Joseph K. Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Birthday_problem&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Birthday Problem on Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Today I Learned This Part I: What are word2vec Embeddings?</title>
      <link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid>
      <description>

&lt;p&gt;Recently Quora put out a &lt;a href=&#34;https://www.kaggle.com/c/quora-question-pairs&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Question similarity&lt;/a&gt; competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p&gt;

&lt;p&gt;Till now whenever I heard the term word2vec I visualized it as a way to create a bag of words vector for a sentence.&lt;/p&gt;

&lt;p&gt;For those who don&amp;rsquo;t know &lt;em&gt;bag of words&lt;/em&gt;:
If we have a series of sentences(documents)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;This is good       - [1,1,1,0,0]&lt;/li&gt;
&lt;li&gt;This is bad        - [1,1,0,1,0]&lt;/li&gt;
&lt;li&gt;This is awesome    - [1,1,0,0,1]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Bag of words would encode it using &lt;em&gt;0:This 1:is 2:good 3:bad 4:awesome&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;But it is much more powerful than that.&lt;/p&gt;

&lt;p&gt;What word2vec does is that it creates vectors for words.
What I mean by that is that we have a 300 dimensional vector for every word(common bigrams too) in a dictionary.&lt;/p&gt;

&lt;h2 id=&#34;how-does-that-help&#34;&gt;How does that help?&lt;/h2&gt;

&lt;p&gt;We can use this for multiple scenarios but the most common are:&lt;/p&gt;

&lt;p&gt;A. &lt;em&gt;Using word2vec embeddings we can find out similarity between words&lt;/em&gt;.
Assume you have to answer if these two statements signify the same thing:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;President greets press in Chicago&lt;/li&gt;
&lt;li&gt;Obama speaks to media in Illinois.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we do a sentence similarity metric or a bag of words approach to compare these two sentences we will get a pretty low score.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/word2vecembed.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;But with a word encoding we can say that&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;President is similar to Obama&lt;/li&gt;
&lt;li&gt;greets is similar to speaks&lt;/li&gt;
&lt;li&gt;press is similar to media&lt;/li&gt;
&lt;li&gt;Chicago is similar to Illinois&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;B. &lt;em&gt;Encode Sentences&lt;/em&gt;: I read a &lt;a href=&#34;https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt; from Abhishek Thakur a prominent kaggler.(Must Read). What he did was he used these word embeddings to create a 300 dimensional vector for every sentence.&lt;/p&gt;

&lt;p&gt;His Approach: Lets say the sentence is &amp;ldquo;What is this&amp;rdquo;
And lets say the embedding for every word is given in 4 dimension(normally 300 dimensional encoding is given)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;what : [.25 ,.25 ,.25 ,.25]&lt;/li&gt;
&lt;li&gt;is   : [  1 ,  0 ,  0 ,  0]&lt;/li&gt;
&lt;li&gt;this : [ .5 ,  0 ,  0 , .5]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then the vector for the sentence is normalized elementwise addition of the vectors. i.e.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Elementwise addition : [.25+1+0.5, 0.25+0+0 , 0.25+0+0, .25+0+.5] = [1.75, .25, .25, .75]
divided by
math.sqrt(1.25^2 + .25^2 + .25^2 + .75^2) = 1.5
gives:[1.16, .17, .17, 0.5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus I can convert any sentence to a vector  of a fixed dimension(decided by the embedding). To find similarity between two sentences I can use a variety of distance/similarity metrics.&lt;/p&gt;

&lt;p&gt;C. Also It enables us to do algebraic manipulations on words which was not possible before. For example: What is king - man + woman ?&lt;/p&gt;

&lt;p&gt;Guess what it comes out to be : &lt;em&gt;Queen&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;application-coding&#34;&gt;Application/Coding:&lt;/h2&gt;

&lt;p&gt;Now lets get down to the coding part as we know a little bit of fundamentals.&lt;/p&gt;

&lt;p&gt;First of all we download a custom word embedding from Google. There are many other embeddings too.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above file is pretty big. Might take some time. Then moving on to coding.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; gensim.models &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; word2vec
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gensim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;KeyedVectors&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_word2vec_format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/GoogleNews-vectors-negative300.bin.gz&amp;#39;&lt;/span&gt;, binary&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;1-starting-simple-lets-find-out-similar-words-want-to-find-similar-words-to-python&#34;&gt;1. Starting simple, lets find out similar words. Want to find similar words to python?&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;most_similar(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px&#34;&gt;
[(u&#39;pythons&#39;, 0.6688377261161804),&lt;br&gt;
 (u&#39;Burmese_python&#39;, 0.6680364608764648),&lt;br&gt;
 (u&#39;snake&#39;, 0.6606293320655823),&lt;br&gt;
 (u&#39;crocodile&#39;, 0.6591362953186035),&lt;br&gt;
 (u&#39;boa_constrictor&#39;, 0.6443519592285156),&lt;br&gt;
 (u&#39;alligator&#39;, 0.6421656608581543),&lt;br&gt;
 (u&#39;reptile&#39;, 0.6387745141983032),&lt;br&gt;
 (u&#39;albino_python&#39;, 0.6158879995346069),&lt;br&gt;
 (u&#39;croc&#39;, 0.6083582639694214),&lt;br&gt;
 (u&#39;lizard&#39;, 0.601341724395752)]&lt;br&gt;
 &lt;/div&gt;

&lt;h3 id=&#34;2-now-we-can-use-this-model-to-find-the-solution-to-the-equation&#34;&gt;2. Now we can use this model to find the solution to the equation:&lt;/h3&gt;

&lt;p&gt;What is king - man + woman?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;most_similar(positive &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;king&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;woman&amp;#39;&lt;/span&gt;],negative &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;man&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px&#34;&gt;
[(u&#39;queen&#39;, 0.7118192315101624),&lt;br&gt;
 (u&#39;monarch&#39;, 0.6189674139022827),&lt;br&gt;
 (u&#39;princess&#39;, 0.5902431011199951),&lt;br&gt;
 (u&#39;crown_prince&#39;, 0.5499460697174072),&lt;br&gt;
 (u&#39;prince&#39;, 0.5377321839332581),&lt;br&gt;
 (u&#39;kings&#39;, 0.5236844420433044),&lt;br&gt;
 (u&#39;Queen_Consort&#39;, 0.5235946178436279),&lt;br&gt;
 (u&#39;queens&#39;, 0.5181134343147278),&lt;br&gt;
 (u&#39;sultan&#39;, 0.5098593235015869),&lt;br&gt;
 (u&#39;monarchy&#39;, 0.5087412595748901)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;You can do plenty of freaky/cool things using this:&lt;/p&gt;

&lt;h3 id=&#34;3-lets-say-you-wanted-a-girl-and-had-a-girl-name-like-emma-in-mind-but-you-got-a-boy-so-what-is-the-male-version-for-emma&#34;&gt;3. Lets say you wanted a girl and had a girl name like emma in mind but you got a boy. So what is the male version for emma?&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;most_similar(positive &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;emma&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;he&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mr&amp;#39;&lt;/span&gt;],negative &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;she&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mrs&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;female&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px&#34;&gt;
[(u&#39;sanchez&#39;, 0.4920658469200134),&lt;br&gt;
 (u&#39;kenny&#39;, 0.48300960659980774),&lt;br&gt;
 (u&#39;alves&#39;, 0.4684845209121704),&lt;br&gt;
 (u&#39;gareth&#39;, 0.4530612826347351),&lt;br&gt;
 (u&#39;bellamy&#39;, 0.44884198904037476),&lt;br&gt;
 (u&#39;gibbs&#39;, 0.445194810628891),&lt;br&gt;
 (u&#39;dos_santos&#39;, 0.44508373737335205),&lt;br&gt;
 (u&#39;gasol&#39;, 0.44387346506118774),&lt;br&gt;
 (u&#39;silva&#39;, 0.4424275755882263),&lt;br&gt;
 (u&#39;shaun&#39;, 0.44144102931022644)]&lt;br&gt;&lt;br&gt;
&lt;/div&gt;

&lt;h3 id=&#34;4-find-which-word-doesn-t-belong-to-a-list-https-github-com-dhammack-word2vecexample-blob-master-main-py&#34;&gt;4. Find which word doesn&amp;rsquo;t belong to a &lt;a href=&#34;https://github.com/dhammack/Word2VecExample/blob/master/main.py&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;list&lt;/a&gt;?&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;doesnt_match(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;math shopping reading science&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I think staple doesn&amp;rsquo;t belong in this list!&lt;/p&gt;

&lt;h2 id=&#34;other-cool-things&#34;&gt;Other Cool Things&lt;/h2&gt;

&lt;h3 id=&#34;1-recommendations&#34;&gt;1. Recommendations:&lt;/h3&gt;

&lt;div style=&#34;margin-top: 4px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/recommendationpaper.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href=&#34;https://arxiv.org/abs/1603.04259&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt;, the authors have shown that itembased CF can be cast in the same framework of word embedding.&lt;/p&gt;

&lt;h3 id=&#34;2-some-other-examples-http-byterot-blogspot-in-2015-06-five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-nlp-gensim-html-that-people-have-seen-after-using-their-own-embeddings&#34;&gt;2. Some other &lt;a href=&#34;http://byterot.blogspot.in/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;examples&lt;/a&gt; that people have seen after using their own embeddings:&lt;/h3&gt;

&lt;p&gt;Library - Books = Hall&lt;br&gt;
Obama + Russia - USA = Putin&lt;br&gt;
Iraq - Violence = Jordan&lt;br&gt;
President - Power = Prime Minister (Not in India Though)&lt;br&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-seeing-the-above-i-started-playing-with-it-a-little&#34;&gt;3.Seeing the above I started playing with it a little.&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Is this model sexist?&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;most_similar(positive &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;donald_trump&amp;#34;&lt;/span&gt;],negative &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;brain&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;font-size:80%;color:black;font-family: helvetica;line-height:18px;margin-top:8px;margin-left:20px&#34;&gt;
[(u&#39;novak&#39;, 0.40405112504959106),&lt;br&gt;
 (u&#39;ozzie&#39;, 0.39440611004829407),&lt;br&gt;
 (u&#39;democrate&#39;, 0.39187556505203247),&lt;br&gt;
 (u&#39;clinton&#39;, 0.390536367893219),&lt;br&gt;
 (u&#39;hillary_clinton&#39;, 0.3862358033657074),&lt;br&gt;
 (u&#39;bnp&#39;, 0.38295692205429077),&lt;br&gt;
 (u&#39;klaar&#39;, 0.38228923082351685),&lt;br&gt;
 (u&#39;geithner&#39;, 0.380607008934021),&lt;br&gt;
 (u&#39;bafana_bafana&#39;, 0.3801495432853699),&lt;br&gt;
 (u&#39;whitman&#39;, 0.3790769875049591)]&lt;br&gt;
&lt;/div&gt;

&lt;p&gt;Whatever it is doing it surely feels like magic. Next time I will try to write more on how it works once I understand it fully.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      <description>

&lt;p&gt;I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.&lt;/p&gt;

&lt;p&gt;To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?&lt;/p&gt;

&lt;p&gt;But it is not magic. And this is a way an aspiring guy could take to become a &lt;b&gt;&lt;u&gt;self-trained data scientist&lt;/u&gt;&lt;/b&gt;. Follow in order. I have tried to include everything that comes to my mind. So here goes:&lt;/p&gt;

&lt;h2 id=&#34;1-stat-110-introduction-to-probability-joe-blitzstein-harvard-university-http-projects-iq-harvard-edu-stat110-youtube&#34;&gt;1. &lt;a href=&#34;http://projects.iq.harvard.edu/stat110/youtube&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The one stat course you gotta take&lt;/em&gt;. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.&lt;/p&gt;

&lt;p&gt;This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.&lt;/p&gt;

&lt;p&gt;One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gamblerâ€™s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.&lt;/p&gt;

&lt;p&gt;It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.&lt;/p&gt;

&lt;p&gt;He got a textbook out based on this course which is clearly a great text:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li3&amp;tag=mlwhizcon-20&amp;linkId=7254baef925507e0d8dfd07cca2f519d&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=B00MMOJ19I&amp;Format=_SL250_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li3&amp;o=1&amp;a=B00MMOJ19I&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;h2 id=&#34;2-data-science-cs109-http-cs109-github-io-2015&#34;&gt;2. &lt;a href=&#34;http://cs109.github.io/2015/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Data Science CS109&lt;/a&gt;: -&lt;/h2&gt;

&lt;p&gt;Again by Professor Blitzstein. Again an awesome course. &lt;em&gt;Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts&lt;/em&gt;. You will learn about &lt;em&gt;Python Libraries&lt;/em&gt; like &lt;strong&gt;Numpy,Pandas&lt;/strong&gt; for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:&lt;/p&gt;

&lt;div style=&#34;color:black; background-color: #E9DAEE;&#34;&gt;
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.
&lt;/div&gt;

&lt;h2 id=&#34;3-cs229-andrew-ng-https-click-linksynergy-com-fs-bin-click-id-lvarvwc5bd0-offerid-495576-248-type-3-subid-0&#34;&gt;3. &lt;a href=&#34;https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=495576.248&amp;amp;type=3&amp;amp;subid=0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;CS229: Andrew Ng&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;After doing these two above courses you will gain the status of what I would like to call a &lt;strong&gt;&amp;ldquo;Beginner&amp;rdquo;&lt;/strong&gt;. Congrats!!!. &lt;em&gt;You know stuff, you know how to implement stuff&lt;/em&gt;. Yet you do not fully understand all the math and grind that goes behind all this.&lt;/p&gt;

&lt;p&gt;Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms.  I will put this course as the &lt;em&gt;one course you gotta take&lt;/em&gt; as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.&lt;/p&gt;

&lt;p&gt;Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website &lt;a href=&#34;http://www.mlyearning.org/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D&amp;rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.&lt;/p&gt;

&lt;h2 id=&#34;4-intro-to-hadoop-mapreduce-udacity-https-www-udacity-com-course-intro-to-hadoop-and-mapreduce-ud617&#34;&gt;4. &lt;a href=&#34;https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Intro to Hadoop &amp;amp; Mapreduce - Udacity&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Let us first focus on the literal elephant in the room - Hadoop.&lt;/em&gt; Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.&lt;/p&gt;

&lt;p&gt;Once  you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem.
Read this amazing Blog Post from Michael Noll: &lt;a href=&#34;http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Writing An Hadoop MapReduce Program In Python - Michael G. Noll&lt;/a&gt;.  Just read the basic mapreduce codes. Don&amp;rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now try to solve these two problems from the CS109 Harvard course from 2013:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; First, grab the file word_list.txt from &lt;a href=&#34;https://raw.github.com/cs109/content/master/labs/lab8/word_list.txt&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.  This contains a list of six-letter words. To keep things simple, all of  the words consist of lower-case letters only.Write a mapreduce job that  finds all anagrams in word_list.txt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;B.&lt;/strong&gt; For the next problem, download the file &lt;a href=&#34;https://raw.github.com/cs109/content/master/labs/lab8/baseball_friends.csv&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;baseball_friends.csv&lt;/a&gt;. Each row of this csv file contains the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A person&amp;rsquo;s name&lt;/li&gt;
&lt;li&gt;The team that person is rooting for &amp;ndash; either &amp;ldquo;Cardinals&amp;rdquo; or &amp;ldquo;Red Sox&amp;rdquo;&lt;/li&gt;
&lt;li&gt;A list of that person&amp;rsquo;s friends, which could have arbitrary length&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;For  example:&lt;/em&gt; The first line tells us that Aaden is a Red Sox friend and he  has 65  friends, who are all listed here. For this problem, it&amp;rsquo;s safe to  assume  that all of the names are unique and that the friendship  structure is  symmetric (i.e. if Alannah shows up in Aaden&amp;rsquo;s friends list, then Aaden will show up in Alannah&amp;rsquo;s friends list).
Write  an mr job that lists each person&amp;rsquo;s name, their favorite  team, the  number of Red Sox fans they are friends with, and the number  of  Cardinals fans they are friends with.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to do this yourself.&lt;/strong&gt; Don&amp;rsquo;t use the mrjob (pronounced Mr. Job) way that  they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.&lt;/p&gt;

&lt;p&gt;If you are done with these, you can safely call yourself as someone who could &lt;strong&gt;&amp;ldquo;think in Mapreduce&amp;rdquo;&lt;/strong&gt; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:&lt;br&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_techniques/&#34;&gt;Hadoop Mapreduce Streaming Tricks and Techniques&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are someone who likes learning from a book you can get:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Hadoop-Definitive-Storage-Analysis-Internet/dp/1491901632/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543345&amp;sr=1-1&amp;keywords=hadoop+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=e0a6c64497866b874326afa08a069654&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491901632&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491901632&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-spark-in-memory-big-data-tool&#34;&gt;5. Spark - In memory Big Data tool.&lt;/h2&gt;

&lt;p&gt;Now  comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.edx.org/xseries/data-science-engineering-apacher-sparktm&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Data Science and Engineering with ApacheÂ® Sparkâ„¢&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have written a little bit about Basic data processing with Spark here. Take a look:
&lt;a href=&#34;https://mlwhiz.com/blog/2015/09/07/spark_basics_explained/&#34;&gt;Learning Spark using Python: Basics and Applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also take a look at some of the projects I did as part of course at &lt;a href=&#34;http://www.github.com/MLWhiz/Spark_Projects/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you would like a book to read:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491912766/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543902&amp;sr=1-1&amp;keywords=spark+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=85591cf408de278e23e8570b7e9c284b&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491912766&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491912766&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t go through the courses, &lt;strong&gt;try solving the same two problems above that you solved by Hadoop using Spark too&lt;/strong&gt;. Otherwise the problem sets in the courses are more than enough.&lt;/p&gt;

&lt;h2 id=&#34;6-understand-linux-shell&#34;&gt;6. Understand Linux Shell:&lt;/h2&gt;

&lt;p&gt;Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn&amp;rsquo;t emphasize how much time shell saves for me everyday.&lt;/p&gt;

&lt;p&gt;Read these tutorials by me for doing that:&lt;br&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/&#34;&gt;Shell Basics every Data Scientist Should know -Part I&lt;/a&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/&#34;&gt;Shell Basics every Data Scientist Should know - Part II(AWK)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you would like a course you can go for &lt;a href=&#34;https://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1#!&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;this course on edX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want a book, go for:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Linux-Command-Line-Complete-Introduction/dp/1593273894/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490544715&amp;sr=1-1&amp;keywords=the+linux+command+line&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=9f155a16f16c7ae34e682e0e0312ee8f&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1593273894&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1593273894&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Congrats you are an &amp;ldquo;Hacker&amp;rdquo; now.&lt;/strong&gt; You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &lt;strong&gt;&amp;ldquo;All Roads lead to Rome&amp;rdquo;&lt;/strong&gt; as long as you are running.&lt;/p&gt;

&lt;h2 id=&#34;7-learn-statistical-inference-and-bayesian-statistics-https-click-linksynergy-com-fs-bin-click-id-lvarvwc5bd0-offerid-467035-204-type-3-subid-0&#34;&gt;7. &lt;a href=&#34;https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=467035.204&amp;amp;type=3&amp;amp;subid=0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Learn Statistical Inference and Bayesian Statistics&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I took the previous version of the specialization which was a single course taught by Mine Ã‡etinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
You can also use these books:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/dp/1943450056/ref=as_li_ss_il?m=A3EEBE82C3HYRD&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=cfd246ebddfde379bc01dcb2c467c199&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1943450056&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1943450056&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a href=&#34;https://www.amazon.com/Probability-Statistics-4th-Morris-DeGroot/dp/0321500466/ref=as_li_ss_il?ie=UTF8&amp;qid=1490547535&amp;sr=8-1&amp;keywords=degroot+statistics&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=fc106a3b8c56be8baf34793816762ec8&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=0321500466&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=0321500466&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;h2 id=&#34;8-deep-learning&#34;&gt;8. Deep Learning&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.fast.ai/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Intro&lt;/a&gt; - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://cs231n.github.io/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced&lt;/a&gt; - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Bonus&lt;/a&gt; - A free online book by Michael Nielsen.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://amzn.to/2npItnM&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced Math Book&lt;/a&gt; - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow&lt;/p&gt;

&lt;h2 id=&#34;9-algorithms-graph-algorithms-recommendation-systems-pagerank-and-more-https-www-youtube-com-watch-v-xoa5v9ao7s0-list-pllsst5z-dsk9jdlct8t62vtzwyw9lnepv&#34;&gt;9. &lt;a href=&#34;https://www.youtube.com/watch?v=xoA5v9AO7S0&amp;amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This course used to be there on Coursera but now only video links on youtube available.
You can learn from this book too:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Mining-Massive-Datasets-Jure-Leskovec/dp/1107077230/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=ba893a022640a279d427fd0c5ea44c1a&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1107077230&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1107077230&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Apart from that if you want to learn about Python and the basic intricacies of the language you can take the &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=495576.1921197134&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fcomputer-fundamentals&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Computer Science Mini Specialization from RICE university&lt;/a&gt;&lt;/strong&gt; too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.&lt;/p&gt;

&lt;h2 id=&#34;10-advanced-maths&#34;&gt;10. Advanced Maths:&lt;/h2&gt;

&lt;p&gt;Couldn&amp;rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Linear Algebra By Gilbert Strang&lt;/a&gt; - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Multivariate Calculus - MIT OCW&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Convex Optimization&lt;/a&gt; - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.&lt;/p&gt;

&lt;p&gt;The Machine learning field is evolving and new advancements are made every day. That&amp;rsquo;s why I didn&amp;rsquo;t put a third tier. The maximum I can call myself is a &amp;ldquo;Hacker&amp;rdquo; and my learning continues. Hope you do the same.&lt;/p&gt;

&lt;p&gt;Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.&lt;/p&gt;

&lt;p&gt;Till then. Ciao!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top advice for a Data Scientist</title>
      <link>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/thinklikeds.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:&lt;/p&gt;

&lt;h2 id=&#34;1-beware-of-the-clean-data-syndrome&#34;&gt;1. Beware of the Clean Data Syndrome&lt;/h2&gt;

&lt;p&gt;You need to ask yourself questions even before you start working on the data. &lt;strong&gt;Does this data make sense?&lt;/strong&gt; Falsely assuming that the data is clean could lead you towards wrong Hypotheses. Apart from that, you can discern a lot of important patterns by looking at discrepancies in the data. For example, if you notice that a particular column has more than 50% values missing, you might think about not using the column. Or you may think that some of the data collection instrument has some error.&lt;/p&gt;

&lt;p&gt;Or let&amp;rsquo;s say you have a distribution of Male vs Female as 90:10 in a Female Cosmetic business. You may assume clean data and show the results as it is or you can use common sense and ask if the labels are switched.&lt;/p&gt;

&lt;h2 id=&#34;2-manage-outliers-wisely&#34;&gt;2. Manage Outliers wisely&lt;/h2&gt;

&lt;p&gt;Outliers can help you understand more about the people who are using your website/product 24 hours a day. But including them while building models will skew the models a lot.&lt;/p&gt;

&lt;h2 id=&#34;3-keep-an-eye-out-for-the-abnormal&#34;&gt;3. Keep an eye out for the Abnormal&lt;/h2&gt;

&lt;p&gt;Be on the &lt;strong&gt;lookout for something out of the obvious&lt;/strong&gt;. If you find something you may have hit gold.&lt;/p&gt;

&lt;p&gt;For example, &lt;a href=&#34;https://www.fastcompany.com/1783127/flickr-founders-glitch-can-game-wants-you-play-nice-be-blockbuster&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Flickr started up as a Multiplayer game&lt;/a&gt;. Only when the founders noticed that people were using it as a photo upload service, did they pivot.&lt;/p&gt;

&lt;p&gt;Another example: fab.com started up as fabulis.com, a site to help gay men meet people. One of the site&amp;rsquo;s popular features was the &amp;ldquo;Gay deal of the Day&amp;rdquo;. One day the deal was for Hamburgers - and half of the buyers were women. This caused the team to realize that there was a market for selling goods to women. So Fabulis pivoted to fab as a flash sale site for designer products.&lt;/p&gt;

&lt;h2 id=&#34;4-start-focussing-on-the-right-metrics&#34;&gt;4. Start Focussing on the right metrics&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beware of Vanity metrics&lt;/strong&gt; For example, # of active users by itself doesn&amp;rsquo;t divulge a lot of information. I would rather say &amp;ldquo;5% MoM increase in active users&amp;rdquo; rather than saying &amp;ldquo; 10000 active users&amp;rdquo;. Even that is a vanity metric as active users would always increase. I would rather keep a track of percentage of users that are active to know how my product is performing.&lt;/li&gt;
&lt;li&gt;Try to find out a &lt;strong&gt;metric that ties with the business goal&lt;/strong&gt;. For example, Average Sales/User for a particular month.&lt;/li&gt;
&lt;/ul&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;5-statistics-may-lie-too&#34;&gt;5. Statistics may lie too&lt;/h2&gt;

&lt;p&gt;Be critical of everything that gets quoted to you. Statistics has been used to lie in advertisements, in workplaces and a lot of other marketing venues in the past. People will do anything to get sales or promotions.&lt;/p&gt;

&lt;p&gt;For example: &lt;a href=&#34;http://marketinglaw.osborneclarke.com/retailing/colgates-80-of-dentists-recommend-claim-under-fire/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Do you remember Colgateâ€™s claim that 80% of dentists recommended their brand?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This statistic seems pretty good at first. It turns out that at the time of surveying the dentists, they could choose several brands â€” not just one. So other brands could be just as popular as Colgate.&lt;/p&gt;

&lt;p&gt;Another Example: &lt;strong&gt;&amp;ldquo;99 percent Accurate&amp;rdquo; doesn&amp;rsquo;t mean shit&lt;/strong&gt;. Ask me to create a cancer prediction model and I could give you a 99 percent accurate model in a single line of code. How? Just predict &amp;ldquo;No Cancer&amp;rdquo; for each one. I will be accurate may be more than 99% of the time as Cancer is a pretty rare disease. Yet I have achieved nothing.&lt;/p&gt;

&lt;h2 id=&#34;6-understand-how-probability-works&#34;&gt;6. Understand how probability works&lt;/h2&gt;

&lt;p&gt;It happened during the summer of 1913 in a Casino in Monaco. Gamblers watched in amazement as a casino&amp;rsquo;s roulette wheel landed on black 26 times in a row. And since the probability of a Red vs Black is exactly half, they were certain that red was &amp;ldquo;due&amp;rdquo;. It was a field day for the Casino. A perfect example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gambler&#39;s_fallacy&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Gambler&amp;rsquo;s fallacy&lt;/a&gt;, aka the Monte Carlo fallacy.&lt;/p&gt;

&lt;p&gt;And This happens in real life. &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2538147&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;People tend to avoid long strings of the same answer&lt;/a&gt;. Sometimes sacrificing accuracy of judgment for the sake of getting a pattern of decisions that looks fairer or probable.&lt;/p&gt;

&lt;p&gt;For example, An admissions officer may reject the next application if he has approved three applications in a row, even if the application should have been accepted on merit.&lt;/p&gt;

&lt;h2 id=&#34;7-correlation-does-not-equal-causation&#34;&gt;7. Correlation Does Not Equal Causation&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/corr_caus.png&#34;  height=&#34;400&#34; width=&#34;500&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The Holy Grail of a Data scientist toolbox. To see something for what it is. Just because two variables move together in tandem doesn&amp;rsquo;t necessarily mean that one causes the another. There have been hilarious examples for this in the past. Some of my favorites are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Looking at the firehouse department data you infer that the more firemen are sent to a fire, the more damage is done.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When investigating the cause of crime in New York City in the 80s, an academic found a strong correlation between the amount of serious crime committed and the amount of ice cream sold by street vendors! Obviously, there was an unobserved variable causing both. Summers are when the crime is the greatest and when the most ice cream is sold. So Ice cream sales don&amp;rsquo;t cause crime. Neither crime increases ice cream sales.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;8-more-data-may-help&#34;&gt;8. More data may help&lt;/h2&gt;

&lt;p&gt;Sometimes getting extra data may work wonders. You might be able to model the real world more closely by looking at the problem from all angles. Look for extra data sources.&lt;/p&gt;

&lt;p&gt;For example, Crime data in a city might help banks provide a better credit line to a person living in a troubled neighborhood and in turn increase the bottom line.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shell Basics every Data Scientist Should know - Part II(AWK)</title>
      <link>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</link>
      <pubDate>Sun, 11 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/</guid>
      <description>

&lt;p&gt;Yesterday I got introduced to awk programming on the shell and is it cool. It lets you do stuff on the command line which you never imagined. As a matter of fact, it&amp;rsquo;s a whole data analytics software in itself when you think about it. You can do selections, groupby, mean, median, sum, duplication, append. You just ask. There is no limit actually.&lt;/p&gt;

&lt;p&gt;And it is easy to learn.&lt;/p&gt;

&lt;p&gt;In this post, I will try to give you a brief intro about how you could add awk to your daily work-flow.&lt;/p&gt;

&lt;p&gt;Please see my previous &lt;a href=&#34;http://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/&#34; target=&#34;_blank&#34; &gt;post&lt;/a&gt; if you want some background or some basic to intermediate understanding of shell commands.&lt;/p&gt;

&lt;h2 id=&#34;basics-fundamentals&#34;&gt;Basics/ Fundamentals&lt;/h2&gt;

&lt;p&gt;So let me start with an example first. Say you wanted to sum a column in a comma delimited file. How would you do that in shell?&lt;/p&gt;

&lt;p&gt;Here is the command. The great thing about awk is that it took me nearly 5 sec to write this command. I did not have to open any text editor to write a python script.&lt;/p&gt;

&lt;p&gt;It lets you do adhoc work quickly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum += $5 } END { print sum }&amp;#39;&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;44662539172
&lt;/pre&gt;

&lt;p&gt;See the command one more time. There is a basic structure to the awk command&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;

&lt;pre&gt;&lt;span class=&#34;n&#34;&gt;BEGIN&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;END&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;An awk program consists of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An optional BEGIN segment : In the begin part we initialize our variables before we even start reading from the file or the standard input.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pattern - action pairs: In the middle part we Process the input data. You put multiple pattern action pairs when you want to do multiple things with the same line.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An optional END segment: In the end part we do something we want to do when we have reached the end of file.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An awk command is called on a file using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{SOMETHING HERE} {SOMETHING HERE: could put Multiple Blocks Like this} END {SOMETHING HERE}&amp;#39;&lt;/span&gt; file.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You also need to know about these preinitialized variables that awk keeps track of.:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;FS : field separator. Default is whitespace (1 or more spaces or tabs). If you are using any other seperator in the file you should specify it in the Begin Part.&lt;/li&gt;
&lt;li&gt;RS : record separator. Default record separator is newline. Can be changed in BEGIN action.&lt;/li&gt;
&lt;li&gt;NR : NR is the variable whose value is the number of the current record. You normally use it in the action blocks in the middle.&lt;/li&gt;
&lt;li&gt;NF : The Number of Fields after the single line has been split up using FS.&lt;/li&gt;
&lt;li&gt;Dollar variables : awk splits up the line which is coming to it by using the given FS and keeps the split parts in the $ variables. For example column 1 is in &lt;code&gt;$1&lt;/code&gt;, column 2 is in &lt;code&gt;$2&lt;/code&gt;. &lt;code&gt;$0&lt;/code&gt; is the string representation of the whole line. Note that if you want to access last column you don&amp;rsquo;t have to count. You can just use &lt;code&gt;$NF&lt;/code&gt;. For second last column you can use &lt;code&gt;$(NF-1)&lt;/code&gt;. Pretty handy. Right.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So If you are with me till here, the hard part is done. Now the fun part starts. Lets look at the first awk command again and try to understand it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum += $5 } END { print sum }&amp;#39;&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So there is a begin block. Remember before we read any line. We initialize sum to 0 and FS to &amp;ldquo;,&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Now as awk reads its input line by line it increments sum by the value in column 5(as specified by &lt;code&gt;$5&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Note that there is no pattern specified here so awk will do the action for every line.&lt;/p&gt;

&lt;p&gt;When awk has completed reading the file it prints out the sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What if you wanted mean?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We could create a cnt Variable:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{ sum=0;cnt=0; FS=&amp;#34;,&amp;#34;} { sum += $5; cnt+=1 } END { print sum/cnt }&amp;#39;&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;1.86436e+06
&lt;/pre&gt;

&lt;p&gt;or better yet, use our friend NR which bash is already keeping track of:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{ sum=0; FS=&amp;#34;,&amp;#34;} { sum += $5 } END { print sum/NR }&amp;#39;&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;1.86436e+06
&lt;/pre&gt;

&lt;h2 id=&#34;filter-a-file&#34;&gt;Filter a file&lt;/h2&gt;

&lt;p&gt;In the mean and sum awk commands we did not put any pattern in our middle commands. Let us use a simple pattern now. Suppose we have a file Salaries.csv which contains:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;head salaries.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;yearID,teamID,lgID,playerID,salary
1985,BAL,AL,murraed02,1472819
1985,BAL,AL,lynnfr01,1090000
1985,BAL,AL,ripkeca01,800000
1985,BAL,AL,lacyle01,725000
1985,BAL,AL,flanami01,641667
1985,BAL,AL,boddimi01,625000
1985,BAL,AL,stewasa01,581250
1985,BAL,AL,martide01,560000
1985,BAL,AL,roeniga01,558333
&lt;/pre&gt;

&lt;p&gt;I want to filter records for players who who earn more than 22 M in 2013 just because I want to. You just do:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;} $5&amp;gt;=22000000 &amp;amp;&amp;amp; $1==2013{print $0}&amp;#39;&lt;/span&gt; Salaries.csv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;2013,DET,AL,fieldpr01,23000000
2013,MIN,AL,mauerjo01,23000000
2013,NYA,AL,rodrial01,29000000
2013,NYA,AL,wellsve01,24642857
2013,NYA,AL,sabatcc01,24285714
2013,NYA,AL,teixema01,23125000
2013,PHI,NL,leecl02,25000000
2013,SFN,NL,linceti01,22250000
&lt;/pre&gt;

&lt;p&gt;Cool right. Now let me explain it a little bit. The part in the command &amp;ldquo;&lt;code&gt;$5&lt;/code&gt;&amp;gt;=22000000 &amp;amp;&amp;amp; &lt;code&gt;$1&lt;/code&gt;==2013&amp;rdquo; is called a pattern. It says that print this line(&lt;code&gt;$0&lt;/code&gt;) if and only if the Salary(&lt;code&gt;$5&lt;/code&gt;) is more than 22M and(&amp;amp;&amp;amp;) year(&lt;code&gt;$1&lt;/code&gt;) is equal to 2013. If the incoming record(line) does not satisfy this pattern it never reaches the inner block.&lt;/p&gt;

&lt;p&gt;So Now you could do basic Select SQL at the command line only if you had:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The logic Operators:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;== equality operator; returns TRUE is both sides are equal&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;!= inverse equality operator&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;amp;&amp;amp; logical AND&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;|| logical OR&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;! logical NOT&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;= relational operators&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Normal Arithmetic Operators:&lt;/strong&gt; +, -, /, *, %, ^&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some String Functions:&lt;/strong&gt; length, substr, split&lt;/p&gt;

&lt;h2 id=&#34;groupby&#34;&gt;GroupBy&lt;/h2&gt;

&lt;p&gt;Now you will say: &amp;ldquo;Hey Dude SQL without groupby is incomplete&amp;rdquo;. You are right and for that we can use the associative array. Lets just see the command first and then I will explain. So lets create another useless use case(or may be something useful to someone :)) We want to find out the number of records for each year in the file. i.e we want to find the distribution of years in the file. Here is the command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    {my_array[$1]=my_array[$1]+1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    END{
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    for (k in my_array){if(k!=&amp;#34;yearID&amp;#34;)print k&amp;#34;|&amp;#34;my_array[k]};
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    }&amp;#39;&lt;/span&gt; Salaries.csv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;1990|867
1991|685
1996|931
1997|925
...
&lt;/pre&gt;

&lt;p&gt;Now I would like to tell you a secret. You don&amp;rsquo;t really need to declare the variables you want to use in awk. So you did not really needed to define sum, cnt variables before. I only did that because it is good practice. If you don&amp;rsquo;t declare a user defined variable in awk, awk assumes it to be null or zero depending on the context. So in the command above we don&amp;rsquo;t declare our myarray in the begin block and that is fine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Associative Array&lt;/strong&gt;: The variable myarray is actually an associative array. i.e. It stores data in a key value format.(Python dictionaries anyone). The same array could keep integer keys and String keys. For example, I can do this in a single code.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;

&lt;pre&gt;&lt;span class=&#34;n&#34;&gt;myarray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&#34;key&#34;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;myarray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&#39;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mlwhiz&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;For Loop for associative arrays&lt;/strong&gt;: I could use a for loop to read associative array&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;

&lt;pre&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DO&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SOMETHING&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;cp&#34;&gt;# Assigns to k each Key of array (unordered)&lt;/span&gt;
&lt;span class=&#34;cp&#34;&gt;# Element is array[k]&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;If Statement&lt;/strong&gt;:Uses a syntax like C for the if statement. the else block is optional:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;

&lt;pre&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;DO&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SOMETHING&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;DO&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SOMETHING&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;p&gt;So lets dissect the above command now.&lt;/p&gt;

&lt;p&gt;I set the File separator to &amp;ldquo;,&amp;rdquo; in the beginning. I use the first column as the key of myarray. If the key exists I increment the value by 1.&lt;/p&gt;

&lt;p&gt;At the end, I loop through all the keys and print out key value pairs separated by &amp;ldquo;|&amp;rdquo;&lt;/p&gt;

&lt;p&gt;I know that the header line in my file contains &amp;ldquo;yearID&amp;rdquo; in column 1 and I don&amp;rsquo;t want &amp;lsquo;yearID|1&amp;rsquo; in the output. So I only print when Key is not equal to &amp;lsquo;yearID&amp;rsquo;.&lt;/p&gt;

&lt;h2 id=&#34;groupby-with-case-statement&#34;&gt;GroupBy with case statement:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat Salaries.csv | awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN{FS=&amp;#34;,&amp;#34;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    $5&amp;lt;100000{array5[&amp;#34;[0-100000)&amp;#34;]+=1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    $5&amp;gt;=100000&amp;amp;&amp;amp;$5&amp;lt;250000{array5[&amp;#34;[100000,250000)&amp;#34;]=array5[&amp;#34;[100000,250000)&amp;#34;]+1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    $5&amp;gt;=250000&amp;amp;&amp;amp;$5&amp;lt;500000{array5[&amp;#34;[250000-500000)&amp;#34;]=array5[&amp;#34;[250000-500000)&amp;#34;]+1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    $5&amp;gt;=500000&amp;amp;&amp;amp;$5&amp;lt;1000000{array5[&amp;#34;[500000-1000000)&amp;#34;]=array5[&amp;#34;[500000-1000000)&amp;#34;]+1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    $5&amp;gt;=1000000{array5[&amp;#34;[1000000)&amp;#34;]=array5[&amp;#34;[1000000)&amp;#34;]+1}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    END{
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    print &amp;#34;VAR Distrib:&amp;#34;;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    for (v in array5){print v&amp;#34;|&amp;#34;array5[v]}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&#34;&gt;VAR Distrib:
[250000-500000)|8326
[0-100000)|2
[1000000)|23661
[100000,250000)|9480
&lt;/pre&gt;

&lt;p&gt;Here we used multiple pattern-action blocks to create a case statement.&lt;/p&gt;

&lt;h2 id=&#34;for-the-brave&#34;&gt;For The Brave:&lt;/h2&gt;

&lt;p&gt;This is a awk code that I wrote to calculate the Mean,Median,min,max and sum of a column simultaneously. Try to go through the code and understand it.I have added comments too. Think of this as an exercise. Try to run this code and play with it. You may learn some new tricks in the process. If you don&amp;rsquo;t understand it do not worry. Just get started writing your own awk codes, you will be able to understand it in very little time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Create a New file named A.txt to keep only the salary column.&lt;/span&gt;
    cat Salaries.csv | cut -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt; -f &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &amp;gt; A.txt
    FILENAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A.txt&amp;#34;&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# The first awk counts the number of lines which are numeric. We use a regex here to check if the column is numeric or not.&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#39;;&amp;#39; stands for Synchronous execution i.e sort only runs after the awk is over.&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# The output of both commands are given to awk command which does the whole work.&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# So Now the first line going to the second awk is the number of lines in the file which are numeric.&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# and from the second to the end line the file is sorted.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;BEGIN {c=0} $1 ~ /^[-0-9]*(\.[0-9]*)?$/ {c=c+1;} END {print c;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$FILENAME&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;            sort -n &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$FILENAME&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; | awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      BEGIN {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        c = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        sum = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        med1_loc = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        med2_loc = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        med1_val = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        med2_val = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        min = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        max = 0;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      NR==1 {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        LINES = $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        # We check whether numlines is even or odd so that we keep only
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        # the locations in the array where the median might be.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        if (LINES%2==0) {med1_loc = LINES/2-1; med2_loc = med1_loc+1;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        if (LINES%2!=0) {med1_loc = med2_loc = (LINES-1)/2;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      $1 ~ /^[-0-9]*(\.[0-9]*)?$/  &amp;amp;&amp;amp;  NR!=1 {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        # setting min value
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        if (c==0) {min = $1;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        # middle two values in array
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        if (c==med1_loc) {med1_val = $1;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        if (c==med2_loc) {med2_val = $1;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        c++
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        sum += $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        max = $1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      END {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        ave = sum / c
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        median = (med1_val + med2_val ) / 2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;sum:&amp;#34; sum
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;count:&amp;#34; c
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;mean:&amp;#34; ave
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;median:&amp;#34; median
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;min:&amp;#34; min
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        print &amp;#34;max:&amp;#34; max
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&lt;/span&gt;

&amp;lt;pre style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF112&amp;#34;&lt;/span&gt;&amp;gt;sum:44662539172
count:23956
mean:1.86436e+06
median:507950
min:0
max:33000000
&amp;lt;/pre&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;endnote&#34;&gt;Endnote:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;awk&lt;/strong&gt; is an awesome tool and there are a lot of use-cases where it can make your life simple. There is a sort of a learning curve, but I think that it would be worth it in the long term. I have tried to give you a taste of awk and I have covered a lot of ground here in this post. To tell you a bit more there, awk is a full programming language. There are for loops, while loops, conditionals, booleans, functions and everything else that you would expect from a programming language. So you could look more still.&lt;/p&gt;

&lt;p&gt;To learn more about awk you can use this &lt;a href=&#34;http://ir.nmu.org.ua/bitstream/handle/123456789/143548/ecf2f2d8a72e7c3cffca0036a73aeed4.pdf?sequence=1&amp;amp;&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;book&lt;/a&gt;. This book is a free resource and you could learn more about awk and use cases.&lt;/p&gt;

&lt;p&gt;Or if you like to have your book binded and in paper like me you can buy this book, which is a gem:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.amazon.com/gp/product/1565922255/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=1565922255&amp;amp;linkCode=as2&amp;amp;tag=mlwhizcon-20&amp;amp;linkId=YC37WW67AJHS3T6S&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;amp;ASIN=1565922255&amp;amp;Format=_SL250_&amp;amp;ID=AsinImage&amp;amp;MarketPlace=US&amp;amp;ServiceVersion=20070822&amp;amp;WS=1&amp;amp;tag=mlwhizcon-20&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;img src=&#34;http://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=1565922255&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Do leave comments in case you find more use-cases for awk or if you want me to write on new use-cases. Or just comment weather you liked it or not and how I could improve as I am also new and trying to learn more of this.&lt;/p&gt;

&lt;p&gt;Till then Ciao !!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shell Basics every Data Scientist Should know -Part I</title>
      <link>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/</guid>
      <description>

&lt;p&gt;Shell Commands are powerful. And life would be like &lt;strong&gt;hell without shell&lt;/strong&gt; is how I like to say it(And that is probably the reason that I dislike windows).&lt;/p&gt;

&lt;p&gt;Consider a case when you have a 6 GB pipe-delimited file sitting on your laptop and you want to find out the count of distinct values in one particular column. You can probably do this in more than one way. You could put that file in a database and run SQL Commands, or you could write a python/perl script.&lt;/p&gt;

&lt;p&gt;Probably whatever you do it won&amp;rsquo;t be simpler/less time consuming than this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | cut -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; -f &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; | sort | uniq | wc -l&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;30
&lt;/pre&gt;

&lt;p&gt;And this will &lt;strong&gt;run way faster&lt;/strong&gt; than whatever you do with perl/python script.&lt;/p&gt;

&lt;p&gt;Now this command says&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use the &lt;strong&gt;cat&lt;/strong&gt; command to print/stream the contents of the file to stdout.&lt;/li&gt;
&lt;li&gt;Pipe the streaming contents from our cat command to the next command &lt;strong&gt;cut&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;cut&lt;/strong&gt; commands specifies the delimiter by the argument &lt;strong&gt;-d&lt;/strong&gt; and the column by the argument &lt;strong&gt;-f&lt;/strong&gt; and streams the output to stdout.&lt;/li&gt;
&lt;li&gt;Pipe the streaming content to the &lt;strong&gt;sort&lt;/strong&gt; command which sorts the input and streams only the distinct values to the stdout. It takes the argument &lt;strong&gt;-u&lt;/strong&gt; that specifies that we only need unique values.&lt;/li&gt;
&lt;li&gt;Pipe the output to the wc -l command which counts the number of lines in the input.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is a &lt;strong&gt;lot going on here&lt;/strong&gt; and I will try my best to ensure that &lt;strong&gt;you will be able to understand most of it by the end of this Blog post&lt;/strong&gt;.Although I will also try to explain more advanced concepts than the above command in this post.&lt;/p&gt;

&lt;p&gt;Now, I use shell commands extensively at my job. I will try to explain the usage of each of the commands based on use cases that I counter nearly daily at may day job as a data scientist.&lt;/p&gt;

&lt;h2 id=&#34;some-basic-commands-in-shell&#34;&gt;Some Basic Commands in Shell:&lt;/h2&gt;

&lt;p&gt;There are a lot of times when you just need to know a little bit about the data. You just want to see may be a couple of lines to inspect a file. One way of doing this is opening the txt/csv file in the notepad. And that is probably the best way for small files. But you could also do it in the shell using:&lt;/p&gt;

&lt;h3 id=&#34;1-cat&#34;&gt;1. cat&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;
yearID|teamID|lgID|playerID|salary
1985|BAL|AL|murraed02|1472819
1985|BAL|AL|lynnfr01|1090000
1985|BAL|AL|ripkeca01|800000
1985|BAL|AL|lacyle01|725000
1985|BAL|AL|flanami01|641667
1985|BAL|AL|boddimi01|625000
1985|BAL|AL|stewasa01|581250
1985|BAL|AL|martide01|560000
1985|BAL|AL|roeniga01|558333
&lt;/pre&gt;

&lt;p&gt;Now the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cat_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;cat&lt;/a&gt; command prints the whole file in the terminal window for you.I have not shown the whole file here.&lt;/p&gt;

&lt;p&gt;But sometimes the files will be so big that you wont be able to open them up in notepad++ or any other software utility and there the cat command will shine.&lt;/p&gt;

&lt;h3 id=&#34;2-head-and-tail&#34;&gt;2. Head and Tail&lt;/h3&gt;

&lt;p&gt;Now you might ask me why would you print the whole file in the terminal itself? Generally I won&amp;rsquo;t. But I just wanted to tell you about the cat command. For the use case when you want only the top/bottom n lines of your data you will generally use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Head_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;head&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/Tail_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;tail&lt;/a&gt; commands. You can use them as below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;head data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID|teamID|lgID|playerID|salary
1985|BAL|AL|murraed02|1472819
1985|BAL|AL|lynnfr01|1090000
1985|BAL|AL|ripkeca01|800000
1985|BAL|AL|lacyle01|725000
1985|BAL|AL|flanami01|641667
1985|BAL|AL|boddimi01|625000
1985|BAL|AL|stewasa01|581250
1985|BAL|AL|martide01|560000
1985|BAL|AL|roeniga01|558333
&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;head -n &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID|teamID|lgID|playerID|salary
1985|BAL|AL|murraed02|1472819
1985|BAL|AL|lynnfr01|1090000
&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tail data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;2013|WAS|NL|bernaro01|1212500
2013|WAS|NL|tracych01|1000000
2013|WAS|NL|stammcr01|875000
2013|WAS|NL|dukeza01|700000
2013|WAS|NL|espinda01|526250
2013|WAS|NL|matthry01|504500
2013|WAS|NL|lombast02|501250
2013|WAS|NL|ramoswi01|501250
2013|WAS|NL|rodrihe03|501000
2013|WAS|NL|moorety01|493000
&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tail -n &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;2013|WAS|NL|rodrihe03|501000
2013|WAS|NL|moorety01|493000
&lt;/pre&gt;

&lt;p&gt;Notice the structure of the shell command here.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;

&lt;pre&gt;&lt;span class=&#34;n&#34;&gt;CommandName&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg1name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg1value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg2name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arg2value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filename&lt;/span&gt;
&lt;/pre&gt;

&lt;/div&gt;

&lt;h3 id=&#34;3-piping&#34;&gt;3. Piping&lt;/h3&gt;

&lt;p&gt;Now we could have also written the same command as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | head&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID|teamID|lgID|playerID|salary
1985|BAL|AL|murraed02|1472819
1985|BAL|AL|lynnfr01|1090000
1985|BAL|AL|ripkeca01|800000
1985|BAL|AL|lacyle01|725000
1985|BAL|AL|flanami01|641667
1985|BAL|AL|boddimi01|625000
1985|BAL|AL|stewasa01|581250
1985|BAL|AL|martide01|560000
1985|BAL|AL|roeniga01|558333
&lt;/pre&gt;

&lt;p&gt;This brings me to one of the most important concepts of Shell usage - &lt;a href=&#34;https://en.wikipedia.org/wiki/Pipeline_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;piping&lt;/strong&gt;&lt;/a&gt;. You won&amp;rsquo;t be able to utilize the full power the shell provides without using this concept. And the concept is actually simple.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Just read the &amp;ldquo;|&amp;rdquo; in the command as &amp;ldquo;pass the data on to&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So I would read the above command as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;cat&lt;/em&gt;(print) the whole data to stream, &lt;strong&gt;pass the data on to&lt;/strong&gt; &lt;em&gt;head&lt;/em&gt; so that it can just give me the first few lines only.&lt;/p&gt;

&lt;p&gt;So did you understood what piping did? &lt;strong&gt;It is providing us a way to use our basic commands in a consecutive manner&lt;/strong&gt;. There are a lot of commands that are fairly basic and it lets us use these basic commands in sequence to do some fairly non trivial things.&lt;/p&gt;

&lt;p&gt;Now let me tell you about a couple of more commands before I show you how we can &lt;strong&gt;chain&lt;/strong&gt; them to do fairly advanced tasks.&lt;/p&gt;

&lt;h3 id=&#34;4-wc&#34;&gt;4. wc&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Wc_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;wc&lt;/a&gt; is a fairly useful shell utility/command that lets us &lt;strong&gt;count the number of lines(-l)&lt;/strong&gt;, &lt;strong&gt;words(-w)&lt;/strong&gt; or &lt;strong&gt;characters(-c)&lt;/strong&gt; in a given file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;wc -l data.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;23957 data.txt
&lt;/pre&gt;

&lt;h3 id=&#34;5-grep&#34;&gt;5. grep&lt;/h3&gt;

&lt;p&gt;You may want to print all the lines in your file which have a particular word. Or as a Data case you might like to see the salaries for the team BAL in 2000. In this case we have printed all the lines in the file which contain &amp;ldquo;2000|BAL&amp;rdquo;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Grep&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;grep&lt;/a&gt; is your friend.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2000|BAL&amp;#34;&lt;/span&gt; data.txt | head&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;2000|BAL|AL|belleal01|12868670
2000|BAL|AL|anderbr01|7127199
2000|BAL|AL|mussimi01|6786032
2000|BAL|AL|ericksc01|6620921
2000|BAL|AL|ripkeca01|6300000
2000|BAL|AL|clarkwi02|6000000
2000|BAL|AL|johnsch04|4600000
2000|BAL|AL|timlimi01|4250000
2000|BAL|AL|deshide01|4209324
2000|BAL|AL|surhobj01|4146789
&lt;/pre&gt;

&lt;p&gt;you could also use regular expressions with grep.&lt;/p&gt;

&lt;h3 id=&#34;6-sort&#34;&gt;6. sort&lt;/h3&gt;

&lt;p&gt;You may want to &lt;a href=&#34;https://en.wikipedia.org/wiki/Sort_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;sort&lt;/a&gt; your dataset on a particular column.Sort is your friend. Say you want to find out the top 10 maximum salaries given to any player in your dataset.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sort -t &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; -k &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; -r -n data.txt | head -10&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;2010|NYA|AL|rodrial01|33000000
2009|NYA|AL|rodrial01|33000000
2011|NYA|AL|rodrial01|32000000
2012|NYA|AL|rodrial01|30000000
2013|NYA|AL|rodrial01|29000000
2008|NYA|AL|rodrial01|28000000
2011|LAA|AL|wellsve01|26187500
2005|NYA|AL|rodrial01|26000000
2013|PHI|NL|leecl02|25000000
2013|NYA|AL|wellsve01|24642857
&lt;/pre&gt;

&lt;p&gt;So there are certainly a lot of options in this command. Lets go through them one by one.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;-t&lt;/strong&gt;: Which delimiter to use?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-k&lt;/strong&gt;: Which column to sort on?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-n&lt;/strong&gt;: If you want Numerical Sorting. Dont use this option if you want Lexographical sorting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-r&lt;/strong&gt;: I want to sort Descending. Sorts Ascending by Default.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;7-cut&#34;&gt;7. cut&lt;/h3&gt;

&lt;p&gt;This command lets you select certain columns from your data. Sometimes you may want to look at just some of the columns in your data. As in you may want to look only at the year, team and salary and not the other columns. &lt;a href=&#34;https://en.wikipedia.org/wiki/Cut_(Unix)&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;cut&lt;/a&gt; is the command to use.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cut -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; -f &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,2,5 data.txt | head&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID|teamID|salary
1985|BAL|1472819
1985|BAL|1090000
1985|BAL|800000
1985|BAL|725000
1985|BAL|641667
1985|BAL|625000
1985|BAL|581250
1985|BAL|560000
1985|BAL|558333
&lt;/pre&gt;

&lt;p&gt;The options are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;-d&lt;/strong&gt;: Which delimiter to use?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-f&lt;/strong&gt;: Which column/columns to cut?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;8-uniq&#34;&gt;8. uniq&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Uniq&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;uniq&lt;/a&gt; is a little bit tricky as in you will want to use this command in sequence with sort. This command removes sequential duplicates. So in conjunction with sort it can be used to get the distinct values in the data. For example if I wanted to find out 10 distinct teamIDs in data, I would use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | cut -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; -f &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; | sort | uniq | head&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;ANA
ARI
ATL
BAL
BOS
CAL
CHA
CHN
CIN
CLE
&lt;/pre&gt;

&lt;p&gt;This command could be used with argument &lt;strong&gt;-c&lt;/strong&gt; to count the occurrence of these distinct values. Something akin to &lt;strong&gt;count distinct&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | cut -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; -f &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; | sort | uniq -c | head&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;247 ANA
458 ARI
838 ATL
855 BAL
852 BOS
368 CAL
812 CHA
821 CHN
46 CIN
867 CLE
&lt;/pre&gt;

&lt;h2 id=&#34;some-other-utility-commands-for-other-operations&#34;&gt;Some Other Utility Commands for Other Operations&lt;/h2&gt;

&lt;p&gt;Some Other command line tools that you could use without going in the specifics as the specifics are pretty hard.&lt;/p&gt;

&lt;h3 id=&#34;1-change-delimiter-in-a-file&#34;&gt;1. Change delimiter in a file&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Find and Replace Magic.&lt;/strong&gt;: You may want to replace certain characters in file with something else using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Tr_%28Unix%29&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;tr&lt;/a&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | tr &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt; |  head -4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID,teamID,lgID,playerID,salary
1985,BAL,AL,murraed02,1472819
1985,BAL,AL,lynnfr01,1090000
1985,BAL,AL,ripkeca01,800000
&lt;/pre&gt;

&lt;p&gt;or the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sed&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;sed&lt;/strong&gt;&lt;/a&gt; command&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | sed -e &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;s/|/,/g&amp;#39;&lt;/span&gt; | head -4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;yearID,teamID,lgID,playerID,salary
1985,BAL,AL,murraed02,1472819
1985,BAL,AL,lynnfr01,1090000
1985,BAL,AL,ripkeca01,800000
&lt;/pre&gt;

&lt;h3 id=&#34;2-sum-of-a-column-in-a-file&#34;&gt;2. Sum of a column in a file&lt;/h3&gt;

&lt;p&gt;Using the &lt;a href=&#34;https://en.wikipedia.org/wiki/AWK&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;awk&lt;/a&gt; command you could find the sum of column in file. Divide it by the number of lines and you can get the mean.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | awk -F &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;|&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{ sum += $5 } END { printf sum }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;44662539172
&lt;/pre&gt;

&lt;p&gt;awk is a powerful command which is sort of a whole language in itself. Do see the wiki page for &lt;a href=&#34;https://en.wikipedia.org/wiki/AWK&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;awk&lt;/a&gt; for a lot of great usecases of awk. I also wrote a post on awk as a second part in this series. Check it &lt;a href=&#34;http://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;HERE&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-find-the-files-in-a-directory-that-satisfy-a-certain-condition&#34;&gt;3. Find the files in a directory that satisfy a certain condition&lt;/h3&gt;

&lt;p&gt;You can do this by using the find command. Lets say you want to &lt;strong&gt;find all the .txt files&lt;/strong&gt; in the current working dir that &lt;strong&gt;start with lowercase h&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;h*.txt&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;./hamlet.txt
&lt;/pre&gt;

&lt;p&gt;To find &lt;strong&gt;all .txt files starting with h regarless of case&lt;/strong&gt; we could use regex.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[Hh]*.txt&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;./hamlet.txt
./Hamlet1.txt
&lt;/pre&gt;

&lt;h3 id=&#34;4-passing-file-list-as-argument&#34;&gt;4. Passing file list as Argument.&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Xargs&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;xargs&lt;/a&gt; was suggested by Gaurav in the comments, so I read about it and it is actually a very nice command which you could use in a variety of use cases.&lt;/p&gt;

&lt;p&gt;So if you just use a pipe, any command/utility receives data on STDIN (the standard input stream) as a raw pile of data that it can sort through one line at a time. However some programs don&amp;rsquo;t accept their commands on standard in. For example the rm command(which is used to remove files), touch command(used to create file with a given name) or a certain python script you wrote(which takes command line arguments). They expect it to be spelled out in the arguments to the command.&lt;/p&gt;

&lt;p&gt;For example: rm takes a file name as a parameter on the command line like so: rm file1.txt. If I wanted to &lt;strong&gt;delete all &amp;lsquo;.txt&amp;rsquo; files starting with &amp;ldquo;h/H&amp;rdquo;&lt;/strong&gt; from my working directory, the below command won&amp;rsquo;t work because rm expects a file as an input.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[hH]*.txt&amp;#34;&lt;/span&gt; | rm&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;usage: rm [-f | -i] [-dPRrvW] file ...
unlink file
&lt;/pre&gt;

&lt;p&gt;To get around it we can use the xargs command which reads the STDIN stream data and converts each line into space separated arguments to the command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[hH]*.txt&amp;#34;&lt;/span&gt; | xargs&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;./hamlet.txt ./Hamlet1.txt
&lt;/pre&gt;

&lt;p&gt;Now you could use rm to remove all .txt files that start with h/H. A word of advice: Always see the output of xargs first before using rm.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[hH]*.txt&amp;#34;&lt;/span&gt; | xargs rm&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Another usage of xargs could be in conjunction with grep to &lt;strong&gt;find all files that contain a given string&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;find . -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.txt&amp;#34;&lt;/span&gt; | xargs grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;honest soldier&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-size:50%; padding:7px; margin:0em;  background-color:#FFF122&#34;&gt;./Data1.txt:O, farewell, honest soldier;
./Data2.txt:O, farewell, honest soldier;
./Data3.txt:O, farewell, honest soldier;
&lt;/pre&gt;

&lt;p&gt;Hopefully You could come up with varied uses building up on these examples. One other use case could be to use this for &lt;strong&gt;passing arguments to a python script&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;other-cool-tricks&#34;&gt;Other Cool Tricks&lt;/h2&gt;

&lt;p&gt;Sometimes you want your data that you got by some command line utility(Shell commands/ Python scripts) not to be shown on stdout but stored in a textfile. You can use the &lt;strong&gt;&amp;rdquo;&amp;gt;&amp;rdquo;&lt;/strong&gt; operator for that. For Example: You could have stored the file after replacing the delimiters in the previous example into anther file called newdata.txt as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat data.txt | tr &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt; &amp;gt; newdata.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I really got confused between &lt;strong&gt;&amp;rdquo;|&amp;rdquo;&lt;/strong&gt; (piping) and &lt;strong&gt;&amp;rdquo;&amp;gt;&amp;rdquo;&lt;/strong&gt; (to_file) operations a lot in the beginning. One way to remember is that you should only use &lt;strong&gt;&amp;rdquo;&amp;gt;&amp;rdquo;&lt;/strong&gt; when you want to write something to a file. &lt;strong&gt;&amp;rdquo;|&amp;rdquo; cannot be used to write to a file.&lt;/strong&gt; Another operation you should know about is the &lt;strong&gt;&amp;rdquo;&amp;gt;&amp;gt;&amp;rdquo;&lt;/strong&gt; operation. It is analogous to &lt;strong&gt;&amp;rdquo;&amp;gt;&amp;rdquo;&lt;/strong&gt; but it appends to an existing file rather that replacing the file and writing over.&lt;/p&gt;

&lt;p&gt;If you would like to know more about commandline, which I guess you would, here are some books that I would recommend for a beginner:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.amazon.com/gp/product/1593273894/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=1593273894&amp;amp;linkCode=as2&amp;amp;tag=mlwhizcon-20&amp;amp;linkId=IXZOHV6FHPTYCBCT&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;amp;ASIN=1593273894&amp;amp;Format=_SL250_&amp;amp;ID=AsinImage&amp;amp;MarketPlace=US&amp;amp;ServiceVersion=20070822&amp;amp;WS=1&amp;amp;tag=mlwhizcon-20&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;img src=&#34;http://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=1593273894&#34; alt=&#34;&#34; /&gt; &lt;a href=&#34;http://www.amazon.com/gp/product/0596009658/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0596009658&amp;amp;linkCode=as2&amp;amp;tag=mlwhizcon-20&amp;amp;linkId=2ZHHZIAJBFW3BFF7&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;amp;ASIN=0596009658&amp;amp;Format=_SL250_&amp;amp;ID=AsinImage&amp;amp;MarketPlace=US&amp;amp;ServiceVersion=20070822&amp;amp;WS=1&amp;amp;tag=mlwhizcon-20&#34; alt=&#34;&#34; /&gt;&lt;/a&gt;&lt;img src=&#34;http://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0596009658&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first book is more of a fun read at leisure type of book. THe second book is a little more serious. Whatever suits you.&lt;/p&gt;

&lt;p&gt;So, this is just the tip of the iceberg. Although I am not an expert in shell usage, these commands reduced my workload to a large extent. If there are some shell commands you use on a regular basis or some shell command that are cool, do tell in the comments. I would love to include it in the blogpost.&lt;/p&gt;

&lt;p&gt;I wrote a blogpost on awk as a second part of this post. Check it &lt;a href=&#34;http://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>