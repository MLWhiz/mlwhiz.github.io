<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 13 Apr 2022 13:34:49 +0100</lastBuildDate><atom:link href="https://mlwhiz.com/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title><link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid><description>&lt;p>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.&lt;/p></description></item><item><title>Stop Worrying and Create your Deep Learning Server in 30 minutes</title><link>https://mlwhiz.com/blog/2020/05/25/dls/</link><pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/25/dls/</guid><description>&lt;p>I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.&lt;/p></description></item><item><title>Using Deep Learning for End to End Multiclass Text Classification</title><link>https://mlwhiz.com/blog/2020/05/24/multitextclass/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/24/multitextclass/</guid><description>&lt;p>






 
 
 
 
 

 
 
 

 
 &lt;img
 sizes="(min-width: 35em) 1200px, 100vw"
 srcset='
 
 /images/multitextclass/main_hu5347609766070859065.png 500w
 
 
 , /images/multitextclass/main_hu15737587202097698753.png 800w
 
 
 , /images/multitextclass/main_hu7039699309994025813.png 1200w
 
 
 , /images/multitextclass/main_hu5618161544359752641.png 1500w 
 '
 src="https://mlwhiz.com/images/multitextclass/main.png"

 alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence">
 

&lt;/p></description></item><item><title>Adding Interpretability to Multiclass Text Classification models</title><link>https://mlwhiz.com/blog/2019/11/08/interpret_models/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/08/interpret_models/</guid><description>&lt;p>Explain Like I am 5.&lt;/p>
&lt;p>It is the basic tenets of learning for me where I try to distill any concept in a more palatable form. As Feynman said:&lt;/p></description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>&lt;p>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p></description></item><item><title>NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid><description>&lt;p>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the 

&lt;a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/">third post&lt;/a>
, I delved deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.&lt;/p></description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>&lt;p>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>&lt;p>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong>Quora Insincere questions classification&lt;/strong> in which I got a rank of &lt;strong>&lt;code>182/4037&lt;/code>&lt;/strong>. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p></description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>&lt;p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the various &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In this post, I will try to take you through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong>Deep learning models&lt;/strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>&lt;p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p></description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/artificial-neural-network.png" height="350" width="700" >&lt;/center>
&lt;/div>
&lt;p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the &lt;strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic&lt;/strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.&lt;/p></description></item><item><title>What Kagglers are using for Text Classification</title><link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid><description>&lt;p>With the problem of Image Classification is more or less solved by Deep learning, &lt;em>Text Classification is the next new developing theme in deep learning&lt;/em>. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that?&lt;/p></description></item><item><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid><description>&lt;p>Recently Quora put out a 

&lt;a href="https://www.kaggle.com/c/quora-question-pairs" target="_blank" rel="nofollow noopener">Question similarity&lt;/a>
 competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p></description></item></channel></rss>