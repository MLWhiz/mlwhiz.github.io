<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Productivity on MLWhiz</title>
    <link>https://mlwhiz.com/tags/productivity/</link>
    <description>Recent content in Productivity on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/productivity/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Create an Awesome Development Setup for Data Science using Atom</title>
      <link>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/atom_for_data_science/main.png"></media:content>
      

      
      <description>Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of “Coding one code block at a time,” as I like to call it, and it helps me to think more clearly while helping me make my code more modular.
Yet, Jupyter is not suitable for some tasks in its present form.</description>

      <content:encoded>  
        
        <![CDATA[  Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of “Coding one code block at a time,” as I like to call it, and it helps me to think more clearly while helping me make my code more modular.
Yet, Jupyter is not suitable for some tasks in its present form. And the most prominent is when I have to work with .py files. And one will need to work with .py files whenever they want to push your code to production or change other people’s code. So, until now, I used sublime text to edit Python files, and I found it excellent. But recently, when I looked at the Atom editor, my loyalties seemed to shift when I saw the multiple out of the box options provided by it.
Now, the real power to Atom comes from the various packages you can install. In this post, I will talk about the packages that help make Atom just the most hackable and wholesome development environment ever.
Installing Atom and Some Starting Tweaks Before we even begin, we need to install Atom. You can do it from the main website here. The installation process is pretty simple, whatever your platform is. For Linux, I just downloaded the .deb file and double-clicked it. Once you have installed Atom, You can look at doing some tweaks:
 Open Core settings in Atom using Ctrl&#43;Shift&#43;P and typing settings therein. This Ctrl&#43;Shift&#43;P command is going to be one of the most important commands in Atom as it lets you navigate and run a lot of commands.  Accessing the Settings window using Ctrl&#43;Shift&#43;P
 Now go to the Editor menu and Uncheck “Soft Tabs”. This is done so that TAB key registers as a TAB and not two spaces. If you want you can also activate “Soft Wrap” which wraps the text if the text exceeds the window width.  My preferred settings for soft-wrap and soft-tabs.
Now, as we have Atom installed, we can look at some of the most awesome packages it provides. And the most important of them is GitHub.
1. Commit to Github without leaving Editor Are you fed up with leaving your text editor to use terminal every time you push a commit to Github? If your answer is yes, Atom solves this very problem by letting you push commits without you ever leaving the text editor window.
This is one of the main features that pushed me towards Atom from Sublime Text. I like how this functionality comes preloaded with Atom and it doesn’t take much time to set it up.
To start using it, click on the GitHub link in the right bottom of the Atom screen, and the Atom screen will prompt you to log in to your Github to provide access. It is a one-time setup, and once you log in and give the token generated to Atom, you will be able to push your commits from the Atom screen itself without navigating to the terminal window.
  ![]  ![]   The process to push a commit is:
 Change any file or multiple files.
 Click on Git on the bottom right corner.
 Stage the Changes
 Write a commit message.
 Click on Push in the bottom right corner.
 And we are done:)
  Below, I am pushing a very simple commit to Github, where I add a title to my Markdown file. Its a GIF file, so it might take some time to load.
Committing in Atom
2. Write Markdown with real-time preview I am always torn between the medium editor vs. Markdown whenever I write blog posts for my site. For one, I prefer using Markdown when I have to use Math symbols for my post or have to use custom HTML. But, I also like the Medium editor as it is WYSIWYG(What You See Is What You Get). And with Atom, I have finally found the perfect markdown editor for me, which provides me with Markdown as well as WYSIWYG. And it has now become a default option for me to create any README.md files for GitHub.
Using Markdown in Atom is again a piece of cake and is activated by default. To see a live preview with Markdown in Atom:
 Use Ctrl&#43;Shift&#43;M to open Markdown Preview Pane.
 Whatever changes you do in the document will reflect near real-time in the preview window.
  Markdown Split Screen editor
3. Minimap — A navigation map for Large code files Till now, we haven’t installed any new package to Atom, so let’s install an elementary package as our first package. This package is called minimap, and it is something that I like to have from my Sublime Text days. It lets you have a side panel where you can click and reach any part of the code. Pretty useful for large files.
To install a package, you can go to settings and click on Install Packages. Ctrl&#43;Shift&#43;P &amp;gt; Settings &amp;gt; &#43; Install &amp;gt; Minimap&amp;gt; Install
Installing Minimap or any package
Once you install the package, you can see the minimap on the side of your screen.
Sidebar to navigate large files with ease
4. Python Autocomplete with function definitions in Text Editor An editor is never really complete until it provides you with some autocomplete options for your favorite language. Atom integrates well with Kite, which tries to integrate AI and autocomplete.
So, to enable autocomplete with Kite, we can use the package named autocomplete-python in Atom. The install steps remain the same as before. i.e.
Ctrl&#43;Shift&#43;P &amp;gt; Settings &amp;gt; &#43; Install &amp;gt; autocomplete-python&amp;gt; Install
You will also see the option of using Kite along with it. I usually end up using Kite instead of Jedi(Another autocomplete option). This is how it looks when you work on a Python document with Kite autocompletion.
Autocomplete with Kite lets you see function definitions too.
5. Hydrogen — Run Python code in Jupyter environment Want to run Python also in your Atom Editor with any Jupyter Kernel? There is a way for that too. We just need to install “Hydrogen” using the same method as before. Once Hydrogen is installed you can use it by:
 Run the command on which your cursor is on using Ctrl&#43;Enter.
 Select any Kernel from the Kernel Selection Screen. I select pyt kernel from the list.
 Now I can continue working in pyt kernel.
  Runnin command using Ctrl&#43;Enter will ask you which environment to use.
Sometimes it might happen that you don’t see an environment/kernel in Atom. In such cases, you can install ipykernel to make that kernel visible to Jupyter as well as Atom.
Here is how to make a new kernel and make it visible in Jupyter/Atom:
conda create -n exampleenv python=3.7 conda activate exampleenv conda install -c anaconda ipykernel python -m ipykernel install --user --name=exampleenv  Once you run these commands, your kernel will be installed. You can now update the Atom’s kernel list by using:
Ctrl&#43;Shift&#43;P &amp;gt;Hydrogen: Update Kernels
And your kernel should now be available in your Atom editor.
6. Search Stack Overflow from your Text Editor Stack Overflow is an integral part of any developer’s life. But you know what the hassle is? To leave the coding environment and go to Chrome to search for every simple thing you need to do. And we end up doing it back and forth throughout the day. So, what if we can access Stack Overflow from Atom? You can do precisely that through the “ask-stack” package, which lets one search for questions on SO. We can access it using Ctrl&#43;Alt&#43;A
Access Stack Overflow in Atom using Ctrl&#43;Alt&#43;A.
Some other honorable mentions of packages you could use are:
 Teletype: Do Pair Coding.
 Linter: Checks code for Stylistic and Programmatic errors. To enable linting in Python, You can use “linter” and “python-linters”.
 Highlight Selected: Highlight all occurrences of a text by double-clicking or selecting the text with a cursor.
 Atom-File-Icons: Provides you with file icons in the left side tree view. Looks much better than before, right?
  Icons for files
Conclusion In this post, I talked about how I use Atom in my Python Development flow.
There are a plethora of other packages in Atom which you may like, and you can look at them to make your environment even more customizable. Or one can even write their own packages as well as Atom is called as the “Most Hackable Editor”.
If you want to learn about Python and not exactly a Python editor, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out. Also, here are my course recommendations to become a Data Scientist in 2020.
I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</title>
      <link>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/deployment_fastapi/main.png"></media:content>
      

      
      <description>Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS.</description>

      <content:encoded>  
        
        <![CDATA[  Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS. None of those seemed particularly great or complete to me.
So, I tried to do this myself using some help from FastAPI documentation. In this post, we will look at predominantly four things:
 Setting Up an Amazon Instance
 Creating a FastAPI API for Object Detection
 Deploying FastAPI using Docker
 An End to End App with UI
  So, without further ado, let’s get started.
You can skip any part you feel you are versed with though I would expect you to go through the whole post, long as it may be, as there’s a lot of interconnection between concepts.
1. Setting Up Amazon Instance Before we start with using the Amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a single sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine.” Here we are trying to set up the machine where we will deploy our FastAPI API.
 In the first step, you need to choose the AMI template for the machine. I am selecting the 18.04 Ubuntu Server since Ubuntu.
   In the second step, I select the t2.xlarge machine, which has 4 CPUs and 16GB RAM rather than the free tier since I want to use an Object Detection model and will need some resources.   Keep pressing Next until you reach the “6. Configure Security Group” tab. This is the most crucial step here. You will need to add a rule with Type: “HTTP” and Port Range:80.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch, you might need to create a new key pair. Here I am creating a new key pair named fastapi and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state; it should be showing “Running.”   Also, to note here are the Public DNS(IPv4) address and the IPv4 public IP. We will need it to connect to this machine. For me, they are:  Public DNS (IPv4): ec2-18-237-28-174.us-west-2.compute.amazonaws.com IPv4 Public IP: 18.237.28.174   Once you have that run the following commands in the folder, you saved the fastapi.pem file. If the file is named fastapi.txt you might need to rename it to fastapi.pem.  # run fist command if fastapi.txt gets downloaded. # mv fastapi.txt fastapi.pem chmod 400 fastapi.pem ssh -i &amp;quot;fastapi.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Now we have got our Amazon instance up and running. We can move on here to the real part of the post.
2. Creating a FastAPI API for Object Detection Before we deploy an API, we need to have an API with us, right? In one of my last posts, I had written a simple tutorial to understand FastAPI and API basics. Do read the post if you want to understand FastAPI basics.
So, here I will try to create an Image detection API. As for how to pass the Image data to the API? The idea is — What is an image but a string? An image is just made up of bytes, and we can encode these bytes as a string. We will use the base64 string representation, which is a popular way to get binary data to ASCII characters. And, we will pass this string representation to give an image to our API.
A. Some Image Basics: What is Image, But a String? So, let us first see how we can convert an Image to a String. We read the binary data from an image file using the ‘rb’ flag and turn it into a base64 encoded data representation using the base64.b64encode function. We then use the decode to utf-8 function to get the base encoded data into human-readable characters. Don’t worry if it doesn’t make a lot of sense right now. Just understand that any data is binary, and we can convert binary data to its string representation using a series of steps.
As a simple example, if I have a simple image like below, we can convert it to a string using:
import base64 with open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;, &amp;#34;rb&amp;#34;) as image_file: base64str = base64.b64encode(image_file.read()).decode(&amp;#34;utf-8&amp;#34;) Here I have got a string representation of a file named dog_with_ball.png on my laptop.
Great, we now have a string representation of an image. And, we can send this string representation to our FastAPI. But we also need to have a way to read an image back from its string representation. After all, our image detection API using PyTorch and any other package needs to have an image object that they can predict, and those methods don’t work on a string.
So here is a way to create a PIL image back from an image’s base64 string. Mostly we just do the reverse steps in the same order. We encode in ‘utf-8’ using .encode. We then use base64.b64decode to decode to bytes. We use these bytes to create a bytes object using io.BytesIO and use Image.open to open this bytes IO object as a PIL image, which can easily be used as an input to my PyTorch prediction code.*** Again simply, it is just a way to convert base64 image string to an actual image.***
import base64 import io from PIL import Image def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img So does this function work? Let’s see for ourselves. We can use just the string to get back the image.
And we have our happy dog back again. Looks better than the string.
B. Writing the Actual FastAPI code So, as now we understand that our API can get an image as a string from our user, let’s create an object detection API that makes use of this image as a string and outputs the bounding boxes for the object with the object classes as well.
Here, I will be using a Pytorch pre-trained fasterrcnn_resnet50_fpn detection model from the torchvision.models for object detection, which is trained on the COCO dataset to keep the code simple, but one can use any model. You can look at these posts if you want to train your custom image classification or image detection model using Pytorch.
Below is the full code for the FastAPI. Although it may look long, we already know all the parts. In this code, we essentially do the following steps:
 Create our fast API app using the FastAPI() constructor.
 Load our model and the classes it was trained on. I got the list of classes from the PyTorch docs.
 We also defined a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user. Here the end-user gives the base64str and some score threshold for object detection prediction.
 We add a function called base64str_to_PILImage which does just what it is named.
 And we write a predict function called get_predictionbase64 which returns a dict of bounding boxes and classes using a base64 string representation of an image and a threshold as an input. We also add @app.put(“/predict”) on top of this function to define our endpoint. If you need to understand put and endpoint refer to my previous post on FastAPI.
  from fastapi import FastAPI from pydantic import BaseModel import torchvision from torchvision import transforms import torch from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from PIL import Image import numpy as np import cv2 import io, json import base64 app = FastAPI() # load a pre-trained Model and convert it to eval mode. # This model loads just once when we start the API. model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) COCO_INSTANCE_CATEGORY_NAMES = [ &amp;#39;__background__&amp;#39;, &amp;#39;person&amp;#39;, &amp;#39;bicycle&amp;#39;, &amp;#39;car&amp;#39;, &amp;#39;motorcycle&amp;#39;, &amp;#39;airplane&amp;#39;, &amp;#39;bus&amp;#39;, &amp;#39;train&amp;#39;, &amp;#39;truck&amp;#39;, &amp;#39;boat&amp;#39;, &amp;#39;traffic light&amp;#39;, &amp;#39;fire hydrant&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;stop sign&amp;#39;, &amp;#39;parking meter&amp;#39;, &amp;#39;bench&amp;#39;, &amp;#39;bird&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;horse&amp;#39;, &amp;#39;sheep&amp;#39;, &amp;#39;cow&amp;#39;, &amp;#39;elephant&amp;#39;, &amp;#39;bear&amp;#39;, &amp;#39;zebra&amp;#39;, &amp;#39;giraffe&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;backpack&amp;#39;, &amp;#39;umbrella&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;handbag&amp;#39;, &amp;#39;tie&amp;#39;, &amp;#39;suitcase&amp;#39;, &amp;#39;frisbee&amp;#39;, &amp;#39;skis&amp;#39;, &amp;#39;snowboard&amp;#39;, &amp;#39;sports ball&amp;#39;, &amp;#39;kite&amp;#39;, &amp;#39;baseball bat&amp;#39;, &amp;#39;baseball glove&amp;#39;, &amp;#39;skateboard&amp;#39;, &amp;#39;surfboard&amp;#39;, &amp;#39;tennis racket&amp;#39;, &amp;#39;bottle&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;wine glass&amp;#39;, &amp;#39;cup&amp;#39;, &amp;#39;fork&amp;#39;, &amp;#39;knife&amp;#39;, &amp;#39;spoon&amp;#39;, &amp;#39;bowl&amp;#39;, &amp;#39;banana&amp;#39;, &amp;#39;apple&amp;#39;, &amp;#39;sandwich&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;broccoli&amp;#39;, &amp;#39;carrot&amp;#39;, &amp;#39;hot dog&amp;#39;, &amp;#39;pizza&amp;#39;, &amp;#39;donut&amp;#39;, &amp;#39;cake&amp;#39;, &amp;#39;chair&amp;#39;, &amp;#39;couch&amp;#39;, &amp;#39;potted plant&amp;#39;, &amp;#39;bed&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;dining table&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;toilet&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;tv&amp;#39;, &amp;#39;laptop&amp;#39;, &amp;#39;mouse&amp;#39;, &amp;#39;remote&amp;#39;, &amp;#39;keyboard&amp;#39;, &amp;#39;cell phone&amp;#39;, &amp;#39;microwave&amp;#39;, &amp;#39;oven&amp;#39;, &amp;#39;toaster&amp;#39;, &amp;#39;sink&amp;#39;, &amp;#39;refrigerator&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;book&amp;#39;, &amp;#39;clock&amp;#39;, &amp;#39;vase&amp;#39;, &amp;#39;scissors&amp;#39;, &amp;#39;teddy bear&amp;#39;, &amp;#39;hair drier&amp;#39;, &amp;#39;toothbrush&amp;#39; ] model.eval() # define the Input class class Input(BaseModel): base64str : str threshold : float def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img @app.put(&amp;#34;/predict&amp;#34;) def get_predictionbase64(d:Input): &amp;#39;&amp;#39;&amp;#39; FastAPI API will take a base 64 image as input and return a json object &amp;#39;&amp;#39;&amp;#39; # Load the image img = base64str_to_PILImage(d.base64str) # Convert image to tensor transform = transforms.Compose([transforms.ToTensor()]) img = transform(img) # get prediction on image pred = model([img]) pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][&amp;#39;labels&amp;#39;].numpy())] pred_boxes = [[(float(i[0]), float(i[1])), (float(i[2]), float(i[3]))] for i in list(pred[0][&amp;#39;boxes&amp;#39;].detach().numpy())] pred_score = list(pred[0][&amp;#39;scores&amp;#39;].detach().numpy()) pred_t = [pred_score.index(x) for x in pred_score if x &amp;gt; d.threshold][-1] pred_boxes = pred_boxes[:pred_t&#43;1] pred_class = pred_class[:pred_t&#43;1] return {&amp;#39;boxes&amp;#39;: pred_boxes, &amp;#39;classes&amp;#39; : pred_class} C. Local Before Global: Test the FastAPI code locally Before we move on to AWS, let us check if the code works on our local machine. We can start the API on our laptop using:
uvicorn fastapiapp:app --reload  The above means that your API is now running on your local server, and the &amp;ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &amp;ndash;reload flag when you put the API in production.
You should see something like:
You can now try to access this API and see if it works using the requests module:
import requests,json payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&amp;#34;,data = payload) data_dict = response.json() And so we get our results using the API. This image contains a dog and a sports ball. We also have corner 1 (x1,y1) and corner 2 (x2,y2) coordinates of our bounding boxes.
D. Lets Visualize Although not strictly necessary, we can visualize how the results look in our Jupyter notebook:
from PIL import Image import numpy as np import cv2 import matplotlib.pyplot as plt def PILImage_to_cv2(img): return np.asarray(img) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) # Write the prediction class plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() img = Image.open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) Here is the output:
Here you will note that I got the image from the local file system, and that sort of can be considered as cheating as we don’t want to save every file that the user sends to us through a web UI. We should have been able to use the same base64string object that we also had to create this image. Right?
Not to worry, we could do that too. Remember our base64str_to_PILImage function? We could have used that also.
img = base64str_to_PILImage(base64str) drawboundingbox(img, data_dict[&#39;boxes&#39;], data_dict[&#39;classes&#39;])  That looks great. We have our working FastAPI, and we also have our amazon instance. We can now move on to Deployment.
3. Deployment on Amazon ec2 Till now, we have created an AWS instance and, we have also created a FastAPI that takes as input a base64 string representation of an image and returns bounding boxes and the associated class. But all the FastAPI code still resides in our local machine. How do we put it on the ec2 server? And run predictions on the cloud.
A. Install Docker We will deploy our app using docker, as is suggested by the fastAPI creator himself. I will try to explain how docker works as we go. The below part may look daunting but it just is a series of commands and steps. So stay with me.
We can start by installing docker using:
sudo apt-get update sudo apt install docker.io  We then start the docker service using:
sudo service docker start  B. Creating the folder structure for docker └── dockerfastapi ├── Dockerfile ├── app │ └── main.py └── requirements.txt  Here dockerfastapi is our project’s main folder. And here are the different files in this folder:
i. requirements.txt: Docker needs a file, which tells it which all libraries are required for our app to run. Here I have listed all the libraries I used in my Fastapi API.
numpy opencv-python matplotlib torchvision torch fastapi pydantic  ii. Dockerfile: The second file is Dockerfile.
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app COPY requirements.txt . RUN pip --no-cache-dir install -r requirements.txt  How Docker works?: You can skip this section, but it will help to get some understanding of how docker works.
The dockerfile can be thought of something like a sh file,which contains commands to create a docker image that can be run in a container. One can think of a docker image as an environment where everything like Python and Python libraries is installed. A container is a unit which is just an isolated box in our system that uses a dockerimage. The advantage of using docker is that we can create multiple docker images and use them in multiple containers. For example, one image might contain python36, and another can contain python37. And we can spawn multiple containers in a single Linux server.
Our Dockerfile contains a few things:
 FROM command: Here the first line FROM specifies that we start with tiangolo’s (FastAPI creator) Docker image. As per his site: “This image has an “auto-tuning” mechanism included so that you can just add your code and get that same high performance automatically. And without making sacrifices”. What we are doing is just starting from an image that installs python3.7 for us along with some added configurations for uvicorn and gunicorn ASGI servers and a start.sh file for ASGI servers automatically. For adventurous souls, particularly commandset1 and commandset2 get executed through a sort of a daisy-chaining of commands.
 COPY command: We can think of a docker image also as a folder that contains files and such. Here we copy our app folder and the requirements.txt file, which we created earlier to our docker image.
 RUN Command: We run pip install command to install all our python dependencies using the requirements.txt file that is now on the docker image.
  iii. main.py: This file contains the fastapiapp.py code we created earlier. Remember to keep the name of the file main.py only.
C. Docker Build We have got all our files in the required structure, but we haven’t yet used any docker command. We will first need to build an image containing all dependencies using Dockerfile.
We can do this simply by:
sudo docker build -t myimage .  This downloads, copies and installs some files and libraries from tiangolo’s image and creates an image called myimage. This myimage has python37 and some python packages as specified by requirements.txt file.
We will then just need to start a container that runs this image. We can do this using:
sudo docker run -d --name mycontainer -p 80:80 myimage  This will create a container named mycontainer which runs our docker image myimage. The part 80:80 connects our docker container port 80 to our Linux machine port 80.
And actually that’s it. At this point, you should be able to open the below URL in your browser.
# &amp;lt;IPV4 public IP&amp;gt;/docs URL: 18.237.28.174/docs  And we can check our app programmatically using:
payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://18.237.28.174/predict](http://18.237.28.174/predict)&amp;#34;,data = payload) data_dict = response.json() print(data_dict) &amp;gt; # Yup, finally our API is deployed.
D. Troubleshooting as the real world is not perfect All the above was good and will just work out of the box if you follow the exact instructions, but the real world doesn’t work like that. You will surely get some errors along the way and would need to debug your code. So to help you with that, some docker commands may come handy:
 Logs: When we ran our container using sudo docker run we don’t get a lot of info, and that is a big problem when you are debugging. You can see the real-time logs using the below command. If you see an error here, you will need to change your code and build the image again.   sudo docker logs -f mycontainer   Starting and Stopping Docker: Sometimes, it might help just to restart your docker. In that case, you can use:   sudo service docker stop sudo service docker start   Listing images and containers: Working with docker, you will end up creating images and containers, but you won’t be able to see them in the working directory. You can list your images and containers using:   sudo docker container ls sudo docker image ls   Deleting unused docker images or containers: You might need to remove some images or containers as these take up a lot of space on the system. Here is how you do that.   # the prune command removes the unused containers and images sudo docker system prune # delete a particular container sudo docker rm mycontainer # remove myimage sudo docker image rm myimage # remove all images sudo docker image prune — all   Checking localhost:The Linux server doesn’t have a browser, but we can still see the browser output though it’s a little ugly:   curl localhost   Develop without reloading image again and again: For development, it’s useful to be able just to change the contents of the code on our machine and test it live, without having to build the image every time. In that case, it’s also useful to run the server with live auto-reload automatically at every code change. Here, we use our app directory on our Linux machine, and we replace the default (/start.sh) with the development alternative /start-reload.sh during development. After everything looks fine, we can build our image again run it inside the container.   sudo docker run -d -p 80:80 -v $(pwd):/app myimage /start-reload.sh  If this doesn’t seem sufficient, adding here a docker cheat sheet containing useful docker commands:
4. An End to End App with UI We are done here with our API creation, but we can also create a UI based app using Streamlit using our FastAPI API. This is not how you will do it in a production setting (where you might have developers making apps using react, node.js or javascript)but is mostly here to check the end-to-end flow of how to use an image API. I will host this barebones Streamlit app on local rather than the ec2 server, and it will get the bounding box info and classes from the FastAPI API hosted on ec2.
If you need to learn more about how streamlit works, you can check out this post. Also, if you would want to deploy this streamlit app also to ec2, here is a tutorial again.
Here is the flow of the whole app with UI and FastAPI API on ec2:
Project Architecture
The most important problems we need to solve in our streamlit app are:
How to get an image file from the user using Streamlit? A. Using File uploader: We can use the file uploader using:
bytesObj = st.file_uploader(“Choose an image file”) The next problem is, what is this bytesObj we get from the streamlit file uploader? In streamlit, we will get a bytesIO object from the file_uploader and we will need to convert it to base64str for our FastAPI app input. This can be done using:
def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) base64str = bytesioObj_to_base64str(bytesObj) B. Using URL: We can also get an image URL from the user using text_input.
url = st.text_input(‘Enter URL’) We can then get image from URL in base64 string format using the requests module and base64 encode and utf-8 decode:
def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) base64str = ImgURL_to_base64str(url) And here is the complete code of our Streamlit app. You have seen most of the code in this post already.
import streamlit as st import base64 import io import requests,json from PIL import Image import cv2 import numpy as np import matplotlib.pyplot as plt import requests import random # use file uploader object to recieve image # Remember that this bytes object can be used only once def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) # Image conversion functions def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img def PILImage_to_cv2(img): return np.asarray(img) def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() st.markdown(&amp;#34;&amp;lt;h1&amp;gt;Our Object Detector App using FastAPI&amp;lt;/h1&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) bytesObj = st.file_uploader(&amp;#34;Choose an image file&amp;#34;) st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h2&amp;gt;or&amp;lt;/h2&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) url = st.text_input(&amp;#39;Enter URL&amp;#39;) if bytesObj or url: # In streamlit we will get a bytesIO object from the file_uploader # and we convert it to base64str for our FastAPI if bytesObj: base64str = bytesioObj_to_base64str(bytesObj) elif url: base64str = ImgURL_to_base64str(url) # We will also create the image in PIL Image format using this base64 str # Will use this image to show in matplotlib in streamlit img = base64str_to_PILImage(base64str) # Run FastAPI payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;http://18.237.28.174/predict&amp;#34;,data = payload) data_dict = response.json() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;App Result&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) st.pyplot() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;FastAPI Response&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) st.write(data_dict) We can run this streamlit app in local using:
streamlit run streamlitapp.py  And we can see our app running on our localhost:8501. Works well with user-uploaded images as well as URL based images. Here is a cat image for some of you cat enthusiasts as well.
 So that’s it. We have created a whole workflow here to deploy image detection models through FastAPI on ec2 and utilizing those results in Streamlit. I hope this helps your woes around deploying models in production. You can find the code for this post as well as all my posts at my GitHub repository.
Let me know if you like this post and if you would like to include Docker or FastAPI or Streamlit in your day to day deployment needs. I am also looking to create a much detailed post on Docker so follow me up to stay tuned with my writing as well. Details below.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman’s Guide for Data Scientists to create APIs in minutes</title>
      <link>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/fastapi_for_data_scientists/main.png"></media:content>
      

      
      <description>Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.
As Data Science and Web developers try to collaborate, API’s become an essential piece of the puzzle to make codes as well as skills more modular.</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.
As Data Science and Web developers try to collaborate, API’s become an essential piece of the puzzle to make codes as well as skills more modular. In fact, in the same way, that a data scientist can’t be expected to know much about Javascript or nodeJS, a frontend developer should be able to get by without knowing any Data Science Language. And APIs do play a considerable role in this abstraction.
But, APIs are confusing. I myself have been confused a lot while creating and sharing them with my development teams who talk in their API terminology like GET request, PUT request, endpoint, Payloads, etc.
This post will be about simplifying and understanding how APIs work, explaining some of the above terms, and creating an API using the excellent API building framework called FastAPI, which makes creating APIs a breeze.
What is an API? Before we go any further, we need to understand what an API is. According to Wikipedia:
 An application programming interface (API) is a computing interface which defines interactions between multiple software intermediaries. It defines the kinds of calls or requests that can be made, how to make them, the data formats that should be used, the conventions to follow, etc.
 The way I like to understand an API is that it’s an “online function,” a function that I can call online.
For example:
I can have a movie API, which returns me names of drama movies when I pass the “animation” genre as input.
The advantage of using such a sort of mechanism is that the API user doesn’t get access to the whole dataset or source code and yet they can get all the information they need. This is how many services on the internet like Amazon Rekognition, which is an image and video API, or Google Natural Language API, which is an NLP API works. They provide us access to some great functions without letting us have the source code, which is often valuable and kept hidden. For example, I can send an image to Amazon Rekognition API, and it can provide me with Face detection and Analysis.
For example, here is a free API floated by Open Movie DB, which lets us search for movies using parameters:
[http://www.omdbapi.com/?i=tt3896198&amp;amp;apikey=9ea43e94](http://www.omdbapi.com/?i=tt3896198&amp;amp;apikey=9ea43e94)  Here I provided the IMDB id for the movie Guardians of the Galaxy 2, using the i parameter for the API. If you open this link in your browser, you will see the whole information of the movie as per the Open Movie Database
Output from OMDB
But before we go any further, let’s understand some terms:
 Endpoint: In the above API call, the endpoint is : http://www.omdbapi.com/ . Simply this is the location of where the function code is running.
 API Access Key: Most of the public APIs will have some access key, which you can request. For OMDB API, I had to register and get the API key which is 9ea43e94.
 ? Operator:This operator is used to specify the parameters we want to send to the API or our “online function.” Here we give two params to our API i.e., IMDB movie ID and API Access Key using the ? operator. Since there are multiple inputs, we use &amp;amp; operator also.
  Why FastAPI? “If you’re looking to learn one modern framework for building REST APIs, check out FastAPI […] It’s fast, easy to use and easy to learn […]” — spaCy creators
While Python has many frameworks to build APIs, the most common being Flask and Tornado, FastAPI is much better than available alternatives in its ease of usage as it seems much more pythonic in comparison with Flask.
Also, FastAPI is fast. As the Github docs say, “Very high performance, on par with NodeJS and Go.” We can also check the latency benchmarks for ourselves.
That is around a speedup by a factor of 2 when compared to Flask and that too without a lot of code change. This means a huge deal when it comes to building an API that can serve millions of customers as it can reduce production efforts and also use less expensive hardware to serve.
So enough of comparison and talk, let’s try to use FastAPI to create our API.
How to write an API with FastAPI? One of the most common use cases for Data Science is how to create an API for getting a model’s prediction? Let us assume that we have a Titanic Survival model in place that predicts if a person will survive or not. And, it needs a person’s age and sex as input params to predict. We will create this API using FastAPI in two ways: GET and PUT. Don’t worry; I will explain each as we go.
What is GET? — In a GET request, we usually try to retrieve data using query parameters that are embedded in the query string itself. For example, in the OMDB API, we use the GET request to specify the movie id and access key as part of the query itself.
What is PUT? — An alternative to the GET request is the PUT request, where we send parameters using a payload, as we will see in the second method. The payload is not part of the query string, and thus PUT is more secure. It will become more clear when you see the second part.
But before we go any further, we need to install FastAPI and uvicorn ASGI server with:
pip install fastapi pip install uvicorn  1. The GET Way: A simple FastAPI method to writing a GET API for our titanic model use case is as follows:
from fastapi import FastAPI app = FastAPI() @app.get(&amp;quot;/predict&amp;quot;) def predict_complex_model(age: int,sex:str): # Assume a big and complex model here. For this test I am using a simple rule based model if age&amp;lt;10 or sex==&#39;F&#39;: return {&#39;survived&#39;:1} else: return {&#39;survived&#39;:0}  Save the above code in a file named fastapiapp.py and then you can run it using the below command on terminal.
$ uvicorn fastapiapp:app --reload  The above means that your API is now running on your server, and the &amp;ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &amp;ndash;reload flag when you put the API in production. Now you can visit the below path in your browser, and you will get the prediction results:
[http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M](http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M)  What happens is as you hit the command in your browser, it calls the http://127.0.0.1:8000/predict endpoint which in turn calls the associated method predict_complex_model with the with params age=10 and sex=&amp;rsquo;M&amp;rsquo;
So, it allows us to use our function from a browser, but that’s still not very helpful. Your developer friend needs to use your predict function to show output on a frontend website. How can you provide him with access to this function?
It is pretty simple. If your developer friend also uses Python, for example, he can use the requests module like below:
import requests age = 15 sex = &amp;quot;F&amp;quot; response = requests.get(f&amp;quot;[http://127.0.0.1:8000/predict?age={age}&amp;amp;sex={](http://127.0.0.1:8000/predict?age=10&amp;amp;sex=M)sex}&amp;quot;) output = response.json()  So we can get the output from the running API(on the server) into our Python Program. A Javascript user would use Javascript Request Library, and a nodeJS developer will use something similar to do this in nodeJS. We will just need to provide them with the endpoint and parameters required.
To test your API, you could also go to the:
[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  Where you will find a GUI way to test your API.
But as we said earlier, THIS IS NOT SECURE as GET parameters are passed via URL. This means that parameters get stored in server logs and browser history. This is not intended. Further, this toy example just had two input parameters, so we were able to do it this way, think of a case where we need to provide many parameters to our predict function.
In such a case or I dare say in most of the cases, we use the PUT API.
2. The PUT Way Using the PUT API, we can call any function by providing a payload to the function. A payload is nothing but a JSON dictionary of input parameters that doesn’t get appended to the query string and is thus much more secure than GET.
Here is the minimal example where we do that same thing as before using PUT. We just change the content of fastapiapp.py to:
from fastapi import FastAPI from pydantic import BaseModel class Input(BaseModel): age : int sex : str app = FastAPI() [@app](http://twitter.com/app).put(&amp;quot;/predict&amp;quot;) def predict_complex_model(d:Input): if d.age&amp;lt;10 or d.sex==&#39;F&#39;: return {&#39;survived&#39;:1} else: return {&#39;survived&#39;:0}  note that we use app.put here in place of app.get previously. We also needed to provide a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user while previously in GET, we validated the inputs using the function parameter list. Also, this time you won’t be able to see your content using a URL on the web. For example, using the browser to point to the endpoint location gives:
So, we can check using the programmatic way using requests in Python again:
import requests,json payload = json.dumps({ &amp;quot;age&amp;quot;: 10, &amp;quot;sex&amp;quot;: &amp;quot;F&amp;quot; }) response = requests.put(&amp;quot;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&amp;quot;,data = payload) response.json()  Notice that we use requests.put here and we provide the payload using the data param in the requests.put function and we also make use of json library to convert our payload to JSON from a dict object.
We could also have used the GUI way as before using:
[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  And, we are done with creating our API. It was simple for a change.
FastAPI makes the API creation, which used to be one of the dreaded parts of the Data Science process, much more intuitive, easy, and Fast.
You can find the code for this post as well as all my posts at my GitHub repository.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title>
      <link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dlrig/main.png"></media:content>
      

      
      <description>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.</description>

      <content:encoded>  
        
        <![CDATA[  Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.
Now, every time I create the whole deep learning setup from an installation viewpoint, I end up facing similar challenges. It’s like running around in circles with all these various dependencies and errors. This time also I had to try many things before the whole configuration came to life without errors.
So this time, I made it a point to document everything while installing all the requirements and their dependencies in my own system.
This post is about setting up your own Linux Ubuntu 18.04 system for deep learning with everything you might need.
If a pre-built deep learning system is preferred, I can recommend Exxact’s line of workstations and servers.
I assume that you have a fresh Ubuntu 18.04 installation. I am taking inspiration from Slav Ivanov’s excellent post in 2017 on creating a Deep Learning box. You can call it the 2020 version for the same post from a setup perspective, but a lot of the things have changed from then, and there are a lot of caveats with specific CUDA versions not supported by Tensorflow and Pytorch.
Starting up Before we do anything with our installation, we need to update our Linux system to the latest packages. We can do this simply by using:
sudo apt-get update sudo apt-get --assume-yes upgrade sudo apt-get --assume-yes install tmux build-essential gcc g&#43;&#43; make binutils sudo apt-get --assume-yes install software-properties-common sudo apt-get --assume-yes install git  The Process So now we have everything set up we want to install the following four things:
 GPU Drivers: Why is your PC not supporting high graphic resolutions? Or how would your graphics cards talk to your python interfaces?
 CUDA: A layer to provide access to the GPU’s instruction set and parallel computation units. In simple words, it allows us a way to write code for GPUs
 CuDNN: a library that provides Primitives for Deep Learning Network
 Pytorch, Tensorflow, and Rapids: higher-level APIs to code Deep Neural Networks
  1. GPU Drivers The first step is to add the latest NVIDIA drivers. You can choose the GPU product type, Linux 64 bit, and download Type as “Linux Long-Lived” for the 18.04 version.
Clicking on search will take you to a downloads page:
From where you can download the driver file NVIDIA-Linux-x86_64–440.44.run and run it using:
chmod &#43;x NVIDIA-Linux-x86_64–440.44.run sudo sh NVIDIA-Linux-x86_64–440.44.run  For you, the file may be named differently, depending on the latest version.
2. CUDA We will now need to install the CUDA toolkit. Somehow the CUDA toolkit 10.2 is still not supported by Pytorch and Tensorflow, so we will go with CUDA Toolkit 10.1, which is supported by both.
Also, the commands on the product page for CUDA 10.1 didn’t work for me and the commands I ended up using are:
sudo apt-key adv --fetch-keys [http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub](http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub) &amp;amp;&amp;amp; echo &amp;quot;deb [https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64) /&amp;quot; | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -o Dpkg::Options::=&amp;quot;--force-overwrite&amp;quot; install cuda-10-1 cuda-drivers  The next step is to create the LD_LIBRARY_PATH and append to the PATH variable the path where CUDA got installed. Just run this below command on your terminal.
echo &#39;export PATH=/usr/local/cuda-10.1/bin${PATH:&#43;:${PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:&#43;:${LD_LIBRARY_PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc &amp;amp;&amp;amp; sudo ldconfig  After this, one can check if CUDA is installed correctly by using:
nvcc --version  As you can see, the CUDA Version is 10.1 as we wanted. Also, check if you can use the command:
nvidia-smi  For me, it showed an error when I used it the first time, but a simple reboot solved the issue. And both my NVIDIA graphic cards show up in all their awesome glory. Don’t worry that the display says the CUDA version supported is 10.2. I was also confused, but it is just the maximum CUDA version supported by the graphics driver that is shown in nvidia-smi.
3.CuDNN What is the use of all these libraries if we are not going to train neural nets? CuDNN provides various primitives for Deep Learning, which are later used by PyTorch/TensorFlow.
But we first need to get a developer account first to install CuDNN. Once you fill-up the signup form, you will see the screen below. Select the cuDNN version that applies to your CUDA version. For me, the CUDA version is 10.1, so I select the second one.
Once you select the appropriate CuDNN version the screen expands:
For my use case, I needed to download three files for Ubuntu 18.04:
[cuDNN Runtime Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Developer Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-dev_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-doc_7.6.5.32-1%2Bcuda10.1_amd64.deb)  After downloading these files, you can install using these commands. You can also see the exact commands if anything changes in the future:
# Install the runtime library: sudo dpkg -i libcudnn7_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the developer library: sudo dpkg -i libcudnn7-dev_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the code samples and cuDNN User Guide(Optional): sudo dpkg -i libcudnn7-doc_7.6.5.32-1&#43;cuda10.1_amd64.deb  4. Anaconda, Pytorch, Tensorflow, and Rapids And finally, we reach the crux. We will install the software which we will interface with most of the times.
We need to install Python with virtual environments. I have downloaded python3 as it is the most stable version as of now, and it is time to say goodbye to Python 2.7. It was great while it lasted. And we will also install Pytorch and Tensorflow. I prefer them both for specific tasks as applicable.
You can go to the anaconda distribution page and download the package.
Once downloaded you can simply run the shell script:
sudo sh Anaconda3-2019.10-Linux-x86_64.sh  You will also need to run these commands on your shell to add some commands to your ~/.bashrc file, and update the conda distribution with the latest libraries versions.
cat &amp;gt;&amp;gt; ~/.bashrc &amp;lt;&amp;lt; &#39;EOF&#39; export PATH=$HOME/anaconda3/bin:${PATH} EOF source .bashrc conda upgrade -y --all  The next step is creating a new environment for your deep learning pursuits or using an existing one. I created a new Conda environment using:
conda create --name py37  Here py37 is the name we provide to this new conda environment. You can activate this conda environment using:
conda activate py37  You should now be able to see something like:
Notice the py37 at the start of command in terminal
We can now add all our required packages to this environment using pip or conda. The latest version 1.3, as seen from the pytorch site, is not yet available for CUDA 10.2, as I already mentioned, so we are in luck with CUDA 10.1. Also, we will need to specify the version of TensorFlow as 2.1.0, as this version was built using 10.1 CUDA.
I also install RAPIDS, which is a library to get your various data science workloads to GPUs. Why use GPUs only for deep learning and not for Data processing? You can get the command to install rapids from the rapids release selector:
sudo apt install python3-pip conda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.11 python=3.7 cudatoolkit=10.1 pip install torchvision  Since PyTorch installation interfered with TensorFlow, I installed TensorFlow in another environment.
conda create --name tf conda activate tf pip install --upgrade tensorflow  Now we can check if the TF and Pytorch installations are correctly done by using the below commands in their own environments:
# Should print True python3 -c &amp;quot;import tensorflow as tf; print(tf.test.is_gpu_available())&amp;quot; # should print cuda python3 -c &amp;quot;import torch; print(torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;))&amp;quot;  If the install is showing some errors for TensorFlow or the GPU test is failing, you might want to add these two additional lines at the end of your bashrc file and restart the terminal:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 export CUDA_HOME=/usr/local/cuda  You might also want to install jupyter lab or jupyter notebook. Thanks to the developers, the process is as easy as just running jupyter labor jupyter notebook in your terminal, whichever you do prefer. I personally like notebook better without all the unnecessary clutter.
Conclusion In this post, I talked about all the software you are going to need to install in your deep learning rig without hassle.
You might still need some help and face some problems for which my best advice would be to check out the different NVIDIA and Stack Overflow forums.
So we have got our deep learning rig setup, and its time for some tests now. In the next few posts, I am going to do some benchmarking on the GPUs and will try to write more on various deep Learning libraries one can include in their workflow. So stay tuned.
Continue Learning If you want to learn more about Deep Learning, here is an excellent course. You can start for free with the 7-day Free Trial.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to run your ML model Predictions 50 times faster?</title>
      <link>https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/hummingbird_faster_ml_preds/main.png"></media:content>
      

      
      <description>With the advent of so many computing and serving frameworks, it is getting stressful day by day for the developers to put a model into production. If the question of what model performs best on my data was not enough, now the question is what framework to choose for serving a model trained with Sklearn or LightGBM or PyTorch. And new frameworks are being added as each day passes.</description>

      <content:encoded>  
        
        <![CDATA[  With the advent of so many computing and serving frameworks, it is getting stressful day by day for the developers to put a model into production. If the question of what model performs best on my data was not enough, now the question is what framework to choose for serving a model trained with Sklearn or LightGBM or PyTorch. And new frameworks are being added as each day passes.
So is it imperative for a Data Scientist to learn a different framework because a Data Engineer is comfortable with that, or conversely, does a Data Engineer need to learn a new platform that the Data Scientist favors?
Add to that the factor of speed and performance that these various frameworks offer, and the question suddenly becomes even more complicated.
So, I was pleasantly surprised when I came across the Hummingbird project on Github recently, which aims to answer this question or at least takes a positive step in the right direction.
So, What is HummingBird? As per their documentation:
 Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models. Thanks to Hummingbird, users can benefit from:
(1) all the current and future optimizations implemented in neural network frameworks;
(2) native hardware acceleration;
(3) having a unique platform to support both traditional and neural network models, and have all of this
(4) without having to re-engineer their models.
 Put even more simply; you can now convert your models written in Scikit-learn or Xgboost or LightGBM into PyTorch models and gain the performance benefits of Pytorch while inferencing.
As of right now, Here is the list of operators Hummingbird supports with more on the way.
A Simple Example We can start by installing Hummingbird, which is as simple as:
pip install hummingbird-ml  To use hummingbird, I will begin with a minimal example on a small random classification Dataset. We start by creating a sample dataset with 100,000 rows and using a RandomForestClassifier on top of that.
import numpy as np from sklearn.ensemble import RandomForestClassifier from hummingbird.ml import convert # Create some random data for binary classification from sklearn import datasets X, y = datasets.make_classification(n_samples=100000, n_features=28) # Create and train a model (scikit-learn RandomForestClassifier in this case) skl_model = RandomForestClassifier(n_estimators=1000, max_depth=10) skl_model.fit(X, y)  What hummingbird helps us with is to convert this sklearn model into a PyTorch model by just using the simple command:
# Using Hummingbird to convert the model to PyTorch model = convert(skl_model, &#39;pytorch&#39;) print(type(model)) -------------------------------------------------------- hummingbird.ml._container.PyTorchBackendModel  We can now load our new Pytorch model to GPU using:
model.to(&#39;cuda&#39;)  This is great. So, we can convert from sklearn model to a PyTorch model, which should run faster on a GPU. But by how much?
Let us see a simple performance comparison.
Comparison 1. Batch Mode We will start by using the sklearn model to predict the whole train dataset and check out the time it takes.
We can do the same with our new PyTorch model:
That is a speedup of 9580&amp;frasl;195 ~ 50x.
2. Single Example Prediction We predict a single example here to see how the model would perform in a real-time setting. The sklearn model:
vs. Pytorch model
That is again a speedup of 79.6&amp;frasl;1.6 ~50x.
Small Caveat A small caveat I experienced is that the predictions from the sklearn model and the hummingbird PyTorch model were not exactly the same.
For example, here are the predictions I got from both models:
Yes, sometimes, they differ in the 7th digit, which might be a function of the conversion process. I think that it won’t change the final 1 or 0 predictions much. We can also check that:
scikit_1_0 = scikit_preds[:,1]&amp;gt;0.5 hb_1_0 = hb_preds[:,1]&amp;gt;0.5 print(len(scikit_1_0) == sum(scikit_1_0==hb_1_0)) ------------------------------------------------------------ True  So, for this case, both the models exactly gave the same 1 or 0 predictions for the whole dataset of 100,000 rows.
So I guess it is okay.
Conclusion The developers at Microsoft are still working on adding many more operators which range from models to feature engineering like MinMaxScaler or LabelEncoder to the code, and I am hopeful that they will further develop and improve this project. Here is the roadmap to development if you are interested.
Although Hummingbird is not perfect yet, it is the first system able to run classical ML inference DNN frameworks and proves them mature enough to be used as generic compilers. I will try to include it in my development workflow when it comes to making predictions at high throughput or latency.
You can find the code for this post as well as all my posts at my GitHub repository.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Stop Worrying and Create your Deep Learning Server in 30 minutes</title>
      <link>https://mlwhiz.com/blog/2020/05/25/dls/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/05/25/dls/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dls/main.png"></media:content>
      

      
      <description>I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating.</description>

      <content:encoded>  
        
        <![CDATA[  I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating. Going through all those Stack Overflow threads. Often wondering what has gone wrong.
So is there a way to do this more efficiently?
It turns out there is. In this blog, I will try to set up a deep learning server on EC2 with minimal effort so that I could focus on more important things.
This blog consists explicitly of two parts:
 Setting up an Amazon EC2 Machine with preinstalled deep learning libraries.
 Setting Up Jupyter Notebook using TMUX and SSH tunneling.
  Don’t worry; it’s not as difficult as it sounds. Just follow the steps and click Next.
Setting up Amazon EC2 Machine I am assuming that you have an AWS account, and you have access to the AWS Console. If not, you might need to sign up for an Amazon AWS account.
 First of all, we need to go to the Services tab to access the EC2 dashboard.   On the EC2 Dashboard, you can start by creating your instance.   Amazon provides Community AMIs(Amazon Machine Image) with Deep Learning software preinstalled. To access these AMIs, you need to look in the community AMIs and search for “Ubuntu Deep Learning” in the Search Tab. You can choose any other Linux flavor, but I have found Ubuntu to be most useful for my Deep Learning needs. In the present setup, I will use The Deep Learning AMI (Ubuntu 18.04) Version 27.0   Once you select an AMI, you can select the Instance Type. It is here you specify the number of CPUs, Memory, and GPUs you will require in your system. Amazon provides a lot of options to choose from based on one’s individual needs. You can filter for GPU instances using the “Filter by” filter.  In this tutorial, I have gone with p2.xlarge instance, which provides NVIDIA K80 GPU with 2,496 parallel processing cores and 12GiB of GPU memory. To know about different instance types, you can look at the documentation here and the pricing here.
 You can change the storage that is attached to the machine in the 4th step. It is okay if you don’t add storage upfront, as you can also do this later. I change the storage from 90 GB to 500 GB as most of the deep learning needs will require proper storage.   That’s all, and you can Launch the Instance after going to the Final Review instance settings Screen. Once you click on Launch, you will see this screen. Just type in any key name in the Key Pair Name and click on “Download key pair”. Your key will be downloaded to your machine by the name you provided. For me, it got saved as “aws_key.pem”. Once you do that, you can click on “Launch Instances”.  Keep this key pair safe as this will be required whenever you want to login to your instance.
 You can now click on “View Instances” on the next page to see your instance. This is how your instance will look like:   To connect to your instance, Just open a terminal window in your Local machine and browse to the folder where you have kept your key pair file and modify some permissions.
chmod 400 aws_key.pem
  Once you do that, you will be able to connect to your instance by SSHing. The SSH command will be of the form:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For me, the command was:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@ec2-54-202-223-197.us-west-2.compute.amazonaws.com  Also, keep in mind that the Public DNS might change once you shut down your instance.
 You have already got your machine up and ready. This machine contains different environments that have various libraries you might need. This particular machine has MXNet, Tensorflow, and Pytorch with different versions of python. And the best thing is that we get all this preinstalled, so it just works out of the box.  Setting Up Jupyter Notebook But there are still a few things you will require to use your machine fully. One of them being Jupyter Notebooks. To set up Jupyter Notebooks with your Machine, I recommend using TMUX and tunneling. Let us go through setting up the Jupyter notebook step by step.
1. Using TMUX to run Jupyter Notebook We will first use TMUX to run the Jupyter notebook on our instance. We mainly use this so that our notebook still runs even if the terminal connection gets lost.
To do this, you will need to create a new TMUX session using:
tmux new -s StreamSession  Once you do that, you will see a new screen with a green border at the bottom. You can start your Jupyter Notebook in this machine using the usual jupyter notebook command. You will see something like:
It will be beneficial to copy the login URL so that we will be able to get the token later when we try to login to our jupyter notebook later. In my case, it is:
[http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872](http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872&amp;amp;token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872)  The next step is to detach our TMUX session so that it continues running in the background even when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)You will come back to the initial screen with the message that you have detached from your TMUX session.
If you want, you can reattach to the session again using:
tmux attach -t StreamSession  2. SSH Tunneling to access the notebook on your Local Browser The second step is to tunnel into the Amazon instance to be able to get the Jupyter notebook on your Local Browser. As we can see, the Jupyter Notebook is actually running on the localhost on the Cloud instance. How do we access it? We use SSH tunneling. Worry not, it is straightforward fill in the blanks. Just use this command on your local machine terminal window:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L &amp;lt;Local Machine Port&amp;gt;:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For this case, I have used:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L 8001:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)ec2-54-202-223-197.us-west-2.compute.amazonaws.com  This means that I will be able to use the Jupyter Notebook If I open the localhost:8001 in my local machine browser. And I surely can. We can now just input the token that we already have saved in one of our previous steps to access the notebook. For me the token is 5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872
You can just login using your token and voila we get the notebook in all its glory.
You can now choose to work on a new project by selecting any of the different environments you want. You can come from Tensorflow or Pytorch or might be willing to get the best of both worlds. This notebook will not disappoint you.
Troubleshooting It might happen that once the machine is restarted, you face some problems with the NVIDIA graphics card. Specifically, in my case, the nvidia-smi command stopped working. If you encounter this problem, the solution is to download the graphics driver from the NVIDIA website.
Above are the settings for the particular AMI I selected. Once you click on Search you will be able to see the next page:
Just copy the download link by right-clicking and copying the link address. And run the following commands on your machine. You might need to change the link address and the file name in this.
# When nvidia-smi doesnt work: wget [https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla](https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla) sudo sh NVIDIA-Linux-x86_64-410.129-diagnostic.run --no-drm --disable-nouveau --dkms --silent --install-libglvnd modinfo nvidia | head -7 sudo modprobe nvidia  Stop Your Instance And that’s it. You have got and up and running Deep Learning machine at your disposal, and you can work with it as much as you want. Just keep in mind to stop the instance whenever you stop working, so you won’t need to pay Amazon when you are not working on your instance. You can do it on the instances page by right-clicking on your instance. Just note that when you need to log in again to this machine, you will need to get the Public DNS (IPv4) address from the instance page back as it might have changed.
Conclusion I have always found it a big chore to set up a deep learning environment.
In this blog, we set up a new Deep Learning server on EC2 in minimal time by using Deep Learning Community AMI, TMUX, and Tunneling for the Jupyter Notebooks. This server comes preinstalled with all the deep learning libraries you might need at your work, and it just works out of the box.
So what are you waiting for? Just get started with Deep Learning with your own server.
If you want to learn more about AWS and how to use it in production settings and deploying models, I would like to call out an excellent course on AWS. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>5 Online Courses you can take for free during COVID-19 Epidemic</title>
      <link>https://mlwhiz.com/blog/2020/03/27/covidcourses/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/03/27/covidcourses/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/covidcourses/main.jpeg"></media:content>
      

      
      <description>With Coronavirus on the prowl, there has been a huge demand across the world for MOOCs as schools and universities continue to shut down.
So, I find it great that providers like Coursera are hosting a lot of excellent courses on their site for free, but they are a little hard to find among all the paid courses.
While these courses are not providing verified certificates if you take them for free, in my view, it is the knowledge that matters than having a few certifications.</description>

      <content:encoded>  
        
        <![CDATA[  With Coronavirus on the prowl, there has been a huge demand across the world for MOOCs as schools and universities continue to shut down.
So, I find it great that providers like Coursera are hosting a lot of excellent courses on their site for free, but they are a little hard to find among all the paid courses.
While these courses are not providing verified certificates if you take them for free, in my view, it is the knowledge that matters than having a few certifications.
TLDR; With thousands of individuals laid off from this crisis, I believe it is crucial to get learning resources out now to people. So here is a list of courses that are great and free to learn.
1. Machine Learning Yes, you heard it right, Coursera is providing the Game Changer Machine Learning course by Andrew Ng for free right now.
As for my review, I think this is the one course that should be done by everyone interested in Machine Learning. For one, it contains the maths behind many of the Machine Learning algorithms and secondly Andrew Ng is a great instructor. Believe it or not, Andrew Ng not only taught but also motivated me to learn data science when I first started.
As for the curriculum, this course has a little of everything — Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.
You might also want to go through a few of my posts while going through this course:
 The Hitchhiker’s Guide to Feature Extraction
 The 5 Classification Evaluation metrics every Data Scientist must know
 The 5 Feature Selection Algorithms every Data Scientist should know
 The Simple Math behind 3 Decision Tree Splitting criterions
  2. Algorithms Algorithms and data structures are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are essential all the same.
Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.
They will require the same zeal to crack as your Data Science interviews, and thus, you might want to give some time for the study of algorithms and Data structure and algorithms questions.
This series of two courses offered by Robert Sedgewick covers all the essential algorithms and data structures. The first part of this course covers the elementary data structures, sorting, and searching algorithms, while the second part focuses on the graph and string-processing algorithms.
You might also like to look at a few of my posts while trying to understand some of the material in these courses.
 3 Programming concepts for Data Scientists A simple introduction to Linked Lists for Data Scientists Dynamic Programming for Data Scientists  3. Bayesian Statistics: From Concept to Data Analysis  “Facts are stubborn things, but statistics are pliable.” ― Mark Twain
 The war between a frequentist and bayesian is never over.
In this course, you will learn about MLE, priors, posteriors, conjugate priors, and a whole lot of other practical scenarios where we can use Bayesian Statistics. All in all, a well-packaged course which explains both frequentist and bayesian approach to statistics.
From the course website:
 This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach.
 4. Practical Time Series Analysis Have you heard about ARIMA models, Stationarity in time series, etc. and have been boggled by these terms? This course aims to teach Time series from a fairly mathematical perspective. I was not able to find such a course for a fairly long time. And now it is free for all.
From the course website:
 In practical Time Series Analysis we look at data sets that represent sequential information, such as stock prices, annual rainfall, sunspot activity, the price of agricultural products, and more. We look at several mathematical models that might be used to describe the processes which generate these types of data
 If you want to use XGBoost or Tree-based models for time series analysis, do take a look at one of my previous post here:
 Using Gradient Boosting for Time Series prediction tasks  5. Getting Started with AWS for Machine Learning  The secret: it’s not what you know, it’s what you show.
 There are a lot of things to consider while building a great machine learning system. But often it happens that we, as data scientists, only worry about certain parts of the project.
But do we ever think about how we will deploy our models once we have them?
I have seen a lot of ML projects, and a lot of them are doomed to fail as they don’t have a set plan for production from the onset.
Having a good platform and understanding how that platform deploys machine Learning apps will make all the difference in the real world. This course on AWS for implementing Machine Learning applications promises just that.
 This course will teach you:
1. How to build, train and deploy a model using Amazon SageMaker with built-in algorithms and Jupyter Notebook instance.
2. How to build intelligent applications using Amazon AI services like Amazon Comprehend, Amazon Rekognition, Amazon Translate and others.
 You might also look at this post of mine, where I try to talk about apps and explain how to plan for Production.
 How to write Web apps using simple Python for Data Scientists? How to Deploy a Streamlit App using an Amazon Free ec2 instance? Take your Machine Learning Models to Production with these 5 simple steps  More Free Courses Also, don’t worry if you don’t want to learn the above ones. I have collected a list of some highly-rated courses that are free to audit before writing this post. You can download the excel file here. So have a stab at whatever you want to learn.
Continue Learning I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Share your Projects even more easily with this New Streamlit Feature</title>
      <link>https://mlwhiz.com/blog/2020/02/23/streamlitrec/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/23/streamlitrec/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlitrec/main.png"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
As Sten Sootla says in his satire piece which I thoroughly enjoyed:</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
As Sten Sootla says in his satire piece which I thoroughly enjoyed:
 The secret: it’s not what you know, it’s what you show.
 This is where StreamLit comes in and provides a way to create web apps just using Python. I have been keeping close tabs on this excellent product for the past few months. In my last few posts, I talked about Working with Streamlit and how to Deploy the streamlit app using ec2. I have also been in constant touch with the Streamlit team while they have been working continuously to make the user experience even better by releasing additional features.
So, have you ever had a problem with explaining how the app works to the stakeholders/business partners? Having to set up multiple calls with different stakeholders in different countries and explaining the whole process again and again?
Or have you worked on a project that you want to share on social media? LinkedIn, Youtube, and the like?
With their new version, Streamlit has released a new feature called “Record a Screencast” which will solve this problem for you.
How? Read on.
Setting up So to check this new feature out, which is a part of Streamlit’s version 0.55.0 offering, we need to first install or upgrade streamlit. Do this by using this command:
pip install --upgrade streamlit  We also need to run Streamlit. Here I will use the demo app. You can also use any of your own apps.
streamlit hello  You should see something like below:
A tab also opens up in your browser, where you can try their demo. If that doesn’t open up in the browser, you can manually go to the Local URL http://localhost:8501/ too.
Recording the Screencast Now the time has come to record our screencast to share with the world. You can find the option to record the screencast using the top-right menu in Streamlit.
Once you click on that, you will get the option to record audio, and you can select the aptly named “Start Recording” button to start recording.
You can then choose what you want to share — just your streamlit app or your entire desktop. One can choose to share the whole desktop if they need to go forth between different programs like Excel sheets, powerpoints, and the streamlit app, for example. Here I choose to show just the “Streamlit” App and click share.
Your screencast has now started, and you can record the explanation session for your shareholders now. Once you are done with the recording, you can click on the top-right menu again and select stop recording. Or conveniently press escape to end the recording session.
You will be able to preview and save the session video you recorded as a .webm file, which you can aim to send to your shareholders and even share on LinkedIn/twitter/youtube for your personal projects.
And that’s it. The process is pretty simple and doesn’t need any additional software installation from our side.
Endnotes Streamlit has democratized the whole process of creating apps.
I honestly like the way Streamlit is working on developing its product, keeping in mind all the pain points of its users. With this iteration, they have resolved one more pain point where users struggle to showcase their work in a meaningful way on social media sites or to explain the workings of an app multiple times to the shareholders.
On top of that, Streamlit is a free and open-source rather than a proprietary web app that works out of the box. I couldn’t recommend it more.
Also, do let me know if you want to request any additional features in Streamlit in the comments section. I will make sure to pass it on to the Streamlit team.
If you want to learn more about using Streamlit to create and deploy apps, take a look at my other posts:
 How to write Web apps using simple Python for Data Scientists? How to Deploy a Streamlit App using an Amazon Free ec2 instance?  If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to Deploy a Streamlit App using an Amazon Free ec2 instance?</title>
      <link>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/22/streamlitec2/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlitec2/main.png"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
In my last post on Streamlit, I talked about how to write Web apps using simple Python for Data Scientists.
But still, a major complaint, if you would check out the comment section of that post, was regarding the inability to deploy Streamlit apps over the web.
And it was a valid complaint.
 A developer can’t show up with his laptop every time the client wanted to use the app. What is the use of such an app?
 So in this post, we will go one step further deploy our Streamlit app over the Web using an Amazon Free ec2 instance.
Setting up the Amazon Instance Before we start with using the amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a simple sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine”. Here we are trying to set up the machine where we will deploy our Streamlit app.
 In the first step, you need to choose the AMI template for the machine. I select the 18.04 Ubuntu Server since it is applicable for the Free Tier. And Ubuntu.
   In the second step, I select the t2.micro instance as again it is the one which is eligible for the free tier. As you can see t2.micro is just a single CPU instance with 512 MB RAM. You can opt for a bigger machine if you are dealing with a powerful model or are willing to pay.   Keep pressing Next until you reach the “6. Configure Security Group” tab. You will need to add a rule with Type: “Custom TCP Rule”, Port Range:8501, and Source: Anywhere. We use the port 8501 here since it is the custom port used by Streamlit.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch you might need to create a new key pair. Here I am creating a new key pair named streamlit and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state, it should be showing “Running”   Select your instance, and copy the Public DNS(IPv4) Address from the description. It should be something starting with ec2.
 Once you have that run the following commands in the folder you saved the streamlit.pem file. I have masked some of the information here.
   chmod 400 streamlit.pem ssh -i &amp;quot;streamlit.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Installing Required Libraries Whoa, that was a handful. After all the above steps you should be able to see the ubuntu prompt for the virtual machine. We will need to set up this machine to run our app. I am going to be using the same streamlit_football_demo app that I used in my previous post.
We start by installing miniconda and adding its path to the environment variable.
sudo apt-get update wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p ~/miniconda echo &amp;quot;PATH=$PATH:$HOME/miniconda/bin&amp;quot; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc  We then install additional dependencies for our app to run. That means I install streamlit and plotly_express.
pip install streamlit pip install plotly_express  And our machine is now prepped and ready to run.
Running Streamlit on Amazon ec2 As I am set up with the instance, I can get the code for my demo app from Github. Or you can choose to create or copy another app as you wish.
git clone https://github.com/MLWhiz/streamlit_football_demo.git cd streamlit_football_demo streamlit run helloworld.py  Now you can go to a browser and type the external URL to access your app. In my case the address is http://35.167.158.251:8501. Here is the output. This app will be up right now if you want to play with it.
A Very Small Problem Though We are up and running with our app for the world to see. But whenever you are going to close the SSH terminal window the process will stop and so will your app.
So what do we do?
TMUX to the rescue. TMUX allows us to keep running our sessions even after we leave the terminal window. It also helps with a lot of other things but I will just go through the steps we need.
First, we stop our app using Ctrl&#43;C and install tmux
sudo apt-get install tmux  We start a new tmux session using the below command. We keep the name of our session as StreamSession. You could use any name here.
tmux new -s StreamSession  You can see that the session name is “StreamSession” at the bottom of the screen. You can now start running streamlit in the tmux session.
streamlit run helloworld.py  You will be able to see your app at the External URL. The next step is to detach our TMUX session so that it continues running in the background when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)
You can now close your SSH session and the app will continue running at the External URL.
And Voila! We are up and running.
Pro TMUX Tip: You can reattach to the same session by using the attach command below. The best part is that you can close your SSH shell and then maybe come back after some hours and reattach to a session and keep working from wherever you were when you closed the SSH shell.
tmux attach -t StreamSession  Simple Troubleshooting: If your app is not hosting at 8501, it means that an instance of streamlit app is already running on your system and you will need to stop that. You can do so by first finding the process ID
ps aux | grep streamlit  You will see something like:
ubuntu 20927 2.4 18.8 713780 189580 pts/3 Sl&#43; 19:55 0:26 /home/ubuntu/miniconda/bin/python /home/ubuntu/miniconda/bin/streamlit run helloworld.py  You will need to kill this process. You can do this simply by
kill -9 20947  Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more. If you want to learn more about how to create awesome web apps with Streamlit then read up my last post.
In this post, we deployed a simple web app on AWS using amazon ec2.
In the process of doing this, we created our own Amazon ec2 instance, logged into the SSH shell, installed miniconda and dependencies, ran our Streamlit application and learned about TMUX. Enough learning for a day?
So go and show on these Mad skills. To end on a lighter note, as Sten Sootla says in his satire piece which I thoroughly enjoyed:
 The secret: it’s not what you know, it’s what you show.
 If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his excellent third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Add this single word to make your Pandas Apply faster</title>
      <link>https://mlwhiz.com/blog/2020/02/20/swifter/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/02/20/swifter/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/swifter/main.png"></media:content>
      

      
      <description>We as data scientists have got laptops with quad-core, octa-core, turbo-boost. We work with servers with even more cores and computing power.
But do we really utilize the raw power we have at hand?
Sometimes we get limited by the limitation of tools at our disposal. And sometimes we are not willing to write all that extraneous code to save a couple of minutes. And end up realizing only later that time optimization would have helped in the long run.</description>

      <content:encoded>  
        
        <![CDATA[  We as data scientists have got laptops with quad-core, octa-core, turbo-boost. We work with servers with even more cores and computing power.
But do we really utilize the raw power we have at hand?
Sometimes we get limited by the limitation of tools at our disposal. And sometimes we are not willing to write all that extraneous code to save a couple of minutes. And end up realizing only later that time optimization would have helped in the long run.
So, can we do better?
Yes, Obviously.
Previously, I had written on how to make your apply function faster-using multiprocessing, but thanks to the swifter library, it is even more trivial now.
This post is about using the computing power we have at hand and applying it to Pandas DataFrames using Swifter.
Problem Statement We have got a huge pandas data frame, and we want to apply a complex function to it which takes a lot of time.
For this post, I will generate some data with 25M rows and 4 columns.
Can use parallelization easily to get extra performance out of our code?
import pandas as pd import numpy as np pdf = pd.DataFrame(np.random.randint(0,100,size=(25000000, 4)),columns=list(&#39;abcd&#39;))  The Data looks like:
Data Sample
Parallelization using just a single change Relax and Parallelize !!!
Let’s set up a simple experiment.
We will try to create a new column in our dataframe. We can do this simply by using apply-lambda in Pandas.
def func(a,b): if a&amp;gt;50: return True elif b&amp;gt;75: return True else: return False pdf[&#39;e&#39;] = pdf.apply(lambda x : func(x[&#39;a&#39;],x[&#39;b&#39;]),axis=1)  The above code takes ~10 Minutes to run. And we are just doing a simple calculation on 2 columns here.
Can we do better and what would it take?
Yes, we can do better just by adding a “magic word” — Swifter.
But first, you need to install swifter, which is as simple as:
conda install -c conda-forge swifter  You can then just import and append swifter keyword before the apply to use it.
import swifter pdf[&#39;e&#39;] = pdf.**swifter**.apply(lambda x : func(x[&#39;a&#39;],x[&#39;b&#39;]),axis=1)  So, Does this work? Yes. It does. We get a 2x improvement in run time vs. just using the function as it is.
So what exactly is happening here? Source: How increasing data size effects performances for Dask, Pandas and Swifter?
Swifter chooses the best way to implement the apply possible for your function by either vectorizing your function or using Dask in the backend to parallelize your function or by maybe using simple pandas apply if the dataset is small.
In this particular case, Swifter is using Dask to parallelize our apply functions with the default value of npartitions = cpu_count()*2.
For the MacBook I am working on the CPU Count is 6 and the hyperthreading is 2. Thus CPU Count is 12 and that makes npartitions=24.
We could also choose to set n_partitions ourselves. Though I have observed the default value works just fine in most cases sometimes you might be able to tune this as well to gain additional speedups.
For example: Below I set n_partitions=12 and we get a 2x speedup again. Here reducing our number of partitions results in smaller run times as the data movement cost between the partitions is high.
Conclusion  Parallelization is not a silver bullet; it is buckshot.
 Parallelization won’t solve all your problems, and you would still have to work on optimizing your functions, but it is a great tool to have in your arsenal.
Time never comes back, and sometimes we have a shortage of it. At these times we need parallelization to be at our disposal with a single word.
And that word is swifter.
Continue Learning Also if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to write Web apps using simple Python for Data Scientists?</title>
      <link>https://mlwhiz.com/blog/2019/12/07/streamlit/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/07/streamlit/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/streamlit/main.jpeg"></media:content>
      

      
      <description>A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn.</description>

      <content:encoded>  
        
        <![CDATA[  A Machine Learning project is never really complete if we don’t have a good way to showcase it.
While in the past, a well-made visualization or a small PPT used to be enough for showcasing a data science project, with the advent of dashboarding tools like RShiny and Dash, a good data scientist needs to have a fair bit of knowledge of web frameworks to get along.
And Web frameworks are hard to learn. I still get confused in all that HTML, CSS, and Javascript with all the hit and trials, for something seemingly simple to do.
Not to mention the many ways to do the same thing, making it confusing for us data science folks for whom web development is a secondary skill.
So, are we doomed to learn web frameworks? Or to call our developer friend for silly doubts in the middle of the night?
This is where StreamLit comes in and delivers on its promise to create web apps just using Python.
 Zen of Python: Simple is better than complex and Streamlit makes it dead simple to create apps.
 This post is about understanding how to create apps that support data science projects using Streamlit.
To understand more about the architecture and the thought process that led to streamlit, have a look at this excellent post by one of the original developers/founder Adrien Treuille.
Installation Installation is as simple as running the command:
pip install streamlit
To see if our installation is successful, we can just run:
streamlit hello
This should show you a screen that says:
You can go to the local URL: localhost:8501 in your browser to see a Streamlit app in action. The developers have provided some cool demos that you can play with. Do take your time and feel the power of the tool before coming back.
Streamlit Hello World Streamlit aims to make app development easy using simple Python.
So let us write a simple app to see if it delivers on that promise.
Here I start with a simple app which we will call the Hello World of streamlit. Just paste the code given below in a file named helloworld.py
import streamlit as st x = st.slider(&amp;#39;x&amp;#39;) st.write(x, &amp;#39;squared is&amp;#39;, x * x) And, on the terminal run:
streamlit run helloworld.py And voila, you should be able to see a simple app in action in your browser at localhost:8501 that allows you to move a slider and gives the result.
It was pretty easy. In the above app, we used two features from Streamlit:
 the st.slider widget that we can slide to change the output of the web app.
 and the versatile st.write command. I am amazed at how it can write anything from charts, dataframes, and simple text. More on this later.
  Important: Remember that every time we change the widget value, the whole app runs from top to bottom.
Streamlit Widgets Widgets provide us a way to control our app. The best place to read about the widgets is the API reference documentation itself but I will describe some most prominent ones that you might end up using.
1. Slider streamlit.slider(label, min_value=None, max_value=None, value=None, step=None, format=None) We already saw st.slider in action above. It can be used with min_value,max_value, and step for getting inputs in a range.
2. Text Input The simplest way to get user input be it some URL input or some text input for sentiment analysis. It just needs a single label for naming the textbox.
import streamlit as st url = st.text_input(&amp;#39;Enter URL&amp;#39;) st.write(&amp;#39;The Entered URL is&amp;#39;, url) This is how the app looks:
Tip: You can just change the file helloworld.py and refresh the browser. The way I work is to open and change helloworld.py in sublime text and see the changes in the browser side by side.
3. Checkbox One use case for checkboxes is to hide or show/hide a specific section in an app. Another could be setting up a boolean value in the parameters for a function.st.checkbox() takes a single argument, which is the widget label. In this app, the checkbox is used to toggle a conditional statement.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) if st.checkbox(&amp;#39;Show dataframe&amp;#39;): st.write(df) 4. SelectBox We can use st.selectbox to choose from a series or a list. Normally a use case is to use it as a simple dropdown to select values from a list.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) option = st.selectbox( &amp;#39;Which Club do you like best?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, option) 5. MultiSelect We can also use multiple values from a dropdown. Here we use st.multiselect to get multiple values as a list in the variable options
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) options = st.multiselect( &amp;#39;What are your favorite clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) st.write(&amp;#39;You selected:&amp;#39;, options) Creating Our Simple App Step by Step So much for understanding the important widgets. Now, we are going to create a simple app using multiple widgets at once.
To start simple, we will try to visualize our football data using streamlit. It is pretty much simple to do this with the help of the above widgets.
import streamlit as st import pandas as pd import numpy as np df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) # Filter dataframe new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] # write dataframe to screen st.write(new_df) Our simple app looks like:
That was easy. But it seems pretty basic right now. Can we add some charts?
Streamlit currently supports many libraries for plotting.Plotly, Bokeh, Matplotlib, Altair, and Vega charts being some of them. Plotly Express also works, although they didn’t specify it in the docs. It also has some inbuilt chart types that are “native” to Streamlit, like st.line_chart and st.area_chart.
We will work with plotly_express here. Here is the code for our simple app. We just used four calls to streamlit. Rest is all simple python.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = pd.read_csv(&amp;#34;football_data.csv&amp;#34;) clubs = st.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # create figure using plotly express fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) Improvements In the start we said that each time we change any widget, the whole app runs from start to end. This is not feasible when we create apps that will serve deep learning models or complicated machine learning models. Streamlit covers us in this aspect by introducing Caching.
1. Caching In our simple app. We read the pandas dataframe again and again whenever a value changes. While it works for the small data we have, it will not work for big data or when we have to do a lot of processing on the data. Let us use caching using the st.cache decorator function in streamlit like below.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) Or for more complex and time taking functions that need to run only once(think loading big Deep Learning models), using:
@st.cache def complex_func(a,b): DO SOMETHING COMPLEX # Won&amp;#39;t run again and again. complex_func(a,b) When we mark a function with Streamlit’s cache decorator, whenever the function is called streamlit checks the input parameters that you called the function with.
If this is the first time Streamlit has seen these params, it runs the function and stores the result in a local cache.
When the function is called the next time, if those params have not changed, Streamlit knows it can skip executing the function altogether. It just uses the results from the cache.
2. Sidebar For a cleaner look based on your preference, you might want to move your widgets into a sidebar, something like Rshiny dashboards. This is pretty simple. Just add st.sidebar in your widget’s code.
import streamlit as st import pandas as pd import numpy as np import plotly_express as px df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) # Plot! st.plotly_chart(fig) 3. Markdown? I love writing in Markdown. I find it less verbose than HTML and much more suited for data science work. So, can we use Markdown with the streamlit app?
Yes, we can. There are a couple of ways to do this. In my view, the best one is to use Magic commands. Magic commands allow you to write markdown as easily as comments. You could also have used the command st.markdown
import streamlit as st import pandas as pd import numpy as np import plotly_express as px &amp;#39;&amp;#39;&amp;#39; # Club and Nationality App This very simple webapp allows you to select and visualize players from certain clubs and certain nationalities. &amp;#39;&amp;#39;&amp;#39; df = st.cache(pd.read_csv)(&amp;#34;football_data.csv&amp;#34;) clubs = st.sidebar.multiselect(&amp;#39;Show Player for clubs?&amp;#39;, df[&amp;#39;Club&amp;#39;].unique()) nationalities = st.sidebar.multiselect(&amp;#39;Show Player from Nationalities?&amp;#39;, df[&amp;#39;Nationality&amp;#39;].unique()) new_df = df[(df[&amp;#39;Club&amp;#39;].isin(clubs)) &amp;amp; (df[&amp;#39;Nationality&amp;#39;].isin(nationalities))] st.write(new_df) # Create distplot with custom bin_size fig = px.scatter(new_df, x =&amp;#39;Overall&amp;#39;,y=&amp;#39;Age&amp;#39;,color=&amp;#39;Name&amp;#39;) &amp;#39;&amp;#39;&amp;#39; ### Here is a simple chart between player age and overall &amp;#39;&amp;#39;&amp;#39; st.plotly_chart(fig) Conclusion Streamlit has democratized the whole process to create apps, and I couldn’t recommend it more.
In this post, we created a simple web app. But the possibilities are endless. To give an example here is face GAN from the streamlit site. And it works by just using the same guiding ideas of widgets and caching.
I love the default colors and styles that the developers have used, and I found it much more comfortable than using Dash, which I was using until now for my demos. You can also include audio and video in your streamlit apps.
On top of that, Streamlit is a free and open-source rather than a proprietary web app that just works out of the box.
In the past, I had to reach out to my developer friends for any single change in a demo or presentation; now it is relatively trivial to do that.
 I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
 I don’t have an idea if it will perform well in a production environment yet, but its a boon for the small proof of concept projects and demos. I aim to use it more in my workflow from now on, and considering the capabilities it provides without all the hard work, I think you should too.
You can find the full code for the final app here.
If you want to learn about the best strategies for creating Visualizations, I would like to call out an excellent course about Data Visualization and applied plotting from the University of Michigan, which is a part of a pretty good Data Science Specialization with Python in itself. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Ultimate Guide to using the Python regex module</title>
      <link>https://mlwhiz.com/blog/2019/09/01/regex/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/09/01/regex/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/regex/1.png"></media:content>
      

      
      <description>One of the main tasks while working with text data is to create a lot of text-based features.
One could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text.
While it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python’s regex module.
For example, let&amp;amp;rsquo;s say you are tasked with finding the number of punctuations in a particular piece of text.</description>

      <content:encoded>  
        
        <![CDATA[  One of the main tasks while working with text data is to create a lot of text-based features.
One could like to find out certain patterns in the text, emails if present in a text as well as phone numbers in a large text.
While it may sound fairly trivial to achieve such functionalities it is much simpler if we use the power of Python’s regex module.
For example, let&amp;rsquo;s say you are tasked with finding the number of punctuations in a particular piece of text. Using text from Dickens here.
How do you normally go about it?
A simple enough way is to do something like:
target = [&amp;#39;;&amp;#39;,&amp;#39;.&amp;#39;,&amp;#39;,&amp;#39;,&amp;#39;–&amp;#39;] string = &amp;#34;It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.**&amp;#34; num_puncts = 0 for punct in target: if punct in string: num_puncts&#43;=string.count(punct) print(num_puncts) 19  And that is all but fine if we didn’t have the re module at our disposal. With re it is simply 2 lines of code:
import re pattern = r&amp;#34;[;.,–]&amp;#34; print(len(re.findall(pattern,string)))  19  This post is about one of the most commonly used regex patterns and some regex functions I end up using regularly.
What is regex? In simpler terms, a regular expression(regex) is used to find patterns in a given string.
The pattern we want to find could be anything.
We can create patterns that resemble an email or a mobile number. We can create patterns that find out words that start with a and ends with z from a string.
In the above example:
import re pattern = r&amp;#39;[,;.,–]&amp;#39; print(len(re.findall(pattern,string))) The pattern we wanted to find out was r’[,;.,–]’. This pattern captures any of the 4 characters we wanted to capture. I find regex101 a great tool for testing patterns. This is how the pattern looks when applied to the target string.
As we can see we are able to find all the occurrences of ,;.,– in the target string as required.
I use the above tool whenever I need to test a regex. Much faster than running a python program again and again and much easier to debug.
So now we know that we can find patterns in a target string but how do we really create these patterns?
Creating Patterns The first thing we need to learn while using regex is how to create patterns.
I will go through some most commonly used patterns one by one.
As you would think, the simplest pattern is a simple string.
pattern = r&amp;#39;times&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; print(len(re.findall(pattern,string))) But that is not very useful. To help with creating complex patterns regex provides us with special characters/operators. Let us go through some of these operators one by one. Please wait for the gifs to load.
1. the [] operator This is the one we used in our first example. We want to find one instance of any character within these square brackets.
[abc]- will find all occurrences of a or b or c.
[a-z]- will find all occurrences of a to z.
[a-z0–9A-Z]- will find all occurrences of a to z, A to Z and 0 to 9.
We can easily use this pattern as below in Python:
pattern = r&amp;#39;[a-zA-Z]&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; print(len(re.findall(pattern,string))) There are other functionalities in regex apart from .findall but we will get to them a little bit later.
2. The dot Operator The dot operator(.) is used to match a single instance of any character except the newline character.
The best part about the operators is that we can use them in conjunction with one another.
For example, We want to find out the substrings in the string that start with small d or Capital D and end with e with a length of 6.
3. Some Meta Sequences There are some patterns that we end up using again and again while using regex. And so regex has created a few shortcuts for them. The most useful shortcuts are:
\w, Matches any letter, digit or underscore. Equivalent to [a-zA-Z0–9_]
\W, Matches anything other than a letter, digit or underscore.
\d, Matches any decimal digit. Equivalent to [0–9].
\D, Matches anything other than a decimal digit.
4. The Plus and Star operator The dot character is used to get a single instance of any character. What if we want to find more.
The Plus character &#43;, is used to signify 1 or more instance of the leftmost character.
The Star character *, is used to signify 0 or more instance of the leftmost character.
For example, if we want to find out all substrings that start with d and end with e, we can have zero characters or more characters between d and e. We can use: d\w*e
If we want to find out all substrings that start with d and end with e with at least one character between d and e, we can use: d\w&#43;e
We could also have used a more generic approach using {} \w{n} - Repeat \w exactly n number of times.
\w{n,} - Repeat \w at least n times or more.
\w{n1, n2} - Repeat \w at least n1 times but no more than n2 times.
5. ^ Caret Operator and $ Dollar operator. ^ Matches the start of a string, and $ Matches the end of the string.
6. Word Boundary This is an important concept.
Did you notice how I always matched substring and never a word in the above examples?
So, what if we want to find all words that start with d?
Can we use d\w* as the pattern? Let&amp;rsquo;s see using the web tool.
Regex Functions Till now we have only used the findall function from the re package, but it also supports a lot more functions. Let us look into the functions one by one.
1. findall We already have used findall. It is one of the regex functions I end up using most often. Let us understand it a little more formally.
Input: Pattern and test string
Output: List of strings.
#USAGE: pattern = r&amp;#39;[iI]t&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; matches = re.findall(pattern,string) for match in matches: print(match) It it  2. Search Input: Pattern and test string
Output: Location object for the first match.
#USAGE: pattern = r&amp;#39;[iI]t&amp;#39; string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; location = re.search(pattern,string) print(location) &amp;lt;_sre.SRE_Match object; span=(0, 2), match=&#39;It&#39;&amp;gt;  We can get this location object’s data using
print(location.group()) &#39;It&#39;  3. Substitute This is another great functionality. When you work with NLP you sometimes need to substitute integers with X’s. Or you might need to redact some document. Just the basic find and replace in any of the text editors.
Input: search pattern, replacement pattern, and the target string
Output: Substituted string
string = &amp;#34;It was the best of times, it was the worst of times.&amp;#34; string = re.sub(r&amp;#39;times&amp;#39;, r&amp;#39;life&amp;#39;, string) print(string) It was the best of life, it was the worst of life.  Some Case Studies: Regex is used in many cases when validation is required. You might have seen prompts on websites like “This is not a valid email address”. While such a prompt could be written using multiple if and else conditions, regex is probably the best for such use cases.
1. PAN Numbers In India, we have got PAN Numbers for Tax identification rather than SSN numbers in the US. The basic validation criteria for PAN is that it must have all its letters in uppercase and characters in the following order:
&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;char&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;digit&amp;gt;&amp;lt;char&amp;gt;  So the question is:
Is ‘ABcDE1234L’ a valid PAN?
How would you normally attempt to solve this without regex? You will most probably write a for loop and keep an index going through the string. With regex it is as simple as below:
match=re.search(r&amp;#39;[A-Z]{5}[0–9]{4}[A-Z]&amp;#39;,&amp;#39;ABcDE1234L&amp;#39;) if match: print(True) else: print(False) False  2. Find Domain Names Sometimes we have got a large text document and we have got to find out instances of telephone numbers or email IDs or domain names from the big text document.
For example, Suppose you have this text:
&amp;lt;div class=&amp;#34;reflist&amp;#34; style=&amp;#34;list-style-type: decimal;&amp;#34;&amp;gt; &amp;lt;ol class=&amp;#34;references&amp;#34;&amp;gt; &amp;lt;li id=&amp;#34;cite_note-1&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [&amp;#34;Train (noun)&amp;#34;](http://www.askoxford.com/concise_oed/train?view=uk). &amp;lt;i&amp;gt;(definition – Compact OED)&amp;lt;/i&amp;gt;. Oxford University Press&amp;lt;span class=&amp;#34;reference-accessdate&amp;#34;&amp;gt;. Retrieved 2008-03-18&amp;lt;/span&amp;gt;.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.atitle=Train&#43;%28noun%29&amp;amp;rft.genre=article&amp;amp;rft_id=http%3A%2F%2Fwww.askoxford.com%2Fconcise_oed%2Ftrain%3Fview%3Duk&amp;amp;rft.jtitle=%28definition&#43;%E2%80%93&#43;Compact&#43;OED%29&amp;amp;rft.pub=Oxford&#43;University&#43;Press&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-2&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;#34;reference-text&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;citation book&amp;#34;&amp;gt;Atchison, Topeka and Santa Fe Railway (1948). &amp;lt;i&amp;gt;Rules: Operating Department&amp;lt;/i&amp;gt;. p. 7.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.au=Atchison%2C&#43;Topeka&#43;and&#43;Santa&#43;Fe&#43;Railway&amp;amp;rft.aulast=Atchison%2C&#43;Topeka&#43;and&#43;Santa&#43;Fe&#43;Railway&amp;amp;rft.btitle=Rules%3A&#43;Operating&#43;Department&amp;amp;rft.date=1948&amp;amp;rft.genre=book&amp;amp;rft.pages=7&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-3&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [Hydrogen trains](http://www.hydrogencarsnow.com/blog2/index.php/hydrogen-vehicles/i-hear-the-hydrogen-train-a-comin-its-rolling-round-the-bend/)&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-4&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [Vehicle Projects Inc. Fuel cell locomotive](http://www.bnsf.com/media/news/articles/2008/01/2008-01-09a.html)&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-5&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^&amp;lt;/b&amp;gt;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;#34;reference-text&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;citation book&amp;#34;&amp;gt;Central Japan Railway (2006). &amp;lt;i&amp;gt;Central Japan Railway Data Book 2006&amp;lt;/i&amp;gt;. p. 16.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.au=Central&#43;Japan&#43;Railway&amp;amp;rft.aulast=Central&#43;Japan&#43;Railway&amp;amp;rft.btitle=Central&#43;Japan&#43;Railway&#43;Data&#43;Book&#43;2006&amp;amp;rft.date=2006&amp;amp;rft.genre=book&amp;amp;rft.pages=16&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;li id=&amp;#34;cite_note-6&amp;#34;&amp;gt;&amp;lt;span class=&amp;#34;mw-cite-backlink&amp;#34;&amp;gt;&amp;lt;b&amp;gt;^ [&amp;#34;Overview Of the existing Mumbai Suburban Railway&amp;#34;](http://web.archive.org/web/20080620033027/http://www.mrvc.indianrail.gov.in/overview.htm). _Official webpage of Mumbai Railway Vikas Corporation_. Archived from [the original](http://www.mrvc.indianrail.gov.in/overview.htm) on 2008-06-20&amp;lt;span class=&amp;#34;reference-accessdate&amp;#34;&amp;gt;. Retrieved 2008-12-11&amp;lt;/span&amp;gt;.&amp;lt;/span&amp;gt;&amp;lt;span title=&amp;#34;ctx_ver=Z39.88-2004&amp;amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&amp;amp;rft.atitle=Overview&#43;Of&#43;the&#43;existing&#43;Mumbai&#43;Suburban&#43;Railway&amp;amp;rft.genre=article&amp;amp;rft_id=http%3A%2F%2Fwww.mrvc.indianrail.gov.in%2Foverview.htm&amp;amp;rft.jtitle=Official&#43;webpage&#43;of&#43;Mumbai&#43;Railway&#43;Vikas&#43;Corporation&amp;amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;#34; class=&amp;#34;Z3988&amp;#34;&amp;gt;&amp;lt;span style=&amp;#34;display:none;&amp;#34;&amp;gt; &amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/li&amp;gt; &amp;lt;/ol&amp;gt; &amp;lt;/div&amp;gt; And you need to find out all the primary domains from this text- askoxford.com;bnsf.com;hydrogencarsnow.com;mrvc.indianrail.gov.in;web.archive.org
How would you do this?
match=re.findall(r&amp;#39;http(s:|:)\/\/([www.|ww2.|)([0-9a-z.A-Z-]*\.\w{2,3})&amp;#39;,string)](http://www.|ww2.|)([0-9a-z.A-Z-]*\.\w{2,3})&amp;#39;,string))for elem in match:print(elem) (&#39;:&#39;, &#39;www.&#39;, &#39;askoxford.com&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;hydrogencarsnow.com&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;bnsf.com&#39;) (&#39;:&#39;, &#39;&#39;, &#39;web.archive.org&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;mrvc.indianrail.gov.in&#39;) (&#39;:&#39;, &#39;www.&#39;, &#39;mrvc.indianrail.gov.in&#39;)  | is the or operator here and match returns tuples where the pattern part inside () is kept.
3. Find Email Addresses: Below is a regex to find email addresses in a long text.
match=re.findall(r&amp;#39;([\w0-9-._]&#43;@[\w0-9-.]&#43;[\w0-9]{2,3})&amp;#39;,string) These are advanced examples but if you try to understand these examples for yourself you should be fine with the info provided.
Conclusion While it might look a little daunting at first, regex provides a great degree of flexibility when it comes to data manipulation, creating features and finding patterns.
I use it quite regularly when I work with text data and it can also be included while working on data validation tasks.
I am also a fan of the regex101 tool and use it frequently to check my regexes. I wonder if I would be using regexes as much if not for this awesome tool.
Also if you want to learn more about NLP here is an excellent course. You can start for free with the 7-day Free Trial.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Minimal Pandas Subset for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pandas_subset/1.jpeg"></media:content>
      

      
      <description>Pandas is a vast library.
Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.
Still, I generally have some issues with it.
There are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.</description>

      <content:encoded>  
        
        <![CDATA[    Pandas is a vast library.
Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.
Still, I generally have some issues with it.
There are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.
This has inspired me to come up with a minimal subset of pandas functions I use while coding.
I have tried it all, and currently, I stick to a particular way. It is like a mind map.
Sometimes because it is fast and sometimes because it’s more readable and sometimes because I can do it with my current knowledge. And sometimes because I know that a particular way will be a headache in the long run(think multi-index)
This post is about handling most of the data manipulation cases in Python using a straightforward, simple, and matter of fact way.
With a sprinkling of some recommendations throughout.
I will be using a data set of 1,000 popular movies on IMDB in the last ten years. You can also follow along in the Kaggle Kernel.
Some Default Pandas Requirements As good as the Jupyter notebooks are, some things still need to be specified when working with Pandas.
***Sometimes your notebook won’t show you all the columns. Sometimes it will display all the rows if you print the dataframe. ***You can control this behavior by setting some defaults of your own while importing Pandas. You can automate it using this addition to your notebook.
For instance, this is the setting I use.
import pandas as pd # pandas defaults pd.options.display.max_columns = 500 pd.options.display.max_rows = 500 Reading Data with Pandas The first thing we do is reading the data source and so here is the code for that.
df = pd.read_csv(&amp;#34;IMDB-Movie-Data.csv&amp;#34;) Recommendation: I could also have used pd.read_table to read the file. The thing is that pd.read_csv has default separator as , and thus it saves me some code. I also genuinely don’t understand the use of pd.read_table
If your data is in some SQL Datasource, you could have used the following code. You get the results in the dataframe format.
# Reading from SQL Datasource import MySQLdb from pandas import DataFrame from pandas.io.sql import read_sql db = MySQLdb.connect(host=&amp;#34;localhost&amp;#34;, # your host, usually localhost user=&amp;#34;root&amp;#34;, # your username passwd=&amp;#34;password&amp;#34;, # your password db=&amp;#34;dbname&amp;#34;) # name of the data base query = &amp;#34;SELECT * FROM tablename&amp;#34; df = read_sql(query, db) Data Snapshot Always useful to see some of the data.
You can use simple head and tail commands with an option to specify the number of rows.
# top 5 rows df.head() # top 50 rows df.head(50) # last 5 rows df.tail() # last 50 rows df.tail(50) You can also see simple dataframe statistics with the following commands.
# To get statistics of numerical columns df.describe() # To get maximum value of a column. When you take a single column you can think of it as a list and apply functions you would apply to a list. You can also use min for instance. print(max(df[&amp;#39;rating&amp;#39;])) # no of rows in dataframe print(len(df)) # Shape of Dataframe print(df.shape) 9.0 1000 (1000,12)  Recommendation: Generally working with Jupyter notebook,I make it a point of having the first few cells in my notebook containing these snapshots of the data. This helps me see the structure of the data whenever I want to. If I don’t follow this practice, I notice that I end up repeating the .head() command a lot of times in my code.
Handling Columns in Dataframes a. Selecting a column For some reason Pandas lets you choose columns in two ways. Using the dot operator like df.Title and using square brackets like df[&#39;Title&#39;]
I prefer the second version, mostly. Why?
There are a couple of reasons you would be better off with the square bracket version in the longer run.
 If your column name contains spaces, then the dot version won’t work. For example, df.Revenue (Millions) won’t work while df[&#39;Revenue (Millions)&#39;] will.
 It also won’t work if your column name is count or mean or any of pandas predefined functions.
 Sometimes you might need to create a for loop over your column names in which your column name might be in a variable. In that case, the dot notation will not work. For Example, This works:
  colname = &amp;#39;height&amp;#39; df[colname] While this doesn’t:
colname = &amp;#39;height&amp;#39; df.colname Trust me. Saving a few characters is not worth it.
Recommendation: Stop using the dot operator. It is a construct that originated from a different language&amp;reg; and respectfully should be left there.
b. Getting Column Names in a list You might need a list of columns for some later processing.
columnnames = df.columns c. Specifying user-defined Column Names: Sometimes you want to change the column names as per your taste. I don’t like spaces in my column names, so I change them as such.
df.columns = [&amp;#39;Rank&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Genre&amp;#39;, &amp;#39;Description&amp;#39;, &amp;#39;Director&amp;#39;, &amp;#39;Actors&amp;#39;, &amp;#39;Year&amp;#39;, &amp;#39;Runtime_Minutes&amp;#39;, &amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Revenue_Millions&amp;#39;, &amp;#39;Metascore&amp;#39;] I could have used another way.
This is the one case where both of the versions are important. When I have to change a lot of column names, I go with the way above. When I have to change the name of just one or two columns I use:
df.rename(columns = {&amp;#39;Revenue (Millions)&amp;#39;:&amp;#39;Rev_M&amp;#39;,&amp;#39;Runtime (Minutes)&amp;#39;:&amp;#39;Runtime_min&amp;#39;},inplace=True) d. Subsetting specific columns: Sometimes you only need to work with particular columns in a dataframe. e.g., to separate numerical and categorical columns, or remove unnecessary columns. Let’s say in our example; we don’t need the description, director, and actor column.
df = df[[&amp;#39;Rank&amp;#39;, &amp;#39;Title&amp;#39;, &amp;#39;Genre&amp;#39;, &amp;#39;Year&amp;#39;,&amp;#39;Runtime_min&amp;#39;, &amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Rev_M&amp;#39;, &amp;#39;Metascore&amp;#39;]] e. Seeing column types: Very useful while debugging. If your code throws an error that you cannot add a str and int, you will like to run this command.
df.dtypes Applying Functions on DataFrame: Apply and Lambda apply and lambda are some of the best things I have learned to use with pandas.
I use apply and lambda anytime I get stuck while building a complex logic for a new column or filter.
a. Creating a Column You can create a new column in many ways.
If you want a column that is a sum or difference of columns, you can pretty much use simple basic arithmetic. Here I get the average rating based on IMDB and Normalized Metascore.
df[&amp;#39;AvgRating&amp;#39;] = (df[&amp;#39;Rating&amp;#39;] &#43; df[&amp;#39;Metascore&amp;#39;]/10)/2 But sometimes we may need to build complex logic around the creation of new columns.
To give you a convoluted example, let’s say that we want to build a custom movie score based on a variety of factors.
Say, If the movie is of the thriller genre, I want to add 1 to the IMDB rating subject to the condition that IMDB rating remains less than or equal to 10. And If a movie is a comedy I want to subtract one from the rating.
How do we do that?
Whenever I get a hold of such complex problems, I use apply/lambda. Let me first show you how I will do this.
def custom_rating(genre,rating): if &amp;#39;Thriller&amp;#39; in genre: return min(10,rating&#43;1) elif &amp;#39;Comedy&amp;#39; in genre: return max(0,rating-1) else: return rating df[&amp;#39;CustomRating&amp;#39;] = df.apply(lambda x: custom_rating(x[&amp;#39;Genre&amp;#39;],x[&amp;#39;Rating&amp;#39;]),axis=1) The general structure is:
 You define a function that will take the column values you want to play with to come up with your logic. Here the only two columns we end up using are genre and rating.
 You use an apply function with lambda along the row with axis=1. The general syntax is:
  df.apply(lambda x: func(x[&amp;#39;col1&amp;#39;],x[&amp;#39;col2&amp;#39;]),axis=1) You should be able to create pretty much any logic using apply/lambda since you just have to worry about the custom function.
b. Filtering a dataframe Pandas make filtering and subsetting dataframes pretty easy. You can filter and subset dataframes using normal operators and &amp;amp;,|,~ operators.
# Single condition: dataframe with all movies rated greater than 8 df_gt_8 = df[df[&amp;#39;Rating&amp;#39;]&amp;gt;8] # Multiple conditions: AND - dataframe with all movies rated greater than 8 and having more than 100000 votes And_df = df[(df[&amp;#39;Rating&amp;#39;]&amp;gt;8) &amp;amp; (df[&amp;#39;Votes&amp;#39;]&amp;gt;100000)] # Multiple conditions: OR - dataframe with all movies rated greater than 8 or having a metascore more than 90 Or_df = df[(df[&amp;#39;Rating&amp;#39;]&amp;gt;8) | (df[&amp;#39;Metascore&amp;#39;]&amp;gt;80)] # Multiple conditions: NOT - dataframe with all emovies rated greater than 8 or having a metascore more than 90 have to be excluded Not_df = df[~((df[&amp;#39;Rating&amp;#39;]&amp;gt;8) | (df[&amp;#39;Metascore&amp;#39;]&amp;gt;80))] Pretty simple stuff.
But sometimes we may need to do complex filtering operations.
And sometimes we need to do some operations which we won’t be able to do using just the above format.
For instance: Let us say we want to filter those rows where the number of words in the movie title is greater than or equal to than 4.
How would you do it?
Trying the below will give you an error. Apparently, you cannot do anything as simple as split with a series.
new_df = df[len(df[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;))&amp;gt;=4] AttributeError: &#39;Series&#39; object has no attribute &#39;split&#39;  One way is first to create a column which contains no of words in the title using apply and then filter on that column.
#create a new column df[&amp;#39;num_words_title&amp;#39;] = df.apply(lambda x : len(x[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;)),axis=1) #simple filter on new column new_df = df[df[&amp;#39;num_words_title&amp;#39;]&amp;gt;=4] And that is a perfectly fine way as long as you don’t have to create a lot of columns. But I prefer this:
new_df = df[df.apply(lambda x : len(x[&amp;#39;Title&amp;#39;].split(&amp;#34; &amp;#34;))&amp;gt;=4,axis=1)] What I did here is that my apply function returns a boolean which can be used to filter.
Now once you understand that you just have to create a column of booleans to filter, you can use any function/logic in your apply statement to get however complex a logic you want to build.
Let us see another example. I will try to do something a little complex to show the structure.
We want to find movies for which the revenue is less than the average revenue for that particular year?
year_revenue_dict = df.groupby([&amp;#39;Year&amp;#39;]).agg({&amp;#39;Rev_M&amp;#39;:np.mean}).to_dict()[&amp;#39;Rev_M&amp;#39;] def bool_provider(revenue, year): return revenue&amp;lt;year_revenue_dict[year] new_df = df[df.apply(lambda x : bool_provider(x[&amp;#39;Rev_M&amp;#39;],x[&amp;#39;Year&amp;#39;]),axis=1)] We have a function here which we can use to write any logic. That provides a lot of power for advanced filtering as long as we can play with simple variables.
c. Change Column Types I even use apply to change the column types since I don’t want to remember the syntax for changing column type and also since it lets me do much more complicated things.
The usual syntax to change column type is astype in Pandas. So if I had a column named price in my data in an str format. I could do this:
df[&amp;#39;Price&amp;#39;] = newDf[&amp;#39;Price&amp;#39;].astype(&amp;#39;int&amp;#39;) But sometimes it won’t work as expected.
You might get the error: ValueError: invalid literal for long() with base 10: ‘13,000’. That is you cannot cast a string with “,” to an int. To do that we first have to get rid of the comma.
After facing this problem time and again, I have stopped using astype altogether now and just use apply to change column types.
df[&amp;#39;Price&amp;#39;] = df.apply(lambda x: int(x[&amp;#39;Price&amp;#39;].replace(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;)),axis=1) And lastly, there is progress_apply progress_apply is a function that comes with tqdm package.
And this has saved me a lot of time.
Sometimes when you have got a lot of rows in your data, or you end up writing a pretty complex apply function, you will see that apply might take a lot of time.
I have seen apply taking hours when working with Spacy. In such cases, you might like to see the progress bar with apply.
You can use tqdm for that.
After the initial imports at the top of your notebook, just replace apply with progress_apply and everything remains the same.
from tqdm import tqdm, tqdm_notebook tqdm_notebook().pandas() df.progress_apply(lambda x: custom_rating_function(x[&amp;#39;Genre&amp;#39;],x[&amp;#39;Rating&amp;#39;]),axis=1) And you get progress bars.
Recommendation:vWhenever you see that you have to create a column with custom complex logic, think of apply and lambda. Try using progress_apply too.
Aggregation on Dataframes: groupby groupby will come up a lot of times whenever you want to aggregate your data. Pandas lets you do this efficiently with the groupby function.
There are a lot of ways that you can use groupby. I have seen a lot of versions, but I prefer a particular style since I feel the version I use is easy, intuitive, and scalable for different use cases.
df.groupby(list of columns to groupby on).aggregate({&amp;#39;colname&amp;#39;:func1, &amp;#39;colname2&amp;#39;:func2}).reset_index() Now you see it is pretty simple. You just have to worry about supplying two primary pieces of information.
 List of columns to groupby on, and
 A dictionary of columns and functions you want to apply to those columns
  reset_index() is a function that resets the index of a dataframe. I apply this function ALWAYS whenever I do a groupby, and you might think of it as a default syntax for groupby operations.
Let us check out an example.
# Find out the sum of votes and revenue by year import numpy as np df.groupby([&amp;#39;Year&amp;#39;]).aggregate({&amp;#39;Votes&amp;#39;:np.sum, &amp;#39;Rev_M&amp;#39;:np.sum}).reset_index() You might also want to group by more than one column. It is fairly straightforward.
df.groupby([&amp;#39;Year&amp;#39;,&amp;#39;Genre&amp;#39;]).aggregate({&amp;#39;Votes&amp;#39;:np.sum, &amp;#39;Rev_M&amp;#39;:np.sum}).reset_index() Recommendation: Stick to one syntax for groupby. Pick your own if you don’t like mine but stick to one.
Dealing with Multiple Dataframes: Concat and Merge: a. concat Sometimes we get data from different sources. Or someone comes to you with multiple files with each file having data for a particular year.
How do we create a single dataframe from a single dataframe?
Here we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic filter operations we already know.
movies_2006 = df[df[&amp;#39;Year&amp;#39;]==2006] movies_2007 = df[df[&amp;#39;Year&amp;#39;]==2007] Here we start with two dataframes: movies_2006 containing info for movies released in 2006 and movies_2007 containing info for movies released in 2007. We want to create a single dataframe that includes movies from both 2006 and 2007
movies_06_07 = pd.concat([movies_2006,movies_2007]) b. merge Most of the data that you will encounter will never come in a single file. One of the files might contain ratings for a particular movie, and another might provide the number of votes for a movie.
In such a case we have two dataframes which need to be merged so that we can have all the information in a single view.
Here we will create our use case artificially since we just have a single file. We are creating two dataframes first using the basic column subset operations we already know.
rating_dataframe = df[[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;]] votes_dataframe = df[[&amp;#39;Title&amp;#39;,&amp;#39;Votes&amp;#39;]] We need to have all this information in a single dataframe. How do we do this?
rating_vote_df = pd.merge(rating_dataframe,votes_dataframe,on=&amp;#39;Title&amp;#39;,how=&amp;#39;left&amp;#39;) rating_vote_df.head() We provide this merge function with four attributes- 1st DF, 2nd DF, join on which column and the joining criteria:[&#39;left&#39;,&#39;right&#39;,&#39;inner&#39;,&#39;outer&#39;]
Recommendation: I usually always end up using left join. You will rarely need to join using outer or right. Actually whenever you need to do a right join you actually just really need a left join with the order of dataframes reversed in the merge function.
Reshaping Dataframes: Melt and pivot_table(reverseMelt) Most of the time, we don’t get data in the exact form we want.
For example, sometimes we might have data in columns which we might need in rows.
Let us create an artificial example again. You can look at the code below that I use to create the example, but really it doesn’t matter.
genre_set = set() for genre in df[&amp;#39;Genre&amp;#39;].unique(): for g in genre.split(&amp;#34;,&amp;#34;): genre_set.add(g) for genre in genre_set: df[genre] = df[&amp;#39;Genre&amp;#39;].apply(lambda x: 1 if genre in x else 0) working_df = df[[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;, &amp;#39;Votes&amp;#39;, &amp;#39;Rev_M&amp;#39;]&#43;list(genre_set)] working_df.head() So we start from a working_df like this:
Now, this is not particularly a great structure to have data in. We might like it better if we had a dataframe with only one column Genre and we can have multiple rows repeated for the same movie. So the movie ‘Prometheus’ might be having three rows since it has three genres. How do we make that work?
We use melt:
reshaped_df = pd.melt(working_df,id_vars = [&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;,&amp;#39;Votes&amp;#39;,&amp;#39;Rev_M&amp;#39;],value_vars = list(genre_set),var_name = &amp;#39;Genre&amp;#39;, value_name =&amp;#39;Flag&amp;#39;) reshaped_df.head() So in this melt function, we provided five attributes:
 dataframe_name = working_df
 id_vars: List of vars we want in the current form only.
 value_vars: List of vars we want to melt/put in the same column
 var_name: name of the column for value_vars
 value_name: name of the column for value of value_vars
  There is still one thing remaining. For Prometheus, we see that it is a thriller and the flag is 0. The flag 0 is unnecessary data we can filter out, and we will have our results. We keep only the genres with flag 1
reshaped_df = reshaped_df[reshaped_df[&amp;#39;Flag&amp;#39;]==1] What if we want to go back?
We need the values in a column to become multiple columns. How? We use pivot_table
re_reshaped_df = reshaped_df.pivot_table(index=[&amp;#39;Title&amp;#39;,&amp;#39;Rating&amp;#39;,&amp;#39;Votes&amp;#39;,&amp;#39;Rev_M&amp;#39;], columns=&amp;#39;Genre&amp;#39;, values=&amp;#39;Flag&amp;#39;, aggfunc=&amp;#39;sum&amp;#39;).reset_index() re_reshaped_df.head() We provided four attributes to the pivot_table function.
 index: We don’t want to change these column structures
 columns: explode this column into multiple columns
 values: use this column to aggregate
 aggfunc: the aggregation function.
  We can then fill the missing values by 0 using fillna
re_reshaped_df=re_reshaped_df.fillna(0) Recommendation: Multiple columns to one column: melt and One column to multiple columns: pivot_table . There are other ways to do melt — stack and different ways to do pivot_table: pivot,unstack. Stay away from them and just use melt and pivot_table. There are some valid reasons for this like unstack and stack will create multi-index and we don’t want to deal with that, and pivot cannot take multiple columns as the index.
Conclusion  With Pandas, less choice is more
 Here I have tried to profile some of the most useful functions in pandas I end up using most often.
Pandas is a vast library with a lot of functionality and custom options. That makes it essential that you should have a mindmap where you stick to a particular syntax for a specific thing.
Here I have shared mine, and you can proceed with it and make it better as your understanding of the library grows.
I hope you found this post useful and worth your time. I tried to make this as simple as possible, but you may always ask me or see the documentation for doubts.
Whole code and data are posted in the Kaggle Kernel.
Also, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more of such posts in the future too. Let me know what you think about them. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>3 Great Additions for your Jupyter Notebooks</title>
      <link>https://mlwhiz.com/blog/2019/06/28/jupyter_extensions/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/06/28/jupyter_extensions/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/extensions/nbext_snippets.gif"></media:content>
      

      
      <description>I love Jupyter notebooks and the power they provide.
They can be used to present findings as well as share code in the most effective manner which was not easy with the previous IDEs.
Yet there is something still amiss.
There are a few functionalities I aspire in my text editor which don’t come by default in Jupyter.
But fret not. Just like everything in Python, Jupyter too has third-party extensions.</description>

      <content:encoded>  
        
        <![CDATA[    I love Jupyter notebooks and the power they provide.
They can be used to present findings as well as share code in the most effective manner which was not easy with the previous IDEs.
Yet there is something still amiss.
There are a few functionalities I aspire in my text editor which don’t come by default in Jupyter.
But fret not. Just like everything in Python, Jupyter too has third-party extensions.
This post is about some of the most useful extensions I found.
1. Collapsible Headings The one extension, I like most is collapsible headings.
It makes the flow of the notebook easier to comprehend and also helps in creating presentable notebooks.
To get this one, install the jupyter_contrib_nbextensions package with this command on the terminal window:
conda install -c conda-forge jupyter_contrib_nbextensions  Once the package is installed, we can start jupyter notebook using:
jupyter notebook  Once you go to the home page of your jupyter notebook, you can see that a new tab for NBExtensions is created.
  And we can get a lot of extensions using this package.
  This is how it looks:
  2. Automatic Imports   Automation is the future.
One thing that bugs me is that whenever I open a new Jupyter notebook in any of my data science projects, I need to copy paste a lot of libraries and default options for some of them.
To tell you about some of the usual imports I use:
 Pandas and numpy — In my view, Python must make these two as a default import.
 Seaborn, matplotlib, plotly_express
 change some pandas and seaborn default options.
  Here is the script that I end up pasting over and over again.
import pandas as pd import numpy as np import plotly_express as px import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline *# We dont Probably need the Gridlines. Do we? If yes comment this line* sns.set(style=&amp;#34;ticks&amp;#34;) # pandas defaults pd.options.display.max_columns = 500 pd.options.display.max_rows = 500 Is there a way I can automate this?
Just go to the nbextensions tab and select the snippets extension.
You will need to make the following changes to the snippets.json file. You can find this file at /miniconda3/envs/py36/share/jupyter/nbextensions/snippets location. The py36 in this location here is my conda virtualenv. It took me some time to find this location for me. Yours might be different. Please note that you don’t have to change at the site-packages location.
{ &amp;#34;snippets&amp;#34; : [ { &amp;#34;name&amp;#34; : &amp;#34;example&amp;#34;, &amp;#34;code&amp;#34; : [ &amp;#34;# This is an example snippet!&amp;#34;, &amp;#34;# To create your own, add a new snippet block to the&amp;#34;, &amp;#34;# snippets.json file in your jupyter nbextensions directory:&amp;#34;, &amp;#34;# /nbextensions/snippets/snippets.json&amp;#34;, &amp;#34;import this&amp;#34; ] }, { &amp;#34;name&amp;#34; : &amp;#34;default&amp;#34;, &amp;#34;code&amp;#34; : [ &amp;#34;# This is A snippet for all data related tasks&amp;#34;, &amp;#34;import pandas as pd&amp;#34; &amp;#34;import numpy as np&amp;#34; &amp;#34;import plotly_express as px&amp;#34; &amp;#34;import seaborn as sns&amp;#34; &amp;#34;import matplotlib.pyplot as plt&amp;#34; &amp;#34;%matplotlib inline&amp;#34; &amp;#34;# We dont Probably need the Gridlines. Do we? If yes comment this line&amp;#34; &amp;#34;sns.set(style=&amp;#39;ticks&amp;#39;)&amp;#34; &amp;#34;# pandas defaults&amp;#34; &amp;#34;pd.options.display.max_columns = 500&amp;#34; &amp;#34;pd.options.display.max_rows = 500&amp;#34; ] } ] } You can see this extension in action below.
  Pretty cool. Right? I also use this to create basic snippets for my deep learning notebooks and NLP based notebooks.
3. Execution Time We have used %time as well as decorator based timer functions to measure time for our functions. You can also use this excellent extension to do that.
Plus it looks great.
Just select the ExecutionTime extension from the NBextensions list and you will have an execution result at the bottom of the cell after every cell execution as well as the time when the cell was executed.
  Other Extensions   NBExtensions has a lot of extensions. Some other extensions from NBExtensions I like and you might want to look at:
 Limit Output: Ever had your notebook hang since you printed a lot of text in your notebook. This extension limits the number of characters that can be printed below a code cell
 2to3Convertor: Having problems with your old python2 notebooks. Tired of changing the print statements. This one is a good one.
     Live Markdown Preview: Some of us like writing our blogs using Markdown in a jupyter notebook. Sometimes it can be hectic as you make errors in writing. Now you can see Live-preview of the rendered output of markdown cells while editing their source.    Conclusion I love how there is a package for everything with Python. And that holds good with the Jupyter notebook too.
The jupyter_contrib_nbextensions package works great out of the box.
It has made my life a lot easier when it comes to checking execution times, scrolling through the notebook, and repetitive tasks.
There are many other extensions this package does provide. Do take a look at them and try to see which ones you find useful.
Also, if you want to learn more about Python 3, I would like to call out an excellent course on Learn Intermediate level Python from the University of Michigan. Do check it out.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>