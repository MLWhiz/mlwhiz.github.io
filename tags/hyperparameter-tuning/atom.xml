<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Hyperparameter Tuning on MLWhiz</title>
    <link>https://mlwhiz.com/tags/hyperparameter-tuning/</link>
    <description>Recent content in Hyperparameter Tuning on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/hyperparameter-tuning/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Hyperopt - A bayesian Parameter Tuning Framework</title>
      <link>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://1.gravatar.com/avatar/14e38645b7816711ca19e971e879c63b?s=180d=identiconr=G"></media:content>
      

      
      <description>Recently I was working on a in-class competition from the &amp;amp;ldquo;How to win a data science competition&amp;amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using XGBoost for time series prediction tasks.
The one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed.</description>

      <content:encoded>  
        
        <![CDATA[  Recently I was working on a in-class competition from the &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using XGBoost for time series prediction tasks.
The one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed. Left the machine with hyperopt in the night. And in the morning I had my results. It was really awesome and I did avoid a lot of hit and trial.
What really is Hyperopt? From the site:
 Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.
 What the above means is that it is a optimizer that could minimize/maximize the loss function/accuracy(or whatever metric) for you.
All of us are fairly known to cross-grid search or random-grid search. Hyperopt takes as an input a space of hyperparams in which it will search, and moves according to the result of past trials.
To know more about how it does this, take a look at this paper by J Bergstra. Here is the documentation from github.
How? Let me just put the code first. This is how I define the objective function. The objective function takes space(the hyperparam space) as the input and returns the loss(The thing you want to minimize.Or negative of the thing you want to maximize)
(X,y) and (Xcv,ycv) are the train and cross validation dataframes respectively.
We have defined a hyperparam space by using the variable space which is actually just a dictionary. We could choose different distributions for different parameter values.
We use the fmin function from the hyperopt package to minimize our fn through the space.
from sklearn.metrics import mean_squared_error import xgboost as xgb from hyperopt import hp, fmin, tpe, STATUS_OK, Trials import numpy as np def objective(space): print(space) clf = xgb.XGBRegressor(n_estimators =1000,colsample_bytree=space[&amp;#39;colsample_bytree&amp;#39;], learning_rate = .3, max_depth = int(space[&amp;#39;max_depth&amp;#39;]), min_child_weight = space[&amp;#39;min_child_weight&amp;#39;], subsample = space[&amp;#39;subsample&amp;#39;], gamma = space[&amp;#39;gamma&amp;#39;], reg_lambda = space[&amp;#39;reg_lambda&amp;#39;],) eval_set = [( X, y), ( Xcv, ycv)] clf.fit(X, y, eval_set=eval_set, eval_metric=&amp;#34;rmse&amp;#34;, early_stopping_rounds=10,verbose=False) pred = clf.predict(Xcv) mse_scr = mean_squared_error(ycv, pred) print &amp;#34;SCORE:&amp;#34;, np.sqrt(mse_scr) #change the metric if you like return {&amp;#39;loss&amp;#39;:mse_scr, &amp;#39;status&amp;#39;: STATUS_OK } space ={&amp;#39;max_depth&amp;#39;: hp.quniform(&amp;#34;x_max_depth&amp;#34;, 4, 16, 1), &amp;#39;min_child_weight&amp;#39;: hp.quniform (&amp;#39;x_min_child&amp;#39;, 1, 10, 1), &amp;#39;subsample&amp;#39;: hp.uniform (&amp;#39;x_subsample&amp;#39;, 0.7, 1), &amp;#39;gamma&amp;#39; : hp.uniform (&amp;#39;x_gamma&amp;#39;, 0.1,0.5), &amp;#39;colsample_bytree&amp;#39; : hp.uniform (&amp;#39;x_colsample_bytree&amp;#39;, 0.7,1), &amp;#39;reg_lambda&amp;#39; : hp.uniform (&amp;#39;x_reg_lambda&amp;#39;, 0,1) } trials = Trials() best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials) print best Finally: Running the above gives us pretty good hyperparams for our learning algorithm. In fact I bagged up the results from multiple hyperparam settings and it gave me the best score on the LB. If you like this and would like to get more information about such things, subscribe to the mailing list on the right hand side. Also I would definitely recommend this course about winning Kaggle competitions by Kazanova, Kaggle rank 3 . Do take a look.
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>