<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on MLWhiz</title>
    <link>https://mlwhiz.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/machine-learning/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Hyperopt - A bayesian Parameter Tuning Framework</title>
      <link>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</guid>
      <description>

&lt;p&gt;Recently I was working on a in-class competition from the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;How to win a data science competition&amp;rdquo;&lt;/a&gt; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using &lt;a href=&#34;https://mlwhiz.com/blog/2017/12/26/how_to_win_a_data_science_competition/&#34;&gt;XGBoost for time series prediction&lt;/a&gt; tasks.&lt;/p&gt;

&lt;p&gt;The one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed. Left the machine with hyperopt in the night. And in the morning I had my results. It was really awesome and I did avoid a lot of hit and trial.&lt;/p&gt;

&lt;h2 id=&#34;what-really-is-hyperopt&#34;&gt;What really is Hyperopt?&lt;/h2&gt;

&lt;p&gt;From the site:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What the above means is that it is a optimizer that could minimize/maximize the loss function/accuracy(or whatever metric) for you.&lt;/p&gt;

&lt;p&gt;All of us are fairly known to cross-grid search or random-grid search. Hyperopt takes as an input a space of hyperparams in which it will search, and moves according to the result of past trials.&lt;/p&gt;

&lt;p&gt;To know more about how it does this, take a look at this &lt;a href=&#34;https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; by J Bergstra.
Here is the &lt;a href=&#34;https://github.com/hyperopt/hyperopt/wiki/FMin&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; from github.&lt;/p&gt;

&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;

&lt;p&gt;Let me just put the code first. This is how I define the objective function. The objective function takes space(the hyperparam space) as the input and returns the loss(The thing you want to minimize.Or negative of the thing you want to maximize)&lt;/p&gt;

&lt;p&gt;(X,y) and (Xcv,ycv) are the train and cross validation dataframes respectively.&lt;/p&gt;

&lt;p&gt;We have defined a hyperparam space by using the variable &lt;code&gt;space&lt;/code&gt; which is actually just a dictionary. We could choose different distributions for different parameter values.&lt;/p&gt;

&lt;p&gt;We use the &lt;code&gt;fmin&lt;/code&gt; function from the hyperopt package to minimize our &lt;code&gt;fn&lt;/code&gt; through the &lt;code&gt;space&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; mean_squared_error
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; xgboost &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; xgb
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; hyperopt &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; hp, fmin, tpe, STATUS_OK, Trials
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;objective&lt;/span&gt;(space):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(space)
    clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xgb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;XGBRegressor(n_estimators &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;,colsample_bytree&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;colsample_bytree&amp;#39;&lt;/span&gt;],
                           learning_rate &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,
                            max_depth &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;]),
                            min_child_weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;min_child_weight&amp;#39;&lt;/span&gt;],
                            subsample &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;subsample&amp;#39;&lt;/span&gt;],
                           gamma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt;],
                           reg_lambda &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; space[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;reg_lambda&amp;#39;&lt;/span&gt;],)

    eval_set  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [( X, y), ( Xcv, ycv)]

    clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X, y,
            eval_set&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;eval_set, eval_metric&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rmse&amp;#34;&lt;/span&gt;,
            early_stopping_rounds&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)

    pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(Xcv)
    mse_scr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mean_squared_error(ycv, pred)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SCORE:&amp;#34;&lt;/span&gt;, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(mse_scr)
    &lt;span style=&#34;color:#75715e&#34;&gt;#change the metric if you like&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;:mse_scr, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;status&amp;#39;&lt;/span&gt;: STATUS_OK }


space &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;max_depth&amp;#39;&lt;/span&gt;: hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;quniform(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x_max_depth&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;min_child_weight&amp;#39;&lt;/span&gt;: hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;quniform (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x_min_child&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;subsample&amp;#39;&lt;/span&gt;: hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x_subsample&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt; : hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x_gamma&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;),
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;colsample_bytree&amp;#39;&lt;/span&gt; : hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x_colsample_bytree&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;reg_lambda&amp;#39;&lt;/span&gt; : hp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x_reg_lambda&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    }


trials &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Trials()
best &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fmin(fn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;objective,
            space&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;space,
            algo&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tpe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;suggest,
            max_evals&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,
            trials&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;trials)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; best&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;finally&#34;&gt;Finally:&lt;/h2&gt;

&lt;p&gt;Running the above gives us pretty good hyperparams for our learning algorithm.
In fact I bagged up the results from multiple hyperparam settings and it gave me the best score on the LB.
If you like this and would like to get more information about such things, subscribe to the mailing list on the right hand side.
Also I would definitely recommend this &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; about winning Kaggle competitions by Kazanova, Kaggle rank 3 . Do take a look.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dictvectorizer for One Hot Encoding of Categorical Data</title>
      <link>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</link>
      <pubDate>Tue, 30 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</guid>
      <description>

&lt;h2 id=&#34;the-problem&#34;&gt;THE PROBLEM:&lt;/h2&gt;

&lt;p&gt;Recently I was working on the Criteo Advertising Competition on Kaggle. The competition was a classification problem which basically involved predicting the click through rates based on several features provided in the train data. Seeing the size of the data (11 GB Train), I felt that going with Vowpal Wabbit might be a better option.&lt;/p&gt;

&lt;p&gt;But after getting to an CV error of .47 on the Kaggle LB and being stuck there , I felt the need to go back to Scikit learn. While SciKit learn seemed to have a partial_fit method in SGDClassifier, I still could not find a partial_fit method in the OneHotEncoder or DictVectorizer class which made me look to the internet again. Now while I could find many advices on how to use OneHotEncoding and DictVectorizer on small data, I cannot find something relate to data too big to store in the memory. How do I OneHotEncode such a large data file?&lt;/p&gt;

&lt;h2 id=&#34;dictvectorizer&#34;&gt;DICTVECTORIZER&lt;/h2&gt;

&lt;p&gt;How does a DictVectorizer works. There is a lot of stuff around the net for this but I dint get to understand much around it. This blog from Zygmuntz of Fastml came to rescue then. Although still it didn’t resolve how to apply that to such large amount of data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.feature_extraction &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DictVectorizer &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; DV
&lt;span style=&#34;color:#75715e&#34;&gt;# Create Vectorizer&lt;/span&gt;
vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DV( sparse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False )
&lt;span style=&#34;color:#75715e&#34;&gt;# Read the whole Data&lt;/span&gt;
traindata &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(train_file, header&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, sep&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;, names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; colnames)
&lt;span style=&#34;color:#75715e&#34;&gt;# Retain the categorical Columns&lt;/span&gt;
train_df   &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; traindata[cat_col]
&lt;span style=&#34;color:#75715e&#34;&gt;# Convert Panda Data frame to Dict&lt;/span&gt;
train_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_dict()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values()
&lt;span style=&#34;color:#75715e&#34;&gt;# Create Fit&lt;/span&gt;
vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(test_dict)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;the-data&#34;&gt;THE DATA&lt;/h2&gt;

&lt;p&gt;The data was basically comprised of 40 Features with: 1. First two Columns as ID, Label 2. Next 13 columns Continuous columns labelled I1-I13 3. Next 26 Columns Categorical labelled C1-C26 Further the categorical columns were very sparse and some of the categorical variables could take more than a million different values.&lt;/p&gt;

&lt;h2 id=&#34;the-workarounds&#34;&gt;THE WORKAROUNDS&lt;/h2&gt;

&lt;p&gt;The main problem that I faced was that I could not fit that much data in a DataFrame, even when I have a machine of 16GB, and that lead me to think that do I have a need for such a large data frame. And that lead me to the first part of the solution. I don’t need to load the whole data at once. I just needed to create another dictionary with all the possible combinations and then fit my dictvectorizer on it.&lt;/p&gt;

&lt;p&gt;I know that it is a lot to take in, so let’s take an example to understand it: Let’s say we have a data of infinite size, which has 3 categorical variables: C1 could take values 1-100 C2 could take values 1-3 C3 could take values 1-1000 Then we just have to find which category could take the maximum number of values (i.e. C3 in the above case) and make a dict which contains other categories replicated to contain as many values In other words, we need to make a dict like: {C1 : [1,2,3,……,97,98,99,100]*10  , C2 : [1,2,3]*333+[1]  , C3: [1….1000]} Notice the star sign at the last of the list. That means that for every key in the dict the number of values is now 1000(i.e. the maximum number of features).&lt;/p&gt;

&lt;p&gt;And so that is what I did. After we have the Vectorizer Fit, the next task was to transform the data. I took the data transformed it and sent it to my model line by line. P.S. Don’t store the transformed data as around a 100000 records takes ~ 10GB of Hard Disk Space due to the high number of features.&lt;/p&gt;

&lt;p&gt;Hope you find it Informative and happy learning.&lt;/p&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
  </channel>
</rss>