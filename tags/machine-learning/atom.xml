<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Machine Learning on MLWhiz</title>
    <link>https://mlwhiz.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/machine-learning/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Hyperopt - A bayesian Parameter Tuning Framework</title>
      <link>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/</guid>
      

      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.comhttps://1.gravatar.com/avatar/14e38645b7816711ca19e971e879c63b?s=180&amp;d=identicon&amp;r=G"></media:content>

      

      
      <description>Recently I was working on a in-class competition from the &amp;ldquo;How to win a data science competition&amp;rdquo; Coursera course. You can start for free with the 7-day Free Trial. Learned a lot of new things from that about using XGBoost for time series prediction tasks.
The one thing that I tried out in this competition was the Hyperopt package - A bayesian Parameter Tuning Framework. And I was literally amazed.</description>
    </item>
    

    <item>
      <title>Dictvectorizer for One Hot Encoding of Categorical Data</title>
      <link>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</link>
      <pubDate>Tue, 30 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/30/dictvectorizer_one_hot_encoding/</guid>
      

      
      <description>THE PROBLEM: Recently I was working on the Criteo Advertising Competition on Kaggle. The competition was a classification problem which basically involved predicting the click through rates based on several features provided in the train data. Seeing the size of the data (11 GB Train), I felt that going with Vowpal Wabbit might be a better option.
But after getting to an CV error of .47 on the Kaggle LB and being stuck there , I felt the need to go back to Scikit learn.</description>
    </item>
    
  </channel>
</rss>