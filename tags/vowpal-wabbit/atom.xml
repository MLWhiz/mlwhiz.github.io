<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Vowpal Wabbit on MLWhiz</title>
    <link>https://mlwhiz.com/tags/vowpal-wabbit/</link>
    <description>Recent content in Vowpal Wabbit on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Dec 2014 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/vowpal-wabbit/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Exploring Vowpal Wabbit with the Avazu Clickthrough Prediction Challenge</title>
      <link>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/12/01/exploring_vowpal_wabbit_avazu/</guid>
      
      

      
      <description>In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.</description>

      <content:encoded>  
        
        <![CDATA[ In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding.
For this competition, we have provided 11 days worth of Avazu data to build and test prediction models. Can you find a strategy that beats standard classification algorithms? The winning models from this competition will be released under an open-source license.
Data Fields Loading Data ## Loading the data  import pandas as pd import numpy as np import string as stri #too large data not keeping it in memory. # will be using line by line scripting. #data = pd.read_csv(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/train&amp;#34;) Since the data is too large around 6 gb , we will proceed by doing line by line analysis of data. We will try to use vowpal wabbit first of all as it is an online model and it also gives us the option of minimizing log loss as a default. It is also very fast to run and will give us quite an intuition as to how good our prediction can be.
I will use all the variables in the first implementation and we will rediscover things as we move on
Running Vowpal Wabbit Creating data in vowpal format (One Time Only) from datetime import datetime def csv_to_vw(loc_csv, loc_output, train=True): start = datetime.now() print(&amp;#34;\nTurning %sinto %s. Is_train_set? %s&amp;#34;%(loc_csv,loc_output,train)) i = open(loc_csv, &amp;#34;r&amp;#34;) j = open(loc_output, &amp;#39;wb&amp;#39;) counter=0 with i as infile: line_count=0 for line in infile: # to counter the header if line_count==0: line_count=1 continue # The data has all categorical features #numerical_features = &amp;#34;&amp;#34; categorical_features = &amp;#34;&amp;#34; counter = counter&#43;1 #print counter line = line.split(&amp;#34;,&amp;#34;) if train: #working on the date column. We will take day , hour a = line[2] new_date= datetime(int(&amp;#34;20&amp;#34;&#43;a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(&amp;#34;%A&amp;#34;) hour= a[6:8] categorical_features &#43;= &amp;#34; |hr %s&amp;#34; % hour categorical_features &#43;= &amp;#34; |day %s&amp;#34; % day # 24 columns in data  for i in range(3,24): if line[i] != &amp;#34;&amp;#34;: categorical_features &#43;= &amp;#34;|c%s%s&amp;#34; % (str(i),line[i]) else: a = line[1] new_date= datetime(int(&amp;#34;20&amp;#34;&#43;a[0:2]),int(a[2:4]),int(a[4:6])) day = new_date.strftime(&amp;#34;%A&amp;#34;) hour= a[6:8] categorical_features &#43;= &amp;#34; |hr %s&amp;#34; % hour categorical_features &#43;= &amp;#34; |day %s&amp;#34; % day for i in range(2,23): if line[i] != &amp;#34;&amp;#34;: categorical_features &#43;= &amp;#34; |c%s%s&amp;#34; % (str(i&#43;1),line[i]) #Creating the labels #print &amp;#34;a&amp;#34; if train: #we care about labels if line[1] == &amp;#34;1&amp;#34;: label = 1 else: label = -1 #we set negative label to -1 #print (numerical_features) #print categorical_features j.write( &amp;#34;%s&amp;#39;%s%s\n&amp;#34; % (label,line[0],categorical_features)) else: #we dont care about labels #print ( &amp;#34;1 &amp;#39;%s |i%s |c%s\n&amp;#34; % (line[0],numerical_features,categorical_features) ) j.write( &amp;#34;1 &amp;#39;%s%s\n&amp;#34; % (line[0],categorical_features) ) #Reporting progress #print counter if counter % 1000000 == 0: print(&amp;#34;%s\t%s&amp;#34;%(counter, str(datetime.now() - start))) print(&amp;#34;\n%sTask execution time:\n\t%s&amp;#34;%(counter, str(datetime.now() - start))) #csv_to_vw(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/train&amp;#34;, &amp;#34;/Users/RahulAgarwal/kaggle_cpr/click.train_original_data.vw&amp;#34;,train=True) #csv_to_vw(&amp;#34;/Users/RahulAgarwal/kaggle_cpr/test&amp;#34;, &amp;#34;/Users/RahulAgarwal/kaggle_cpr/click.test_original_data.vw&amp;#34;,train=False) Running Vowpal Wabbit on the data The Vowpal Wabbit will be run on the command line itself.
Training VW:
vw click.train_original_data.vw -f click.model.vw --loss_function logistic Testing VW:
vw click.test_original_data.vw -t -i click.model.vw -p click.preds.txt Creating Kaggle Submission File import math def zygmoid(x): return 1 / (1 &#43; math.exp(-x)) with open(&amp;#34;kaggle.click.submission.csv&amp;#34;,&amp;#34;wb&amp;#34;) as outfile: outfile.write(&amp;#34;id,click\n&amp;#34;) for line in open(&amp;#34;click.preds.txt&amp;#34;): row = line.strip().split(&amp;#34; &amp;#34;) try: outfile.write(&amp;#34;%s,%f\n&amp;#34;%(row[1],zygmoid(float(row[0])))) except: pass This solution ranked 211/371 submissions at the time and the leaderboard score was 0.4031825 while the best leaderboard score was 0.3901120
Next Steps   Create a better VW model
 Shuffle the data before making the model as the VW algorithm is an online learner and might have given more preference to the latest data provide high weights for clicks as data is skewed. How Much? tune VW algorithm using vw-hypersearch. What should be tuned? Use categorical features like |C1 &amp;ldquo;C1&amp;rdquo;&amp;amp;&amp;ldquo;1&amp;rdquo;    Create a XGBoost Model.
  Create a Sofia-ML Model and see how it works on this data.
  ]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>