<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Big Data on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/tags/big-data/</link><description>Recent content in Big Data on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 07 Jul 2023 00:36:31 +0100</lastBuildDate><atom:link href="https://mlwhiz.com/tags/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>The Most Complete Guide to pySpark DataFrames</title><link>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</guid><description>&lt;p>Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.&lt;/p></description></item><item><title>Practical Spark Tips for Data Scientists</title><link>https://mlwhiz.com/blog/2020/03/20/practicalspark/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/03/20/practicalspark/</guid><description>&lt;p>&lt;em>&lt;strong>I know — Spark is sometimes frustrating to work with.&lt;/strong>&lt;/em>&lt;/p></description></item><item><title>5 Ways to add a new column in a PySpark Dataframe</title><link>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</guid><description>&lt;p>&lt;em>&lt;strong>Too much data is getting generated day by day.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>Although sometimes we can manage our big data using tools like 

&lt;a href="https://towardsdatascience.com/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=---------5------------------" target="_blank" rel="nofollow noopener">Rapids&lt;/a>
 or 

&lt;a href="https://towardsdatascience.com/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8?source=---------11------------------" target="_blank" rel="nofollow noopener">Parallelization&lt;/a>
, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.&lt;/p></description></item><item><title>100x faster Hyperparameter Search Framework with Pyspark</title><link>https://mlwhiz.com/blog/2020/02/22/hyperspark/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/22/hyperspark/</guid><description>&lt;p>Recently I was working on tuning hyperparameters for a huge Machine Learning model.&lt;/p></description></item><item><title>Data Scientists, The 5 Graph Algorithms that you should know</title><link>https://mlwhiz.com/blog/2019/09/02/graph_algs/</link><pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/09/02/graph_algs/</guid><description>&lt;p>We as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.&lt;/p></description></item><item><title>How did I learn Data Science?</title><link>https://mlwhiz.com/blog/2019/08/12/resources/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/08/12/resources/</guid><description>&lt;p>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.&lt;/p></description></item><item><title>The Hitchhikers guide to handle Big Data using Spark</title><link>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</guid><description>&lt;p>Big Data has become synonymous with Data engineering.&lt;/p>
&lt;p>But the line between Data Engineering and Data scientists is blurring day by day.&lt;/p></description></item><item><title>To all Data Scientists - The one Graph Algorithm you need to know</title><link>https://mlwhiz.com/blog/2018/12/07/connected_components/</link><pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/07/connected_components/</guid><description>&lt;p>Graphs provide us with a very useful data structure. They can help us to find structure within our data. With the advent of Machine learning and big data we need to get as much information as possible about our data. Learning a little bit of graph theory can certainly help us with that.&lt;/p></description></item><item><title>Top Data Science Resources on the Internet right now</title><link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link><pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid><description>&lt;p>I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.&lt;/p></description></item><item><title>Learning Spark using Python: Basics and Applications</title><link>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</link><pubDate>Mon, 07 Sep 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</guid><description>&lt;p>I generally have a use case for &lt;a href="https://hadoop.apache.org/" target="_blank" rel="nofollow">Hadoop&lt;/a> in my daily job. It has made my life easier in a sense that I am able to get results which I was not able to see with SQL queries. But still I find it painfully slow.
I have to write procedural programs while I work. As in merge these two datasets and then filter and then merge another dataset and then filter using some condition and yada-yada.
You get the gist. And in hadoop its painstakingly boring to do this. You have to write more than maybe 3 Mapreduce Jobs. One job will read the data line by line and write to the disk.&lt;/p></description></item><item><title>Hadoop Mapreduce Streaming Tricks and Techniques</title><link>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</link><pubDate>Sat, 09 May 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_technique/</guid><description>&lt;p>I have been using Hadoop a lot now a days and thought about writing some of the novel techniques that a user could use to get the most out of the Hadoop Ecosystem.&lt;/p></description></item><item><title>Learning pyspark – Installation – Part 1</title><link>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</link><pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</guid><description>&lt;p>This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.&lt;/p></description></item><item><title>Hadoop, Mapreduce and More – Part 1</title><link>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</link><pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</guid><description>&lt;p>It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all.
What I understood finally is that Hadoop is basically comprised of 3 elements:&lt;/p></description></item></channel></rss>