<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mapreduce on MLWhiz</title>
    <link>https://mlwhiz.com/tags/mapreduce/</link>
    <description>Recent content in Mapreduce on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Sep 2014 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/mapreduce/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Learning pyspark – Installation – Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/28/learning_pyspark/</guid>
      <description>

&lt;p&gt;This is part one of a learning series of pyspark, which is a python binding to the spark program written in Scala.&lt;/p&gt;

&lt;p&gt;The installation is pretty simple. These steps were done on Mac OS Mavericks but should work for Linux too. Here are the steps for the installation:&lt;/p&gt;

&lt;h2 id=&#34;1-download-the-binaries&#34;&gt;1. Download the Binaries:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;Spark : http:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;spark&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apache&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;org&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;downloads&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;html
Scala : http:&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;www&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scala&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;lang&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;org&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;download&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;

Dont use Latest Version of Scala, Use Scala &lt;span style=&#34;color:#ae81ff&#34;&gt;2.10&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;2-add-these-lines-to-your-bash-profile&#34;&gt;2. Add these lines to your .bash_profile:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;export SCALA_HOME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;your_path_to_scala
export SPARK_HOME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;your_path_to_spark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;3-build-spark-this-will-take-time&#34;&gt;3. Build Spark(This will take time):&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;brew install sbt
cd $SPARK_HOME
sbt/sbt assembly&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;4-start-the-pyspark-shell&#34;&gt;4. Start the Pyspark Shell:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$SPARK_HOME/bin/pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And Voila. You are running pyspark on your Machine&lt;/p&gt;

&lt;p&gt;To check that everything is properly installed, Lets run a simple program:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parallelize([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should return 3.
So Now Just Run Hadoop On your Machine and then run pyspark Using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /usr/local/hadoop/
bin/start-all.sh
jps
$SPARK_HOME/bin/pyspark&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Hadoop, Mapreduce and More – Part 1</title>
      <link>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</link>
      <pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2014/09/27/hadoop_mapreduce/</guid>
      <description>

&lt;p&gt;It has been some time since I was stalling learning Hadoop. Finally got some free time and realized that Hadoop may not be so difficult after all.
What I understood finally is that Hadoop is basically comprised of 3 elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A File System&lt;/li&gt;
&lt;li&gt;Map – Reduce&lt;/li&gt;
&lt;li&gt;Its many individual Components.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s go through each of them one by one.&lt;/p&gt;

&lt;h2 id=&#34;1-hadoop-as-a-file-system&#34;&gt;1. Hadoop as a File System:&lt;/h2&gt;

&lt;p&gt;One of the main things that Hadoop provides is cheap data storage. What happens intrinsically is that the Hadoop system takes a file, cuts it into chunks and keeps those chunks at different places in a cluster. Suppose you have a big big file in your local system and you want that file to be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;On the cloud for easy access&lt;/li&gt;
&lt;li&gt;Processable in human time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The one thing you can look forward to is Hadoop.&lt;/p&gt;

&lt;p&gt;Assuming that you have got hadoop installed on the amazon cluster you are working on.&lt;/p&gt;

&lt;h3 id=&#34;start-the-hadoop-cluster&#34;&gt;Start the Hadoop Cluster:&lt;/h3&gt;

&lt;p&gt;You need to run the following commands to start the hadoop cluster(Based on location of hadoop installation directory):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /usr/local/hadoop/
bin/start-all.sh
jps&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Adding File to HDFS:&lt;/strong&gt; Every command in Hadoop starts with hadoop fs and the rest of it works like the UNIX syntax. To add a file “purchases.txt” to the hdfs system:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;hadoop fs -put purchases.txt /usr/purchases.txt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;2-hadoop-for-map-reduce&#34;&gt;2. Hadoop for Map-Reduce:&lt;/h2&gt;

&lt;p&gt;MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster.&lt;/p&gt;

&lt;p&gt;While Hadoop is implemented in Java, you can use almost any language to do map-reduce in hadoop using hadoop streaming. Suppose you have a big file containing the Name of store and sales of store each hour. And you want to find out the sales per store using map-reduce. Lets Write a sample code for that:&lt;/p&gt;

&lt;p&gt;InputFile&lt;/p&gt;

&lt;pre style=&#34;font-family:courier new,monospace; background-color:#f6c6529c; color:#000000&#34;&gt;A,300,12:00
B,234,1:00
C,234,2:00
D,123,3:00
A,123,1:00
B,346,2:00
&lt;/pre&gt;

&lt;p&gt;Mapper.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mapper&lt;/span&gt;():
    &lt;span style=&#34;color:#75715e&#34;&gt;# The Mapper takes inputs from stdin and prints out store name and value&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
        data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;)
        storeName,Value,time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(storeName,Value)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Reducer.py&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;reducer&lt;/span&gt;():
    &lt;span style=&#34;color:#75715e&#34;&gt;# The reducer takes inputs from mapper and prints out aggregated store name and value&lt;/span&gt;
    salesTotal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    oldKey &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sys&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stdin:
        data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;#Adding a little bit of Defensive programming&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(data) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        curKey,curVal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; oldKey adn oldKey &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; curKey:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(oldKey,salesTotal)
            salesTotal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        oldKey&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;curKey
        salesTotal &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; curVal
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; oldkey&lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt;None:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{0},{1}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(oldKey,salesTotal)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running the program on shell using pipes&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;textfile.txt | ./mapper.py | sort | ./reducer.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running the program on mapreduce using Hadoop Streaming&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;hadoop jar contrib/streaming/hadoop-*streaming*.jar /
-file mapper.py -mapper mapper.py /
-file reducer.py -reducer reducer.py /
-input /inputfile -output /outputfile&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;3-hadoop-components&#34;&gt;3. Hadoop Components:&lt;/h2&gt;

&lt;p&gt;Now if you have been following Hadoop you might have heard about Apache, Cloudera, HortonWorks etc. All of these are Hadoop vendors who provide Hadoop Along with its components. I will talk about the main component of Hadoop here – Hive.
So what exactly is Hive: Hive is a SQL like interface to map-reduce queries. So if you don’t understand all the hocus-pocus of map-reduce but know SQL, you can do map-reduce via Hive.
Seems Promising? It is.
While the syntax is mainly SQL, it is still a little different and there are some quirks that we need to understand to work with Hive.
First of all lets open hive command prompt: For that you just have to type “hive”, and voila you are in.
Here are some general commands&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;show databases  &lt;span style=&#34;color:#75715e&#34;&gt;#   -- See all Databases&lt;/span&gt;
use database     &lt;span style=&#34;color:#75715e&#34;&gt;#     -- Use a particular Database&lt;/span&gt;
show tables       &lt;span style=&#34;color:#75715e&#34;&gt;#     -- See all tables in a particular Database&lt;/span&gt;
describe table    &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Creating an external table:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;CREATE EXTERNAL TABLE IF NOT EXISTS BXDataSet
&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;ISBN STRING,BookTitle STRING, ImageURLL STRING&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
ROW FORMAT DELIMITED  FIELDS TERMINATED BY ‘;’ STORED AS TEXTFILE;
LOAD DATA INPATH ‘/user/book.csv’ OVERWRITE INTO TABLE BXDataSet;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The query commands work the same way as in SQL. You can do all the group by and hive will automatically convert it in map-reduce:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;select&lt;/span&gt; * from tablename;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Stay Tuned for Part 2 – Where we will talk about another components of Hadoop – PIG
To learn more about hadoop in the meantime these are the books I recommend:&lt;/p&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/1491901632/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491901632&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=4122280e94f7bbd0ceebc9d13e60d103&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1491901632&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1491901632&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
  </channel>
</rss>