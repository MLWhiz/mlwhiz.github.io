<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Computer Vision on MLWhiz</title>
    <link>https://mlwhiz.com/tags/computer-vision/</link>
    <description>Recent content in Computer Vision on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/computer-vision/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>The Most Complete Guide to PyTorch for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pytorch_guide/main.png"></media:content>
      

      
      <description>PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.
I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch.</description>

      <content:encoded>  
        
        <![CDATA[  PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.
I remember picking PyTorch up only after some extensive experimentation a couple of years back. To tell you the truth, it took me a lot of time to pick it up but am I glad that I moved from Keras to PyTorch. With its high customizability and pythonic syntax,PyTorch is just a joy to work with, and I would recommend it to anyone who wants to do some heavy lifting with Deep Learning.
So, in this PyTorch guide, I will try to ease some of the pain with PyTorch for starters and go through some of the most important classes and modules that you will require while creating any Neural Network with Pytorch.
But, that is not to say that this is aimed at beginners only as I will also talk about the high customizability PyTorch provides and will talk about custom Layers, Datasets, Dataloaders, and Loss functions.
So let’s get some coffee ☕ ️and start it up.
Tensors Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts.
1. Create a Tensor We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of more things with tensors just like you can do with NumPy arrays.
# Using torch.Tensor t = torch.Tensor([[1,2,3],[3,4,5]]) print(f&amp;#34;Created Tensor Using torch.Tensor:\n{t}&amp;#34;) # Using torch.randn t = torch.randn(3, 5) print(f&amp;#34;Created Tensor Using torch.randn:\n{t}&amp;#34;) # using torch.[ones|zeros](*size) t = torch.ones(3, 5) print(f&amp;#34;Created Tensor Using torch.ones:\n{t}&amp;#34;) t = torch.zeros(3, 5) print(f&amp;#34;Created Tensor Using torch.zeros:\n{t}&amp;#34;) # using torch.randint - a tensor of size 4,5 with entries between 0 and 10(excluded) t = torch.randint(low = 0,high = 10,size = (4,5)) print(f&amp;#34;Created Tensor Using torch.randint:\n{t}&amp;#34;) # Using from_numpy to convert from Numpy Array to Tensor a = np.array([[1,2,3],[3,4,5]]) t = torch.from_numpy(a) print(f&amp;#34;Convert to Tensor From Numpy Array:\n{t}&amp;#34;) # Using .numpy() to convert from Tensor to Numpy array t = t.numpy() print(f&amp;#34;Convert to Numpy Array From Tensor:\n{t}&amp;#34;) 2. Tensor Operations Again, there are a lot of operations you can do on these tensors. The full list of functions can be found here.
A = torch.randn(3,4) W = torch.randn(4,2) # Multiply Matrix A and W t = A.mm(W) print(f&amp;#34;Created Tensor t by Multiplying A and W:\n{t}&amp;#34;) # Transpose Tensor t t = t.t() print(f&amp;#34;Transpose of Tensor t:\n{t}&amp;#34;) # Square each element of t t = t**2 print(f&amp;#34;Square each element of Tensor t:\n{t}&amp;#34;) # return the size of a tensor print(f&amp;#34;Size of Tensor t using .size():\n{t.size()}&amp;#34;) Note: What are PyTorch Variables? In the previous versions of Pytorch, Tensor and Variables used to be different and provided different functionality, but now the Variable API is deprecated, and all methods for variables work with Tensors. So, if you don’t know about them, it’s fine as they re not needed, and if you know them, you can forget about them.
The nn.Module Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects. nn.Module lets you create your Deep Learning models as a class. You can inherit from nn.Moduleto define any model as a class. Every model class necessarily contains an__init__ procedure block and a block for the forward pass.
 In the __init__ part, the user can define all the layers the network is going to have but doesn&amp;rsquo;t yet define how those layers would be connected to each other.
 In the forward pass block, the user defines how data flows from one layer to another inside the network.
  So, put simply, any network we define will look like:
class myNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) x = self.lin2(x) return x Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do).
class myCrazyNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = nn.Linear(784, 30) self.lin2 = nn.Linear(30, 784) self.lin3 = nn.Linear(30, 10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x_lin1 = self.lin1(x) x_lin2 = x &#43; self.lin2(x_lin1) x_lin2 = self.lin1(x_lin2) x = self.lin3(x_lin2) return x We can also check if the neural network forward pass works. I usually do that by first creating some random input and just passing that through the network I have created.
x = torch.randn((100,784)) model = myCrazyNeuralNet() model(x).size() -------------------------- torch.Size([100, 10]) A word about Layers Pytorch is pretty powerful, and you can actually create any new experimental layer by yourself using nn.Module. For example, rather than using the predefined Linear Layer nn.Linear from Pytorch above, we could have created our custom linear layer.
class myCustomLinearLayer(nn.Module): def __init__(self,in_size,out_size): super().__init__() self.weights = nn.Parameter(torch.randn(in_size, out_size)) self.bias = nn.Parameter(torch.zeros(out_size)) def forward(self, x): return x.mm(self.weights) &#43; self.bias You can see how we wrap our weights tensor in nn.Parameter. This is done to make the tensor to be considered as a model parameter. From PyTorch docs:
 Parameters are *Tensor* subclasses, that have a very special property when used with Module - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear in parameters() iterator
 As you will later see, the model.parameters() iterator will be an input to the optimizer. But more on that later.
Right now, we can now use this custom layer in any PyTorch network, just like any other layer.
class myCustomNeuralNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin1 = myCustomLinearLayer(784,10) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin1(x) return x x = torch.randn((100,784)) model = myCustomNeuralNet() model(x).size() ------------------------------------------ torch.Size([100, 10]) But then again, Pytorch would not be so widely used if it didn’t provide a lot of ready to made layers used very frequently in wide varieties of Neural Network architectures. Some examples are: nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.BatchNorm2d, nn.Dropout, nn.Embedding, nn.GRU/nn.LSTM, nn.Softmax, nn.LogSoftmax, nn.MultiheadAttention, nn.TransformerEncoder, nn.TransformerDecoder
I have linked all the layers to their source where you could read all about them, but to show how I usually try to understand a layer and read the docs, I would try to look at a very simple convolutional layer here.
So, a Conv2d Layer needs as input an Image of height H and width W, with Cin channels. Now, for the first layer in a convnet, the number of in_channels would be 3(RGB), and the number of out_channels can be defined by the user. The kernel_size mostly used is 3x3, and the stride normally used is 1.
To check a new layer which I don’t know much about, I usually try to see the input as well as output for the layer like below where I would first initialize the layer:
conv_layer = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (3,3), stride = 1, padding=1) And then pass some random input through it. Here 100 is the batch size.
x = torch.randn((100,3,24,24)) conv_layer(x).size() -------------------------------- torch.Size([100, 64, 24, 24]) So, we get the output from the convolution operation as required, and I have sufficient information on how to use this layer in any Neural Network I design.
Datasets and DataLoaders How would we pass data to our Neural nets while training or while testing? We can definitely pass tensors as we have done above, but Pytorch also provides us with pre-built Datasets to make it easier for us to pass data to our neural nets. You can check out the complete list of datasets provided at torchvision.datasets and torchtext.datasets. But, to give a concrete example for datasets, let’s say we had to pass images to an Image Neural net using a folder which has images in this structure:
data train sailboat kayak . .  We can use torchvision.datasets.ImageFolder dataset to get an example image like below:
from torchvision import transforms from torchvision.datasets import ImageFolder traindir = &amp;#34;data/train/&amp;#34; t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = ImageFolder(root=traindir,transform=t) print(&amp;#34;Num Images in Dataset:&amp;#34;, len(train_dataset)) print(&amp;#34;Example Image and Label:&amp;#34;, train_dataset[2]) This dataset has 847 images, and we can get an image and its label using an index. Now we can pass images one by one to any image neural network using a for loop:
for i in range(0,len(train_dataset)): image ,label = train_dataset[i] pred = model(image) But that is not optimal. We want to do batching. We can actually write some more code to append images and labels in a batch and then pass it to the Neural network. But Pytorch provides us with a utility iterator torch.utils.data.DataLoader to do precisely that. Now we can simply wrap our train_dataset in the Dataloader, and we will get batches instead of individual examples.
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) We can simply iterate with batches using:
for image_batch, label_batch in train_dataloader: print(image_batch.size(),label_batch.size()) break ------------------------------------------------- torch.Size([64, 3, 224, 224]) torch.Size([64]) So actually, the whole process of using datasets and Dataloaders becomes:
t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = torchvision.datasets.ImageFolder(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch) You can look at this particular example in action in my previous blogpost on Image classification using Deep Learning here.
This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case.
Understanding Custom Datasets To write our custom datasets, we can make use of the abstract class torch.utils.data.Dataset provided by Pytorch. We need to inherit this Dataset class and need to define two methods to create a custom Dataset.
 __len__ : a function that returns the size of the dataset. This one is pretty simple to write in most cases.
 __getitem__: a function that takes as input an index i and returns the sample at index i.
  For example, we can create a simple custom dataset that returns an image and a label from a folder. See that most of the tasks are happening in __init__ part where we use glob.glob to get image names and do some general preprocessing.
from glob import glob from PIL import Image from torch.utils.data import Dataset class customImageFolderDataset(Dataset): &amp;#34;&amp;#34;&amp;#34;Custom Image Loader dataset.&amp;#34;&amp;#34;&amp;#34; def __init__(self, root, transform=None): &amp;#34;&amp;#34;&amp;#34; Args: root (string): Path to the images organized in a particular folder structure. transform: Any Pytorch transform to be applied &amp;#34;&amp;#34;&amp;#34; # Get all image paths from a directory self.image_paths = glob(f&amp;#34;{root}/*/*&amp;#34;) # Get the labels from the image paths self.labels = [x.split(&amp;#34;/&amp;#34;)[-2] for x in self.image_paths] # Create a dictionary mapping each label to a index from 0 to len(classes). self.label_to_idx = {x:i for i,x in enumerate(set(self.labels))} self.transform = transform def __len__(self): # return length of dataset return len(self.image_paths) def __getitem__(self, idx): # open and send one image and label img_name = self.image_paths[idx] label = self.labels[idx] image = Image.open(img_name) if self.transform: image = self.transform(image) return image,self.label_to_idx[label] Also, note that we open our images one at a time in the __getitem__ method and not while initializing. This is not done in __init__ because we don&amp;rsquo;t want to load all our images in the memory and just need to load the required ones.
We can now use this dataset with the utility Dataloader just like before. It works just like the previous dataset provided by PyTorch but without some utility functions.
t = transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor()]) train_dataset = customImageFolderDataset(root=traindir,transform=t) train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=True, num_workers=10) for image_batch, label_batch in train_dataloader: pred = myImageNeuralNet(image_batch) Understanding Custom DataLoaders This particular section is a little advanced and can be skipped going through this post as it will not be needed in a lot of situations. But I am adding it for completeness here.
So let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes.
class BiLSTM(nn.Module): def __init__(self): super().__init__() self.hidden_size = 64 drp = 0.1 max_features, embed_size = 10000,300 self.embedding = nn.Embedding(max_features, embed_size) self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True) self.linear = nn.Linear(self.hidden_size*4 , 64) self.relu = nn.ReLU() self.dropout = nn.Dropout(drp) self.out = nn.Linear(64, 1) def forward(self, x): h_embedding = self.embedding(x) h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0)) h_lstm, _ = self.lstm(h_embedding) avg_pool = torch.mean(h_lstm, 1) max_pool, _ = torch.max(h_lstm, 1) conc = torch.cat(( avg_pool, max_pool), 1) conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) return out This network expects its input to be of shape (batch_size, seq_length) and works with any seq_length. We can check this by passing our model two random batches with different sequence lengths(10 and 25).
model = BiLSTM() input_batch_1 = torch.randint(low = 0,high = 10000, size = (100,**10**)) input_batch_2 = torch.randint(low = 0,high = 10000, size = (100,**25**)) print(model(input_batch_1).size()) print(model(input_batch_2).size()) ------------------------------------------------------------------ torch.Size([100, 1]) torch.Size([100, 1]) Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance.
So, how do we do this? Let’s write a very simple custom dataset class first.
class CustomTextDataset(Dataset): &amp;#39;&amp;#39;&amp;#39; Simple Dataset initializes with X and y vectors We start by sorting our X and y vectors by sequence lengths &amp;#39;&amp;#39;&amp;#39; def __init__(self,X,y=None): self.data = list(zip(X,y)) # Sort by length of first element in tuple self.data = sorted(self.data, key=lambda x: len(x[0])) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] Also, let’s generate some random data which we will use with this custom Dataset.
import numpy as np train_data_size = 1024 sizes = np.random.randint(low=50,high=300,size=(train_data_size,)) X = [np.random.randint(0,10000, (sizes[i])) for i in range(train_data_size)] y = np.random.rand(train_data_size).round() #checking one example in dataset print((X[0],y[0])) Example of one random sequence and label. Each integer in the sequence corresponds to a word in the sentence.
We can use the custom dataset now using:
train_dataset = CustomTextDataset(X,y) If we now try to use the Dataloader on this dataset with batch_size&amp;gt;1, we will get an error. Why is that?
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10) for xb,yb in train_dataloader: print(xb.size(),yb.size()) This happens because the sequences have different lengths, and our data loader expects our sequences of the same length. Remember that in the previous image example, we resized all images to size 224 using the transforms, so we didn’t face this error.
So, how do we iterate through this dataset so that each batch has sequences with the same length, but different batches may have different sequence lengths?
We can use collate_fn parameter in the DataLoader that lets us define how to stack sequences in a particular batch. To use this, we need to define a function that takes as input a batch and returns (x_batch, y_batch ) with padded sequence lengths based on max_sequence_length in the batch. The functions I have used in the below function are simple NumPy operations. Also, the function is properly commented so you can understand what is happening.
def collate_text(batch): # get text sequences in batch data = [item[0] for item in batch] # get labels in batch target = [item[1] for item in batch] # get max_seq_length in batch max_seq_len = max([len(x) for x in data]) # pad text sequences based on max_seq_len data = [np.pad(p, (0, max_seq_len - len(p)), &amp;#39;constant&amp;#39;) for p in data] # convert data and target to tensor data = torch.LongTensor(data) target = torch.LongTensor(target) return [data, target] We can now use this collate_fn with our Dataloader as:
train_dataloader = DataLoader(train_dataset,batch_size = 64, shuffle=False, num_workers=10,collate_fn = collate_text) for xb,yb in train_dataloader: print(xb.size(),yb.size()) It will work this time as we have provided a custom collate_fn. And see that the batches have different sequence lengths now. Thus we would be able to train our BiLSTM using variable input sizes just like we wanted.
Training a Neural Network We know how to create a neural network using nn.Module. But how to train it? Any neural network that has to be trained will have a training loop that will look something similar to below:
num_epochs = 5 for epoch in range(num_epochs): # Set model to train mode model.train() for x_batch,y_batch in train_dataloader: # Clear gradients optimizer.zero_grad() # Forward pass - Predicted outputs pred = model(x_batch) # Find Loss and backpropagation of gradients loss = loss_criterion(pred, y_batch) loss.backward() # Update the parameters optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: pred = model(x_batch) val_loss = loss_criterion(pred, y_batch) In the above code, we are running five epochs and in each epoch:
 We iterate through the dataset using a data loader.
 In each iteration, we do a forward pass using model(x_batch)
 We calculate the Loss using a loss_criterion
 We back-propagate that loss using loss.backward() call. We don&amp;rsquo;t have to worry about the calculation of the gradients at all, as this simple call does it all for us.
 Take an optimizer step to change the weights in the whole network using optimizer.step(). This is where weights of the network get modified using the gradients calculated in loss.backward() call.
 We go through the validation data loader to check the validation score/metrics. Before doing validation, we set the model to eval mode using model.eval().Please note we don&amp;rsquo;t back-propagate losses in eval mode.
  Till now, we have talked about how to use nn.Module to create networks and how to use Custom Datasets and Dataloaders with Pytorch. So let&amp;rsquo;s talk about the various options available for Loss Functions and Optimizers.
Loss functions Pytorch provides us with a variety of loss functions for our most common tasks, like Classification and Regression. Some most used examples are nn.CrossEntropyLoss , nn.NLLLoss , nn.KLDivLoss and nn.MSELoss. You can read the documentation of each loss function, but to explain how to use these loss functions, I will go through the example of nn.NLLLoss
The documentation for NLLLoss is pretty succinct. As in, this loss function is used for Multiclass classification, and based on the documentation:
 the input expected needs to be of size (batch_size x Num_Classes ) — These are the predictions from the Neural Network we have created.
 We need to have the log-probabilities of each class in the input — To get log-probabilities from a Neural Network, we can add a LogSoftmax Layer as the last layer of our network.
 The target needs to be a tensor of classes with class numbers in the range(0, C-1) where C is the number of classes.
  So, we can try to use this Loss function for a simple classification network. Please note the LogSoftmax layer after the final linear layer. If you don&amp;rsquo;t want to use this LogSoftmax layer, you could have just used nn.CrossEntropyLoss
class myClassificationNet(nn.Module): def __init__(self): super().__init__() # Define all Layers Here self.lin = nn.Linear(784, 10) self.logsoftmax = nn.LogSoftmax(dim=1) def forward(self, x): # Connect the layer Outputs here to define the forward pass x = self.lin(x) x = self.logsoftmax(x) return x Let’s define a random input to pass to our network to test it:
# some random input: X = torch.randn(100,784) y = torch.randint(low = 0,high = 10,size = (100,)) And pass it through the model to get predictions:
model = myClassificationNet() preds = model(X) We can now get the loss as:
criterion = nn.NLLLoss() loss = criterion(preds,y) loss ------------------------------------------ tensor(2.4852, grad_fn=&amp;lt;NllLossBackward&amp;gt;) Custom Loss Function Defining your custom loss functions is again a piece of cake, and you should be okay as long as you use tensor operations in your loss function. For example, here is the customMseLoss
def customMseLoss(output,target): loss = torch.mean((output - target)**2) return loss You can use this custom loss just like before. But note that we don’t instantiate the loss using criterion this time as we have defined it as a function.
output = model(x) loss = customMseLoss(output, target) loss.backward() If we wanted, we could have also written it as a class using nn.Module , and then we would have been able to use it as an object. Here is an NLLLoss custom example:
class CustomNLLLoss(nn.Module): def __init__(self): super().__init__() def forward(self, x, y): # x should be output from LogSoftmax Layer log_prob = -1.0 * x # Get log_prob based on y class_index as loss=-mean(ylogp) loss = log_prob.gather(1, y.unsqueeze(1)) loss = loss.mean() return loss criterion = CustomNLLLoss() loss = criterion(preds,y) Optimizers Once we get gradients using the loss.backward() call, we need to take an optimizer step to change the weights in the whole network. Pytorch provides a variety of different ready to use optimizers using the torch.optim module. For example: torch.optim.Adadelta , torch.optim.Adagrad , torch.optim.RMSprop and the most widely used torch.optim.Adam.
To use the most used Adam optimizer from PyTorch, we can simply instantiate it with:
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999)) And then use optimizer.zero_grad() and optimizer.step() while training the model.
I am not discussing how to write custom optimizers as it is an infrequent use case, but if you want to have more optimizers, do check out the pytorch-optimizer library, which provides a lot of other optimizers used in research papers. Also, if you anyhow want to create your own optimizers, you can take inspiration using the source code of implemented optimizers in PyTorch or pytorch-optimizers.
Other optimizers from pytorch-optimizer library
Using GPU/Multiple GPUs Till now, whatever we have done is on the CPU. If you want to use a GPU, you can put your model to GPU using model.to(&#39;cuda&#39;). Or if you want to use multiple GPUs, you can use nn.DataParallel. Here is a utility function that checks the number of GPUs in the machine and sets up parallel training automatically using DataParallel if needed.
# Whether to train on a gpu train_on_gpu = torch.cuda.is_available() print(f&amp;#39;Train on gpu: {train_on_gpu}&amp;#39;)# Number of gpus if train_on_gpu: gpu_count = torch.cuda.device_count() print(f&amp;#39;{gpu_count} gpus detected.&amp;#39;) if gpu_count &amp;gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to(&amp;#39;cuda&amp;#39;) if multi_gpu: model = nn.DataParallel(model) The only thing that we will need to change is that we will load our data to GPU while training if we have GPUs. It’s as simple as adding a few lines of code to our training loop.
num_epochs = 5 for epoch in range(num_epochs): model.train() for x_batch,y_batch in train_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() optimizer.zero_grad() pred = model(x_batch) loss = loss_criterion(pred, y_batch) loss.backward() optimizer.step() model.eval() for x_batch,y_batch in valid_dataloader: if train_on_gpu: x_batch,y_batch = x_batch.cuda(), y_batch.cuda() pred = model(x_batch) val_loss = loss_criterion(pred, y_batch) Conclusion Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this.
You can find the code for this post here on my GitHub repo, where I keep codes for all my blogs.
If you want to learn more about Pytorch using a course based structure, take a look at the Deep Neural Networks with PyTorch course by IBM on Coursera. Also, if you want to know more about Deep Learning, I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman’s Introduction to GANs for Data Scientists using PyTorch</title>
      <link>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pyt_gan/main.png"></media:content>
      

      
      <description>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects.</description>

      <content:encoded>  
        
        <![CDATA[  Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and faces generated by artificial intelligence. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.
In my view, GANs will change the way we generate video games and special effects. Using this approach, we could create realistic textures or characters on demand.
So in this post, we’re going to look at the generative adversarial networks behind AI-generated images, and help you to understand how to create and build your similar application with PyTorch. We’ll try to keep the post as intuitive as possible for those of you just starting out, but we’ll try not to dumb it down too much.
At the end of this article, you’ll have a solid understanding of how General Adversarial Networks (GANs) work, and how to build your own.
Task Overview In this post, we will create unique anime characters using the Anime Face Dataset. It is a dataset consisting of 63,632 high-quality anime faces in a number of styles. It’s a good starter dataset because it’s perfect for our goal.
We will be using Deep Convolutional Generative Adversarial Networks (DC-GANs) for our project. Though we’ll be using it to generate the faces of new anime characters, DC-GANs can also be used to create modern fashion styles, general content creation, and sometimes for data augmentation as well.
But before we get into the coding, let’s take a quick look at how GANs work.
INTUITION: Brief Intro to GANs for Generating Fake Images GANs typically employ two dueling neural networks to train a computer to learn the nature of a dataset well enough to generate convincing fakes. One of these Neural Networks generates fakes (the generator), and the other tries to classify which images are fake (the discriminator). These networks improve over time by competing against each other.
Perhaps imagine the generator as a robber and the discriminator as a police officer. The more the robber steals, the better he gets at stealing things. But at the same time, the police officer also gets better at catching the thief. Well, in an ideal world, anyway.
The losses in these neural networks are primarily a function of how the other network performs:
 Discriminator network loss is a function of generator network quality: Loss is high for the discriminator if it gets fooled by the generator’s fake images.
 Generator network loss is a function of discriminator network quality: Loss is high if the generator is not able to fool the discriminator.
  In the training phase, we train our discriminator and generator networks sequentially, intending to improve performance for both. The end goal is to end up with weights that help the generator to create realistic-looking images. In the end, we’ll use the generator neural network to generate high-quality fake images from random noise.
The Generator architecture One of the main problems we face when working with GANs is that the training is not very stable. So we have to come up with a generator architecture that solves our problem and also results in stable training. The diagram below is taken from the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, which explains the DC-GAN generator architecture.
Though it might look a little bit confusing, essentially you can think of a generator neural network as a black box which takes as input a 100 dimension normally generated vector of numbers and gives us an image:
So how do we create such an architecture? Below, we use a dense layer of size 4x4x1024 to create a dense vector out of the 100-d vector. We then reshape the dense vector in the shape of an image of 4×4 with 1024 filters, as shown in the following figure:
Note that we don’t have to worry about any weights right now as the network itself will learn those during training.
Once we have the 1024 4×4 maps, we do upsampling using a series of transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, however, we don’t halve the number of maps. We reduce the maps to 3 for each RGB channel since we need three channels for the output image.
Now, What are Transpose convolutions? Put simply, transposing convolutions provides us with a way to upsample images. In a convolution operation, we try to go from a 4×4 image to a 2×2 image. But when we transpose convolutions, we convolve from 2×2 to 4×4 as shown in the following figure:
Some of you may already know that unpooling is commonly used for upsampling input feature maps in convolutional neural networks (CNN). So why don’t we use unpooling here?
The reason comes down to the fact that unpooling does not involve any learning. However, transposed convolution is learnable, so it’s preferred. Later in the article, we’ll see how the parameters can be learned by the generator.
The Discriminator architecture Now that we’ve covered the generator architecture, let’s look at the discriminator as a black box. In practice, it contains a series of convolutional layers with a dense layer at the end to predict if an image is fake or not. You can see an example in the figure below:
Every image convolutional neural network works by taking an image as input, and predicting if it is real or fake using a sequence of convolutional layers.
Data preprocessing and visualization Before going any further with our training, we preprocess our images to a standard size of 64x64x3. We will also need to normalize the image pixels before we train our GAN. You can see the process in the code below, which I’ve commented on for clarity.
# Root directory for dataset dataroot = &amp;#34;anime_images/&amp;#34; # Number of workers for dataloader workers = 2 # Batch size during training batch_size = 128 # Spatial size of training images. All images will be resized to this size using a transformer. image_size = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # We can use an image folder dataset the way we have it setup. # Create the dataset dataset = datasets.ImageFolder(root=dataroot, transform=transforms.Compose([ transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])) # Create the dataloader dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers) # Decide which device we want to run on device = torch.device(&amp;#34;cuda:0&amp;#34; if (torch.cuda.is_available() and ngpu &amp;gt; 0) else &amp;#34;cpu&amp;#34;) # Plot some training images real_batch = next(iter(dataloader)) plt.figure(figsize=(8,8)) plt.axis(&amp;#34;off&amp;#34;) plt.title(&amp;#34;Training Images&amp;#34;) plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) The resultant output of the code is as follows:
So Many different Characters — Can our Generator understand the patterns?
Implementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.
Generating noise vector for Generator We need to generate the noise which we want to convert to an image using our generator architecture.
We use a normal distribution
to generate the noise vector:
nz = 100 noise = torch.randn(64, nz, 1, 1, device=device)  Generator architecture The generator is the most crucial part of the GAN. Here, we’ll create a generator by adding some transposed convolution layers to upsample the noise vector to an image. You’ll notice that this generator architecture is not the same as the one given in the DC-GAN paper I linked above.
In order to make it a better fit for our data, I had to make some architectural changes. I added a convolution layer in the middle and removed all dense layers from the generator architecture to make it fully convolutional.
I also used a lot of Batchnorm layers and leaky ReLU activation. The following code block is the function I will use to create the generator:
# Size of feature maps in generator ngf = 64 # Number of channels in the training images. For color images this is 3 nc = 3 # Size of z latent vector (i.e. size of generator input noise) nz = 100 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is noise, going into a convolution # Transpose 2D conv layer 1. nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), nn.BatchNorm2d(ngf * 8), nn.ReLU(True), # Resulting state size - (ngf*8) x 4 x 4 i.e. if ngf= 64 the size is 512 maps of 4x4 # Transpose 2D conv layer 2. nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 4), nn.ReLU(True), # Resulting state size -(ngf*4) x 8 x 8 i.e 8x8 maps # Transpose 2D conv layer 3. nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf * 2), nn.ReLU(True), # Resulting state size. (ngf*2) x 16 x 16 # Transpose 2D conv layer 4. nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), nn.BatchNorm2d(ngf), nn.ReLU(True), # Resulting state size. (ngf) x 32 x 32 # Final Transpose 2D conv layer 5 to generate final image. # nc is number of channels - 3 for 3 image channel nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), # Tanh activation to get final normalized image nn.Tanh() # Resulting state size. (nc) x 64 x 64 ) def forward(self, input): &amp;#39;&amp;#39;&amp;#39; This function takes as input the noise vector&amp;#39;&amp;#39;&amp;#39; return self.main(input) Now we can instantiate the model using the generator class. We are keeping the default weight initializer for PyTorch even though the paper says to initialize the weights using a mean of 0 and std dev of 0.2. The default weights initializer from Pytorch is more than good enough for our project.
# Create the generator netG = Generator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &amp;gt; 1): netG = nn.DataParallel(netG, list(range(ngpu))) # Print the model print(netG)  We can see the final generator model:
The Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.
# Number of channels in the training images. For color images this is 3 nc = 3 # Size of feature maps in discriminator ndf = 64 class Discriminator(nn.Module): def __init__(self, ngpu): super(Discriminator, self).__init__() self.ngpu = ngpu self.main = nn.Sequential( # input is (nc) x 64 x 64 nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf) x 32 x 32 nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 2), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*2) x 16 x 16 nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 4), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*4) x 8 x 8 nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), nn.BatchNorm2d(ndf * 8), nn.LeakyReLU(0.2, inplace=True), # state size. (ndf*8) x 4 x 4 nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), nn.Sigmoid() ) def forward(self, input): return self.main(input) Now we can instantiate the discriminator exactly as we did the generator.
# Create the Discriminator netD = Discriminator(ngpu).to(device) # Handle multi-gpu if desired if (device.type == &#39;cuda&#39;) and (ngpu &amp;gt; 1): netD = nn.DataParallel(netD, list(range(ngpu))) # Print the model print(netD)  Here is the architecture of the discriminator:
Training Understanding how the training works in GAN is essential. It’s interesting, too; we can see how training the generator and discriminator together improves them both at the same time.
Now that we have our discriminator and generator models, next we need to initialize separate optimizers for them.
# Initialize BCELoss function criterion = nn.BCELoss() # Create batch of latent vectors that we will use to visualize # the progression of the generator fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Establish convention for real and fake labels during training real_label = 1. fake_label = 0. # Setup Adam optimizers for both G and D # Learning rate for optimizers lr = 0.0002 # Beta1 hyperparam for Adam optimizers beta1 = 0.5 optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) The Training Loop This is the main area where we need to understand how the blocks we’ve created will assemble and work together.
# Lists to keep track of progress/Losses img_list = [] G_losses = [] D_losses = [] iters = 0 # Number of training epochs num_epochs = 50 # Batch size during training batch_size = 128 print(&amp;#34;Starting Training Loop...&amp;#34;) # For each epoch for epoch in range(num_epochs): # For each batch in the dataloader for i, data in enumerate(dataloader, 0): ############################ # (1) Update D network: maximize log(D(x)) &#43; log(1 - D(G(z))) # Here we: # A. train the discriminator on real data # B. Create some fake images from Generator using Noise # C. train the discriminator on fake data ########################### # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images using generator # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D&amp;#39;s loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real &#43; errD_fake # Update D optimizerD.step() ############################ # (2) Update G network: maximize log(D(G(z))) # Here we: # A. Find the discriminator output on Fake images # B. Calculate Generators loss based on this output. Note that the label is 1 for generator. # C. Update Generator ########################### netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost # Since we just updated D, perform another forward pass of all-fake batch through D output = netD(fake).view(-1) # Calculate G&amp;#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() # Output training stats every 50th Iteration in an epoch if i % 1000 == 0: print(&amp;#39;[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f/ %.4f&amp;#39; % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2)) # Save Losses for plotting later G_losses.append(errG.item()) D_losses.append(errD.item()) # Check how the generator is doing by saving G&amp;#39;s output on a fixed_noise vector if (iters % 250 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)): #print(iters) with torch.no_grad(): fake = netG(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True)) iters &#43;= 1 It may seem complicated, but I’ll break down the code above step by step in this section. The main steps in every training iteration are:
Step 1: Sample a batch of normalized images from the dataset
for i, data in enumerate(dataloader, 0):  Step 2: Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their labels.
 # Training Discriminator on real data netD.zero_grad() # Format batch real_cpu = data[0].to(device) b_size = real_cpu.size(0) label = torch.full((b_size,), real_label, device=device) # Forward pass real batch through D output = netD(real_cpu).view(-1) # Calculate loss on real batch errD_real = criterion(output, label) # Calculate gradients for D in backward pass errD_real.backward() D_x = output.mean().item() ## Create a batch of fake images # Generate noise to send as input to the generator noise = torch.randn(b_size, nz, 1, 1, device=device) # Generate fake image batch with G fake = netG(noise) label.fill_(fake_label) # Classify fake batch with D output = netD(fake.detach()).view(-1) # Calculate D&#39;s loss on the fake batch errD_fake = criterion(output, label) # Calculate the gradients for this batch errD_fake.backward() D_G_z1 = output.mean().item() # Add the gradients from the all-real and all-fake batches errD = errD_real &#43; errD_fake # Update D optimizerD.step()  Step 3: Backpropagate the errors through the generator by computing the loss gathered from discriminator output on fake images as the input and 1’s as the target while keeping the discriminator as untrainable — This ensures that the loss is higher when the generator is not able to fool the discriminator. You can check it yourself like so: if the discriminator gives 0 on the fake image, the loss will be high i.e., BCELoss(0,1).
 netG.zero_grad() label.fill_(real_label) # fake labels are real for generator cost output = netD(fake).view(-1) # Calculate G&#39;s loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step()  We repeat the steps using the for loop to end up with a good discriminator and generator.
Results The final output of our generator can be seen below. The GAN generates pretty good images for our content editor friends to work with.
The images might be a little crude, but still, this project was a starter for our GAN journey. The field is constantly advancing with better and more complex GAN architectures, so we’ll likely see further increases in image quality from these architectures. Also, keep in mind that these images are generated from a noise vector only: this means the input is some noise, and the output is an image. It’s quite incredible.
ALL THESE IMAGES ARE FAKE
1. Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average, and the variance is also decreasing as we do more steps. It’s possible that training for even more iterations would give us even better results.
plt.figure(figsize=(10,5)) plt.title(&amp;quot;Generator and Discriminator Loss During Training&amp;quot;) plt.plot(G_losses,label=&amp;quot;G&amp;quot;) plt.plot(D_losses,label=&amp;quot;D&amp;quot;) plt.xlabel(&amp;quot;iterations&amp;quot;) plt.ylabel(&amp;quot;Loss&amp;quot;) plt.legend() plt.show()  2. Image Animation at every 250th Iteration in Jupyter Notebook We can choose to see the output as an animation using the below code:
#%%capture fig = plt.figure(figsize=(8,8)) plt.axis(&amp;quot;off&amp;quot;) ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list] ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)HTML(ani.to_jshtml())  You can choose to save an animation object as a gif as well if you want to send them to some friends.
ani.save(&#39;animation.gif&#39;, writer=&#39;imagemagick&#39;,fps=5) Image(url=&#39;animation.gif&#39;)  3. Image generated at every 200 Iter Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases, the images are getting better.
# create a list of 16 images to show every_nth_image = np.ceil(len(img_list)/16) ims = [np.transpose(img,(1,2,0)) for i,img in enumerate(img_list)if i%every_nth_image==0] print(&amp;quot;Displaying generated images&amp;quot;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(20,20)) gs1 = gridspec.GridSpec(4, 4) gs1.update(wspace=0, hspace=0) step = 0 for i,image in enumerate(ims): ax1 = plt.subplot(gs1[i]) ax1.set_aspect(&#39;equal&#39;) fig = plt.imshow(image) # you might need to change some params here fig = plt.text(7,30,&amp;quot;Step: &amp;quot;&#43;str(step),bbox=dict(facecolor=&#39;red&#39;, alpha=0.5),fontsize=12) plt.axis(&#39;off&#39;) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) step&#43;=int(250*every_nth_image) #plt.tight_layout() plt.savefig(&amp;quot;GENERATEDimage.png&amp;quot;,bbox_inches=&#39;tight&#39;,pad_inches=0) plt.show()  Given below is the result of the GAN at different time steps:
Conclusion In this post, we covered the basics of GANs for creating fairly believable fake images. We hope you now have an understanding of generator and discriminator architecture for DC-GANs, and how to build a simple DC-GAN to generate anime images from scratch.
Though this model is not the most perfect anime face generator, using it as a base helps us to understand the basics of generative adversarial networks, which in turn can be used as a stepping stone to more exciting and complex GANs as we move forward.
Look at it this way, as long as we have the training data at hand, we now have the ability to conjure up realistic textures or characters on demand. That is no small feat.
For a closer look at the code for this post, please visit my GitHub repository where you can find the code for this post as well as all my posts.
If you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew Ng. Andrew is a great instructor, and this course is excellent too.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
This post was first published here
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Creating my First Deep Learning &#43; Data Science Workstation</title>
      <link>https://mlwhiz.com/blog/2020/08/09/owndlrig/</link>
      <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/09/owndlrig/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/owndlrig/main.png"></media:content>
      

      
      <description>Creating my workstation has been a dream for me, if nothing else.
I knew the process involved, yet I somehow never got to it. It might have been time or money. Mostly Money.
But this time I just had to do it. I was just fed up with setting up a server on AWS for any small personal project and fiddling with all the installations. Or I had to work on Google Collab notebooks, which have a lot of limitations on running times and network connections.</description>

      <content:encoded>  
        
        <![CDATA[  Creating my workstation has been a dream for me, if nothing else.
I knew the process involved, yet I somehow never got to it. It might have been time or money. Mostly Money.
But this time I just had to do it. I was just fed up with setting up a server on AWS for any small personal project and fiddling with all the installations. Or I had to work on Google Collab notebooks, which have a lot of limitations on running times and network connections. So, I found out some time to create a Deep Learning Rig with some assistance from NVIDIA folks.
The whole process involved a lot of reading up and watching a lot of Youtube videos from Linus Tech Tips. And as it was the first time I was assembling a computer from scratch, it was sort of special too.
Building the DL rig as per your requirements takes up a lot of research. I researched on individual parts, their performance, reviews, and even the aesthetics.
Now, most of the workstation builds I researched were focussed on gaming, so I thought of putting down a Deep Learning Rig Spec as well.
I will try to put all the components I used along with the reasons why I went with those particular parts as well.
***Also, if you want to see how I set up the Deep Learning libraries after setting up the system to use Ubuntu 18.04, you can view ***this definitive guide for Setting up a Deep Learning Workstation.
So why the need for a workstation? The very first answer that comes to my mind is, why not?
I work a lot on deep learning and machine learning applications, and it always has been such a massive headache to churn up a new server and installing all the dependencies every time I start to work on a new project.
Also, it looks great, sits on your desk, is available all the time, and is open to significant customization as per your requirements.
Adding to this the financial aspects of using the GCP or AWS, and I was pretty much sold on the idea of building my rig.
My Build It took me a couple of weeks to come up with the final build.
I knew from the start that I want to have a lot of computing power and also something that would be upgradable in the coming years. Currently, my main priorities were to get a system that could support two NVIDIA RTX Titan cards with NVLink. That would allow me to have 48GB GPU memory at my disposal. Simply awesome.
PS:* The below build might not be the best build, and there may be cheaper alternatives present, but I know for sure that it is the build with the minimal future headache. So I went with it. I also contacted Nvidia to get a lot of suggestions about this particular build and only went forward after they approved of it.
1. Intel i9 9920x 3.5 GHz 12 core Processor Yes, I went with an Intel processor and not an AMD one. My reason for this (though people may differ with me on this) is because Intel has more compatible and related software like Intel’s MKL, which benefits most of the Python libraries I use.
Another and maybe a more important reason, at least for me, was that it was suggested by the people at NVIDIA to go for i9 if I wanted to have a dual RTX Titan configuration. Again zero headaches in the future.
So why this particular one from the Intel range?
I started with 9820X with its ten cores and 9980XE with 18 cores, but the latter stretched my budget a lot. I found that i9–9920X, with its 12 cores and 3.5 GHz processor, fit my budget just fine, and as it is always better to go for the mid-range solution, I went with it.
Now a CPU is the component that decides a lot of other components you are going to end up using.
For example, if you choose an i9 9900X range of CPU, you will have to select an X299 motherboard, or if you are going to use an AMD Threadripper CPU, you will need an X399 Motherboard. So be mindful of choosing the right CPU and motherboard.
2. MSI X299 SLI PLUS ATX LGA2066 Motherboard This was a particularly difficult choice. There are just too many options here. I wanted a Motherboard that could support at least 96GB RAM (again as per the specifications by the NVIDIA Folks for supporting 2 Titans). That meant that I had to have at least six slots if I were to use 16GB RAM Modules as 16x6=96. I got 8 in this one, so it is expandable till 128 GB RAM.
I also wanted to be able to have 2 TB NVMe SSD in my system(in the future), and that meant I needed 2 M.2 ports, which this board has. Or else I would have to go for a much expensive 2TB Single NVMe SSD.
I looked into a lot of options, and based on the ATX Form factor, 4 PCI-E x16 slots, and the reasonable pricing of the board, I ended up choosing this one.
3. Noctua NH-D15 chromax.BLACK 82.52 CFM CPU Cooler Liquid cooling is in rage right now. And initially, I also wanted to go for an AIO cooler, i.e., liquid cooling.
But after talking to a couple of people at NVIDIA as well as scrouging through the internet forums on the pro and cons of both options, I realized that Air cooling is better suited to my needs. So I went for the Noctua NH-D15, which is one of the best Air coolers in the market. So, I went with the best air cooling instead of a mediocre water cooling. And this cooler is SILENT. More on this later.
4. Phanteks Enthoo Pro Tempered Glass Case The next thing to think was a case that is going to be big enough to handle all these components and also be able to provide the required cooling. It was where I spent most of my time while researching.
I mean, we are going to keep 2 Titan RTX, 9920x CPU, 128 GB RAM. It’s going to be a hellish lot of heat in there.
Add to that the space requirements for the Noctua air cooler and the capability to add a lot of fans, and I was left with two options based on my poor aesthetic sense as well as the availability in my country. The options were — Corsair Air 540 ATX and the Phanteks Enthoo Pro Tempered Glass PH-ES614PTG_SWT.
Both of them are exceptional cases, but I went through with the Enthoo Pro as it is a more recently launched case and has a bigger form factor(Full Tower) offers options for more customizable build in the future too.
5. Dual Titan RTX with 3 Slot NVLink These 2 Titan RTX are by far the most important and expensive part of the whole build. These alone take up 80% of the cost, but aren’t they awesome?
I wanted to have a high-performance GPU in my build, and the good folks at NVIDIA were generous enough to send me two of these to test out.
I just love them. The design. The way they look in the build and the fact that they can be combined using a 3 Slot NVLink to provide 48 GB of GPU RAM effectively. Just awesome. If money is an issue, 2 x RTX 2080 Ti would also work fine as well. Only a problem will be that you might need smaller batch sizes training on RTX 2080 Ti, and in some cases, you might not be able to train large models as RTX2080Ti has 11GB RAM only. Also, you won’t be able to use NVLink, which combines the VRAM of multiple GPUs in Titans.
6. Samsung 970 Evo Plus 1 TB NVME Solid State Drive What about storage? NVMe SSD, of course, and the Samsung Evo Plus is the unanimous and most popular winner in this SSD race.
I bought 1 of them till now, but as I have 2 M.2 ports in my motherboard, I will get total storage of 2TB SSD in the future.
You can also get a couple of 2.5&amp;rdquo; SSD for more storage space.
7. Corsair Vengeance LPX 128GB (8x16GB) DDR4 3200 MHz I wanted to have a minimum of 96GB RAM, as suggested by the NVIDIA team. So I said what the heck and went with the full 128 GB RAM without cheaping out.
As you can see, these RAM sticks are not RGB lit, and that is a conscious decision as the Noctua Air Cooler doesn’t provide a lot of clearance for RAM Slots and the RGB ones had a slightly higher height. So keep that in mind. Also, I was never trying to go for an RGB Build anyway as I want to focus on those lit up Titans in my build.
8. Corsair 1200W Power Supply A 1200W power supply is a pretty big one, but that is needed realizing that the estimated wattage of our components at full wattage is going to be ~965W.
I had a couple of options for the power supply from other manufacturers also but went with this one because of Corsair’s name. I would have gone with HX1200i, but it was not available, and AX1200i was much more expensive than this one at my location. But both of them are excellent options apart from this one.
9. Even More Fans The Phanteks case comes up with three fans, but I was recommended to upgrade the intake, and exhaust fans of the case to BeQuiet BL071 PWM Fans as Dual Titans can put out a lot of heat. I have noticed that the temperature of my room is almost 2–3 degrees higher than the outside temperature, as I generally keep the machine on.
To get the best possible airflow, I bought 5 of these. I have put two at the top of the case along with a Phanteks case fan, 2 of them in the front, and one fan at the back of the case.
10. Peripherals The Essentials — A cup of tea and those speakers
This section is not necessary but wanted to put it in for completion.
Given all the power we have got, I didn’t want to cheap out on the peripherals. So I got myself an LG 27UK650 4k monitor for content creation, BenQ EX2780Q 1440p 144hz Gaming Monitor for a little bit of gaming, a Mechanical Cherry MX Red Corsair K68 Keyboard and a Corsair M65 Pro Mouse.
And my build is complete.
Pricing 💰💰💰 I will put the price as per the PCPartPicker site as I have gotten my components from different countries and sources. You can also check the part list at the PCPartPicker site: https://pcpartpicker.com/list/zLVjZf
As you can see, this is pretty expensive by any means (even after getting the GPUs from NVIDIA), but that is the price you pay for certain afflictions, I guess.
Finally In this post, I talked about all the parts you are going to need to assemble your deep learning rig and my reasons for getting these in particular.
You might try to look out for better components or a different design, but this one has been working pretty well for me for quite some time now, and is it fast.
If you want to see how I set up the Deep Learning libraries after setting up the system with these components, you can view this definitive guide for Setting up a Deep Learning Workstation with Ubuntu 18.04
Let me know what you think in the comments.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</title>
      <link>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/deployment_fastapi/main.png"></media:content>
      

      
      <description>Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS.</description>

      <content:encoded>  
        
        <![CDATA[  Just recently, I had written a simple tutorial on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.
That post got quite a good response, but the most asked question was how to deploy the FastAPI API on ec2 and how to use images data rather than simple strings, integers, and floats as input to the API.
I scoured the net for this, but all I could find was some undercooked documentation and a lot of different ways people were taking to deploy using NGINX or ECS. None of those seemed particularly great or complete to me.
So, I tried to do this myself using some help from FastAPI documentation. In this post, we will look at predominantly four things:
 Setting Up an Amazon Instance
 Creating a FastAPI API for Object Detection
 Deploying FastAPI using Docker
 An End to End App with UI
  So, without further ado, let’s get started.
You can skip any part you feel you are versed with though I would expect you to go through the whole post, long as it may be, as there’s a lot of interconnection between concepts.
1. Setting Up Amazon Instance Before we start with using the Amazon ec2 instance, we need to set one up. You might need to sign up with your email ID and set up the payment information on the AWS website. Works just like a single sign-on. From here, I will assume that you have an AWS account and so I am going to explain the next essential parts so you can follow through.
 Go to AWS Management Console using https://us-west-2.console.aws.amazon.com/console.
 On the AWS Management Console, you can select “Launch a Virtual Machine.” Here we are trying to set up the machine where we will deploy our FastAPI API.
 In the first step, you need to choose the AMI template for the machine. I am selecting the 18.04 Ubuntu Server since Ubuntu.
   In the second step, I select the t2.xlarge machine, which has 4 CPUs and 16GB RAM rather than the free tier since I want to use an Object Detection model and will need some resources.   Keep pressing Next until you reach the “6. Configure Security Group” tab. This is the most crucial step here. You will need to add a rule with Type: “HTTP” and Port Range:80.   You can click on “Review and Launch” and finally on the “Launch” button to launch the instance. Once you click on Launch, you might need to create a new key pair. Here I am creating a new key pair named fastapi and downloading that using the “Download Key Pair” button. Keep this key safe as it would be required every time you need to login to this particular machine. Click on “Launch Instance” after downloading the key pair   You can now go to your instances to see if your instance has started. Hint: See the Instance state; it should be showing “Running.”   Also, to note here are the Public DNS(IPv4) address and the IPv4 public IP. We will need it to connect to this machine. For me, they are:  Public DNS (IPv4): ec2-18-237-28-174.us-west-2.compute.amazonaws.com IPv4 Public IP: 18.237.28.174   Once you have that run the following commands in the folder, you saved the fastapi.pem file. If the file is named fastapi.txt you might need to rename it to fastapi.pem.  # run fist command if fastapi.txt gets downloaded. # mv fastapi.txt fastapi.pem chmod 400 fastapi.pem ssh -i &amp;quot;fastapi.pem&amp;quot; ubuntu@&amp;lt;Your Public DNS(IPv4) Address&amp;gt;  Now we have got our Amazon instance up and running. We can move on here to the real part of the post.
2. Creating a FastAPI API for Object Detection Before we deploy an API, we need to have an API with us, right? In one of my last posts, I had written a simple tutorial to understand FastAPI and API basics. Do read the post if you want to understand FastAPI basics.
So, here I will try to create an Image detection API. As for how to pass the Image data to the API? The idea is — What is an image but a string? An image is just made up of bytes, and we can encode these bytes as a string. We will use the base64 string representation, which is a popular way to get binary data to ASCII characters. And, we will pass this string representation to give an image to our API.
A. Some Image Basics: What is Image, But a String? So, let us first see how we can convert an Image to a String. We read the binary data from an image file using the ‘rb’ flag and turn it into a base64 encoded data representation using the base64.b64encode function. We then use the decode to utf-8 function to get the base encoded data into human-readable characters. Don’t worry if it doesn’t make a lot of sense right now. Just understand that any data is binary, and we can convert binary data to its string representation using a series of steps.
As a simple example, if I have a simple image like below, we can convert it to a string using:
import base64 with open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;, &amp;#34;rb&amp;#34;) as image_file: base64str = base64.b64encode(image_file.read()).decode(&amp;#34;utf-8&amp;#34;) Here I have got a string representation of a file named dog_with_ball.png on my laptop.
Great, we now have a string representation of an image. And, we can send this string representation to our FastAPI. But we also need to have a way to read an image back from its string representation. After all, our image detection API using PyTorch and any other package needs to have an image object that they can predict, and those methods don’t work on a string.
So here is a way to create a PIL image back from an image’s base64 string. Mostly we just do the reverse steps in the same order. We encode in ‘utf-8’ using .encode. We then use base64.b64decode to decode to bytes. We use these bytes to create a bytes object using io.BytesIO and use Image.open to open this bytes IO object as a PIL image, which can easily be used as an input to my PyTorch prediction code.*** Again simply, it is just a way to convert base64 image string to an actual image.***
import base64 import io from PIL import Image def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img So does this function work? Let’s see for ourselves. We can use just the string to get back the image.
And we have our happy dog back again. Looks better than the string.
B. Writing the Actual FastAPI code So, as now we understand that our API can get an image as a string from our user, let’s create an object detection API that makes use of this image as a string and outputs the bounding boxes for the object with the object classes as well.
Here, I will be using a Pytorch pre-trained fasterrcnn_resnet50_fpn detection model from the torchvision.models for object detection, which is trained on the COCO dataset to keep the code simple, but one can use any model. You can look at these posts if you want to train your custom image classification or image detection model using Pytorch.
Below is the full code for the FastAPI. Although it may look long, we already know all the parts. In this code, we essentially do the following steps:
 Create our fast API app using the FastAPI() constructor.
 Load our model and the classes it was trained on. I got the list of classes from the PyTorch docs.
 We also defined a new class Input , which uses a library called pydantic to validate the input data types that we will get from the API end-user. Here the end-user gives the base64str and some score threshold for object detection prediction.
 We add a function called base64str_to_PILImage which does just what it is named.
 And we write a predict function called get_predictionbase64 which returns a dict of bounding boxes and classes using a base64 string representation of an image and a threshold as an input. We also add @app.put(“/predict”) on top of this function to define our endpoint. If you need to understand put and endpoint refer to my previous post on FastAPI.
  from fastapi import FastAPI from pydantic import BaseModel import torchvision from torchvision import transforms import torch from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from PIL import Image import numpy as np import cv2 import io, json import base64 app = FastAPI() # load a pre-trained Model and convert it to eval mode. # This model loads just once when we start the API. model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) COCO_INSTANCE_CATEGORY_NAMES = [ &amp;#39;__background__&amp;#39;, &amp;#39;person&amp;#39;, &amp;#39;bicycle&amp;#39;, &amp;#39;car&amp;#39;, &amp;#39;motorcycle&amp;#39;, &amp;#39;airplane&amp;#39;, &amp;#39;bus&amp;#39;, &amp;#39;train&amp;#39;, &amp;#39;truck&amp;#39;, &amp;#39;boat&amp;#39;, &amp;#39;traffic light&amp;#39;, &amp;#39;fire hydrant&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;stop sign&amp;#39;, &amp;#39;parking meter&amp;#39;, &amp;#39;bench&amp;#39;, &amp;#39;bird&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;horse&amp;#39;, &amp;#39;sheep&amp;#39;, &amp;#39;cow&amp;#39;, &amp;#39;elephant&amp;#39;, &amp;#39;bear&amp;#39;, &amp;#39;zebra&amp;#39;, &amp;#39;giraffe&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;backpack&amp;#39;, &amp;#39;umbrella&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;handbag&amp;#39;, &amp;#39;tie&amp;#39;, &amp;#39;suitcase&amp;#39;, &amp;#39;frisbee&amp;#39;, &amp;#39;skis&amp;#39;, &amp;#39;snowboard&amp;#39;, &amp;#39;sports ball&amp;#39;, &amp;#39;kite&amp;#39;, &amp;#39;baseball bat&amp;#39;, &amp;#39;baseball glove&amp;#39;, &amp;#39;skateboard&amp;#39;, &amp;#39;surfboard&amp;#39;, &amp;#39;tennis racket&amp;#39;, &amp;#39;bottle&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;wine glass&amp;#39;, &amp;#39;cup&amp;#39;, &amp;#39;fork&amp;#39;, &amp;#39;knife&amp;#39;, &amp;#39;spoon&amp;#39;, &amp;#39;bowl&amp;#39;, &amp;#39;banana&amp;#39;, &amp;#39;apple&amp;#39;, &amp;#39;sandwich&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;broccoli&amp;#39;, &amp;#39;carrot&amp;#39;, &amp;#39;hot dog&amp;#39;, &amp;#39;pizza&amp;#39;, &amp;#39;donut&amp;#39;, &amp;#39;cake&amp;#39;, &amp;#39;chair&amp;#39;, &amp;#39;couch&amp;#39;, &amp;#39;potted plant&amp;#39;, &amp;#39;bed&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;dining table&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;toilet&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;tv&amp;#39;, &amp;#39;laptop&amp;#39;, &amp;#39;mouse&amp;#39;, &amp;#39;remote&amp;#39;, &amp;#39;keyboard&amp;#39;, &amp;#39;cell phone&amp;#39;, &amp;#39;microwave&amp;#39;, &amp;#39;oven&amp;#39;, &amp;#39;toaster&amp;#39;, &amp;#39;sink&amp;#39;, &amp;#39;refrigerator&amp;#39;, &amp;#39;N/A&amp;#39;, &amp;#39;book&amp;#39;, &amp;#39;clock&amp;#39;, &amp;#39;vase&amp;#39;, &amp;#39;scissors&amp;#39;, &amp;#39;teddy bear&amp;#39;, &amp;#39;hair drier&amp;#39;, &amp;#39;toothbrush&amp;#39; ] model.eval() # define the Input class class Input(BaseModel): base64str : str threshold : float def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img @app.put(&amp;#34;/predict&amp;#34;) def get_predictionbase64(d:Input): &amp;#39;&amp;#39;&amp;#39; FastAPI API will take a base 64 image as input and return a json object &amp;#39;&amp;#39;&amp;#39; # Load the image img = base64str_to_PILImage(d.base64str) # Convert image to tensor transform = transforms.Compose([transforms.ToTensor()]) img = transform(img) # get prediction on image pred = model([img]) pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][&amp;#39;labels&amp;#39;].numpy())] pred_boxes = [[(float(i[0]), float(i[1])), (float(i[2]), float(i[3]))] for i in list(pred[0][&amp;#39;boxes&amp;#39;].detach().numpy())] pred_score = list(pred[0][&amp;#39;scores&amp;#39;].detach().numpy()) pred_t = [pred_score.index(x) for x in pred_score if x &amp;gt; d.threshold][-1] pred_boxes = pred_boxes[:pred_t&#43;1] pred_class = pred_class[:pred_t&#43;1] return {&amp;#39;boxes&amp;#39;: pred_boxes, &amp;#39;classes&amp;#39; : pred_class} C. Local Before Global: Test the FastAPI code locally Before we move on to AWS, let us check if the code works on our local machine. We can start the API on our laptop using:
uvicorn fastapiapp:app --reload  The above means that your API is now running on your local server, and the &amp;ndash;reload flag indicates that the API gets updated automatically when you change the fastapiapp.py file. This is very helpful while developing and testing, but you should remove this &amp;ndash;reload flag when you put the API in production.
You should see something like:
You can now try to access this API and see if it works using the requests module:
import requests,json payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)&amp;#34;,data = payload) data_dict = response.json() And so we get our results using the API. This image contains a dog and a sports ball. We also have corner 1 (x1,y1) and corner 2 (x2,y2) coordinates of our bounding boxes.
D. Lets Visualize Although not strictly necessary, we can visualize how the results look in our Jupyter notebook:
from PIL import Image import numpy as np import cv2 import matplotlib.pyplot as plt def PILImage_to_cv2(img): return np.asarray(img) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) # Write the prediction class plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() img = Image.open(&amp;#34;sample_images/dog_with_ball.jpg&amp;#34;) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) Here is the output:
Here you will note that I got the image from the local file system, and that sort of can be considered as cheating as we don’t want to save every file that the user sends to us through a web UI. We should have been able to use the same base64string object that we also had to create this image. Right?
Not to worry, we could do that too. Remember our base64str_to_PILImage function? We could have used that also.
img = base64str_to_PILImage(base64str) drawboundingbox(img, data_dict[&#39;boxes&#39;], data_dict[&#39;classes&#39;])  That looks great. We have our working FastAPI, and we also have our amazon instance. We can now move on to Deployment.
3. Deployment on Amazon ec2 Till now, we have created an AWS instance and, we have also created a FastAPI that takes as input a base64 string representation of an image and returns bounding boxes and the associated class. But all the FastAPI code still resides in our local machine. How do we put it on the ec2 server? And run predictions on the cloud.
A. Install Docker We will deploy our app using docker, as is suggested by the fastAPI creator himself. I will try to explain how docker works as we go. The below part may look daunting but it just is a series of commands and steps. So stay with me.
We can start by installing docker using:
sudo apt-get update sudo apt install docker.io  We then start the docker service using:
sudo service docker start  B. Creating the folder structure for docker └── dockerfastapi ├── Dockerfile ├── app │ └── main.py └── requirements.txt  Here dockerfastapi is our project’s main folder. And here are the different files in this folder:
i. requirements.txt: Docker needs a file, which tells it which all libraries are required for our app to run. Here I have listed all the libraries I used in my Fastapi API.
numpy opencv-python matplotlib torchvision torch fastapi pydantic  ii. Dockerfile: The second file is Dockerfile.
FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app COPY requirements.txt . RUN pip --no-cache-dir install -r requirements.txt  How Docker works?: You can skip this section, but it will help to get some understanding of how docker works.
The dockerfile can be thought of something like a sh file,which contains commands to create a docker image that can be run in a container. One can think of a docker image as an environment where everything like Python and Python libraries is installed. A container is a unit which is just an isolated box in our system that uses a dockerimage. The advantage of using docker is that we can create multiple docker images and use them in multiple containers. For example, one image might contain python36, and another can contain python37. And we can spawn multiple containers in a single Linux server.
Our Dockerfile contains a few things:
 FROM command: Here the first line FROM specifies that we start with tiangolo’s (FastAPI creator) Docker image. As per his site: “This image has an “auto-tuning” mechanism included so that you can just add your code and get that same high performance automatically. And without making sacrifices”. What we are doing is just starting from an image that installs python3.7 for us along with some added configurations for uvicorn and gunicorn ASGI servers and a start.sh file for ASGI servers automatically. For adventurous souls, particularly commandset1 and commandset2 get executed through a sort of a daisy-chaining of commands.
 COPY command: We can think of a docker image also as a folder that contains files and such. Here we copy our app folder and the requirements.txt file, which we created earlier to our docker image.
 RUN Command: We run pip install command to install all our python dependencies using the requirements.txt file that is now on the docker image.
  iii. main.py: This file contains the fastapiapp.py code we created earlier. Remember to keep the name of the file main.py only.
C. Docker Build We have got all our files in the required structure, but we haven’t yet used any docker command. We will first need to build an image containing all dependencies using Dockerfile.
We can do this simply by:
sudo docker build -t myimage .  This downloads, copies and installs some files and libraries from tiangolo’s image and creates an image called myimage. This myimage has python37 and some python packages as specified by requirements.txt file.
We will then just need to start a container that runs this image. We can do this using:
sudo docker run -d --name mycontainer -p 80:80 myimage  This will create a container named mycontainer which runs our docker image myimage. The part 80:80 connects our docker container port 80 to our Linux machine port 80.
And actually that’s it. At this point, you should be able to open the below URL in your browser.
# &amp;lt;IPV4 public IP&amp;gt;/docs URL: 18.237.28.174/docs  And we can check our app programmatically using:
payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;[http://18.237.28.174/predict](http://18.237.28.174/predict)&amp;#34;,data = payload) data_dict = response.json() print(data_dict) &amp;gt; # Yup, finally our API is deployed.
D. Troubleshooting as the real world is not perfect All the above was good and will just work out of the box if you follow the exact instructions, but the real world doesn’t work like that. You will surely get some errors along the way and would need to debug your code. So to help you with that, some docker commands may come handy:
 Logs: When we ran our container using sudo docker run we don’t get a lot of info, and that is a big problem when you are debugging. You can see the real-time logs using the below command. If you see an error here, you will need to change your code and build the image again.   sudo docker logs -f mycontainer   Starting and Stopping Docker: Sometimes, it might help just to restart your docker. In that case, you can use:   sudo service docker stop sudo service docker start   Listing images and containers: Working with docker, you will end up creating images and containers, but you won’t be able to see them in the working directory. You can list your images and containers using:   sudo docker container ls sudo docker image ls   Deleting unused docker images or containers: You might need to remove some images or containers as these take up a lot of space on the system. Here is how you do that.   # the prune command removes the unused containers and images sudo docker system prune # delete a particular container sudo docker rm mycontainer # remove myimage sudo docker image rm myimage # remove all images sudo docker image prune — all   Checking localhost:The Linux server doesn’t have a browser, but we can still see the browser output though it’s a little ugly:   curl localhost   Develop without reloading image again and again: For development, it’s useful to be able just to change the contents of the code on our machine and test it live, without having to build the image every time. In that case, it’s also useful to run the server with live auto-reload automatically at every code change. Here, we use our app directory on our Linux machine, and we replace the default (/start.sh) with the development alternative /start-reload.sh during development. After everything looks fine, we can build our image again run it inside the container.   sudo docker run -d -p 80:80 -v $(pwd):/app myimage /start-reload.sh  If this doesn’t seem sufficient, adding here a docker cheat sheet containing useful docker commands:
4. An End to End App with UI We are done here with our API creation, but we can also create a UI based app using Streamlit using our FastAPI API. This is not how you will do it in a production setting (where you might have developers making apps using react, node.js or javascript)but is mostly here to check the end-to-end flow of how to use an image API. I will host this barebones Streamlit app on local rather than the ec2 server, and it will get the bounding box info and classes from the FastAPI API hosted on ec2.
If you need to learn more about how streamlit works, you can check out this post. Also, if you would want to deploy this streamlit app also to ec2, here is a tutorial again.
Here is the flow of the whole app with UI and FastAPI API on ec2:
Project Architecture
The most important problems we need to solve in our streamlit app are:
How to get an image file from the user using Streamlit? A. Using File uploader: We can use the file uploader using:
bytesObj = st.file_uploader(“Choose an image file”) The next problem is, what is this bytesObj we get from the streamlit file uploader? In streamlit, we will get a bytesIO object from the file_uploader and we will need to convert it to base64str for our FastAPI app input. This can be done using:
def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) base64str = bytesioObj_to_base64str(bytesObj) B. Using URL: We can also get an image URL from the user using text_input.
url = st.text_input(‘Enter URL’) We can then get image from URL in base64 string format using the requests module and base64 encode and utf-8 decode:
def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) base64str = ImgURL_to_base64str(url) And here is the complete code of our Streamlit app. You have seen most of the code in this post already.
import streamlit as st import base64 import io import requests,json from PIL import Image import cv2 import numpy as np import matplotlib.pyplot as plt import requests import random # use file uploader object to recieve image # Remember that this bytes object can be used only once def bytesioObj_to_base64str(bytesObj): return base64.b64encode(bytesObj.read()).decode(&amp;#34;utf-8&amp;#34;) # Image conversion functions def base64str_to_PILImage(base64str): base64_img_bytes = base64str.encode(&amp;#39;utf-8&amp;#39;) base64bytes = base64.b64decode(base64_img_bytes) bytesObj = io.BytesIO(base64bytes) img = Image.open(bytesObj) return img def PILImage_to_cv2(img): return np.asarray(img) def ImgURL_to_base64str(url): return base64.b64encode(requests.get(url).content).decode(&amp;#34;utf-8&amp;#34;) def drawboundingbox(img, boxes,pred_cls, rect_th=2, text_size=1, text_th=2): img = PILImage_to_cv2(img) class_color_dict = {} #initialize some random colors for each class for better looking bounding boxes for cat in pred_cls: class_color_dict[cat] = [random.randint(0, 255) for _ in range(3)] for i in range(len(boxes)): cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]),int(boxes[i][1][1])), color=class_color_dict[pred_cls[i]], thickness=rect_th) cv2.putText(img,pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size, class_color_dict[pred_cls[i]],thickness=text_th) plt.figure(figsize=(20,30)) plt.imshow(img) plt.xticks([]) plt.yticks([]) plt.show() st.markdown(&amp;#34;&amp;lt;h1&amp;gt;Our Object Detector App using FastAPI&amp;lt;/h1&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) bytesObj = st.file_uploader(&amp;#34;Choose an image file&amp;#34;) st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h2&amp;gt;or&amp;lt;/h2&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) url = st.text_input(&amp;#39;Enter URL&amp;#39;) if bytesObj or url: # In streamlit we will get a bytesIO object from the file_uploader # and we convert it to base64str for our FastAPI if bytesObj: base64str = bytesioObj_to_base64str(bytesObj) elif url: base64str = ImgURL_to_base64str(url) # We will also create the image in PIL Image format using this base64 str # Will use this image to show in matplotlib in streamlit img = base64str_to_PILImage(base64str) # Run FastAPI payload = json.dumps({ &amp;#34;base64str&amp;#34;: base64str, &amp;#34;threshold&amp;#34;: 0.5 }) response = requests.put(&amp;#34;http://18.237.28.174/predict&amp;#34;,data = payload) data_dict = response.json() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;App Result&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;#34;, unsafe_allow_html=True) drawboundingbox(img, data_dict[&amp;#39;boxes&amp;#39;], data_dict[&amp;#39;classes&amp;#39;]) st.pyplot() st.markdown(&amp;#34;&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;FastAPI Response&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;&amp;lt;br&amp;gt;&amp;#34;, unsafe_allow_html=True) st.write(data_dict) We can run this streamlit app in local using:
streamlit run streamlitapp.py  And we can see our app running on our localhost:8501. Works well with user-uploaded images as well as URL based images. Here is a cat image for some of you cat enthusiasts as well.
 So that’s it. We have created a whole workflow here to deploy image detection models through FastAPI on ec2 and utilizing those results in Streamlit. I hope this helps your woes around deploying models in production. You can find the code for this post as well as all my posts at my GitHub repository.
Let me know if you like this post and if you would like to include Docker or FastAPI or Streamlit in your day to day deployment needs. I am also looking to create a much detailed post on Docker so follow me up to stay tuned with my writing as well. Details below.
Continue Learning If you want to learn more about building and putting a Machine Learning model in production, this course on AWS for implementing Machine Learning applications promises just that.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How to Create an End to End Object Detector using Yolov5</title>
      <link>https://mlwhiz.com/blog/2020/08/08/yolov5/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/08/08/yolov5/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://miro.medium.com/max/595/1*QvCHyXdY36jpwoz-2_n9yQ.gif"></media:content>
      

      
      <description>Ultralytics recently launched YOLOv5 amid controversy surrounding its name. For context, the first three versions of YOLO (You Only Look Once) were created by Joseph Redmon. Following this, Alexey Bochkovskiy created YOLOv4 on darknet, which boasted higher Average Precision (AP) and faster results than previous iterations.
Now, Ultralytics has released YOLOv5, with comparable AP and faster inference times than YOLOv4. This has left many asking: is a new version warranted given similar accuracy to YOLOv4?</description>

      <content:encoded>  
        
        <![CDATA[  Ultralytics recently launched YOLOv5 amid controversy surrounding its name. For context, the first three versions of YOLO (You Only Look Once) were created by Joseph Redmon. Following this, Alexey Bochkovskiy created YOLOv4 on darknet, which boasted higher Average Precision (AP) and faster results than previous iterations.
Now, Ultralytics has released YOLOv5, with comparable AP and faster inference times than YOLOv4. This has left many asking: is a new version warranted given similar accuracy to YOLOv4? Whatever the answer may be, it’s definitely a sign of how quickly the detection community is evolving.
Since they first ported YOLOv3, Ultralytics has made it very simple to create and deploy models using Pytorch, so I was eager to try out YOLOv5. As it turns out, Ultralytics has further simplified the process, and the results speak for themselves.
In this article, we’ll create a detection model using YOLOv5, from creating our dataset and annotating it to training and inferencing using their remarkable library. This post focuses on the implementation of YOLOv5, including:
 Creating a toy dataset
 Annotating the image data
 Creating the project structure
 Training YOLOv5
  Creating Custom Dataset You can forgo the first step if you have your image Dataset. Since I don’t have images, I am downloading data from the Open Image Dataset(OID), which is an excellent resource for getting annotated image data that can be used for classification as well as detection. Note that we won’t be using the provided annotations from OID and create our own for the sake of learning.
1. OIDv4 Download Images: To download images from the Open Image dataset, we start by cloning the OIDv4_ToolKit and installing all requirements.
git clone [https://github.com/EscVM/OIDv4_ToolKit](https://github.com/EscVM/OIDv4_ToolKit) cd [OIDv4_ToolKit](https://github.com/EscVM/OIDv4_ToolKit) pip install -r requirements.txt  We can now use the main.py script within this folder to download images as well as labels for multiple classes.
Below I am downloading the data for Cricketball and Football to create our Custom Dataset. That is, we will be creating a dataset with footballs and cricket balls, and the learning task is to detect these balls.
python3 main.py downloader --classes Cricket_ball Football --type_csv all -y --limit 500  The below command creates a directory named “OID” with the following structure:
OID directory structure. We will take only the image files(.jpgs) from here and not the labels as we will annotate manually to create our Custom Dataset, though we can use them if required for a different project.
Before we continue, we will need to copy all the images in the same folder to start our labeling exercise from Scratch. You can choose to do this manually, but this can also be quickly done programmatically using recursive glob function:
import os from glob import glob os.system(&amp;quot;mkdir Images&amp;quot;) images = glob(r&#39;OID/**/*.jpg&#39;, recursive=True) for img in images: os.system(f&amp;quot;cp {img} Images/&amp;quot;)  2. Label Images with HyperLabel We will use a tool called Hyperlabel to label our images. In the past, I have used many tools to create annotations like labelimg, labelbox, etc. but never came across a tool so straightforward and that too open source. The only downside is that you cannot get this tool for Linux and only for Mac and Windows, but I guess that is fine for most of us.
    The best part of this tool is the variety of output formats it provides. Since we want to get the data for Yolo, we will close Yolo Format and export it after being done with our annotations. But you can choose to use this tool if you want to get annotations in JSON format(COCO) or XML format(Pascal VOC) too.
Exporting in Yolo format essentially creates a .txt file for each of our images, which contains the class_id, x_center, y_center, width, and the height of the image. It also creates a file named obj.names , which helps map the class_id to the class name. For example:
       Notice that the coordinates are scaled from 0 to 1 in the annotation file. Also, note that the class_id is 0 for Cricketball and 1 for football as per obj.names file, which starts from 0. There are a few other files we create using this, but we won’t be using them in this example.
Once we have done this, we are mostly set up with our custom dataset and would only need to rearrange some of these files for subsequent training and validation splits later when we train our model. The dataset currently will be a single folder like below containing both the images as well as annotations:
dataset - 0027773a6d54b960.jpg - 0027773a6d54b960.txt - 2bded1f9cb587843.jpg - 2bded1f9cb587843.txt -- --  Setting up the project To train our custom object detector, we will be using Yolov5 from Ultralytics. We start by cloning the repository and installing the dependencies:
# clone repo git clone [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) cd yolov5 pip install -U -r requirements.txt  We then start with creating our own folder named training in which we will keep our custom dataset.
!mkdir training  We start by copying our custom dataset folder in this folder and creating the train validation folders using the simple train_val_folder_split.ipynb notebook. This code below just creates some train and validation folders and populates them with images.
import glob, os import random # put your own path here dataset_path = &#39;dataset&#39; # Percentage of images to be used for the validation set percentage_test = 20 !mkdir data !mkdir data/images !mkdir data/labels !mkdir data/images/train !mkdir data/images/valid !mkdir data/labels/train !mkdir data/labels/valid # Populate the folders p = percentage_test/100 for pathAndFilename in glob.iglob(os.path.join(dataset_path, &amp;quot;*.jpg&amp;quot;)): title, ext = os.path.splitext(os.path.basename(pathAndFilename)) if random.random() &amp;lt;=p : os.system(f&amp;quot;cp {dataset_path}/{title}.jpg data/images/valid&amp;quot;) os.system(f&amp;quot;cp {dataset_path}/{title}.txt data/labels/valid&amp;quot;) else: os.system(f&amp;quot;cp {dataset_path}/{title}.jpg data/images/train&amp;quot;) os.system(f&amp;quot;cp {dataset_path}/{title}.txt data/labels/train&amp;quot;)  After running this, your data folder structure should look like below. It should have two directories images and labels.
We now have to add two configuration files to training folder:
1. Dataset.yaml: We create a file “dataset.yaml” that contains the path of training and validation images and also the classes.
# train and val datasets (image directory or *.txt file with image paths) train: training/data/images/train/ val: training/data/images/valid/ # number of classes nc: 2 # class names names: [&#39;Cricketball&#39;, &#39;Football&#39;]  2. Model.yaml: We can use multiple models ranging from small to large while creating our network. For example, yolov5s.yaml file in the yolov5/models directory is the small Yolo model with 7M parameters, while the yolov5x.yaml is the largest Yolo model with 96M Params. For this project, I will use the yolov5l.yaml which has 50M params. We start by copying the file from yolov5/models/yolov5l.yaml to the training folder and changing nc , which is the number of classes to 2 as per our project requirements.
# parameters nc: 2 # change number of classes depth_multiple: 1.0 # model depth multiple width_multiple: 1.0 # layer channel multiple  Train At this point our training folder looks like:
Once we are done with the above steps, we can start training our model. This is as simple as running the below command, where we provide the locations of our config files and various other params. You can check out the different other options in train.py file, but these are the ones I found noteworthy.
# Train yolov5l on custom dataset for 300 epochs $ python train.py --img 640 --batch 16 --epochs 300--data training/dataset.yaml --cfg training/yolov5l.yaml --weights &#39;&#39;  Sometimes you might get an error with PyTorch version 1.5 in that case run on a single GPU using:
# Train yolov5l on custom dataset for 300 epochs $ python train.py --img 640 --batch 16 --epochs 300--data training/dataset.yaml --cfg training/yolov5l.yaml --weights &#39;&#39; --device 0  Once you start the training, you can check whether the training has been set up by checking the automatically created filetrain_batch0.jpg , which contains the training labels for the first batch and test_batch0_gt.jpg which includes the ground truth for test images. This is how they look for me.
      Left: train_batch0.jpg, Right: test_batch0_gt.jpg
Results To see the results for the training at localhost:6006 in your browser using tensorboard, run this command in another terminal tab
tensorboard --logdir=runs  Here are the various validation metrics. These metrics also get saved in a file results.png at the end of the training run.
Predict Ultralytics Yolov5 provides a lot of different ways to check the results on new data.
To detect some images you can simply put them in the folder named inference/images and run the inference using the best weights as per validation AP:
python detect.py --weights weights/best.pt  You can also detect in a video using the detect.py file:
python detect.py --weights weights/best.pt --source inference/videos/messi.mp4 --view-img --output inference/output  Here I specify that I want to see the output using the — view-img flag, and we store the output at the location inference/output. This will create a .mp4 file in this location. It&amp;rsquo;s impressive that the network can see the ball, the speed at which inference is made here, and also the mindblowing accuracy on never observed data.
You can also use the webcam as a source by specifying the &amp;ndash;source as 0. You can check out the various other options in detect.py file.
Conclusion In this post, I talked about how to create a Yolov5 object detection model using a Custom Dataset. I love the way Ultralytics has made it so easy to create an object detection model.
Additionally, the various ways that they have provided to see the model results make it a complete package I have seen in a long time.
If you would like to experiment with the custom dataset yourself, you can download the annotated data on Kaggle and the code at Github.
If you want to know more about various Object Detection techniques, motion estimation, object tracking in video, etc., I would like to recommend this excellent course on Deep Learning in Computer Vision in the Advanced machine learning specialization. If you wish to know more about how the object detection field has evolved over the years, you can also take a look at my last post on Object detection.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title>
      <link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dlrig/main.png"></media:content>
      

      
      <description>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.</description>

      <content:encoded>  
        
        <![CDATA[  Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.
But this time I just had to do it. So, I found out some free time to create a Deep Learning Rig with a lot of assistance from NVIDIA folks who were pretty helpful. On that note special thanks to Josh Patterson and Michael Cooper.
Now, every time I create the whole deep learning setup from an installation viewpoint, I end up facing similar challenges. It’s like running around in circles with all these various dependencies and errors. This time also I had to try many things before the whole configuration came to life without errors.
So this time, I made it a point to document everything while installing all the requirements and their dependencies in my own system.
This post is about setting up your own Linux Ubuntu 18.04 system for deep learning with everything you might need.
If a pre-built deep learning system is preferred, I can recommend Exxact’s line of workstations and servers.
I assume that you have a fresh Ubuntu 18.04 installation. I am taking inspiration from Slav Ivanov’s excellent post in 2017 on creating a Deep Learning box. You can call it the 2020 version for the same post from a setup perspective, but a lot of the things have changed from then, and there are a lot of caveats with specific CUDA versions not supported by Tensorflow and Pytorch.
Starting up Before we do anything with our installation, we need to update our Linux system to the latest packages. We can do this simply by using:
sudo apt-get update sudo apt-get --assume-yes upgrade sudo apt-get --assume-yes install tmux build-essential gcc g&#43;&#43; make binutils sudo apt-get --assume-yes install software-properties-common sudo apt-get --assume-yes install git  The Process So now we have everything set up we want to install the following four things:
 GPU Drivers: Why is your PC not supporting high graphic resolutions? Or how would your graphics cards talk to your python interfaces?
 CUDA: A layer to provide access to the GPU’s instruction set and parallel computation units. In simple words, it allows us a way to write code for GPUs
 CuDNN: a library that provides Primitives for Deep Learning Network
 Pytorch, Tensorflow, and Rapids: higher-level APIs to code Deep Neural Networks
  1. GPU Drivers The first step is to add the latest NVIDIA drivers. You can choose the GPU product type, Linux 64 bit, and download Type as “Linux Long-Lived” for the 18.04 version.
Clicking on search will take you to a downloads page:
From where you can download the driver file NVIDIA-Linux-x86_64–440.44.run and run it using:
chmod &#43;x NVIDIA-Linux-x86_64–440.44.run sudo sh NVIDIA-Linux-x86_64–440.44.run  For you, the file may be named differently, depending on the latest version.
2. CUDA We will now need to install the CUDA toolkit. Somehow the CUDA toolkit 10.2 is still not supported by Pytorch and Tensorflow, so we will go with CUDA Toolkit 10.1, which is supported by both.
Also, the commands on the product page for CUDA 10.1 didn’t work for me and the commands I ended up using are:
sudo apt-key adv --fetch-keys [http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub](http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub) &amp;amp;&amp;amp; echo &amp;quot;deb [https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64) /&amp;quot; | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -o Dpkg::Options::=&amp;quot;--force-overwrite&amp;quot; install cuda-10-1 cuda-drivers  The next step is to create the LD_LIBRARY_PATH and append to the PATH variable the path where CUDA got installed. Just run this below command on your terminal.
echo &#39;export PATH=/usr/local/cuda-10.1/bin${PATH:&#43;:${PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:&#43;:${LD_LIBRARY_PATH}}&#39; &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc &amp;amp;&amp;amp; sudo ldconfig  After this, one can check if CUDA is installed correctly by using:
nvcc --version  As you can see, the CUDA Version is 10.1 as we wanted. Also, check if you can use the command:
nvidia-smi  For me, it showed an error when I used it the first time, but a simple reboot solved the issue. And both my NVIDIA graphic cards show up in all their awesome glory. Don’t worry that the display says the CUDA version supported is 10.2. I was also confused, but it is just the maximum CUDA version supported by the graphics driver that is shown in nvidia-smi.
3.CuDNN What is the use of all these libraries if we are not going to train neural nets? CuDNN provides various primitives for Deep Learning, which are later used by PyTorch/TensorFlow.
But we first need to get a developer account first to install CuDNN. Once you fill-up the signup form, you will see the screen below. Select the cuDNN version that applies to your CUDA version. For me, the CUDA version is 10.1, so I select the second one.
Once you select the appropriate CuDNN version the screen expands:
For my use case, I needed to download three files for Ubuntu 18.04:
[cuDNN Runtime Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Developer Library for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-dev_7.6.5.32-1%2Bcuda10.1_amd64.deb) [cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)](https://developer.nvidia.com/compute/machine-learning/cudnn/secure/7.6.5.32/Production/10.1_20191031/Ubuntu18_04-x64/libcudnn7-doc_7.6.5.32-1%2Bcuda10.1_amd64.deb)  After downloading these files, you can install using these commands. You can also see the exact commands if anything changes in the future:
# Install the runtime library: sudo dpkg -i libcudnn7_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the developer library: sudo dpkg -i libcudnn7-dev_7.6.5.32-1&#43;cuda10.1_amd64.deb #Install the code samples and cuDNN User Guide(Optional): sudo dpkg -i libcudnn7-doc_7.6.5.32-1&#43;cuda10.1_amd64.deb  4. Anaconda, Pytorch, Tensorflow, and Rapids And finally, we reach the crux. We will install the software which we will interface with most of the times.
We need to install Python with virtual environments. I have downloaded python3 as it is the most stable version as of now, and it is time to say goodbye to Python 2.7. It was great while it lasted. And we will also install Pytorch and Tensorflow. I prefer them both for specific tasks as applicable.
You can go to the anaconda distribution page and download the package.
Once downloaded you can simply run the shell script:
sudo sh Anaconda3-2019.10-Linux-x86_64.sh  You will also need to run these commands on your shell to add some commands to your ~/.bashrc file, and update the conda distribution with the latest libraries versions.
cat &amp;gt;&amp;gt; ~/.bashrc &amp;lt;&amp;lt; &#39;EOF&#39; export PATH=$HOME/anaconda3/bin:${PATH} EOF source .bashrc conda upgrade -y --all  The next step is creating a new environment for your deep learning pursuits or using an existing one. I created a new Conda environment using:
conda create --name py37  Here py37 is the name we provide to this new conda environment. You can activate this conda environment using:
conda activate py37  You should now be able to see something like:
Notice the py37 at the start of command in terminal
We can now add all our required packages to this environment using pip or conda. The latest version 1.3, as seen from the pytorch site, is not yet available for CUDA 10.2, as I already mentioned, so we are in luck with CUDA 10.1. Also, we will need to specify the version of TensorFlow as 2.1.0, as this version was built using 10.1 CUDA.
I also install RAPIDS, which is a library to get your various data science workloads to GPUs. Why use GPUs only for deep learning and not for Data processing? You can get the command to install rapids from the rapids release selector:
sudo apt install python3-pip conda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.11 python=3.7 cudatoolkit=10.1 pip install torchvision  Since PyTorch installation interfered with TensorFlow, I installed TensorFlow in another environment.
conda create --name tf conda activate tf pip install --upgrade tensorflow  Now we can check if the TF and Pytorch installations are correctly done by using the below commands in their own environments:
# Should print True python3 -c &amp;quot;import tensorflow as tf; print(tf.test.is_gpu_available())&amp;quot; # should print cuda python3 -c &amp;quot;import torch; print(torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;))&amp;quot;  If the install is showing some errors for TensorFlow or the GPU test is failing, you might want to add these two additional lines at the end of your bashrc file and restart the terminal:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 export CUDA_HOME=/usr/local/cuda  You might also want to install jupyter lab or jupyter notebook. Thanks to the developers, the process is as easy as just running jupyter labor jupyter notebook in your terminal, whichever you do prefer. I personally like notebook better without all the unnecessary clutter.
Conclusion In this post, I talked about all the software you are going to need to install in your deep learning rig without hassle.
You might still need some help and face some problems for which my best advice would be to check out the different NVIDIA and Stack Overflow forums.
So we have got our deep learning rig setup, and its time for some tests now. In the next few posts, I am going to do some benchmarking on the GPUs and will try to write more on various deep Learning libraries one can include in their workflow. So stay tuned.
Continue Learning If you want to learn more about Deep Learning, here is an excellent course. You can start for free with the 7-day Free Trial.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>End to End Pipeline for setting up Multiclass Image Classification for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/multiclass_image_classification_pytorch/main.png"></media:content>
      

      
      <description>Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?
All of the above are examples of image classification in different settings. Multiclass image classification is a common task in computer vision, where we categorize an image into three or more classes.
In the past, I always used Keras for computer vision projects.</description>

      <content:encoded>  
        
        <![CDATA[  Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?
All of the above are examples of image classification in different settings. Multiclass image classification is a common task in computer vision, where we categorize an image into three or more classes.
In the past, I always used Keras for computer vision projects. However, recently when the opportunity to work on multiclass image classification presented itself, I decided to use PyTorch. I have already moved from Keras to PyTorch for all NLP tasks, so why not vision, too?
 PyTorch is powerful, and I also like its more pythonic structure.
 In this post, we’ll create an end to end pipeline for image multiclass classification using Pytorch.This will include training the model, putting the model’s results in a form that can be shown to business partners, and functions to help deploy the model easily. As an added feature we will look at Test Time Augmentation using Pytorch also.
But before we learn how to do image classification, let’s first look at transfer learning, the most common method for dealing with such problems.
What is Transfer Learning? Transfer learning is the process of repurposing knowledge from one task to another. From a modelling perspective, this means using a model trained on one dataset and fine-tuning it for use with another. But why does it work?
Let’s start with some background. Every year the visual recognition community comes together for a very particular challenge: The Imagenet Challenge. The task in this challenge is to classify 1,000,000 images into 1,000 categories.
This challenge has already resulted in researchers training big convolutional deep learning models. The results have included great models like Resnet50 and Inception.
But, what does it mean to train a neural model? Essentially, it means the researchers have learned the weights for a neural network after training the model on a million images.
So, what if we could get those weights? We could then use them and load them into our own neural networks model to predict on the test dataset, right? Actually, we can go even further than that; we can add an extra layer on top of the neural network these researchers have prepared to classify our own dataset.
 While the exact workings of these complex models is still a mystery, we do know that the lower convolutional layers capture low-level image features like edges and gradients. In comparison, higher convolutional layers capture more and more intricate details, such as body parts, faces, and other compositional features.
 Source: Visualizing and Understanding Convolutional Networks. You can see how the first few layers capture basic shapes, and the shapes become more and more complex in the later layers.
In the example above from ZFNet (a variant of Alexnet), one of the first convolutional neural networks to achieve success on the Imagenet task, you can see how the lower layers capture lines and edges, and the later layers capture more complex features. The final fully-connected layers are generally assumed to capture information that is relevant for solving the respective task, e.g. ZFNet’s fully-connected layers indicate which features are relevant for classifying an image into one of 1,000 object categories.
For a new vision task, it is possible for us to simply use the off-the-shelf features of a state-of-the-art CNN pre-trained on ImageNet, and train a new model on these extracted features.
The intuition behind this idea is that a model trained to recognize animals might also be used to recognize cats vs dogs. In our case, &amp;gt; # a model that has been trained on 1000 different categories has seen a lot of real-world information, and we can use this information to create our own custom classifier.
So that’s the theory and intuition. How do we get it to actually work? Let’s look at some code. You can find the complete code for this post on Github.
Data Exploration We will start with the Boat Dataset from Kaggle to understand the multiclass image classification problem. This dataset contains about 1,500 pictures of boats of different types: buoys, cruise ships, ferry boats, freight boats, gondolas, inflatable boats, kayaks, paper boats, and sailboats. Our goal is to create a model that looks at a boat image and classifies it into the correct category.
Here’s a sample of images from the dataset:
And here are the category counts:
Since the categories “freight boats”, “inflatable boats” , and “boats” don’t have a lot of images; we will be removing these categories when we train our model.
Creating the required Directory Structure Before we can go through with training our deep learning models, we need to create the required directory structure for our images. Right now, our data directory structure looks like:
images sailboat kayak . .  We need our images to be contained in 3 folders train, val and test. We will then train on the images in train dataset, validate on the ones in the val dataset and finally test them on images in the test dataset.
data train sailboat kayak . . val sailboat kayak . . test sailboat kayak . .  You might have your data in a different format, but I have found that apart from the usual libraries, the glob.glob and os.system functions are very helpful. Here you can find the complete data preparation code. Now let’s take a quick look at some of the not-so-used libraries that I found useful while doing data prep.
What is glob.glob? Simply, glob lets you get names of files or folders in a directory using a regex. For example, you can do something like:
from glob import glob categories = glob(“images/*”) print(categories) ------------------------------------------------------------------ [&#39;images/kayak&#39;, &#39;images/boats&#39;, &#39;images/gondola&#39;, &#39;images/sailboat&#39;, &#39;images/inflatable boat&#39;, &#39;images/paper boat&#39;, &#39;images/buoy&#39;, &#39;images/cruise ship&#39;, &#39;images/freight boat&#39;, &#39;images/ferry boat&#39;]  What is os.system? os.system is a function in os library which lets you run any command-line function in python itself. I generally use it to run Linux functions, but it can also be used to run R scripts within python as shown here. For example, I use it in my data preparation to copy files from one directory to another after getting the information from a pandas data frame. I also use f string formatting.
import os for i,row in fulldf.iterrows(): # Boat category cat = row[&#39;category&#39;] # section is train,val or test section = row[&#39;type&#39;] # input filepath to copy ipath = row[&#39;filepath&#39;] # output filepath to paste opath = ipath.replace(f&amp;quot;images/&amp;quot;,f&amp;quot;data/{section}/&amp;quot;) # running the cp command os.system(f&amp;quot;cp &#39;{ipath}&#39; &#39;{opath}&#39;&amp;quot;)  Now since we have our data in the required folder structure, we can move on to more exciting parts.
Data Preprocessing Transforms: 1. Imagenet Preprocessing
In order to use our images with a network trained on the Imagenet dataset, we need to preprocess our images in the same way as the Imagenet network. For that, we need to rescale the images to 224×224 and normalize them as per Imagenet standards. We can use the torchvision transforms library to do that. Here we take a CenterCrop of 224×224 and normalize as per Imagenet standards. The operations defined below happen sequentially. You can find a list of all transforms provided by PyTorch here.
transforms.Compose([ transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])  2. Data Augmentations
We can do a lot more preprocessing for data augmentations. Neural networks work better with a lot of data. Data augmentation is a strategy which we use at training time to increase the amount of data we have.
For example, we can flip the image of a boat horizontally, and it will still be a boat. Or we can randomly crop images or add color jitters. Here is the image transforms dictionary I have used that applies to both the Imagenet preprocessing as well as augmentations. This dictionary contains the various transforms we have for the train, test and validation data as used in this great post. As you’d expect, we don’t apply the horizontal flips or other data augmentation transforms to the test data and validation data because we don’t want to get predictions on an augmented image.
# Image transformations image_transforms = { # Train uses data augmentation &#39;train&#39;: transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]), # Validation does not use augmentation &#39;valid&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), # Test does not use augmentation &#39;test&#39;: transforms.Compose([ transforms.Resize(size=256), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), }  Here is an example of the train transforms applied to an image in the training dataset. Not only do we get a lot of different images from a single image, but it also helps our network become invariant to the object orientation.
ex_img = Image.open(&#39;/home/rahul/projects/compvisblog/data/train/cruise ship/cruise-ship-oasis-of-the-seas-boat-water-482183.jpg&#39;) t = image_transforms[&#39;train&#39;] plt.figure(figsize=(24, 24)) for i in range(16): ax = plt.subplot(4, 4, i &#43; 1) _ = imshow_tensor(t(ex_img), ax=ax) plt.tight_layout()  DataLoaders The next step is to provide the training, validation, and test dataset locations to PyTorch. We can do this by using the PyTorch datasets and DataLoader class. This part of the code will mostly remain the same if we have our data in the required directory structures.
# Datasets from folders traindir = &amp;quot;data/train&amp;quot; validdir = &amp;quot;data/val&amp;quot; testdir = &amp;quot;data/test&amp;quot; data = { &#39;train&#39;: datasets.ImageFolder(root=traindir, transform=image_transforms[&#39;train&#39;]), &#39;valid&#39;: datasets.ImageFolder(root=validdir, transform=image_transforms[&#39;valid&#39;]), &#39;test&#39;: datasets.ImageFolder(root=testdir, transform=image_transforms[&#39;test&#39;]) } # Dataloader iterators, make sure to shuffle dataloaders = { &#39;train&#39;: DataLoader(data[&#39;train&#39;], batch_size=batch_size, shuffle=True,num_workers=10), &#39;val&#39;: DataLoader(data[&#39;valid&#39;], batch_size=batch_size, shuffle=True,num_workers=10), &#39;test&#39;: DataLoader(data[&#39;test&#39;], batch_size=batch_size, shuffle=True,num_workers=10) }  These dataloaders help us to iterate through the dataset. For example, we will use the dataloader below in our model training. The data variable will contain data in the form (batch_size, color_channels, height, width) while the target is of shape (batch_size) and hold the label information.
train_loader = dataloaders[&#39;train&#39;] for ii, (data, target) in enumerate(train_loader):  Modeling 1. Create the model using a pre-trained model Right now these following pre-trained models are available to use in the torchvision library:
 AlexNet
 VGG
 ResNet
 SqueezeNet
 DenseNet
 Inception v3
 GoogLeNet
 ShuffleNet v2
 MobileNet v2
 ResNeXt
 Wide ResNet
 MNASNet
  Here I will be using resnet50 on our dataset, but you can effectively use any other model too as per your choice.
from torchvision import models model = models.resnet50(pretrained=True)  We start by freezing our model weights since we don’t want to change the weights for the renet50 models.
# Freeze model weights for param in model.parameters(): param.requires_grad = False  The next thing we need to do is to replace the linear classification layer in the model by our custom classifier. I have found that to do this, it is better first to see the model structure to determine what is the final linear layer. We can do this simply by printing the model object:
print(model) ------------------------------------------------------------------ ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) . . . . (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) **(fc): Linear(in_features=2048, out_features=1000, bias=True)** )  Here we find that the final linear layer that takes the input from the convolutional layers is named fc
We can now simply replace the fc layer using our custom neural network. This neural network takes input from the previous layer to fc and gives the log softmax output of shape (batch_size x n_classes).
n_inputs = model.fc.in_features model.fc = nn.Sequential( nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4), nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))  Please note that the new layers added now are fully trainable by default.
2. Load the model on GPU We can use a single GPU or multiple GPU(if we have them) using DataParallel from PyTorch. Here is what we can use to detect the GPU as well as the number of GPUs to load the model on GPU. Right now I am training my models on dual NVIDIA Titan RTX GPUs.
# Whether to train on a gpu train_on_gpu = cuda.is_available() print(f&#39;Train on gpu: {train_on_gpu}&#39;) # Number of gpus if train_on_gpu: gpu_count = cuda.device_count() print(f&#39;{gpu_count} gpus detected.&#39;) if gpu_count &amp;gt; 1: multi_gpu = True else: multi_gpu = False if train_on_gpu: model = model.to(&#39;cuda&#39;) if multi_gpu: model = nn.DataParallel(model)  3. Define criterion and optimizers One of the most important things to notice when you are training any model is the choice of loss-function and the optimizer used. Here we want to use categorical cross-entropy as we have got a multiclass classification problem and the Adam optimizer, which is the most commonly used optimizer. But since we are applying a LogSoftmax operation on the output of our model, we will be using the NLL loss.
from torch import optim criteration = nn.NLLLoss() optimizer = optim.Adam(model.parameters())  4. Training the model Given below is the full code used to train the model. It might look pretty big on its own, but essentially what we are doing is as follows:
 Start running epochs. In each epoch-
 Set the model mode to train using model.train().
 Loop through the data using the train dataloader.
 Load your data to the GPU using the data, target = data.cuda(), target.cuda() command
 Set the existing gradients in the optimizer to zero using optimizer.zero_grad()
 Run the forward pass through the batch using output = model(data)
 Compute loss using loss = criterion(output, target)
 Backpropagate the losses through the network using loss.backward()
 Take an optimizer step to change the weights in the whole network using optimizer.step()
 All the other steps in the training loop are just to maintain the history and calculate accuracy.
 Set the model mode to eval using model.eval().
 Get predictions for the validation data using valid_loader and calculate valid_loss and valid_acc
 Print the validation loss and validation accuracy results every print_every epoch.
 Save the best model based on validation loss.
 Early Stopping: If the cross-validation loss doesn’t improve for max_epochs_stop stop the training and load the best available model with the minimum validation loss.
  def train(model, criterion, optimizer, train_loader, valid_loader, save_file_name, max_epochs_stop=3, n_epochs=20, print_every=1): &amp;#34;&amp;#34;&amp;#34;Train a PyTorch Model Params -------- model (PyTorch model): cnn to train criterion (PyTorch loss): objective to minimize optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters train_loader (PyTorch dataloader): training dataloader to iterate through valid_loader (PyTorch dataloader): validation dataloader used for early stopping save_file_name (str ending in &amp;#39;.pt&amp;#39;): file path to save the model state dict max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping n_epochs (int): maximum number of training epochs print_every (int): frequency of epochs to print training stats Returns -------- model (PyTorch model): trained cnn with best weights history (DataFrame): history of train and validation loss and accuracy &amp;#34;&amp;#34;&amp;#34; # Early stopping intialization epochs_no_improve = 0 valid_loss_min = np.Inf valid_max_acc = 0 history = [] # Number of epochs already trained (if using loaded in model weights) try: print(f&amp;#39;Model has been trained for: {model.epochs} epochs.\n&amp;#39;) except: model.epochs = 0 print(f&amp;#39;Starting Training from Scratch.\n&amp;#39;) overall_start = timer() # Main loop for epoch in range(n_epochs): # keep track of training and validation loss each epoch train_loss = 0.0 valid_loss = 0.0 train_acc = 0 valid_acc = 0 # Set to training model.train() start = timer() # Training loop for ii, (data, target) in enumerate(train_loader): # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Clear gradients optimizer.zero_grad() # Predicted outputs are log probabilities output = model(data) # Loss and backpropagation of gradients loss = criterion(output, target) loss.backward() # Update the parameters optimizer.step() # Track train loss by multiplying average loss by number of examples in batch train_loss &#43;= loss.item() * data.size(0) # Calculate accuracy by finding max log probability _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) # Need to convert correct tensor from int to float to average accuracy = torch.mean(correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples in batch train_acc &#43;= accuracy.item() * data.size(0) # Track training progress print( f&amp;#39;Epoch: {epoch}\t{100 * (ii &#43; 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.&amp;#39;, end=&amp;#39;\r&amp;#39;) # After training loops ends, start validation else: model.epochs &#43;= 1 # Don&amp;#39;t need to keep track of gradients with torch.no_grad(): # Set to evaluation mode model.eval() # Validation loop for data, target in valid_loader: # Tensors to gpu if train_on_gpu: data, target = data.cuda(), target.cuda() # Forward pass output = model(data) # Validation loss loss = criterion(output, target) # Multiply average loss times the number of examples in batch valid_loss &#43;= loss.item() * data.size(0) # Calculate validation accuracy _, pred = torch.max(output, dim=1) correct_tensor = pred.eq(target.data.view_as(pred)) accuracy = torch.mean( correct_tensor.type(torch.FloatTensor)) # Multiply average accuracy times the number of examples valid_acc &#43;= accuracy.item() * data.size(0) # Calculate average losses train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) # Calculate average accuracy train_acc = train_acc / len(train_loader.dataset) valid_acc = valid_acc / len(valid_loader.dataset) history.append([train_loss, valid_loss, train_acc, valid_acc]) # Print training and validation results if (epoch &#43; 1) % print_every == 0: print( f&amp;#39;\nEpoch: {epoch} \tTraining Loss: {train_loss:.4f} \tValidation Loss: {valid_loss:.4f}&amp;#39; ) print( f&amp;#39;\t\tTraining Accuracy: {100 * train_acc:.2f}%\tValidation Accuracy: {100 * valid_acc:.2f}%&amp;#39; ) # Save the model if validation loss decreases if valid_loss &amp;lt; valid_loss_min: # Save model torch.save(model.state_dict(), save_file_name) # Track improvement epochs_no_improve = 0 valid_loss_min = valid_loss valid_best_acc = valid_acc best_epoch = epoch # Otherwise increment count of epochs with no improvement else: epochs_no_improve &#43;= 1 # Trigger early stopping if epochs_no_improve &amp;gt;= max_epochs_stop: print( f&amp;#39;\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%&amp;#39; ) total_time = timer() - overall_start print( f&amp;#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch&#43;1):.2f} seconds per epoch.&amp;#39; ) # Load the best state dict model.load_state_dict(torch.load(save_file_name)) # Attach the optimizer model.optimizer = optimizer # Format history history = pd.DataFrame( history, columns=[ &amp;#39;train_loss&amp;#39;, &amp;#39;valid_loss&amp;#39;, &amp;#39;train_acc&amp;#39;, &amp;#39;valid_acc&amp;#39; ]) return model, history # Attach the optimizer model.optimizer = optimizer # Record overall time and print out stats total_time = timer() - overall_start print( f&amp;#39;\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%&amp;#39; ) print( f&amp;#39;{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.&amp;#39; ) # Format history history = pd.DataFrame( history, columns=[&amp;#39;train_loss&amp;#39;, &amp;#39;valid_loss&amp;#39;, &amp;#39;train_acc&amp;#39;, &amp;#39;valid_acc&amp;#39;]) return model, history # Running the model model, history = train( model, criterion, optimizer, dataloaders[&amp;#39;train&amp;#39;], dataloaders[&amp;#39;val&amp;#39;], save_file_name=save_file_name, max_epochs_stop=3, n_epochs=100, print_every=1) Here is the output from running the above code. Just showing the last few epochs. The validation accuracy started at ~55% in the first epoch, and we ended up with a validation accuracy of ~90%.
And here are the training curves showing the loss and accuracy metrics:
Inference and Model Results We want our results in different ways to use our model. For one, we require test accuracies and confusion matrices. All of the code for creating these results is in the code notebook.
1. Test Results The overall accuracy of the test model is:
Overall Accuracy: 88.65 %  Here is the confusion matrix for results on the test dataset.
We can also look at the category wise accuracies. I have also added the train counts to see the results from a new perspective.
2. Visualizing Predictions for Single Image For deployment purposes, it helps to be able to get predictions for a single image. You can get the code from the notebook.
3. Visualizing Predictions for a Category We can also see the category wise results for debugging purposes and presentations.
4. Test results with Test Time Augmentation We can also do test time augmentation to increase our test accuracy. Here I am using a new test data loader and transforms:
# Image transformations tta_random_image_transforms = transforms.Compose([ transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.ColorJitter(), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), # Image net standards transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Imagenet standards ]) # Datasets from folders ttadata = { &#39;test&#39;: datasets.ImageFolder(root=testdir, transform=tta_random_image_transforms) } # Dataloader iterators ttadataloader = { &#39;test&#39;: DataLoader(ttadata[&#39;test&#39;], batch_size=512, shuffle=False,num_workers=10) }  We can then get the predictions on the test set using the below function:
def tta_preds_n_averaged(model, test_loader,n=5): &amp;#34;&amp;#34;&amp;#34;Returns the TTA preds from a trained PyTorch model Params -------- model (PyTorch model): trained cnn for inference test_loader (PyTorch DataLoader): test dataloader Returns -------- results (array): results for each category &amp;#34;&amp;#34;&amp;#34; # Hold results results = np.zeros((len(test_loader.dataset), n_classes)) bs = test_loader.batch_size model.eval() with torch.no_grad(): #aug loop: for _ in range(n): # Testing loop tmp_pred = np.zeros((len(test_loader.dataset), n_classes)) for i,(data, targets) in enumerate(tqdm.tqdm(test_loader)): # Tensors to gpu if train_on_gpu: data, targets = data.to(&amp;#39;cuda&amp;#39;), targets.to(&amp;#39;cuda&amp;#39;) # Raw model output out = model(data) tmp_pred[i*bs:(i&#43;1)*bs] = np.array(out.cpu()) results&#43;=tmp_pred return results/n In the function above, I am applying the tta_random_image_transforms to each image 5 times before getting its prediction. The final prediction is the average of all five predictions. When we use TTA over the whole test dataset, we noticed that the accuracy increased by around 1%
TTA Accuracy: 89.71%  Also, here is the results for TTA compared to normal results category wise:
In this small dataset, the TTA might not seem to add much value, but I have noticed that it adds value with big datasets.
Conclusion In this post, I talked about the end to end pipeline for working on a multiclass image classification project using PyTorch. We worked on creating some readymade code to train a model using transfer learning, visualize the results, use Test time augmentation, and got predictions for a single image so that we can deploy our model when needed using any tool like Streamlit.
You can find the complete code for this post on Github.
If you would like to learn more about Image Classification and Convolutional Neural Networks take a look at the Deep Learning Specialization from Andrew Ng. Also, to learn more about PyTorch and start from the basics, you can take a look at the Deep Neural Networks with PyTorch course offered by IBM.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
This post was first published here.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Stop Worrying and Create your Deep Learning Server in 30 minutes</title>
      <link>https://mlwhiz.com/blog/2020/05/25/dls/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2020/05/25/dls/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dls/main.png"></media:content>
      

      
      <description>I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating.</description>

      <content:encoded>  
        
        <![CDATA[  I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.
You start with installing Anaconda and end up creating different environments for Pytorch and Tensorflow, so they don’t interfere. And in the middle of it, you inevitably end up messing up and starting from scratch. And this often happens multiple times.
It is not just a massive waste of time; it is also mighty(trying to avoid profanity here) irritating. Going through all those Stack Overflow threads. Often wondering what has gone wrong.
So is there a way to do this more efficiently?
It turns out there is. In this blog, I will try to set up a deep learning server on EC2 with minimal effort so that I could focus on more important things.
This blog consists explicitly of two parts:
 Setting up an Amazon EC2 Machine with preinstalled deep learning libraries.
 Setting Up Jupyter Notebook using TMUX and SSH tunneling.
  Don’t worry; it’s not as difficult as it sounds. Just follow the steps and click Next.
Setting up Amazon EC2 Machine I am assuming that you have an AWS account, and you have access to the AWS Console. If not, you might need to sign up for an Amazon AWS account.
 First of all, we need to go to the Services tab to access the EC2 dashboard.   On the EC2 Dashboard, you can start by creating your instance.   Amazon provides Community AMIs(Amazon Machine Image) with Deep Learning software preinstalled. To access these AMIs, you need to look in the community AMIs and search for “Ubuntu Deep Learning” in the Search Tab. You can choose any other Linux flavor, but I have found Ubuntu to be most useful for my Deep Learning needs. In the present setup, I will use The Deep Learning AMI (Ubuntu 18.04) Version 27.0   Once you select an AMI, you can select the Instance Type. It is here you specify the number of CPUs, Memory, and GPUs you will require in your system. Amazon provides a lot of options to choose from based on one’s individual needs. You can filter for GPU instances using the “Filter by” filter.  In this tutorial, I have gone with p2.xlarge instance, which provides NVIDIA K80 GPU with 2,496 parallel processing cores and 12GiB of GPU memory. To know about different instance types, you can look at the documentation here and the pricing here.
 You can change the storage that is attached to the machine in the 4th step. It is okay if you don’t add storage upfront, as you can also do this later. I change the storage from 90 GB to 500 GB as most of the deep learning needs will require proper storage.   That’s all, and you can Launch the Instance after going to the Final Review instance settings Screen. Once you click on Launch, you will see this screen. Just type in any key name in the Key Pair Name and click on “Download key pair”. Your key will be downloaded to your machine by the name you provided. For me, it got saved as “aws_key.pem”. Once you do that, you can click on “Launch Instances”.  Keep this key pair safe as this will be required whenever you want to login to your instance.
 You can now click on “View Instances” on the next page to see your instance. This is how your instance will look like:   To connect to your instance, Just open a terminal window in your Local machine and browse to the folder where you have kept your key pair file and modify some permissions.
chmod 400 aws_key.pem
  Once you do that, you will be able to connect to your instance by SSHing. The SSH command will be of the form:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For me, the command was:
ssh -i &amp;quot;aws_key.pem&amp;quot; ubuntu@ec2-54-202-223-197.us-west-2.compute.amazonaws.com  Also, keep in mind that the Public DNS might change once you shut down your instance.
 You have already got your machine up and ready. This machine contains different environments that have various libraries you might need. This particular machine has MXNet, Tensorflow, and Pytorch with different versions of python. And the best thing is that we get all this preinstalled, so it just works out of the box.  Setting Up Jupyter Notebook But there are still a few things you will require to use your machine fully. One of them being Jupyter Notebooks. To set up Jupyter Notebooks with your Machine, I recommend using TMUX and tunneling. Let us go through setting up the Jupyter notebook step by step.
1. Using TMUX to run Jupyter Notebook We will first use TMUX to run the Jupyter notebook on our instance. We mainly use this so that our notebook still runs even if the terminal connection gets lost.
To do this, you will need to create a new TMUX session using:
tmux new -s StreamSession  Once you do that, you will see a new screen with a green border at the bottom. You can start your Jupyter Notebook in this machine using the usual jupyter notebook command. You will see something like:
It will be beneficial to copy the login URL so that we will be able to get the token later when we try to login to our jupyter notebook later. In my case, it is:
[http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872](http://localhost:8888/?token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872&amp;amp;token=5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872)  The next step is to detach our TMUX session so that it continues running in the background even when you leave the SSH shell. To do this just press Ctrl&#43;B and then D (Don’t press Ctrl when pressing D)You will come back to the initial screen with the message that you have detached from your TMUX session.
If you want, you can reattach to the session again using:
tmux attach -t StreamSession  2. SSH Tunneling to access the notebook on your Local Browser The second step is to tunnel into the Amazon instance to be able to get the Jupyter notebook on your Local Browser. As we can see, the Jupyter Notebook is actually running on the localhost on the Cloud instance. How do we access it? We use SSH tunneling. Worry not, it is straightforward fill in the blanks. Just use this command on your local machine terminal window:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L &amp;lt;Local Machine Port&amp;gt;:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)&amp;lt;Your PublicDNS(IPv4)&amp;gt;  For this case, I have used:
ssh -i &amp;quot;aws_key.pem&amp;quot; -L 8001:localhost:8888 [ubuntu@](mailto:ubuntu@ec2-34-212-131-240.us-west-2.compute.amazonaws.com)ec2-54-202-223-197.us-west-2.compute.amazonaws.com  This means that I will be able to use the Jupyter Notebook If I open the localhost:8001 in my local machine browser. And I surely can. We can now just input the token that we already have saved in one of our previous steps to access the notebook. For me the token is 5ccd01f60971d9fc97fd79f64a5bb4ce79f4d96823ab7872
You can just login using your token and voila we get the notebook in all its glory.
You can now choose to work on a new project by selecting any of the different environments you want. You can come from Tensorflow or Pytorch or might be willing to get the best of both worlds. This notebook will not disappoint you.
Troubleshooting It might happen that once the machine is restarted, you face some problems with the NVIDIA graphics card. Specifically, in my case, the nvidia-smi command stopped working. If you encounter this problem, the solution is to download the graphics driver from the NVIDIA website.
Above are the settings for the particular AMI I selected. Once you click on Search you will be able to see the next page:
Just copy the download link by right-clicking and copying the link address. And run the following commands on your machine. You might need to change the link address and the file name in this.
# When nvidia-smi doesnt work: wget [https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla](https://www.nvidia.in/content/DriverDownload-March2009/confirmation.php?url=/tesla/410.129/NVIDIA-Linux-x86_64-410.129-diagnostic.run&amp;amp;lang=in&amp;amp;type=Tesla) sudo sh NVIDIA-Linux-x86_64-410.129-diagnostic.run --no-drm --disable-nouveau --dkms --silent --install-libglvnd modinfo nvidia | head -7 sudo modprobe nvidia  Stop Your Instance And that’s it. You have got and up and running Deep Learning machine at your disposal, and you can work with it as much as you want. Just keep in mind to stop the instance whenever you stop working, so you won’t need to pay Amazon when you are not working on your instance. You can do it on the instances page by right-clicking on your instance. Just note that when you need to log in again to this machine, you will need to get the Public DNS (IPv4) address from the instance page back as it might have changed.
Conclusion I have always found it a big chore to set up a deep learning environment.
In this blog, we set up a new Deep Learning server on EC2 in minimal time by using Deep Learning Community AMI, TMUX, and Tunneling for the Jupyter Notebooks. This server comes preinstalled with all the deep learning libraries you might need at your work, and it just works out of the box.
So what are you waiting for? Just get started with Deep Learning with your own server.
If you want to learn more about AWS and how to use it in production settings and deploying models, I would like to call out an excellent course on AWS. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Implementing Object Detection and Instance Segmentation for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/12/06/weapons/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/06/weapons/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/weapons/main.png"></media:content>
      

      
      <description>Object Detection is a helpful tool to have in your coding repository.
It forms the backbone of many fantastic industrial applications. Some of them being self-driving cars, medical imaging and face detection.
In my last post on Object detection, I talked about how Object detection models evolved.
But what good is theory, if we can’t implement it?
This post is about implementing and getting an object detector on our custom dataset of weapons.</description>

      <content:encoded>  
        
        <![CDATA[  Object Detection is a helpful tool to have in your coding repository.
It forms the backbone of many fantastic industrial applications. Some of them being self-driving cars, medical imaging and face detection.
In my last post on Object detection, I talked about how Object detection models evolved.
But what good is theory, if we can’t implement it?
This post is about implementing and getting an object detector on our custom dataset of weapons.
The problem we will specifically solve today is that of Instance Segmentation using Mask-RCNN.
Instance Segmentation Can we create masks for each object in the image? Specifically something like:
The most common way to solve this problem is by using Mask-RCNN. The architecture of Mask-RCNN looks like below:
Source
Essentially, it comprises of:
 A backbone network like resnet50/resnet101
 A Region Proposal network
 ROI-Align layers
 Two output layers — one to predict masks and one to predict class and bounding box.
  There is a lot more to it. If you want to learn more about the theory, read my last post&amp;ndash; Demystifying Object Detection and Instance Segmentation for Data Scientists
This post is mostly going to be about the code.
1. Creating your Custom Dataset for Instance Segmentation The use case we will be working on is a weapon detector. A weapon detector is something that can be used in conjunction with street cameras as well as CCTV’s to fight crime. So it is pretty nifty.
So, I started with downloading 40 images each of guns and swords from the open image dataset and annotated them using the VIA tool. Now setting up the annotation project in VIA is petty important, so I will try to explain it step by step.
1. Set up VIA VIA is an annotation tool, using which you can annotate images both bounding boxes as well as masks. I found it as one of the best tools to do annotation as it is online and runs in the browser itself.
To use it, open http://www.robots.ox.ac.uk/~vgg/software/via/via.html
You will see a page like:
The next thing we want to do is to add the different class names in the region_attributes. Here I have added ‘gun’ and ‘sword’ as per our use case as these are the two distinct targets I want to annotate.
2. Annotate the Images I have kept all the files in the folder data. Next step is to add the files we want to annotate. We can add files in the data folder using the “Add Files” button in the VIA tool. And start annotating along with labels as shown below after selecting the polyline tool.
3. Download the annotation file Click on save project on the top menu of the VIA tool.
Save file as via_region_data.json by changing the project name field. This will save the annotations in COCO format.
4. Set up the data directory structure We will need to set up the data directories first so that we can do object detection. In the code below, I am creating a directory structure that is required for the model that we are going to use.
from random import random import os from glob import glob import json # Path to your images image_paths = glob(&amp;#34;data/*&amp;#34;) #Path to your annotations from VIA tool annotation_file = &amp;#39;via_region_data.json&amp;#39; #clean up the annotations a little annotations = json.load(open(annotation_file)) cleaned_annotations = {} for k,v in annotations[&amp;#39;_via_img_metadata&amp;#39;].items(): cleaned_annotations[v[&amp;#39;filename&amp;#39;]] = v # create train and validation directories ! mkdir procdata ! mkdir procdata/val ! mkdir procdata/train train_annotations = {} valid_annotations = {} # 20% of images in validation folder for img in image_paths: # Image goes to Validation folder if random()&amp;lt;0.2: os.system(&amp;#34;cp &amp;#34;&#43; img &#43; &amp;#34; procdata/val/&amp;#34;) img = img.split(&amp;#34;/&amp;#34;)[-1] valid_annotations[img] = cleaned_annotations[img] else: os.system(&amp;#34;cp &amp;#34;&#43; img &#43; &amp;#34; procdata/train/&amp;#34;) img = img.split(&amp;#34;/&amp;#34;)[-1] train_annotations[img] = cleaned_annotations[img] # put different annotations in different folders with open(&amp;#39;procdata/val/via_region_data.json&amp;#39;, &amp;#39;w&amp;#39;) as fp: json.dump(valid_annotations, fp) with open(&amp;#39;procdata/train/via_region_data.json&amp;#39;, &amp;#39;w&amp;#39;) as fp: json.dump(train_annotations, fp) After running the above code, we will get the data in the below folder structure:
- procdata - train - img1.jpg - img2.jpg - via_region_data.json - val - img3.jpg - img4.jpg - via_region_data.json  2. Setup the Coding Environment We will use the code from the matterport/Mask_RCNN GitHub repository. You can start by cloning the repository and installing the required libraries.
git clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip install -r requirements.txt Once we are done with installing the dependencies and cloning the repo, we can start with implementing our project.
We make a copy of the samples/balloon directory in Mask_RCNN folder and create a samples/guns_and_swords directory where we will continue our work:
cp -r samples/balloon samples/guns_and_swords Setting up the Code We start by renaming and changing balloon.py in the samples/guns_and_swords directory to gns.py. The balloon.py file right now trains for one target. I have extended it to use multiple targets. In this file, we change:
 balloonconfig to gnsConfig
 BalloonDataset to gnsDataset : We changed some code here to get the target names from our annotation data and also give multiple targets.
 And some changes in the train function
  Showing only the changed gnsConfig here to get you an idea. You can take a look at the whole gns.py code here.
class gnsConfig(Config): &amp;#34;&amp;#34;&amp;#34;Configuration for training on the toy dataset. Derives from the base Config class and overrides some values. &amp;#34;&amp;#34;&amp;#34; # Give the configuration a recognizable name NAME = &amp;#34;gns&amp;#34; # We use a GPU with 16GB memory, which can fit three image. # Adjust down if you use a smaller GPU. IMAGES_PER_GPU = 3 # Number of classes (including background) NUM_CLASSES = 1 &#43; 2 # Background &#43; sword &#43; gun # Number of training steps per epoch 3. Visualizing Images and Masks Once we are done with changing the gns.py file,we can visualize our masks and images. You can do simply by following this Visualize Dataset.ipynb notebook.
4. Train the MaskRCNN Model with Transfer Learning To train the maskRCNN model, on the Guns and Swords dataset, we need to run one of the following commands on the command line based on if we want to initialise our model with COCO weights or imagenet weights:
# Train a new model starting from pre-trained COCO weights python3 gns.py train — dataset=/path/to/dataset — weights=coco # Resume training a model that you had trained earlier python3 gns.py train — dataset=/path/to/dataset — weights=last # Train a new model starting from ImageNet weights python3 gns.py train — dataset=/path/to/dataset — weights=imagenet The command with weights=last will resume training from the last epoch. The weights are going to be saved in the logs directory in the Mask_RCNN folder.
This is how the loss looks after our final epoch.
Visualize the losses using Tensorboard You can take advantage of tensorboard to visualise how your network is performing. Just run:
tensorboard --logdir ~/objectDetection/Mask_RCNN/logs/gns20191010T1234 You can get the tensorboard at
https://localhost:6006  Here is how our mask loss looks like:
We can see that the validation loss is performing pretty abruptly. This is expected as we only have kept 20 images in the validation set.
5. Prediction on New Images Predicting a new image is also pretty easy. Just follow the prediction.ipynb notebook for a minimal example using our trained model. Below is the main part of the code.
# Function taken from utils.dataset def load_image(image_path): &amp;#34;&amp;#34;&amp;#34;Load the specified image and return a [H,W,3] Numpy array. &amp;#34;&amp;#34;&amp;#34; # Load image image = skimage.io.imread(image_path) # If grayscale. Convert to RGB for consistency. if image.ndim != 3: image = skimage.color.gray2rgb(image) # If has an alpha channel, remove it for consistency if image.shape[-1] == 4: image = image[..., :3] return image # path to image to be predicted image = load_image(&amp;#34;../../../data/2c8ce42709516c79.jpg&amp;#34;) # Run object detection results = model.detect([image], verbose=1) # Display results ax = get_ax(1) r = results[0] a = visualize.display_instances(image, r[&amp;#39;rois&amp;#39;], r[&amp;#39;masks&amp;#39;], r[&amp;#39;class_ids&amp;#39;], dataset.class_names, r[&amp;#39;scores&amp;#39;], ax=ax, title=&amp;#34;Predictions&amp;#34;) This is how the result looks for some images in the validation set:
Improvements The results don’t look very promising and leave a lot to be desired, but that is to be expected because of very less training data(60 images). One can try to do the below things to improve the model performance for this weapon detector.
 We just trained on 60 images due to time constraints. While we used transfer learning the data is still too less — Annotate more data.
 Train for more epochs and longer time. See how validation loss and training loss looks like.
 Change hyperparameters in the mrcnn/config file in the Mask_RCNN directory. For information on what these hyperparameters mean, take a look at my previous post. The main ones you can look at:
  # if you want to provide different weights to different losses LOSS_WEIGHTS ={&amp;#39;rpn_class_loss&amp;#39;: 1.0, &amp;#39;rpn_bbox_loss&amp;#39;: 1.0, &amp;#39;mrcnn_class_loss&amp;#39;: 1.0, &amp;#39;mrcnn_bbox_loss&amp;#39;: 1.0, &amp;#39;mrcnn_mask_loss&amp;#39;: 1.0} # Length of square anchor side in pixels RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512) # Ratios of anchors at each cell (width/height) # A value of 1 represents a square anchor, and 0.5 is a wide anchor RPN_ANCHOR_RATIOS = [0.5, 1, 2] Conclusion In this post, I talked about how to implement Instance segmentation using Mask-RCNN for a custom dataset.
I tried to make the coding part as simple as possible and hope you find the code useful. In the next part of this post, I will deploy this model using a web app. So stay tuned.
You can download the annotated weapons data as well as the code at Github.
If you want to know more about various Object Detection techniques, motion estimation, object tracking in video etc., I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Demystifying Object Detection and Instance Segmentation for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/12/05/od/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/12/05/od/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/od/main.jpeg"></media:content>
      

      
      <description>I like deep learning a lot but Object Detection is something that doesn’t come easily to me.
And Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.
It is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.</description>

      <content:encoded>  
        
        <![CDATA[  I like deep learning a lot but Object Detection is something that doesn’t come easily to me.
And Object detection is important and does have its uses. Most common of them being self-driving cars, medical imaging and face detection.
It is definitely a hard problem to solve. And with so many moving parts and new concepts introduced over the long history of this problem, it becomes even harder to understand.
This post is about distilling that history into an easy explanation and explaining the gory details for Object Detection and Instance Segmentation.
Introduction We all know about the image classification problem. Given an image can you find out the class the image belongs to?
We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets.
 ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.
 But there are lots of other interesting problems in the Image domain:
Source
These problems can be divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:
 Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class?
 Classification&#43;Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number of objects(commonly 1) in the image.
 Object Detection: A More general case of the Classification&#43;Localization problem. In a real-world setting, we don’t know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them?
 Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won’t be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.
  As you can see all the problems have something of a similar flavour but a little different than each other. In this post, I will focus mainly on Object Detection and Instance segmentation as they are the most interesting.I will go through the 4 most famous techniques for object detection and how they improved with time and new ideas.
Classification&#43;Localization So lets first try to understand how we can solve the problem when we have a single object in the image. How to solve the Classification&#43;Localization case.
 💡 Treat localization as a regression problem!
 Input Data Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting, we used to have data in the form (X,y) where X is the image and y used to be the class label.
In the Classification&#43;Localization setting, we will have data normally in the form (X,y), where X is still the image and y is an array containing (class_label, x,y,w,h) where,
x = bounding box top left corner x-coordinate
y = bounding box top left corner y-coordinate
w = width of the bounding box in pixels
h = height of the bounding box in pixels
Model So in this setting, we create a multi-output model which takes an image as the input and has (n_labels &#43; 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).
Loss Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).
 Loss = alpha*Softmax_Loss &#43; (1-alpha)*L2_Loss
 Since these two losses would be on a different scale, the alpha hyper-parameter is something that needs to be tuned.
There is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function.
And herein lies the essence of the whole Deep Learning framework — Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem. And that is what we are going to see a lot going forward.
Object Detection So how does this idea of localization using regression get mapped to Object Detection? It doesn’t.
We don’t have a fixed number of objects. So we can’t have 4 outputs denoting, the bounding box coordinates.
One naive idea could be to apply CNN to many different crops of the image. CNN classifies each crop as an object class or background class. This is intractable. There could be a lot of such crops that you can create.
Region Proposals: So, if just there was just a method(Normally called Region Proposal Network)which could find some smaller number of cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is the basic idea behind RCNN-The first major success in object detection.
And that is what selective search (Uijlings et al, “Selective Search for Object Recognition”, IJCV 2013) provided.
So what are Region Proposals?
 Find “blobby” image regions that are likely to contain objects
 Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU
  So, how exactly the region proposals are made?
Selective Search for Object Recognition: This paper finds regions in two steps.
First, we start with a set of some initial regions using Efficient GraphBased Image Segmentation.
 Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels.
 In this paper they take an approach:
 Each edge (vi , vj )∈ E has a corresponding weight w((vi , vj )), which is a non-negative measure of the similarity between neighboring elements vi and vj . In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ∈ S corresponds to a connected component in a graph.
 Put simply, they use graph-based methods to find connected components in an image and the edges are made on some measure of similarity between pixels.
As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. And that is the second step.
For that, the authors of Selective Search for Object Recognition apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm, they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill to provide us with much better region proposals.
1. R-CNN So now we have our region proposals. How do we exactly use them in R-CNN?
 Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.
 Along with this, the authors have also used a class-specific bounding box regressor, that takes:
Input : (Px, Py, Ph, Pw) — the location of the proposed region.
Target: (Gx, Gy, Gh, Gw) — Ground truth labels for the region.
The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G)
Training R-CNN What is the input to an RCNN?
So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. We train class-specific SVM’s
So every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class-specific data.
Test Time R-CNN At test time we predict detection boxes using class-specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Thus, non-maximum suppression is an integral part of the object detection pipeline.
First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed.
This process is recursively applied on all the remaining boxes until we are left with good bounding boxes only.
Problems with RCNN:  Training is slow.
 Inference (detection) is slow. 47s / image with VGG16 — Since the Convnet needs to be run many times.
  Need for speed. So Fast R-CNN.
2. Fast R-CNN  💡 So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM/classifier to classify those crops.
 Something like:
 From Paper: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.
 💡Idea So the basic idea is to have to run the convolution only once in the image rather than so many convolution networks in R-CNN. Then we can map the ROI proposals using some method and filter the last convolution layer and just run a final classifier on that.
This idea depends a little upon the architecture of the model that gets used too.
So the architecture that the authors have proposed is:
 We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net’s first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network’s last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K &#43; 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.
 Don’t worry if you don’t understand the above. This obviously is a little confusing, so let us break this down. But for that, we need to see VGG16 architecture first.
The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).
Now the location of ROI coordinates is in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512.
As we are using VGG, we have transformed the image (224 x 224 x 3) into (14 x 14 x 512) — i.e. the height and width are divided by 16. We can map ROIs coordinates onto the feature map just by dividing them by 16.
 In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the “things” it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.
 What is ROI pooling?
Remember that the final classifier runs for each crop. And so each crop needs to be of the same size. And that is what ROI Pooling does.
In the above image, our region proposal is (0,3,5,7) in x,y,w,h format.
We divide that area into 4 regions since we want to have an ROI pooling layer of 2x2. We divide the whole area into buckets by rounding 5&amp;frasl;2 and 7&amp;frasl;2 and then just do a max-pool.
How do you do ROI-Pooling on Areas smaller than the target size? if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.
After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K &#43; 1 categories(&#43;1 for Background) and category-specific bounding-box regressors.
Training Fast-RCNN What is the input to a Fast- RCNN?
Pretty much similar to R-CNN: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next, we treat all region proposals with ≥ 0.5 IoU(Intersection over Union) overlap with a ground-truth box as a positive training example for that box’s class and the rest as negative. This time we have a dense layer on top, and we use multi-task loss.
So every ROI becomes a training example. The main difference is that there is a concept of multi-task loss:
A Fast R-CNN network has two sibling output layers.
The first outputs a discrete probability distribution (per RoI), p = (p0, &amp;hellip; , pK), over K &#43; 1 categories. As usual, p is computed by a softmax over the K&#43;1 outputs of a fully connected layer.
The second sibling layer outputs bounding-box regression offsets, t= (tx, ty, tw, th), for each of the K object classes. Each training RoI is labelled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labelled RoI to jointly train for classification and bounding-box regression
Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class.
Problem: Region proposals are still taking up most of the time. Can we reduce the time taken for Region proposals?
3. Faster-RCNN The next question that got asked was: Can the network itself do region proposals?
 The intuition is that: With FastRCNN we’re already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes &#43; bbox coordinates?
 How does the Region Proposal Network work? One of the main ideas in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.
So, first of all, we define anchor centres on the image.
The anchor centres are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224&amp;frasl;14).
This is how anchors look like:
 So we start with some predefined regions we think our objects could be with Anchors.
 Our Region Proposal Network(RPN) classifies which regions have the object and the offset of the object bounding box. Training is done using the same logic. 1 if IOU for anchor with bounding box&amp;gt;0.5 0 otherwise.
 Non-Maximum suppression to reduce region proposals
 Fast RCNN detection network on top of proposals
  Faster-RCNN Loss The whole network is then jointly trained with 4 losses:
 RPN classify object / not object
 RPN regress box coordinates offset
 Final classification score (object classes)
 Final box coordinates offset
  Performance Instance Segmentation Now comes the most interesting part — Instance segmentation. Can we create masks for each individual object in the image? Specifically something like:
Mask-RCNN The same authors come to rescue again. The basic idea is to add another output layer that predicts the mask. And to use ROIAlign instead of ROIPooling.
Source: Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign
Mask R-CNN adopts the same two-stage procedure, with an identical first stage (RPN).
In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.
ROIAlign vs ROIPooling In ROI pooling we lose the exact location-based information. See how we arbitrarily divided our region into 4 different sized boxes. For a classification task, it works well.
But for providing masks on a pixel level, we don’t want to lose this information. And hence we don’t quantize the pooling layer and use bilinear interpolation to find out values that properly aligns the extracted features with the input. See how 0.8 differs from 0.88
Source
Training During training, we define a multi-task loss on each sampled RoI as
L = Lcls &#43; Lbox &#43; Lmask
The classification loss Lcls and bounding-box loss Lbox are identical as in Faster R-CNN. The mask branch has a K × m × m — dimensional output for each RoI, which encodes K binary masks of resolution m × m, one for each of the K classes.
To this, we apply a per-pixel sigmoid and define Lmask as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, Lmask is only defined on the kth mask (other mask outputs do not contribute to the loss).
Mask Prediction The mask layer is K × m × m dimensional where K is the number of classes. The m×m floating-number mask output is resized to the RoI size and binarized at a threshold of 0.5 to get final masks.
Conclusion Congrats for reaching the end. This post was a long one.
In this post, I talked about some of the most important advancements in the field of Object detection and Instance segmentation and tried to explain them as easily as I can.
This is my own understanding of these papers with inputs from many blogs and slides on the internet and I sincerely thank the creators. Let me know if you find something wrong with my understanding.
Object detection is a vast field and there are a lot of other methods that dominate this field. Some of them being U-net, SSD and YOLO.
There is no dearth of resources to learn them so I would encourage you to go and take a look at them. You have got a solid backing/understanding now.
In this post, I didn’t write about coding and implementation. So stay tuned for my next post in which we will train a Mask RCNN model for a custom dataset.
If you want to know more about various Object Detection techniques, motion estimation, object tracking in video etc., I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>An End to End Introduction to GANs</title>
      <link>https://mlwhiz.com/blog/2019/06/17/gans/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/06/17/gans/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/gans/faces.png"></media:content>
      

      
      <description>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.
In this post, I will help the reader to understand how they can create and build such applications on their own.</description>

      <content:encoded>  
        
        <![CDATA[    I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.
In this post, I will help the reader to understand how they can create and build such applications on their own.
I will try to keep this post as intuitive as possible for starters while not dumbing it down too much.
This post is about understanding how GANs work.
Task Overview I will work on creating our own anime characters using anime characters dataset.
The DC-GAN flavor of GANs which I will use here is widely applicable not only to generate Faces or new anime characters; it can also be used to create modern fashion styles, for general content creation and sometimes for data augmentation purposes as well.
As per my view, GANs will change the way video games and special effects are generated. The approach could create realistic textures or characters on demand.
You can find the full code for this chapter in the Github Repository. I have also uploaded the code to Google Colab so that you can try it yourself.
Using DCGAN architecture to generate anime images As always before we get into the coding, it helps to delve a little bit into the theory.
The main idea of DC-GAN’s stemmed from the paper UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS written in 2016 by Alec Radford, Luke Metz, and Soumith Chintala.
Although I am going to explain the paper in the next few sections, do take a look at it. It is an excellent paper.
INTUITION: Brief Intro to GANs for Generating Fake Images   Generator vs. Discriminator    Typically, GANs employ two dueling neural networks to train a computer to learn the nature of a data set well enough to generate convincing fakes.
We can think of this as two systems where one Neural Network works to generate fakes (Generator), and another neural network (Discriminator) tries to classify which image is a fake.
As both generator and discriminator networks do this repetitively, the networks eventually get better at their respective tasks.
Think of this as simple as swordplay. Two noobs start sparring with each other. After a while, both become better at swordplay.
Or you could think of this as a robber(generator) and a policeman(Discriminator). After a lot of thefts, the robber becomes better at thieving while the policeman gets better at catching the robber. In an ideal world.
The Losses in these neural networks are primarily a function of how the other network performs:
 Discriminator network loss is a function of generator network quality- Loss is high for the discriminator if it gets fooled by the generator’s fake images
 Generator network loss is a function of discriminator network quality — Loss is high if the generator is not able to fool the discriminator.
  In the training phase, we train our Discriminator and Generator networks sequentially intending to improve both the Discriminator and Generator performance.
The objective is to end up with weights that help Generators to generate realistic looking images. In the end, we can use the Generator Neural network to generate fake images from Random Noise.
Generator architecture One of the main problems we face with GANs is that the training is not very stable. Thus we have to come up with a Generator architecture that solves our problem and also results in stable training.
     The preceding diagram is taken from the paper, which explains the DC-GAN generator architecture. It might look a little bit confusing.
Essentially we can think of a generator Neural Network as a black box which takes as input a 100 sized normally generated vector of numbers and gives us an image:
     How do we get such an architecture?
In the below architecture, we use a dense layer of size 4x4x1024 to create a dense vector out of this 100-d vector. Then, we reshape this dense vector in the shape of an image of 4x4 with 1024 filters, as shown in the following figure:
  tc   We don’t have to worry about any weights right now as the network itself will learn those while training.
Once we have the 1024 4x4 maps, we do upsampling using a series of Transposed convolutions, which after each operation doubles the size of the image and halves the number of maps. In the last step, though we don’t half the number of maps but reduce it to 3 channels/maps only for each RGB channel since we need three channels for the output image.
Now, What are Transpose convolutions? In most simple terms, transpose convolutions provide us with a way to upsample images. While in the convolution operation we try to go from a 4x4 image to a 2x2 image, in Transpose convolutions, we convolve from 2x2 to 4x4 as shown in the following figure:
     Q: We know that Un-pooling is popularly used for upsampling input feature maps in the convolutional neural network (CNN). Why don’t we use Un-pooling?
It is because un-pooling does not involve any learning. However, transposed convolution is learnable, and that is why we prefer transposed convolutions to un-pooling. Their parameters can be learned by the generator as we will see in some time.
Discriminator architecture Now, as we have understood the generator architecture, here is the discriminator as a black box.
In practice, it contains a series of convolutional layers and a dense layer at the end to predict if an image is fake or not as shown in the following figure:
     Takes an image as input and predicts if it is real/fake. Every image conv net ever.
Data preprocessing and visualization The first thing we want to do is to look at some of the images in the dataset. The following are the python commands to visualize some of the images from the dataset:
filenames = glob.glob(&amp;#39;animeface-character-dataset/*/*.pn*&amp;#39;) plt.figure(figsize=(10, 8)) for i in range(5): img = plt.imread(filenames[i], 0) plt.subplot(4, 5, i&#43;1) plt.imshow(img) plt.title(img.shape) plt.xticks([]) plt.yticks([]) plt.tight_layout() plt.show() The resultant output is as follows:
     We get to see the sizes of the images and the images themselves.
We also need functions to preprocess the images to a standard size of 64x64x3, in this particular case, before proceeding further with our training.
We will also need to normalize the image pixels before we use it to train our GAN. You can see the code it is well commented.
# A function to normalize image pixels. def norm_img(img): &amp;#39;&amp;#39;&amp;#39;A function to Normalize Images. Input: img : Original image as numpy array. Output: Normailized Image as numpy array &amp;#39;&amp;#39;&amp;#39; img = (img / 127.5) - 1 return img def denorm_img(img): &amp;#39;&amp;#39;&amp;#39;A function to Denormailze, i.e. recreate image from normalized image Input: img : Normalized image as numpy array. Output: Original Image as numpy array &amp;#39;&amp;#39;&amp;#39; img = (img &#43; 1) * 127.5 return img.astype(np.uint8) def sample_from_dataset(batch_size, image_shape, data_dir=None): &amp;#39;&amp;#39;&amp;#39;Create a batch of image samples by sampling random images from a data directory. Resizes the image using image_shape and normalize the images. Input: batch_size : Sample size required image_size : Size that Image should be resized to data_dir : Path of directory where training images are placed. Output: sample : batch of processed images &amp;#39;&amp;#39;&amp;#39; sample_dim = (batch_size,) &#43; image_shape sample = np.empty(sample_dim, dtype=np.float32) all_data_dirlist = list(glob.glob(data_dir)) sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size) for index,img_filename in enumerate(sample_imgs_paths): image = Image.open(img_filename) image = image.resize(image_shape[:-1]) image = image.convert(&amp;#39;RGB&amp;#39;) image = np.asarray(image) image = norm_img(image) sample[index,...] = image return sample As you will see, we will be using the preceding defined functions in the training part of our code.
Implementation of DCGAN This is the part where we define our DCGAN. We will be defining our noise generator function, Generator architecture, and Discriminator architecture.
Generating noise vector for Generator   Kids: Normal Noise generators    The following code block is a helper function to create a noise vector of predefined length for a Generator. It will generate the noise which we want to convert to an image using our generator architecture.
We use a normal distribution
to generate the noise vector:
def gen_noise(batch_size, noise_shape): &amp;#39;&amp;#39;&amp;#39; Generates a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) Input: batch_size : size of batch noise_shape: shape of noise vector, normally kept as 100 Output:a numpy vector sampled from normal distribution of shape (batch_size,noise_shape) &amp;#39;&amp;#39;&amp;#39; return np.random.normal(0, 1, size=(batch_size,)&#43;noise_shape) Generator architecture The Generator is the most crucial part of the GAN.
Here, I create a generator by adding some transposed convolution layers to upsample the noise vector to an image.
As you will notice, this generator architecture is not the same as given in the Original DC-GAN paper.
I needed to make some architectural changes to fit our data better, so I added a convolution layer in the middle and removed all dense layers from the generator architecture, making it fully convolutional.
I also use a lot of Batchnorm layers with a momentum of 0.5 and leaky ReLU activation. I use Adam optimizer with β=0.5. The following code block is the function I will use to create the generator:
def get_gen_normal(noise_shape): &amp;#39;&amp;#39;&amp;#39; This function takes as input shape of the noise vector and creates the Keras generator architecture. &amp;#39;&amp;#39;&amp;#39; kernel_init = &amp;#39;glorot_uniform&amp;#39; gen_input = Input(shape = noise_shape) # Transpose 2D conv layer 1. generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = &amp;#34;valid&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(gen_input) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 2. generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 3. generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Transpose 2D conv layer 4. generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # conv 2D layer 1. generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) generator = BatchNormalization(momentum = 0.5)(generator) generator = LeakyReLU(0.2)(generator) # Final Transpose 2D conv layer 5 to generate final image. Filter size 3 for 3 image channel generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(generator) # Tanh activation to get final normalized image generator = Activation(&amp;#39;tanh&amp;#39;)(generator) # defining the optimizer and compiling the generator model. gen_opt = Adam(lr=0.00015, beta_1=0.5) generator_model = Model(input = gen_input, output = generator) generator_model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=gen_opt, metrics=[&amp;#39;accuracy&amp;#39;]) generator_model.summary() return generator_model You can plot the final generator model:
plot_model(generator, to_file=&amp;#39;gen_plot.png&amp;#39;, show_shapes=True, show_layer_names=True)   Generator Architecture    Discriminator architecture Here is the discriminator architecture where I use a series of convolutional layers and a dense layer at the end to predict if an image is fake or not.
Here is the architecture of the discriminator:
def get_disc_normal(image_shape=(64,64,3)): dropout_prob = 0.4 kernel_init = &amp;#39;glorot_uniform&amp;#39; dis_input = Input(shape = image_shape) # Conv layer 1: discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(dis_input) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 2: discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 3:  discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator) # Conv layer 4: discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = &amp;#34;same&amp;#34;, data_format = &amp;#34;channels_last&amp;#34;, kernel_initializer = kernel_init)(discriminator) discriminator = BatchNormalization(momentum = 0.5)(discriminator) discriminator = LeakyReLU(0.2)(discriminator)#discriminator = MaxPooling2D(pool_size=(2, 2))(discriminator) # Flatten discriminator = Flatten()(discriminator) # Dense Layer discriminator = Dense(1)(discriminator) # Sigmoid Activation discriminator = Activation(&amp;#39;sigmoid&amp;#39;)(discriminator) # Optimizer and Compiling model dis_opt = Adam(lr=0.0002, beta_1=0.5) discriminator_model = Model(input = dis_input, output = discriminator) discriminator_model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=dis_opt, metrics=[&amp;#39;accuracy&amp;#39;]) discriminator_model.summary() return discriminator_modelplot_model(discriminator, to_file=&amp;#39;dis_plot.png&amp;#39;, show_shapes=True, show_layer_names=True)   Discriminator Architecture    Training Understanding how the training works in GAN is essential. And maybe a little interesting too.
I start by creating our discriminator and generator using the functions defined in the previous section:
discriminator = get_disc_normal(image_shape) generator = get_gen_normal(noise_shape) The generator and discriminator are then combined to create the final GAN.
discriminator.trainable = False # Optimizer for the GAN opt = Adam(lr=0.00015, beta_1=0.5) #same as generator # Input to the generator gen_inp = Input(shape=noise_shape) GAN_inp = generator(gen_inp) GAN_opt = discriminator(GAN_inp) # Final GAN gan = Model(input = gen_inp, output = GAN_opt) gan.compile(loss = &amp;#39;binary_crossentropy&amp;#39;, optimizer = opt, metrics=[&amp;#39;accuracy&amp;#39;]) plot_model(gan, to_file=&amp;#39;gan_plot.png&amp;#39;, show_shapes=True, show_layer_names=True) This is the architecture of our whole GAN:
The Training Loop This is the main region where we need to understand how the blocks we have created until now assemble and work together to work as one.
# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(&amp;#34;Begin step: &amp;#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) # Genearate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) # Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 10th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir&#43;step_num&#43;&amp;#34;_image.png&amp;#34;) # Create the labels for real and fake data. We don&amp;#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(&amp;#34;Disc: real loss: %ffake loss: %f&amp;#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) # Train the generator using a random vector of noise and its labels (1&amp;#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(&amp;#34;GAN loss: %f&amp;#34; % (gan_metrics[0])) # Log results by opening a file in append mode text_file = open(log_dir&#43;&amp;#34;\\training_log.txt&amp;#34;, &amp;#34;a&amp;#34;) text_file.write(&amp;#34;Step: %dDisc: real loss: %ffake loss: %fGAN loss: %f\n&amp;#34; % (tot_step, dis_metrics_real[0], dis_metrics_fake[0],gan_metrics[0])) text_file.close() # save GAN loss to plot later avg_GAN_loss.append(gan_metrics[0]) end_time = time.time() diff_time = int(end_time - step_begin_time) print(&amp;#34;Step %dcompleted. Time took: %ssecs.&amp;#34; % (tot_step, diff_time)) # save model at every 500 steps if ((tot_step&#43;1) % 500) == 0: print(&amp;#34;-----------------------------------------------------------------&amp;#34;) print(&amp;#34;Average Disc_fake loss: %f&amp;#34; % (np.mean(avg_disc_fake_loss))) print(&amp;#34;Average Disc_real loss: %f&amp;#34; % (np.mean(avg_disc_real_loss))) print(&amp;#34;Average GAN loss: %f&amp;#34; % (np.mean(avg_GAN_loss))) print(&amp;#34;-----------------------------------------------------------------&amp;#34;) discriminator.trainable = False generator.trainable = False # predict on fixed_noise fixed_noise_generate = generator.predict(noise) step_num = str(tot_step).zfill(4) save_img_batch(fixed_noise_generate,img_save_dir&#43;step_num&#43;&amp;#34;fixed_image.png&amp;#34;) generator.save(save_model_dir&#43;str(tot_step)&#43;&amp;#34;_GENERATOR_weights_and_arch.hdf5&amp;#34;) discriminator.save(save_model_dir&#43;str(tot_step)&#43;&amp;#34;_DISCRIMINATOR_weights_and_arch.hdf5&amp;#34;) Don’t worry, I will try to break the above code step by step here. The main steps in every training iteration are:
Step 1: Sample a batch of normalized images from the dataset directory
# Use a fixed noise vector to see how the GAN Images transition through time on a fixed noise. fixed_noise = gen_noise(16,noise_shape) # To keep Track of losses avg_disc_fake_loss = [] avg_disc_real_loss = [] avg_GAN_loss = [] # We will run for num_steps iterations for step in range(num_steps): tot_step = step print(&amp;#34;Begin step: &amp;#34;, tot_step) # to keep track of time per step step_begin_time = time.time() # sample a batch of normalized images from the dataset real_data_X = sample_from_dataset(batch_size, image_shape, data_dir=data_dir) Step2:Generate noise for input to the generator
# Generate noise to send as input to the generator noise = gen_noise(batch_size,noise_shape) Step3:Generate images using random noise using the generator.
# Use generator to create(predict) images fake_data_X = generator.predict(noise) # Save predicted images from the generator every 100th step if (tot_step % 100) == 0: step_num = str(tot_step).zfill(4) save_img_batch(fake_data_X,img_save_dir&#43;step_num&#43;&amp;#34;_image.png&amp;#34;) Step 4:Train discriminator using generator images(Fake images) and real normalized images(Real Images) and their noisy labels.
# Create the labels for real and fake data. We don&amp;#39;t give exact ones and zeros but add a small amount of noise. This is an important GAN training trick real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2 fake_data_Y = np.random.random_sample(batch_size)*0.2 # train the discriminator using data and labels discriminator.trainable = True generator.trainable = False # Training Discriminator seperately on real data dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y) # training Discriminator seperately on fake data dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y) print(&amp;#34;Disc: real loss: %ffake loss: %f&amp;#34; % (dis_metrics_real[0], dis_metrics_fake[0])) # Save the losses to plot later avg_disc_fake_loss.append(dis_metrics_fake[0]) avg_disc_real_loss.append(dis_metrics_real[0]) Step 5:Train the GAN using noise as X and 1&amp;rsquo;s(noisy) as Y while keeping discriminator as untrainable.
# Train the generator using a random vector of noise and its labels (1&amp;#39;s with noise) generator.trainable = True discriminator.trainable = False GAN_X = gen_noise(batch_size,noise_shape) GAN_Y = real_data_Y gan_metrics = gan.train_on_batch(GAN_X,GAN_Y) print(&amp;#34;GAN loss: %f&amp;#34; % (gan_metrics[0])) We repeat the steps using the for loop to end up with a good discriminator and generator.
Results The final output image looks like the following. As we can see, the GAN can generate pretty good images for our content editor friends to work with.
They might be a little crude for your liking, but still, this project was a starter for our GAN journey.
     Loss over the training period Here is the graph generated for the losses. We can see that the GAN Loss is decreasing on average and the variance is decreasing too as we do more steps. One might want to train for even more iterations to get better results.
Image generated at every 1500 steps You can see the output and running code in Colab:
# Generating GIF from PNGs import imageio # create a list of PNGs generated_images = [img_save_dir&#43;str(x).zfill(4)&#43;&amp;#34;_image.png&amp;#34; for x in range(0,num_steps,100)] images = [] for filename in generated_images: images.append(imageio.imread(filename)) imageio.mimsave(img_save_dir&#43;&amp;#39;movie.gif&amp;#39;, images) from IPython.display import Image with open(img_save_dir&#43;&amp;#39;movie.gif&amp;#39;,&amp;#39;rb&amp;#39;) as f: display(Image(data=f.read(), format=&amp;#39;png&amp;#39;)) Given below is the code to generate some images at different training steps. As we can see, as the number of steps increases the images are getting better.
# create a list of 20 PNGs to show generated_images = [img_save_dir&#43;str(x).zfill(4)&#43;&amp;#34;fixed_image.png&amp;#34; for x in range(0,num_steps,1500)] print(&amp;#34;Displaying generated images&amp;#34;) # You might need to change grid size and figure size here according to num images. plt.figure(figsize=(16,20)) gs1 = gridspec.GridSpec(5, 4) gs1.update(wspace=0, hspace=0) for i,image in enumerate(generated_images): ax1 = plt.subplot(gs1[i]) ax1.set_aspect(&amp;#39;equal&amp;#39;) step = image.split(&amp;#34;fixed&amp;#34;)[0] image = Image.open(image) fig = plt.imshow(image) # you might need to change some params here fig = plt.text(20,47,&amp;#34;Step: &amp;#34;&#43;step,bbox=dict(facecolor=&amp;#39;red&amp;#39;, alpha=0.5),fontsize=12) plt.axis(&amp;#39;off&amp;#39;) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.tight_layout() plt.savefig(&amp;#34;GENERATEDimage.png&amp;#34;,bbox_inches=&amp;#39;tight&amp;#39;,pad_inches=0) plt.show() Given below is the result of the GAN at different time steps:
Conclusion In this post, we learned about the basics of GAN. We also learned about the Generator and Discriminator architecture for DC-GANs, and we built a simple DC-GAN to generate anime images from scratch.
This model is not very good at generating fake images, yet we get to understand the basics of GANs with this project, and we are fired up to build more exciting and complex GANs as we go forward.
The DC-GAN flavor of GANs is widely applicable not only to generate Faces or new anime characters, but it can also be used to generate new fashion styles, for general content creation and sometimes for data augmentation purposes as well.
We can now conjure up realistic textures or characters on demand if we have the training data at hand, and that is no small feat.
If you want to know more about deep learning applications and use cases, take a look at the Sequence Models course in the Deep Learning Specialization by Andrew NG. Andrew is a great instructor, and this course is great too.
I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at Medium or Subscribe to my blog.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>A Layman guide to moving from Keras to Pytorch</title>
      <link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/artificial-neural-network.png"></media:content>
      

      
      <description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds.</description>

      <content:encoded>  
        
        <![CDATA[    Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the CuDNNGRU/CuDNNLSTM layers in keras are not deterministic, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: Sentiment Analysis, summarization, dialogue state tracking, to name a few.
Also take a look at my other post: Text Preprocessing Methods for Deep Learning, which talks about different preprocessing techniques you can use for your NLP task and What Kagglers are using for Text Classification, which talks about various deep learning models in use in NLP.
Ok back to the task at hand. While Keras is great to start with deep learning, with time you are going to resent some of its limitations. I sort of thought about moving to Tensorflow. It seemed like a good transition as TF is the backend of Keras. But was it hard? With the whole session.run commands and tensorflow sessions, I was sort of confused. It was not Pythonic at all.
Pytorch helps in that since it seems like the python way to do things. You have things under your control and you are not losing anything on the performance front. In the words of Andrej Karpathy:
I&amp;#39;ve been using PyTorch a few months now and I&amp;#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.
&amp;mdash; Andrej Karpathy (@karpathy) May 26, 2017 
So without further ado let me translate Keras to Pytorch for you.
The Classy way to write your network?   Ok, let us create an example network in keras first which we will try to port into Pytorch. Here I would like to give a piece of advice too. When you try to move from Keras to Pytorch take any network you have and try porting it to Pytorch. It will make you understand Pytorch in a much better way. Here I am trying to write one of the networks that gave pretty good results in the Quora Insincere questions classification challenge for me. This model has all the bells and whistles which at least any Text Classification deep learning network could contain with its GRU, LSTM and embedding layers and also a meta input layer. And thus would serve as a good example. Also if you want to read up more on how the BiLSTM/GRU and Attention model work do visit my post here.
def get_model(features,clipvalue=1.,num_filters=40,dropout=0.1,embed_size=501): features_input = Input(shape=(features.shape[1],)) inp = Input(shape=(maxlen, )) # Layer 1: Word2Vec Embeddings. x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp) # Layer 2: SpatialDropout1D(0.1) x = SpatialDropout1D(dropout)(x) # Layer 3: Bidirectional CuDNNLSTM x = Bidirectional(LSTM(num_filters, return_sequences=True))(x) # Layer 4: Bidirectional CuDNNGRU x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x) # Layer 5: some pooling operations avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features x = concatenate([avg_pool, x_h, max_pool,features_input]) # Layer 7: A dense layer x = Dense(16, activation=&amp;#34;relu&amp;#34;)(x) # Layer 8: A dropout layer x = Dropout(0.1)(x) # Layer 9: Output dense layer with one output for our Binary Classification problem. outp = Dense(1, activation=&amp;#34;sigmoid&amp;#34;)(x) # Some keras model creation and compiling model = Model(inputs=[inp,features_input], outputs=outp) adam = optimizers.adam(clipvalue=clipvalue) model.compile(loss=&amp;#39;binary_crossentropy&amp;#39;, optimizer=adam, metrics=[&amp;#39;accuracy&amp;#39;]) return model So a model in pytorch is defined as a class(therefore a little more classy) which inherits from nn.module . Every class necessarily contains an __init__ procedure block and a block for the forward pass.
 In the __init__ part the user defines all the layers the network is going to have but doesn&amp;rsquo;t yet define how those layers would be connected to each other
 In the forward pass block, the user defines how data flows from one layer to another inside the network.
  Why is this Classy? Obviously classy because of Classes. Duh! But jokes apart, I found it beneficial due to a couple of reasons:
1) It gives you a lot of control on how your network is built.
2) You understand a lot about the network when you are building it since you have to specify input and output dimensions. So ** fewer chances of error**. (Although this one is really up to the skill level)
3) Easy to debug networks. Any time you find any problem with the network just use something like print(&amp;quot;avg_pool&amp;quot;, avg_pool.size()) in the forward pass to check the sizes of the layer and you will debug the network easily
4) You can return multiple outputs from the forward layer. This is pretty helpful in the Encoder-Decoder architecture where you can return both the encoder and decoder output. Or in the case of autoencoder where you can return the output of the model and the hidden layer embedding for the data.
5) Pytorch tensors work in a very similar manner to numpy arrays. For example, I could have used Pytorch Maxpool function to write the maxpool layer but max_pool, _ = torch.max(h_gru, 1) will also work.
6) You can set up different layers with different initialization schemes. Something you won&amp;rsquo;t be able to do in Keras. For example, in the below network I have changed the initialization scheme of my LSTM layer. The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.
7) Wait until you see the training loop in Pytorch You will be amazed at the sort of control it provides.
Now the same model in Pytorch will look like something like this. Do go through the code comments to understand more on how to port.
class Alex_NeuralNet_Meta(nn.Module): def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix): super(Alex_NeuralNet_Meta, self).__init__() # Initialize some parameters for your model self.hidden_size = hidden_size drp = 0.1 # Layer 1: Word2Vec Embeddings. self.embedding = nn.Embedding(max_features, embed_size) self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) self.embedding.weight.requires_grad = False # Layer 2: Dropout1D(0.1) self.embedding_dropout = nn.Dropout2d(0.1) # Layer 3: Bidirectional CuDNNLSTM self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True) for name, param in self.lstm.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 4: Bidirectional CuDNNGRU self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True) for name, param in self.gru.named_parameters(): if &amp;#39;bias&amp;#39; in name: nn.init.constant_(param, 0.0) elif &amp;#39;weight_ih&amp;#39; in name: nn.init.kaiming_normal_(param) elif &amp;#39;weight_hh&amp;#39; in name: nn.init.orthogonal_(param) # Layer 7: A dense layer self.linear = nn.Linear(hidden_size*6 &#43; features.shape[1], lin_size) self.relu = nn.ReLU() # Layer 8: A dropout layer self.dropout = nn.Dropout(drp) # Layer 9: Output dense layer with one output for our Binary Classification problem. self.out = nn.Linear(lin_size, 1) def forward(self, x): &amp;#39;&amp;#39;&amp;#39; here x[0] represents the first element of the input that is going to be passed. We are going to pass a tuple where first one contains the sequences(x[0]) and the second one is a additional feature vector(x[1]) &amp;#39;&amp;#39;&amp;#39; h_embedding = self.embedding(x[0]) # Based on comment by Ivank to integrate spatial dropout. embeddings = h_embedding.unsqueeze(2) # (N, T, 1, K) embeddings = embeddings.permute(0, 3, 2, 1) # (N, K, 1, T) embeddings = self.embedding_dropout(embeddings) # (N, K, 1, T), some features are masked embeddings = embeddings.permute(0, 3, 2, 1) # (N, T, 1, K) h_embedding = embeddings.squeeze(2) # (N, T, K) #h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0))) #print(&amp;#34;emb&amp;#34;, h_embedding.size()) h_lstm, _ = self.lstm(h_embedding) #print(&amp;#34;lst&amp;#34;,h_lstm.size()) h_gru, hh_gru = self.gru(h_lstm) hh_gru = hh_gru.view(-1, 2*self.hidden_size ) #print(&amp;#34;gru&amp;#34;, h_gru.size()) #print(&amp;#34;h_gru&amp;#34;, hh_gru.size()) # Layer 5: is defined dynamically as an operation on tensors. avg_pool = torch.mean(h_gru, 1) max_pool, _ = torch.max(h_gru, 1) #print(&amp;#34;avg_pool&amp;#34;, avg_pool.size()) #print(&amp;#34;max_pool&amp;#34;, max_pool.size()) # the extra features you want to give to the model f = torch.tensor(x[1], dtype=torch.float).cuda() #print(&amp;#34;f&amp;#34;, f.size()) # Layer 6: A concatenation of the last state, maximum pool, average pool and # additional features conc = torch.cat(( hh_gru, avg_pool, max_pool,f), 1) #print(&amp;#34;conc&amp;#34;, conc.size()) # passing conc through linear and relu ops conc = self.relu(self.linear(conc)) conc = self.dropout(conc) out = self.out(conc) # return the final output return out Hope you are still there with me. One thing I would like to emphasize here is that you need to code something up in Pytorch to really understand how it works. And know that once you do that you would be glad that you put in the effort. On to the next section.
Tailored or Readymade: The Best Fit with a highly customizable Training Loop   In the above section I wrote that you will be amazed once you saw the training loop. That was an exaggeration. On the first try you will be a little baffled/confused. But as soon as you read through the loop more than once it will make a lot of intuituve sense. Once again read up the comments and the code to gain a better understanding.
This training loop does k-fold cross-validation on your training data and outputs Out-of-fold train_preds and test_preds averaged over the runs on the test data. I apologize if the flow looks something straight out of a kaggle competition, but if you understand this you would be able to create a training loop for your own workflow. And that is the beauty of Pytorch.
So a brief summary of this loop are as follows:
 Create stratified splits using train data Loop through the splits.  Convert your train and CV data to tensor and load your data to the GPU using the X_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() command Load the model onto the GPU using the model.cuda() command Define Loss function, Scheduler and Optimizer create train_loader and valid_loader` to iterate through batches. Start running epochs. In each epoch  Set the model mode to train using model.train(). Go through the batches in train_loader and run the forward pass Run a scheduler step to change the learning rate Compute loss Set the existing gradients in the optimizer to zero Backpropagate the losses through the network Clip the gradients Take an optimizer step to change the weights in the whole network Set the model mode to eval using model.eval(). Get predictions for the validation data using valid_loader and store in variable valid_preds_fold Calculate Loss and print  After all epochs are done. Predict the test data and store the predictions. These predictions will be averaged at the end of the split loop to get the final test_preds Get Out-of-fold(OOF) predictions for train set using train_preds[valid_idx] = valid_preds_fold These OOF predictions can then be used to calculate the Local CV score for your model.   def pytorch_model_run_cv(x_train,y_train,features,x_test, model_obj, feats = False,clip = True): seed_everything() avg_losses_f = [] avg_val_losses_f = [] # matrix for the out-of-fold predictions train_preds = np.zeros((len(x_train))) # matrix for the predictions on the test set test_preds = np.zeros((len(x_test))) splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train)) for i, (train_idx, valid_idx) in enumerate(splits): seed_everything(i*1000&#43;i) x_train = np.array(x_train) y_train = np.array(y_train) if feats: features = np.array(features) x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda() y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() if feats: kfold_X_features = features[train_idx.astype(int)] kfold_X_valid_features = features[valid_idx.astype(int)] x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda() y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda() model = copy.deepcopy(model_obj) model.cuda() loss_fn = torch.nn.BCEWithLogitsLoss(reduction=&amp;#39;sum&amp;#39;) step_size = 300 base_lr, max_lr = 0.001, 0.003 optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr) ################################################################################################ scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode=&amp;#39;exp_range&amp;#39;, gamma=0.99994) ############################################################################################### train = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold)) valid = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold)) train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True) valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False) print(f&amp;#39;Fold {i &#43; 1}&amp;#39;) for epoch in range(n_epochs): start_time = time.time() model.train() avg_loss = 0. for i, (x_batch, y_batch, index) in enumerate(train_loader): if feats: f = kfold_X_features[index] y_pred = model([x_batch,f]) else: y_pred = model(x_batch) if scheduler: scheduler.batch_step() # Compute and print loss. loss = loss_fn(y_pred, y_batch) optimizer.zero_grad() loss.backward() if clip: nn.utils.clip_grad_norm_(model.parameters(),1) optimizer.step() avg_loss &#43;= loss.item() / len(train_loader) model.eval() valid_preds_fold = np.zeros((x_val_fold.size(0))) test_preds_fold = np.zeros((len(x_test))) avg_val_loss = 0. for i, (x_batch, y_batch,index) in enumerate(valid_loader): if feats: f = kfold_X_valid_features[index] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() avg_val_loss &#43;= loss_fn(y_pred, y_batch).item() / len(valid_loader) valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0] elapsed_time = time.time() - start_time print(&amp;#39;Epoch {}/{} \tloss={:.4f} \tval_loss={:.4f} \ttime={:.2f}s&amp;#39;.format( epoch &#43; 1, n_epochs, avg_loss, avg_val_loss, elapsed_time)) avg_losses_f.append(avg_loss) avg_val_losses_f.append(avg_val_loss) # predict all samples in the test set batch per batch for i, (x_batch,) in enumerate(test_loader): if feats: f = test_features[i * batch_size:(i&#43;1) * batch_size] y_pred = model([x_batch,f]).detach() else: y_pred = model(x_batch).detach() test_preds_fold[i * batch_size:(i&#43;1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0] train_preds[valid_idx] = valid_preds_fold test_preds &#43;= test_preds_fold / len(splits) print(&amp;#39;All \tloss={:.4f} \tval_loss={:.4f} \t&amp;#39;.format(np.average(avg_losses_f),np.average(avg_val_losses_f))) return train_preds, test_preds But Why? Why so much code? Okay. I get it. That was probably a handful. What you could have done with a simple.fit in keras, takes a lot of code to accomplish in Pytorch. But understand that you get a lot of power too. Some use cases for you to understand:
 While in Keras you have prespecified schedulers like ReduceLROnPlateau (and it is a task to write them), in Pytorch you can experiment like crazy. If you know how to write Python you are going to get along just fine Want to change the structure of your model between the epochs. Yeah you can do it. Changing the input size for convolution networks on the fly. And much more. It is only your imagination that will stop you.  Wanna Run it Yourself?   So another small confession here. The code above will not run as is as there are some code artifacts which I have not shown here. I did this in favor of making the post more readable. Like you see the seed_everything, MyDataset and CyclicLR (From Jeremy Howard Course) functions and classes in the code above which are not really included with Pytorch. But fret not my friend. I have tried to write a Kaggle Kernel with the whole running code. You can see the code here and include it in your projects.
If you liked this post, please don&amp;rsquo;t forget to upvote the Kernel too. I will be obliged.
Endnotes and References This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.
 Discussion on 3rd Place winner model in Toxic comment 3rd Place model in Keras by Larry Freeman Pytorch starter Capsule model How to: Preprocessing when using embeddings Improve your Score with some Text Preprocessing Pytorch baseline Pytorch starter  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Object Detection: An End to End Theoretical Perspective</title>
      <link>https://mlwhiz.com/blog/2018/09/22/object_detection/</link>
      <pubDate>Sat, 22 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2018/09/22/object_detection/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/id1.png"></media:content>
      

      
      <description>We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets. ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer&amp;amp;rsquo;s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset.</description>

      <content:encoded>  
        
        <![CDATA[  We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and Transfer Learning using pre-trained nets. ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer&amp;rsquo;s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.  As a side note: if you want to know more about convnets and Transfer Learning I would like to recommend this awesome course on Deep Learning in Computer Vision in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course talks about various CNN architetures and covers a wide variety of problems in the image domain including detection and segmentation.
But there are a lot many interesting problems in the Image domain. The one which we are going to focus on today is the Segmentation, Localization and Detection problem. So what are these problems?
  So these problems are divided into 4 major buckets. In the next few lines I would try to explain each of these problems concisely before we take a deeper dive:
 Semantic Segmentation: Given an image, can we classify each pixel as belonging to a particular class? Classification&#43;Localization: We were able to classify an image as a cat. Great. Can we also get the location of the said cat in that image by drawing a bounding box around the cat? Here we assume that there is a fixed number(commonly 1) in the image. Object Detection: A More general case of the Classification&#43;Localization problem. In a real-world setting, we don&amp;rsquo;t know how many objects are in the image beforehand. So can we detect all the objects in the image and draw bounding boxes around them? Instance Segmentation: Can we create masks for each individual object in the image? It is different from semantic segmentation. How? If you look in the 4th image on the top, we won&amp;rsquo;t be able to distinguish between the two dogs using semantic segmentation procedure as it would sort of merge both the dogs together.  In this post, we will focus mainly on Object Detection.
Classification&#43;Localization So lets first try to understand how we can solve the problem when we have a single object in the image. The Classification&#43;Localization case. Pretty neatly said in the CS231n notes:
Treat localization as a regression problem!    Input Data: Lets first talk about what sort of data such sort of model expects. Normally in an image classification setting we used to have data in the form (X,y) where X is the image and y used to be the class labels. In the Classification&#43;Localization setting we will have data normally in the form (X,y), where X is still the image and y is a array containing (class_label, x,y,w,h) where,
x = bounding box top left corner x-coordinate
y = bounding box top left corner y-coordinate
w = width of bounding box in pixel
h = height of bounding box in pixel
Model: So in this setting we create a multi-output model which takes an image as the input and has (n_labels &#43; 4) output nodes. n_labels nodes for each of the output class and 4 nodes that give the predictions for (x,y,w,h).
Loss: In such a setting setting up the loss is pretty important. Normally the loss is a weighted sum of the Softmax Loss(from the Classification Problem) and the regression L2 loss(from the bounding box coordinates).
$$Loss = alpha*SoftmaxLoss &#43; (1-alpha)*L2Loss$$
Since these two losses would be on a different scale, the alpha hyper-parameter needs to be tuned.
There is one thing I would like to note here. We are trying to do object localization task but we still have our convnets in place here. We are just adding one more output layer to also predict the coordinates of the bounding box and tweaking our loss function. And here in lies the essence of the whole Deep Learning framework - Stack layers on top of each other, reuse components to create better models, and create architectures to solve your own problem. And that is what we are going to see a lot going forward.
Object Detection So how does this idea of localization using regression get mapped to Object Detection? It doesn&amp;rsquo;t. We don&amp;rsquo;t have a fixed number of objects. So we can&amp;rsquo;t have 4 outputs denoting, the bounding box coordinates.
One naive idea could be to apply a CNN to many different crops of the image, CNN classifies each crop as object class or background class. This is intractable. There could be a lot of such crops that you can create.
Region Proposals: If just there was a method(Normally called Region Proposal Network)which could find some cropped regions for us automatically, we could just run our convnet on those regions and be done with object detection. And that is what selective search (Uijlings et al, &amp;ldquo;Selective Search for Object Recognition&amp;rdquo;, IJCV 2013) provided for RCNN.
So what are Region Proposals:
 Find &amp;ldquo;blobby&amp;rdquo; image regions that are likely to contain objects Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU  How the region proposals are being made?
Selective Search for Object Recognition: So this paper starts with a set of some initial regions using 13 Graph-based image segmentation techniques generally represent the problem in terms of a graph G = (V, E) where each node v ∈ V corresponds to a pixel in the image, and the edges in E connect certain pairs of neighboring pixels. A weight is associated with each edge based on some property of the pixels that it connects, such as their image intensities. Depending on the method, there may or may not be an edge connecting each pair of vertices.  In this paper they take an approach: Each edge (vi , vj )∈ E has a corresponding weight w((vi , vj )), which is a non-negative measure of the dissimilarity between neighboring elements vi and vj . In the case of image segmentation, the elements in V are pixels and the weight of an edge is some measure of the dissimilarity between the two pixels connected by that edge (e.g., the difference in intensity, color, motion, location or some other local attribute). In the graph-based approach, a segmentation S is a partition of V into components such that each component (or region) C ∈ S corresponds to a connected component in a graph.   
As you can see if we create bounding boxes around these masks we will be losing a lot of regions. We want to have the whole baseball player in a single bounding box/frame. We need to somehow group these initial regions. For that the authors of Selective Search for Object Recognition apply the Hierarchical Grouping algorithm to these initial regions. In this algorithm they merge most similar regions together based on different notions of similarity based on colour, texture, size and fill.
    RCNN The above selective search is the region proposal they used in RCNN paper. But what is RCNN and how does it use region proposals?
  Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVM.  Along with this, the authors have also used a class specific bounding box regressor, that takes: Input : (Px,Py,Ph,Pw) - the location of the proposed region. Target: (Gx,Gy,Gh,Gw) - Ground truth labels for the region. The goal is to learn a transformation that maps the proposed region(P) to the Ground truth box(G) ### Training RCNN What is the input to an RCNN? So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes) Next we treat all region proposals with ≥ 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box&#39;s class and the rest as negative. We train class specific SVM&#39;s So every region proposal becomes a training example. and the convnet gives a feature vector for that region proposal. We can then train our n-SVMs using the class specific data. ### Test Time RCNN At test time we predict detection boxes using class specific SVMs. We will be getting a lot of overlapping detection boxes at the time of testing. Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes   Problems with RCNN: Training is slow. Inference (detection) is slow. 47s / image with VGG16 - Since the Convnet needs to be run many times.
Need for speed. Hence comes in picture by the same authors:
Fast RCNN So the next idea from the same authors: Why not create convolution map of input image and then just select the regions from that convolutional map? Do we really need to run so many convnets? What we can do is run just a single convnet and then apply region proposal crops on the features calculated by the convnet and use a simple SVM to classify those crops.  Something like:   From Paper: Fig. illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all &#34;background&#34; class and another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes refined bounding-box positions for one of the K classes.  This idea depends a little upon the architecture of the model that get used too. Do we take the 4096 bottleneck layer from VGG16? So the architecture that the authors have proposed is: We experiment with three pre-trained ImageNet [4] networks, each with five max pooling layers and between five and thirteen conv layers (see Section 4.1 for network details). When a pre-trained network initializes a Fast R-CNN network, it undergoes three transformations. First, the last max pooling layer is replaced by a RoI pooling layer that is configured by setting H and W to be compatible with the net&#39;s first fully connected layer (e.g., H = W = 7 for VGG16). Second, the network&#39;s last fully connected layer and softmax (which were trained for 1000-way ImageNet classification) are replaced with the two sibling layers described earlier (a fully connected layer and softmax over K &#43; 1 categories and category-specific bounding-box regressors). Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.  This obviously is a little confusing and &#34;hairy&#34;, let us break this down. But for that, we need to see the VGG16 architecture.   The last pooling layer is 7x7x512. This is the layer the network authors intend to replace by the ROI pooling layers. This pooling layer has got as input the location of the region proposal(xmin_roi,ymin_roi,h_roi,w_roi) and the previous feature map(14x14x512).
  Now the location of ROI coordinates are in the units of the input image i.e. 224x224 pixels. But the layer on which we have to apply the ROI pooling operation is 14x14x512. As we are using VGG we will transform image (224 x 224 x 3) into (14 x 14 x 512) - height and width is divided by 16. we can map ROIs coordinates onto the feature map just by dividing them by 16.
In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the &#34;things&#34; it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.  How the ROI pooling is done?   In the above image our region proposal is (0,3,5,7) and we divide that area into 4 regions since we want to have a ROI pooling layer of 2x2.
How do you do ROI-Pooling on Areas smaller than the target size? if region proposal size is 5x5 and ROI pooling layer of size 7x7. If this happens, we resize to 35x35 just by copying 7 times each cell and then max-pooling back to 7x7.
After replacing the pooling layer, the authors also replaced the 1000 layer imagenet classification layer by a fully connected layer and softmax over K &#43; 1 categories(&#43;1 for Background) and category-specific bounding-box regressors.
Training Fast-RCNN What is the input to an Fast- RCNN?
Pretty much similar: So we have got an image, Region Proposals from the RPN strategy and the ground truths of the labels (labels, ground truth boxes)
Next we treat all region proposals with ≥ 0.5 IoU(Intersection over union) overlap with a ground-truth box as positive training example for that box&amp;rsquo;s class and the rest as negative. This time we have a dense layer on top, and we use multi task loss.
So every ROI becomes a training example. The main difference is that there is concept of multi-task loss:
A Fast R-CNN network has two sibling output layers. The first outputs a discrete probability distribution (per RoI), p = (p0, . . . , pK), over K &#43; 1 categories. As usual, p is computed by a softmax over the K&#43;1 outputs of a fully connected layer. The second sibling layer outputs bounding-box regression offsets, t= (tx , ty , tw, th), for each of the K object classes. Each training RoI is labeled with a ground-truth class u and a ground-truth bounding-box regression target v. We use a multi-task loss L on each labeled RoI to jointly train for classification and bounding-box regression
  Where Lcls is the softmax classification loss and Lloc is the regression loss. u=0 is for BG class and hence we add to loss only when we have a boundary box for any of the other class. Further:
  Problem:   Faster-RCNN The next question that got asked was : Can the network itself do region proposals?
The intuition is that: With FastRCNN we&#39;re already computing an Activation Map in the CNN, why not run the Activation Map through a few more layers to find the interesting regions, and then finish off the forward pass by predicting the classes &#43; bbox coordinates?    How does the Region Proposal Network work? One of the main idea in the paper is the idea of Anchors. Anchors are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.
So first of all we define anchor centers on the image.
  The anchor centers are separated by 16 px in case of VGG16 network as the final convolution layer of (14x14x512) subsamples the image by a factor of 16(224&amp;frasl;14). This is how anchors look like:
   So we start with some predefined regions we think our objects could be with Anchors. Our RPN Classifies which regions have the object and the offset of the object bounding box. 1 if IOU for anchor with bounding box&amp;gt;0.5 0 otherwise. Non-Maximum suppression to reduce region proposals Fast RCNN detection network on top of proposals  Faster-RCNN Loss: The whole network is then jointly trained with 4 losses:
 RPN classify object / not object RPN regress box coordinates offset Final classification score (object classes) Final box coordinates offset  Results:   Disclaimer: This is my own understanding of these papers with inputs from many blogs and slides on the internet. Let me know if you find something wrong with my understanding. I will be sure to correct myself and post.
References:  Transfer Learning CS231 Object detection Lecture Slides Efficient Graph-Based Image Segmentation Rich feature hierarchies for accurate object detection and semantic segmentation(RCNN Paper) Selective Search for Object Recognition ROI Pooling Explanation Faster RCNN Blog StackOverflow Faster RCNN Blog Faster RCNN Blog Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks https://www.slideshare.net/WenjingChen7/deep-learning-for-object-detection  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Today I Learned This Part 2: Pretrained Neural Networks What are they?</title>
      <link>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</link>
      <pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://image.slidesharecdn.com/practicaldeeplearning-160329181459/95/practical-deep-learning-16-638.jpg"></media:content>
      

      
      <description>Deeplearning is the buzz word right now. I was working on the course for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the Dogs vs Cats competition on Kaggle to achieve very good results.
What are pretrained Neural Networks? So let me tell you about the background a little bit. There is a challenge that happens every year in the visual recognition community - The Imagenet Challenge.</description>

      <content:encoded>  
        
        <![CDATA[  Deeplearning is the buzz word right now. I was working on the course for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the Dogs vs Cats competition on Kaggle to achieve very good results.
What are pretrained Neural Networks? So let me tell you about the background a little bit. There is a challenge that happens every year in the visual recognition community - The Imagenet Challenge. The task there is to classify the images in 1000 categories using Image training data. People train big convolutional deep learning models for this challenge.
Now what does training a neural model actually mean? It just means that they learn the weights for a NN. What if we can get the weights they learn? We can use those weights to load them into our own NN model and predict on the test dataset. Right?
But actually we can go further than that. We can add an extra layer on top of the NN they have prepared to classify our own dataset.
In a way you can think of the intermediate features created by the Pretrained neural networks to be the features for the next layer.
Why it works? We are essentially doing the image classification task only. We need to find out edges, shapes, intensities and other features from the images that are given to us. The pretrained model is already pretty good at finding these sort of features. Forget neural nets, if we plug these features into a machine learning algorithm we should be good.
What we actually do here is replace the last layer of the neural network with a new prediction/output layer and train while keeping the weights for all the layers before the second last layer constant.
Code: I assume that you understand Keras a little. If not you can look at the docs. Let us get into coding now. First of all we will create the architecture of the neural network the VGG Team created in 2014. Then we will load the weights.
Import some stuff
import numpy as np from numpy.random import random, permutation from scipy import misc, ndimage from scipy.ndimage.interpolation import zoom import keras from keras import backend as K from keras.utils.data_utils import get_file from keras.models import Sequential, Model from keras.layers.core import Flatten, Dense, Dropout, Lambda from keras.layers import Input from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D from keras.optimizers import SGD, RMSprop, Adam from keras.preprocessing import image VGG has just one type of convolutional block, and one type of fully connected (&amp;lsquo;dense&amp;rsquo;) block. We start by defining the building blocks of our Deep learning model.
def ConvBlock(layers, model, filters): for i in range(layers): model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(filters, 3, 3, activation=&amp;#39;relu&amp;#39;)) model.add(MaxPooling2D((2,2), strides=(2,2))) def FCBlock(model): model.add(Dense(4096, activation=&amp;#39;relu&amp;#39;)) model.add(Dropout(0.5))  Now the input of the VGG Model was images. When the VGG model was trained in 2014, the creators subtracted the average of each of the three (R,G,B) channels first, so that the data for each channel had a mean of zero. Furthermore, their software that expected the channels to be in B,G,R order, whereas Python by default uses R,G,B. We need to preprocess our data to make these two changes, so that it is compatible with the VGG model. We also add some helper functions.
#Mean of each channel as provided by VGG researchers vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1)) def vgg_preprocess(x): x = x - vgg_mean # subtract mean return x[:, ::-1] # reverse axis bgr-&amp;gt;rgb def VGG_16(): model = Sequential() model.add(Lambda(vgg_preprocess, input_shape=(3,224,224))) ConvBlock(2, model, 64) ConvBlock(2, model, 128) ConvBlock(3, model, 256) ConvBlock(3, model, 512) ConvBlock(3, model, 512) model.add(Flatten()) FCBlock(model) FCBlock(model) model.add(Dense(1000, activation=&amp;#39;softmax&amp;#39;)) return model def finetune(model, num_classes): # Drop last layer model.pop() # Make all layers untrainable. i.e fix all weights for layer in model.layers: layer.trainable=False # Add a new layer which is the new output layer model.add(Dense(num_classes, activation=&amp;#39;softmax&amp;#39;)) model.compile(optimizer=Adam(lr=0.001), loss=&amp;#39;categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) return model # A way to generate batches of images def get_batches(path, dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=64, class_mode=&amp;#39;categorical&amp;#39;): return gen.flow_from_directory(path&#43;dirname, target_size=(224,224), class_mode=class_mode, shuffle=shuffle, batch_size=batch_size) The hard part is done now. Just create a VGG object and load the weights.We will need to load pretrained weights into the model too. You can download the &amp;ldquo;VGG16_weights.h5&amp;rdquo; file here
model = VGG_16() model.load_weights(&amp;#39;VGG16_weights.h5&amp;#39;) # Since our dogs vs cat dataset is binary classification model ftmodel = finetune(model,2) print ftmodel.summary()   Showing a little bit of output here. This is how the last layers of our Neural net look after training. Now we have got a architecture which we got to train. Here we are only training to get the last layer weights. As you can see from the trainable params.
path = &amp;#34;dogscats/&amp;#34; batch_size=64 # Iterators to get our images from our datasets. The datasets are folders named train and valid. Both folder contain two directories &amp;#39;dogs&amp;#39; and &amp;#39;cats&amp;#39;. In each directory the corresponding images are kept. batches = get_batches(path,&amp;#39;train&amp;#39;, batch_size=batch_size) val_batches = get_batches(path,&amp;#39;valid&amp;#39;, batch_size=batch_size) # Now run for some epochs till the validation loss stops decreasing. no_of_epochs=1 for epoch in range(no_of_epochs): print &amp;#34;Running epoch: %d&amp;#34; % epoch ftmodel.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=1, validation_data=val_batches, nb_val_samples=val_batches.nb_sample) latest_weights_filename = &amp;#39;ft%d.h5&amp;#39; % epoch ftmodel.save_weights(latest_weights_filename) #Create Predictions on test set. The test images should be in the folder dogscats/test/test_images/ , which is a single directory containing all images. test_batches = get_batches(path, &amp;#39;test&amp;#39;, batch_size=2*batch_size, class_mode=None) preds = ftmodel.predict_generator(test_batches, test_batches.nb_sample) isdog = preds[:,1] image_id = batches.filenames final_submission = np.stack([ids,isdog], axis=1) And we are done!
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>