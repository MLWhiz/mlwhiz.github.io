<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/" >

  
  <channel>
    <title>Statistics on MLWhiz</title>
    <link>https://mlwhiz.com/tags/statistics/</link>
    <description>Recent content in Statistics on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/statistics/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Top 5 Cities for Data Scientists to Thrive In</title>
      <link>https://mlwhiz.com/blog/2019/11/27/cities/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/27/cities/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.comhttps://cdn.pixabay.com/photo/2018/09/27/09/22/web-3706562_1280.jpg"></media:content>
      

      
      <description>Technological developments have paved the way for new niche industries, where professions like data science have appeared.
Data scientists have the knowledge and expertise to perform the work that data analysts do, and then some. They analyze and interpret complex data sets of varying structures, and are able to solve obscure problems with codes, models, and machine-learning algorithms.
As you can see in our post ‘How did I Learn Data Science?</description>

      <content:encoded>  
        
        <![CDATA[  Technological developments have paved the way for new niche industries, where professions like data science have appeared.
Data scientists have the knowledge and expertise to perform the work that data analysts do, and then some. They analyze and interpret complex data sets of varying structures, and are able to solve obscure problems with codes, models, and machine-learning algorithms.
As you can see in our post ‘How did I Learn Data Science?’, there are many steps to be taken if you want to be a specialist in the field.
From learning the basics of probability and statistics, to learning Data Science in Python to create your own work, Machine Learning, Spark, Linux Shell, Inferential Statistics, NLP, and algorithms––aspiring data scientists must train themselves, whether through online programs and tutorials or through local classes with established mentors.
The language of data science is universal, and the field is gaining traction across the globe. Information Week notes there has been an 8% increase in data science job searches from 2017 to 2019, and actual job postings grew by 55%. A lot of professionals spend years learning their trade, and many of them even have master’s degrees, at the very least.
However, Citizen Data Scientists have emerged as well. These people incorporate their own expertise and unique abilities to analytics tasks, as their primary job function is external to the field of analytics. They are said to serve complementary roles to traditional data scientists.
When collaboration between the two occurs, skills diversify and consolidated knowledge is used for analyses that have been successful. Currently, there are over 2 million students studying the data science courses on learning platform Udemy, with the beginner courses being taken nearly 100,000 times. Nowadays, learning is made possible anywhere. Data science is clearly a booming industry, with many countries having taken a huge interest in the field as businesses continue to expand.
Now is the time for data scientists to shine. And here are some of the top cities for them to thrive in:
1. Bengaluru, India India is projected to have the biggest world economy as an area often targeted for outsourcing––including tech.
Bengaluru has been dubbed the Silicon Valley of India, as it has the best analytics market in the country.
Foreign tech firms have likewise been hosted in this city. It’s become a hub for technological advancements, AI labs, and both tech startups and giants who have contributed significantly to the growing sector. People have flocked to Bengaluru because of its cost of living as well, making it even more ideal for graduates just starting out.
2. Dublin, Ireland As Europe’s newest cloud gateway, it houses server farms for big names like Microsoft, Google, Amazon, and Facebook. This is what draws the cream of the crop in terms of data scientists to Dublin. The average income of the top data scientists in the country is roughly $100,000, which is three times the average salary in Ireland. Despite these facts, the National Analytics Maturity Study found that companies are struggling to find competent people to fill in data science and analysis jobs. This dilemma may continue to increase as more companies incorporate data-related jobs in the coming years. Now may be the best time to consider Dublin, if you need a bit of a change.
3. Lexington Park, Maryland, The United States Lexington Park hosts a high number of tech jobs, with almost a fourth of the city being employed in the STEM field. Therefore, it has the highest median STEM wage in the entire country, with most employees earning six figures.
Over 2,500 data science jobs are found in the city, and the area has more data science jobs per thousand, in contrast to any other place in the US.
4. London, United Kingdom London is known as the world’s hub for AI and FinTech. The UK government is highly invested in this field and has participated in a £1 billion deal together with 50 tech-involved businesses worldwide. While in part this comes from its dwindling economy and the uncertainty surrounding the Brexit deal, it continues to offer some of the highest paying and most reputable jobs in the field. They often host events related to the industry, such as the Deep Learning Summit, Strata Data Conference, and ODSC’s European Conference. It also houses the Alan Turing Institute-the national institute of data science and AI.
5. Singapore, Singapore Standing at number 6 in the list of the world’s Smart Cities, Singapore has seen all kinds of developments in self-driving vehicles, AI, and IT. Like the UK, it receives strong government backing in the form of investments for city infrastructure, which makes use of advanced technology. Many tech startups, giants, and VCs have viewed the city as the place to be in Southeast Asia.
For anyone who’s interested in beginning a career in data science and is in need of some helpful guides, check out the MLWHiz Archive
Post written by: Oliver Williams
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The Simple Math behind 3 Decision Tree Splitting criterions</title>
      <link>https://mlwhiz.com/blog/2019/11/12/dtsplits/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/12/dtsplits/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/dtsplits/main.png"></media:content>
      

      
      <description>Decision Trees are great and are useful for a variety of tasks. They form the backbone of most of the best performing models in the industry like XGboost and Lightgbm.
But how do they work exactly? In fact, this is one of the most asked questions in ML/DS interviews.
We generally know they work in a stepwise manner and have a tree structure where we split a node using some feature on some criterion.</description>

      <content:encoded>  
        
        <![CDATA[  Decision Trees are great and are useful for a variety of tasks. They form the backbone of most of the best performing models in the industry like XGboost and Lightgbm.
But how do they work exactly? In fact, this is one of the most asked questions in ML/DS interviews.
We generally know they work in a stepwise manner and have a tree structure where we split a node using some feature on some criterion.
But how do these features get selected and how a particular threshold or value gets chosen for a feature?
In this post, I will talk about three of the main splitting criteria used in Decision trees and why they work. This is something that has been written about repeatedly but never really well enough.
1. Gini Impurity According to Wikipedia,
 Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.
 In simple terms, Gini impurity is the measure of impurity in a node. Its formula is:
where J is the number of classes present in the node and p is the distribution of the class in the node.
So to understand the formula a little better, let us talk specifically about the binary case where we have nodes with only two classes.
So in the below five examples of candidate nodes labelled A-E and with the distribution of positive and negative class shown, which is the ideal condition to be in?
I reckon you would say A or E and you are right. What is the worst situation to be in? C, I suppose as the data is precisely 50:50 in that node.
Now, this all looks good, intuitively. Gini Impurity gives us a way to quantify it.
Let us calculate the Gini impurity for all five nodes separately and check the values.
✅ Gini Impurity works as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Gini Impurity.
We could also see the plot of Gini Impurity for the binary case to verify the above.
❓So how do we exactly use it in a Decision Tree?
Suppose, we have the UCI Heart Disease data. The “target” field refers to the presence of heart disease in the patient. It is 0 (no presence) or 1.
We now already have a measure in place(Gini Impurity) using which we can evaluate a split on a particular variable with a certain threshold(continuous) or value(categorical).
Categorical Variable Splits For simplicity, let us start with a categorical variable — sex.
If we split by Sex, our tree will look like below:
Notice that we use Sex=0 and Sex!=0 so that this generalises well to categories with multiple levels. Our root node has 165 &#43;ve examples and 138 -ve examples. And we get two child nodes when we split by sex.
We already know how to calculate the impurity for a node. So we calculate the impurity of the left child as well as the right child.
I_Left = 1 - (72/96)**2 - (24/96)**2 I_Right = 1 - (93/207)**2 - (114/207)**2 print(&amp;#34;Left Node Impurity:&amp;#34;,I_Left) print(&amp;#34;Right Node Impurity:&amp;#34;,I_Right) Left Node Impurity: 0.375 Right Node Impurity: 0.4948540222642302  We get two numbers here. We need to get a single number which provides the impurity of a single split. So what do we do? Should, we take an average? We can take an average, but what will happen if one node gets only one example and another node has all other examples?
To mitigate the above, we take a weighted average of the two impurities weighted by the number of examples in the individual node. In code:
gender_split_impurity = 96/(96&#43;207)*I_Left &#43; 207/(96&#43;207)*I_Right print(gender_split_impurity) 0.45688047065576126  Continuous Variable Splits We can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and created a tree.
I_Left = 1 - (58/126)**2 - (68/126)**2 I_Right = 1 - (107/177)**2 - (70/177)**2 print(&amp;#34;Left Node Impurity:&amp;#34;,I_Left) print(&amp;#34;Right Node Impurity:&amp;#34;,I_Right) Left Node Impurity: 0.49685059208868737 Right Node Impurity: 0.47815123368125373  Just by looking at both the impurities close to 0.5, we can infer that it is not a good split. Still, we calculate our weighted Gini impurity as before:
chol_split_impurity = 126/(126&#43;177)*I_Left &#43; 177/(126&#43;177)*I_Right print(chol_split_impurity) 0.48592720450414695  Since the chol_split_impurity&amp;gt;gender_split_impurity, we split based on Gender.
In reality, we evaluate a lot of different splits. With different threshold values for a continuous variable. And all the levels for categorical variables. And then choose the split which provides us with the lowest weighted impurity in the child nodes.
2. Entropy Another very popular way to split nodes in the decision tree is Entropy. Entropy is the measure of Randomness in the system. The formula for Entropy is:
where C is the number of classes present in the node and p is the distribution of the class in the node.
So again talking about the binary case we talked about before. What is the value of Entropy for all the 5 cases from A-E?
Entropy values work as expected. Maximum for Node C and the minimum for both A and E. We need to choose the node with Minimum Entropy.
We could also see the plot of Entropy for the binary case to verify the above.
So how do we exactly use Entropy in a Decision Tree?
We are using the Heartrate example as before. We now already have a measure in place(Entropy) using which we can evaluate a split on an individual variable with a certain threshold(continuous) or value(categorical).
Categorical Variable Splits For simplicity, let us start with a categorical variable — sex.
If we split by Sex, our tree will look like below:
If we split on Gender
We already know how to calculate the randomness for a node. So we calculate the randomness of the left child as well as the right child.
E_Left = -(72/96)*np.log2(72/96) - (24/96)*np.log2(24/96) E_Right = -(93/207)*np.log2(93/207) - (114/207)*np.log2(114/207) print(&amp;#34;Left Node Randomness:&amp;#34;,E_Left) print(&amp;#34;Right Node Randomness:&amp;#34;,E_Right) Left Node Randomness: 0.8112781244591328 Right Node Randomness: 0.992563136012236  We get two numbers here. We need to get a single number which provides the Randomness of a single split. So what do we do? We again take a weighted average where we weight by the number of examples in the individual node. In code:
gender_split_randomness = 96/(96&#43;207)*E_Left &#43; 207/(96&#43;207)*E_Right print(gender_split_randomness) 0.9351263006686785  Continuous Variable Splits Again as before, we can split by a continuous variable too. Let us try to split using cholesterol feature in the dataset. We chose a threshold of 250 and create a tree.
E_Left = -(58/126)*np.log2(58/126) - (68/126)*np.log2(68/126) E_Right = -(107/177)*np.log2(107/177) - (70/177)*np.log2(70/177) print(&amp;#34;Left Node Randomness:&amp;#34;,E_Left) print(&amp;#34;Right Node Randomness:&amp;#34;,E_Right) Left Node Randomness: 0.9954515828457715 Right Node Randomness: 0.9682452182690404  Just by looking at both the randomness close to 1, we can infer that it is not a good split. Still, we calculate our weighted Entropy as before:
chol_split_randomness = 126/(126&#43;177)*E_Left &#43; 177/(126&#43;177)*E_Right print(chol_split_randomness) 0.9795587560138196  Since the chol_split_randomness&amp;gt;gender_split_randomness, we split based on Gender. Precisely the same results we got from Gini.
3. Variance Gini Impurity and Entropy work pretty well for the classification scenario.
But what about regression?
In the case of regression, the most common split measure used is just the weighted variance of the nodes. It makes sense too: We want minimum variation in the nodes after the split.
We want a regression task for this. So, we have the data for 50 startups, and we want to predict Profit.
Categorical Variable Splits Let us try a split by a categorical variable ⇒State=Florida.
If we split by State=FL, our tree will look like below:
Overall Variance then is just the weighted sums of individual variances:
overall_variance = 16/(16&#43;34)*Var_Left &#43; 34/(16&#43;34)*Var_Right print(overall_variance) 1570582843  Continuous Variable Splits Again as before, we can split by a continuous variable too. Let us try to split using R&amp;amp;D spend feature in the dataset. We chose a threshold of 100000 and create a tree.
Splitting on R&amp;amp;D
Just by looking at this, we can see it is better than our previous split. So, we find the overall variance in this case:
overall_variance = 14/(14&#43;36)*419828105 &#43; 36/(14&#43;36)*774641406 print(overall_variance) 675293681.7199999  Since the overall_variance(R&amp;amp;D&amp;gt;=100000)&amp;lt; overall_variance(State==FL), we prefer a split based on R&amp;amp;D.
Continue Learning If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>P-value Explained Simply for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2019/11/11/pval/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/11/pval/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/pval/main.png"></media:content>
      

      
      <description>Recently, I got asked about how to explain p-values in simple terms to a layperson. I found that it is hard to do that.
P-Values are always a headache to explain even to someone who knows about them let alone someone who doesn’t understand statistics.
I went to Wikipedia to find something and here is the definition: &amp;amp;gt; In statistical hypothesis testing, the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two groups) would be equal to, or more extreme than, the actual observed results.</description>

      <content:encoded>  
        
        <![CDATA[  Recently, I got asked about how to explain p-values in simple terms to a layperson. I found that it is hard to do that.
P-Values are always a headache to explain even to someone who knows about them let alone someone who doesn’t understand statistics.
I went to Wikipedia to find something and here is the definition: &amp;gt; In statistical hypothesis testing, the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the sample mean difference between two groups) would be equal to, or more extreme than, the actual observed results.
And my first thought was that might be they have written it like this so that nobody could understand it. The problem here lies with a lot of terminology and language that statisticians enjoy to employ.
This post is about explaining p-values in an easy to understand way without all that pretentiousness of statisticians.
A Real-Life problem In our lives, we certainly believe one thing over another.
From the obvious ones like — The earth is round. Or that the earth revolves around the Sun. The Sun rises in the east.
To the more non-obvious ones with varying level of uncertainties - Exercising reduces weight? Or that Trump is going to win/lose in his next election? Or that a particular drug works? Or that sleeping for 8 hours is good for your health?
While the former category is facts, the latter category differs from person to person.
So, what if I come to you and say that exercising does not affect weight?
All the gym-goers may call me not so kind words. But is there a mathematical and logical structure in which someone can disprove me?
This brings us to the notion of Hypothesis testing.
Hypothesis Testing So the statement I made in the above example — exercising does not affect weight. This statement is my Hypothesis. Let’s call it Null hypothesisfor now. For now, it is the status quo as in we consider it to be true.
The Alternative Hypothesis from people who swear by exercising is — exercising does reduce weight.
But how do we test these hypotheses? We collect Data. We collect weight loss data for a sample of 10 people who regularly exercise for over 3 months.
WeightLoss Sample Mean = 2 kg Sample Standard Deviation = 1 kg  Does this prove that exercise does reduce weight? From a cursory look, it sort of looks like that exercising does have its benefits as people who exercise have lost on an average 2 kgs.
But you will find that such clear cut findings are not always the case when you do hypothesis testing. What if the weight loss mean for people who do exercise was just 0.2 kg. Would you still be so sure that exercise does reduce weight?
So how can we quantify this and put some maths behind it all?
Let us set up our experiment to do this.
Experiment Let’s go back to our Hypotheses again:
Hº: Exercising does not affect weight. Or equivalently 𝜇 = 0
Hᴬ: Exercise does reduce weight. Or equivalently 𝜇&amp;gt;0
We see our data sample of 10 people, and we try to find out the value of
Observed Mean(Weightloss of People who exercise) = 2 kg
Observed Sample Standard Deviation = 1 kg
Now a good question to ask ourselves is — Assuming that the null hypothesis is true, what is the probability of observing a sample mean of 2 kg or more extreme than 2 kg?
Assuming we can calculate this — If this probability value is meagre (lesser than a threshold value), we reject our null hypothesis. And otherwise, we fail to reject our null hypothesis. Why fail to reject and not accept? I will answer this later.
This probability value is actually the p-value. Simply, it is just the probability of observing what we observed or extreme results if we assume our null hypothesis to be true.
The statisticians call the threshold as the significance level(𝜶), and in most of the cases, 𝜶 is taken to be 0.05.
So how do we answer: Assuming that the null hypothesis is true, what is the probability of getting a value of 2 kg or more than 2 kg?
And here comes our favourite distribution, Normal distribution in the picture.
The Normal Distribution We create a Sampling Distribution of the mean of the WeightLoss samples assuming our Null hypothesis is True.
Central Limit Theorem: The central limit theorem simply states that if you have a population with mean μ and standard deviation σ, and take random samples from the population, then the distribution of the sample means will be approximately normally distributed with mean as the population mean and standard deviation σ/√n. Where σ is the standard deviation of the sample and n is the number of observations in the sample.
Now we already know the mean of our population as given by our null hypothesis. So, we use that and have a normal distribution whose mean is 0. And whose standard deviation is given by 1/√10
This is, in fact, the distribution of the mean of the samples from the population. We observed a particular value of the mean that is Xobserved = 2 kg.
Now we can use some statistical software to find the area under this particular curve:
from scipy.stats import norm import numpy as np p = 1-norm.cdf(2, loc=0, scale = 1/np.sqrt(10)) print(p) 1.269814253745949e-10  As such, this is a very small probability p-value ( less than the significance level of 0.05) for the mean of a sample to take a value of 2 or more.
And so we can reject our Null hypothesis. And we can call our results statistically significant as in they don’t just occur due to mere chance.
The Z statistic You might have heard about the Z statistic too when you have read about Hypothesis testing. Again as I said, terminology.
That is the extension of basically the same above idea where we use a standard normal with mean 0 and variance 1 as our sampling distribution after transforming our observed value x using:
This makes it easier to use statistical tables. In our running example, our z statistic is:
z = (2-0)/(1/np.sqrt(10)) print(z) 6.324555320336758  Just looking at the Z statistic of &amp;gt;6 should give you an idea that the observed value is at least six standard deviations away and so the p-value should be very less. We can still find the p-value using:
from scipy.stats import norm import numpy as np p = 1-norm.cdf(z, loc=0, scale=1) print(p) 1.269814253745949e-10  As you can see, we get the same answer using the Z statistic.
An Important Distinction So we said before that we reject our null hypothesis as in we got sufficient evidence to prove that our null hypothesis is false.
But what if the p-value was higher than the significance level. Then we say that we fail to reject the null hypothesis. Why don’t we say accept the null hypothesis?
The best intuitive example of this is using trial courts. In a trial court, the null hypothesis is that the accused is not guilty. Then we see some evidence to disprove the null hypothesis.
If we are not able to disprove the null hypotheses the judge doesn’t say that the accused hasn’t committed the crime. The judge only says that based on the given evidence, we are not able to convict the accused.
Another example to drive this point forward: Assuming that we are exploring life on an alien planet. And our null hypothesis(Hº) is that there is no life on the planet. We roam around a few miles for some time and look for people/aliens on that planet. If we see any alien, we can reject the null hypothesis in favour of the alternative.
But if we don’t see any alien, can we definitively say that there is no alien life on the planet or accept our null hypotheses? Maybe we needed to explore more, or perhaps we needed more time and we may have found an alien. So, in this case, we cannot accept the null hypothesis; we can only fail to reject it. Or In Cassie Kozyrkov’s words from whom the example comes, we can say that “we learned nothing interesting”.
 In STAT101 class, they teach you to write a convoluted paragraph when that happens. (“We fail to reject the null hypothesis and conclude that there is insufficient statistical evidence to support the existence of alien life on this planet.”) I’m convinced that the only purpose of this expression is to strain students’ wrists. I’ve always allowed my undergraduate students to write it like it is: we learned nothing interesting.
 In essence, hypothesis testing is just about checking if our observed values make the null hypothesis look ridiculous. If yes, we reject the null hypothesis and call our results statistically significant. And otherwise we have learned nothing interesting, and we continue with our status quo.
Continue Learning If you want to learn more about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data, Mine Çetinkaya-Rundel teaches Inferential Statistics course on coursera and it cannot get simpler than this one. She is a great instructor and explains the fundamentals of Statistical inference nicely.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Adding Interpretability to Multiclass Text Classification models</title>
      <link>https://mlwhiz.com/blog/2019/11/08/interpret_models/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/08/interpret_models/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/interpret/main.png"></media:content>
      

      
      <description>Explain Like I am 5.
It is the basic tenets of learning for me where I try to distill any concept in a more palatable form. As Feynman said:
 I couldn’t do it. I couldn’t reduce it to the freshman level. That means we don’t really understand it.
 So, when I saw the ELI5 library that aims to interpret machine learning models, I just had to try it out.</description>

      <content:encoded>  
        
        <![CDATA[  Explain Like I am 5.
It is the basic tenets of learning for me where I try to distill any concept in a more palatable form. As Feynman said:
 I couldn’t do it. I couldn’t reduce it to the freshman level. That means we don’t really understand it.
 So, when I saw the ELI5 library that aims to interpret machine learning models, I just had to try it out.
One of the basic problems we face while explaining our complex machine learning classifiers to the business is interpretability.
Sometimes the stakeholders want to understand — what is causing a particular result? It may be because the task at hand is very critical and we cannot afford to take a wrong decision. Think of a classifier that takes automated monetary actions based on user reviews.
Or it may be to understand a little bit more about the business/the problem space.
Or it may be to increase the social acceptance of your model.
This post is about interpreting complex text classification models.
The Dataset: To explain how ELI5 works, I will be working with the stack overflow dataset on Kaggle. This dataset contains around 40000 posts and the corresponding tag for the post.
This is how the dataset looks:
And given below is the distribution for different categories.
This is a balanced dataset and thus suited well for our purpose of understanding.
So let us start. You can follow along with the code in this Kaggle Kernel
Staring Simple: Let us first try to use a simple scikit-learn pipeline to build our text classifier which we will try to interpret later. In this pipeline, I will be using a very simple count vectorizer along with Logistic regression.
from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegressionCV from sklearn.pipeline import make_pipeline # Creating train-test Split X = sodata[[&amp;#39;post&amp;#39;]] y = sodata[[&amp;#39;tags&amp;#39;]] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # fitting the classifier vec = CountVectorizer() clf = LogisticRegressionCV() pipe = make_pipeline(vec, clf) pipe.fit(X_train.post, y_train.tags) Let’s see the results we get:
from sklearn import metrics def print_report(pipe): y_actuals = y_test[&amp;#39;tags&amp;#39;] y_preds = pipe.predict(X_test[&amp;#39;post&amp;#39;]) report = metrics.classification_report(y_actuals, y_preds) print(report) print(&amp;#34;accuracy: {:0.3f}&amp;#34;.format(metrics.accuracy_score(y_actuals, y_preds))) print_report(pipe) The above is a pretty simple Logistic regression model and it performs pretty well. We can check out its weights using the below function:
for i, tag in enumerate(clf.classes_): coefficients = clf.coef_[i] weights = list(zip(vec.get_feature_names(),coefficients)) print(&amp;#39;Tag:&amp;#39;,tag) print(&amp;#39;Most Positive Coefficients:&amp;#39;) print(sorted(weights,key=lambda x: -x[1])[:10]) print(&amp;#39;Most Negative Coefficients:&amp;#39;) print(sorted(weights,key=lambda x: x[1])[:10]) print(&amp;#34;--------------------------------------&amp;#34;) ------------------------------------------------------------ OUTPUT: ------------------------------------------------------------ Tag: python Most Positive Coefficients: [(&#39;python&#39;, 6.314761719932758), (&#39;def&#39;, 2.288467823831321), (&#39;import&#39;, 1.4032539284357077), (&#39;dict&#39;, 1.1915110448370732), (&#39;ordered&#39;, 1.1558015932799253), (&#39;print&#39;, 1.1219958415166653), (&#39;tuples&#39;, 1.053837204818975), (&#39;elif&#39;, 0.9642251085198578), (&#39;typeerror&#39;, 0.9595246314353266), (&#39;tuple&#39;, 0.881802590839166)] Most Negative Coefficients: [(&#39;java&#39;, -1.8496383139251245), (&#39;php&#39;, -1.4335540858871623), (&#39;javascript&#39;, -1.3374796382615586), (&#39;net&#39;, -1.2542682749949605), (&#39;printf&#39;, -1.2014123042575882), (&#39;objective&#39;, -1.1635960146614717), (&#39;void&#39;, -1.1433460304246827), (&#39;var&#39;, -1.059642972412936), (&#39;end&#39;, -1.0498078813349798), (&#39;public&#39;, -1.0134828865993966)] -------------------------------------- Tag: ruby-on-rails Most Positive Coefficients: [(&#39;rails&#39;, 6.364037640161158), (&#39;ror&#39;, 1.804826792986176), (&#39;activerecord&#39;, 1.6892552000017307), (&#39;ruby&#39;, 1.41428459023012), (&#39;erb&#39;, 1.3927336940889532), (&#39;end&#39;, 1.3650227017877463), (&#39;rb&#39;, 1.2280121863441906), (&#39;gem&#39;, 1.1988196865523322), (&#39;render&#39;, 1.1035255831838242), (&#39;model&#39;, 1.0813278895692746)] Most Negative Coefficients: [(&#39;net&#39;, -1.5818801311532575), (&#39;php&#39;, -1.3483618692617583), (&#39;python&#39;, -1.201167422237274), (&#39;mysql&#39;, -1.187479885113293), (&#39;objective&#39;, -1.1727511956332588), (&#39;sql&#39;, -1.1418573958542007), (&#39;messageform&#39;, -1.0551060751109618), (&#39;asp&#39;, -1.0342831159678236), (&#39;ios&#39;, -1.0319120624686084), (&#39;iphone&#39;, -0.9400116321217807)] -------------------------------------- .......  And that is all pretty good. We can see the coefficients make sense and we can try to improve our model using this information.
But above was a lot of code. ELI5 makes this exercise pretty simple for us. We just have to use the below command:
import eli5 eli5.show_weights(clf, vec=vec, top=20) Now as you can see the weights value for Python is the same as from the values we got from the function we wrote manually. And it is much prettier and wholesome to explore.
But that is just the tip of the iceberg. ELI5 can also help us to debug our models as we can see below.
Understanding our Simple Text Classification Model Let us now try to find out why a particular example is misclassified. I am using an example which was originally from the class Python but got misclassified as Java:
y_preds = pipe.predict(sodata[&amp;#39;post&amp;#39;]) sodata[&amp;#39;predicted_label&amp;#39;] = y_preds misclassified_examples = sodata[(sodata[&amp;#39;tags&amp;#39;]!=sodata[&amp;#39;predicted_label&amp;#39;])&amp;amp;(sodata[&amp;#39;tags&amp;#39;]==&amp;#39;python&amp;#39;)&amp;amp;(sodata[&amp;#39;predicted_label&amp;#39;]==&amp;#39;java&amp;#39;)] eli5.show_prediction(clf, misclassified_examples[&amp;#39;post&amp;#39;].values[1], vec=vec) In the above example, the classifier predicts Java with a low probability. And we can examine a lot of things going on in the above example to improve our model. For example:
 We get to see that the classifier is taking a lot of digits into consideration(not good)which brings us to the conclusion of cleaning up the digits. Or replacing DateTime objects with a DateTime token.
 Also see that while dictionary has a negative weight for Java, the word dictionaries has a positive weight. So maybe stemming could also help.
 We also see that there are words like &amp;lt;pre&amp;gt;&amp;lt;code&amp;gt; that are influencing our classifier. These words should be removed while cleaning.
 Why is the word date influencing the results? Something to think about.
  We can take a look at more examples to get more such ideas. You get the gist.
Going Deep And Complex This is all good and fine but*** what if models that we use don’t provide weights for the individual features like LSTM?*** It is with these models that explainability can play a very important role.
To understand how to do this, we first create a TextCNN model on our data. Not showing the model creation process in the interest of preserving space but think of it as a series of preprocessing steps and then creating the deep learning model. If interested, you can check out the modelling steps in this Kaggle kernel.
Things get interesting from our point of view when we have a trained black-box model object.
ELI5 provides us with the eli5.lime.TextExplainer to debug our prediction - to check what was important in the document to make a prediction decision.
To use TextExplainer instance, we pass a document to explain and a black-box classifier (a predict function which returns probabilities) to the fit() method. From the documentation this is how our predict function should look like:
 predict (callable) — Black-box classification pipeline. predict should be a function which takes a list of strings (documents) and return a matrix of shape (n_samples, n_classes) with probability values - a row per document and a column per output label.
 So to use ELI5 we will need to define our own function which takes as input a list of strings (documents) and return a matrix of shape (n_samples, n_classes). You can see how we first preprocess and then predict.
def predict_complex(docs): # preprocess the docs as required by our model val_X = tokenizer.texts_to_sequences(docs) val_X = pad_sequences(val_X, maxlen=maxlen) y_preds = model.predict([val_X], batch_size=1024, verbose=0) return y_preds Given below is how we can use TextExplainer. Using the same misclassified example as before in our simple classifier.
import eli5 from eli5.lime import TextExplainer te = TextExplainer(random_state=2019) te.fit(sodata[&amp;#39;post&amp;#39;].values[0], predict_complex) te.show_prediction(target_names=list(encoder.classes_)) This time it doesn’t get misclassified. You can see that the presence of keywords dict and list is what is influencing the decision of our classifier. One can try to see more examples to find more insights.
So how does this work exactly?
TextExplainer generates a lot of texts similar to the document by removing some of the words, and then trains a white-box classifier which predicts the output of the black-box classifier and not the true labels. The explanation we see is for this white-box classifier.
This is, in essence, a little bit similar to the Teacher-Student model distillation, where we use a simpler model to predict outputs from a much more complex teacher model.
Put simply, it tries to create a simpler model that emulates a complex model and then shows us the simpler model weights.
Conclusion  Understanding is crucial. Being able to interpret our models can help us to understand our models better and in turn, explain them better.
 ELI5 provides us with a good way to do this. It works for a variety of models and the documentation for this library is one of the best I have ever seen.
Also, I love the decorated output the ELI5 library provides with the simple and fast way it provides to interpret my models. And debug them too.
To use ELI5 with your models you can follow along with the code in this Kaggle Kernel
Continue Learning If you want to learn more about NLP and how to create Text Classification models, I would like to call out the Natural Language Processing course in the Advanced machine learning specialization. Do check it out. It talks about a lot of beginners to advanced level topics in NLP. You might also like to take a look at some of my posts on NLP in the NLP Learning series.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Classification Evaluation metrics every Data Scientist must know</title>
      <link>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/eval/main.jpeg"></media:content>
      

      
      <description>What do we want to optimize for? Most of the businesses fail to answer this simple question.
Every business problem is a little different, and it should be optimized differently.
We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?
What if we are predicting the number of asteroids that will hit the earth.</description>

      <content:encoded>  
        
        <![CDATA[  What do we want to optimize for? Most of the businesses fail to answer this simple question.
Every business problem is a little different, and it should be optimized differently.
We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?
What if we are predicting the number of asteroids that will hit the earth.
Just say zero all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable. What should we do in such cases?
 Designing a Data Science project is much more important than the modeling itself.
 This post is about various evaluation metrics and how and when to use them.
1. Accuracy, Precision, and Recall: A. Accuracy Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.
Accuracy = (TP&#43;TN)/(TP&#43;FP&#43;FN&#43;TN)
Accuracy is the proportion of true results among the total number of cases examined.
When to use? Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.
Caveats Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.
B. Precision Let’s start with precision, which answers the following question: what proportion of predicted Positives is truly Positive?
Precision = (TP)/(TP&#43;FP)
In the asteroid prediction problem, we never predicted a true positive.
And thus precision=0
When to use? Precision is a valid choice of evaluation metric when we want to be very sure of our prediction. For example: If we are building a system to predict if we should decrease the credit limit on a particular account, we want to be very sure about our prediction or it may result in customer dissatisfaction.
Caveats Being very precise means our model will leave a lot of credit defaulters untouched and hence lose money.
C. Recall Another very useful measure is recall, which answers a different question: what proportion of actual Positives is correctly classified?
Recall = (TP)/(TP&#43;FN)
In the asteroid prediction problem, we never predicted a true positive.
And thus recall is also equal to 0.
When to use? Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.
Caveats Recall is 1 if we predict 1 for all examples.
And thus comes the idea of utilizing tradeoff of precision vs. recall — F1 Score.
2. F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.
The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.
Let us start with a binary prediction problem. We are predicting if an asteroid will hit the earth or not.
So if we say “No” for the whole training set. Our precision here is 0. What is the recall of our positive class? It is zero. What is the accuracy? It is more than 99%.
And hence the F1 score is also 0. And thus we get to know that the classifier that has an accuracy of 99% is basically worthless for our case. And hence it solves our problem.
When to use? We want to have a model with both good precision and recall.
Simply stated the F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low. &amp;gt; # If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
How to Use? You can calculate the F1 score for binary prediction problems using:
from sklearn.metrics import f1_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] f1_score(y_true, y_pred) This is one of my functions which I use to get the best threshold for maximizing F1 score for binary predictions. The below function iterates through possible threshold values to find the one that gives the best F1 score.
# y_pred is an array of predictions def bestThresshold(y_true,y_pred): best_thresh = None best_score = 0 for thresh in np.arange(0.1, 0.501, 0.01): score = f1_score(y_true, np.array(y_pred)&amp;gt;thresh) if score &amp;gt; best_score: best_thresh = thresh best_score = score return best_score , best_thresh Caveats The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.
To solve this, we can do this by creating a weighted F1 metric as below where beta manages the tradeoff between precision and recall.
Here we give β times as much importance to recall as precision.
from sklearn.metrics import fbeta_score y_true = [0, 1, 1, 0, 1, 1] y_pred = [0, 0, 1, 0, 0, 1] fbeta_score(y_true, y_pred,beta=0.5) F1 Score can also be used for Multiclass problems. See this awesome blog post by Boaz Shmueli for details.
3. Log Loss/Binary Crossentropy Log loss is a pretty good evaluation metric for binary classifiers and it is sometimes the optimization objective as well in case of Logistic regression and Neural Networks.
Binary Log loss for an example is given by the below formula where p is the probability of predicting 1.
As you can see the log loss decreases as we are fairly certain in our prediction of 1 and the true label is 1.
When to Use? When the output of a classifier is prediction probabilities. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view of the performance of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.
How to Use? from sklearn.metrics import log_loss # where y_pred are probabilities and y_true are binary class labels log_loss(y_true, y_pred, eps=1e-15) Caveats It is susceptible in case of imbalanced datasets. You might have to introduce class weights to penalize minority errors more or you may use this after balancing your dataset.
4. Categorical Crossentropy The log loss also generalizes to the multiclass problem. The classifier in a multiclass setting must assign a probability to each class for all examples. If there are N samples belonging to M classes, then the Categorical Crossentropy is the summation of -ylogp values:
$y_{ij}$ is 1 if the sample i belongs to class j else 0
$p_{ij}$ is the probability our classifier predicts of sample i belonging to class j.
When to Use? When the output of a classifier is multiclass prediction probabilities. We generally use Categorical Crossentropy in case of Neural Nets. In general, minimizing Categorical cross-entropy gives greater accuracy for the classifier.
How to Use? from sklearn.metrics import log_loss # Where y_pred is a matrix of probabilities with shape ***= (n_samples, n_classes)*** and y_true is an array of class labels log_loss(y_true, y_pred, eps=1e-15) Caveats: It is susceptible in case of imbalanced datasets.
5. AUC AUC is the area under the ROC curve.
AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes
What is the ROC curve?
We have got the probabilities from our classifier. We can use various threshold values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we will have a ROC curve.
Where True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.
Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP&#43;FP)
and False positive rate or FPR is just the proportion of false we are capturing using our algorithm.
1- Specificity = FPR(False Positive Rate)= FP/(TN&#43;FP)
Here we can use the ROC curves to decide on a Threshold value. The choice of threshold value will also depend on how the classifier is intended to be used.
If it is a cancer classification application you don’t want your threshold to be as big as 0.5. Even if a patient has a 0.3 probability of having cancer you would classify him to be 1.
Otherwise, in an application for reducing the limits on the credit card, you don’t want your threshold to be as less as 0.5. You are here a little worried about the negative effect of decreasing limits on customer satisfaction.
When to Use? AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So, for example, if you as a marketer want to find a list of users who will respond to a marketing campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which you will create a list of users to send the marketing campaign.
Another benefit of using AUC is that it is classification-threshold-invariant like log loss. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen, unlike F1 score or accuracy which depend on the choice of threshold.
How to Use? import numpy as np from sklearn.metrics import roc_auc_score y_true = np.array([0, 0, 1, 1]) y_scores = np.array([0.1, 0.4, 0.35, 0.8]) print(roc_auc_score(y_true, y_scores)) Caveats Sometimes we will need well-calibrated probability outputs from our models and AUC doesn’t help with that.
Conclusion An important step while creating our machine learning pipeline is evaluating our different models against each other. A bad choice of an evaluation metric could wreak havoc to your whole system.
So, always be watchful of what you are predicting and how the choice of evaluation metric might affect/alter your final predictions.
Also, the choice of an evaluation metric should be well aligned with the business objective and hence it is a bit subjective. And you can come up with your own evaluation metric as well.
Continue Learning If you want to learn more about how to structure a Machine Learning project and the best practices, I would like to call out his awesome third course named Structuring Machine learning projects in the Coursera Deep Learning Specialization. Do check it out. It talks about the pitfalls and a lot of basic ideas to improve your models.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>4 Graph Algorithms on Steroids for data Scientists with cuGraph</title>
      <link>https://mlwhiz.com/blog/2019/10/20/cugraph/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/10/20/cugraph/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/cugraph/1.jpg"></media:content>
      

      
      <description>We, as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.
We are used to seeing our users in rows with their attributes as columns. But does the real world behave like that?
In a connected world, users cannot be considered as independent entities. They have got certain relationships with each other, and we would sometimes like to include such relationships while building our machine learning models.</description>

      <content:encoded>  
        
        <![CDATA[  We, as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.
We are used to seeing our users in rows with their attributes as columns. But does the real world behave like that?
In a connected world, users cannot be considered as independent entities. They have got certain relationships with each other, and we would sometimes like to include such relationships while building our machine learning models.
Now while in a relational database, we cannot use such relations between different rows(users), in a graph database, it is relatively trivial to do that.
Now, as we know, Python has a great package called Networkx to do this. But the problem with that is that it is not scalable.
A GPU can help solve our scalability problems with its many cores and parallelization. And that is where RAPIDS.ai CuGraph comes in.
 The RAPIDS cuGraph library is a collection of graph analytics that process data found in GPU Dataframes — see cuDF. cuGraph aims to provide a NetworkX-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.
 In this post, I am going to be talking about some of the most essential graph algorithms you should know and how to implement them using Python with cuGraph.
Installation To install cuGraph you can just use the simple command that you can choose from rapids.ai based on your system and configuration.
The command I used is below and I used a nightly build(recommended):
conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge -c anaconda cudf=0.10 cuml=0.10 cugraph=0.10  1. Connected Components We all know how clustering works?
You can think of Connected Components in very layman’s terms as a sort of a hard clustering algorithm which finds clusters/islands in related/connected data.
As a concrete example: Say you have data about roads joining any two cities in the world. And you need to find out all the continents in the world and which city they contain.
How will you achieve that? Come on, give some thought.
The connected components algorithm that we use to do this is based on a special case of BFS/DFS. I won’t talk much about how it works here, but we will see how to get the code up and running using Networkx as well as cuGraph.
Applications From a Retail Perspective: Let us say, we have a lot of customers using a lot of accounts. One way in which we can use the Connected components algorithm is to find out distinct families in our dataset.
We can assume edges(roads) between CustomerIDs based on same credit card usage, or same address or same mobile number, etc. Once we have those connections, we can then run the connected component algorithm on the same to create individual clusters to which we can then assign a family ID.
We can then use these family IDs to provide personalized recommendations based on family needs. We can also use this family ID to fuel our classification algorithms by creating grouped features based on family.
From a Finance Perspective: Another use case would be to capture fraud using these family IDs. If an account has done fraud in the past, it is highly probable that the connected accounts are also susceptible to fraud.
The possibilities are only limited by your imagination.
Code We will be using the Networkx module in Python for creating and analyzing our graphs.
Let us start with an example graph which we are using for our purpose. Contains cities and distance information between them.
We first start by creating a list of edges along with the distances which we will add as the weight of the edge:
edgelist = [[&amp;#39;Mannheim&amp;#39;, &amp;#39;Frankfurt&amp;#39;, 85], [&amp;#39;Mannheim&amp;#39;, &amp;#39;Karlsruhe&amp;#39;, 80], [&amp;#39;Erfurt&amp;#39;, &amp;#39;Wurzburg&amp;#39;, 186], [&amp;#39;Munchen&amp;#39;, &amp;#39;Numberg&amp;#39;, 167], [&amp;#39;Munchen&amp;#39;, &amp;#39;Augsburg&amp;#39;, 84], [&amp;#39;Munchen&amp;#39;, &amp;#39;Kassel&amp;#39;, 502], [&amp;#39;Numberg&amp;#39;, &amp;#39;Stuttgart&amp;#39;, 183], [&amp;#39;Numberg&amp;#39;, &amp;#39;Wurzburg&amp;#39;, 103], [&amp;#39;Numberg&amp;#39;, &amp;#39;Munchen&amp;#39;, 167], [&amp;#39;Stuttgart&amp;#39;, &amp;#39;Numberg&amp;#39;, 183], [&amp;#39;Augsburg&amp;#39;, &amp;#39;Munchen&amp;#39;, 84], [&amp;#39;Augsburg&amp;#39;, &amp;#39;Karlsruhe&amp;#39;, 250], [&amp;#39;Kassel&amp;#39;, &amp;#39;Munchen&amp;#39;, 502], [&amp;#39;Kassel&amp;#39;, &amp;#39;Frankfurt&amp;#39;, 173], [&amp;#39;Frankfurt&amp;#39;, &amp;#39;Mannheim&amp;#39;, 85], [&amp;#39;Frankfurt&amp;#39;, &amp;#39;Wurzburg&amp;#39;, 217], [&amp;#39;Frankfurt&amp;#39;, &amp;#39;Kassel&amp;#39;, 173], [&amp;#39;Wurzburg&amp;#39;, &amp;#39;Numberg&amp;#39;, 103], [&amp;#39;Wurzburg&amp;#39;, &amp;#39;Erfurt&amp;#39;, 186], [&amp;#39;Wurzburg&amp;#39;, &amp;#39;Frankfurt&amp;#39;, 217], [&amp;#39;Karlsruhe&amp;#39;, &amp;#39;Mannheim&amp;#39;, 80], [&amp;#39;Karlsruhe&amp;#39;, &amp;#39;Augsburg&amp;#39;, 250],[&amp;#34;Mumbai&amp;#34;, &amp;#34;Delhi&amp;#34;,400],[&amp;#34;Delhi&amp;#34;, &amp;#34;Kolkata&amp;#34;,500],[&amp;#34;Kolkata&amp;#34;, &amp;#34;Bangalore&amp;#34;,600],[&amp;#34;TX&amp;#34;, &amp;#34;NY&amp;#34;,1200],[&amp;#34;ALB&amp;#34;, &amp;#34;NY&amp;#34;,800]] Now we want to find out distinct continents and their cities from this graph.
First, we will need to create a cudf dataframe with edges in it. Right now, I am creating a pandas dataframe and converting it to cudf dataframe, but in a real-life scenario, we will read from a csv file of edges.
import cugraph import cudf import pandas as pd # create a pandas dataframe of edges pandas_df = pd.DataFrame(edgelist) pandas_df.columns = [&amp;#39;src&amp;#39;,&amp;#39;dst&amp;#39;,&amp;#39;distance&amp;#39;] # create a pandas dataframe of reversed edges as we have a undirected graph rev_pandas_df = pandas_df.copy() rev_pandas_df.columns = [&amp;#39;dst&amp;#39;,&amp;#39;src&amp;#39;,&amp;#39;distance&amp;#39;] rev_pandas_df = rev_pandas_df[[&amp;#39;src&amp;#39;,&amp;#39;dst&amp;#39;,&amp;#39;distance&amp;#39;]] # concat all edges pandas_df = pd.concat([pandas_df,rev_pandas_df]) Now our pandas df contains edges in both directions. And our node names in src and dst columns are in str format. Apparently, cuGraph doesn&amp;rsquo;t like that and only works with integer node IDs.
# CuGraph works with only integer node IDs unique_destinations = set() for [src,dst,dis] in edgelist: unique_destinations.add(src) unique_destinations.add(dst) # create a map of city and a unique id city_id_dict = {} for i, city in enumerate(unique_destinations): city_id_dict[city]=i # create 2 columns that contain the integer IDs for src and dst pandas_df[&amp;#39;src_int&amp;#39;] = pandas_df[&amp;#39;src&amp;#39;].apply(lambda x : city_id_dict[x]) pandas_df[&amp;#39;dst_int&amp;#39;] = pandas_df[&amp;#39;dst&amp;#39;].apply(lambda x : city_id_dict[x]) Now comes the main part that we should focus on:
cuda_g = cudf.DataFrame.from_pandas(pandas_df) # cugraph needs node IDs to be int32 and weights to be float cuda_g[&amp;#39;src_int&amp;#39;] = cuda_g[&amp;#39;src_int&amp;#39;].astype(np.int32) cuda_g[&amp;#39;dst_int&amp;#39;] = cuda_g[&amp;#39;dst_int&amp;#39;].astype(np.int32) cuda_g[&amp;#39;distance&amp;#39;] = cuda_g[&amp;#39;distance&amp;#39;].astype(np.float) G = cugraph.Graph() G.add_edge_list(cuda_g[&amp;#34;src_int&amp;#34;],cuda_g[&amp;#34;dst_int&amp;#34;] , cuda_g[&amp;#39;distance&amp;#39;]) cugraph.weakly_connected_components(G) The output of the last call is a cudf dataframe.
As we can see, the labels correspond to Connected Components ID.
2. Shortest Path Continuing with the above example only, we are given a graph with the cities of Germany and the respective distance between them.
You want to find out how to go from Frankfurt (The starting node) to Munchen by covering the shortest distance.
The algorithm that we use for this problem is called Dijkstra. In Dijkstra’s own words:
 What is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city. It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in ’59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually that algorithm became, to my great amazement, one of the cornerstones of my fame. — Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001[3]
 Applications  Variations of the Dijkstra algorithm is used extensively in Google Maps to find the shortest routes.
 You are in a Walmart Store. You have different Aisles and distance between all the aisles. You want to provide the shortest pathway to the customer from Aisle A to Aisle D.
   You have seen how LinkedIn shows up 1st-degree connections, 2nd-degree connections. What goes on behind the scenes?  Code We already have our Graph as before. We can find the shortest distance from a source node to all nodes in the graph.
# get distances from source node 0 distances = cugraph.sssp(G, 0) # filter infinite distances distances = cugraph.traversal.filter_unreachable(distances) distances Now if we have to find the path between node 0 and 14 we can use the distances cudf.
# Getting the path is as simple as: path = [] dest = 14 while dest != 0: dest = distances[distances[&amp;#39;vertex&amp;#39;] == dest][&amp;#39;predecessor&amp;#39;].values[0] path.append(dest) # reverse the list and print print(path[::-1]) [0, 11, 9]  3. Pagerank This is the page sorting algorithm that powered google for a long time. It assigns scores to pages based on the number and quality of incoming and outgoing links.
Applications Pagerank can be used anywhere where we want to estimate node importance in any network.
 It has been used for finding the most influential papers using citations.
 Has been used by Google to rank pages
 It can be used to rank tweets- User and Tweets as nodes. Create Link between user if user A follows user B and Link between user and Tweets if user tweets/retweets a tweet.
 Recommendation engines
  Code For this exercise, we are going to be using Facebook social network data.
# Loading the file as cudf fb_cudf = cudf.read_csv(&amp;#34;facebook_combined.txt&amp;#34;, sep=&amp;#39; &amp;#39;, names=[&amp;#39;src&amp;#39;, &amp;#39;dst&amp;#39;],dtype =[&amp;#39;int32&amp;#39;,&amp;#39;int32&amp;#39;]) # adding reverse edges also rev_fb_cudf = fb_cudf[[&amp;#39;dst&amp;#39;,&amp;#39;src&amp;#39;]] rev_fb_cudf.columns = [&amp;#39;src&amp;#39;,&amp;#39;dst&amp;#39;] fb_cudf = cudf.concat([fb_cudf,rev_fb_cudf]) Creating the graph
# creating the graph fb_G = cugraph.Graph() fb_G.add_edge_list(fb_cudf[&amp;#34;src&amp;#34;],fb_cudf[&amp;#34;dst&amp;#34;]) Now we want to find the users having high influence capability.
Intuitively, the Pagerank algorithm will give a higher score to a user who has a lot of friends who in turn have a lot of FB Friends.
# Call cugraph.pagerank to get the pagerank scores fb_pagerank = cugraph.pagerank(fb_G) fb_pagerank.sort_values(by=&amp;#39;pagerank&amp;#39;,ascending=False).head() 4. Link Prediction Continuing along with our Facebook example. You might have seen recommended friends in your Facebook account. How can we create our small recommender?
Can we predict which edges will be connected in the future based on current edges?
A straightforward and fast approach to do this is by using the Jaccard Coefficient.
Applications There could be many applications of link predictions. We could predict
 Authors who are going to connect for co-authorships in a citation network
 Who will become friends in a social network?
  Idea We calculate the Jaccard coefficient between two nodes i and j as :
Where the numerator is the number of common neighbors of i and j, and the denominator is the total number of distinct neighbors of i and j.
So in the figure, the half red and green nodes are the common neighbors of both A and B. And they have a total of 5 distinct neighbors. So the JaccardCoeff(A, B) is 2&amp;frasl;5
Code We first create a cudf_nodes cudf with all possible node combinations.
max_vertex_id = fb_pagerank[&amp;#39;vertex&amp;#39;].max() data = [] for x in range(0,max_vertex_id&#43;1): for y in range(0,max_vertex_id&#43;1): data.append([x,y]) cudf_nodes =cudf.from_pandas(pd.DataFrame(data)) cudf_nodes.columns = [&amp;#39;src&amp;#39;,&amp;#39;dst&amp;#39;] cudf_nodes[&amp;#39;src&amp;#39;] = cudf_nodes[&amp;#39;src&amp;#39;].astype(np.int32) cudf_nodes[&amp;#39;dst&amp;#39;] = cudf_nodes[&amp;#39;dst&amp;#39;].astype(np.int32) We can then calculate the Jaccard coefficient between nodes as:
jaccard_coeff_between_nodes = cugraph.link_prediction.jaccard(fb_G,cudf_nodes[&amp;#34;src&amp;#34;],cudf_nodes[&amp;#34;dst&amp;#34;]) jaccard_coeff_between_nodes.head() But we are still not done. We need to remove the edges where the source==destination and the edges which are already present in the graph. We will do this using simple join and filter operations which work particularly similar to pandas.
jaccard_coeff_between_nodes=jaccard_coeff_between_nodes[jaccard_coeff_between_nodes[&amp;#39;source&amp;#39;]!=jaccard_coeff_between_nodes[&amp;#39;destination&amp;#39;]] fb_cudf.columns = [&amp;#39;source&amp;#39;, &amp;#39;destination&amp;#39;] fb_cudf[&amp;#39;edgeflag&amp;#39;]=1 jaccard_coeff_joined_with_edges = jaccard_coeff_between_nodes.merge(fb_cudf,on= [&amp;#39;source&amp;#39;, &amp;#39;destination&amp;#39;],how=&amp;#39;left&amp;#39;) # We just want to see the jaccard coeff of new edges new_edges_jaccard_coeff = jaccard_coeff_joined_with_edges[jaccard_coeff_joined_with_edges[&amp;#39;edgeflag&amp;#39;]!=1] This is our final sorted dataframe with the Jaccard coefficient between unconnected nodes. We know what friends to recommend to our platform users.
new_edges_jaccard_coeff.sort_values(by=&amp;#39;jaccard_coeff&amp;#39;,ascending=False) Basic Network Statistics There are a lot of basic measures which you want to know about your network.
Here is how you get them in your network
print(&amp;#34;Number of Nodes&amp;#34;,fb_G.number_of_nodes()) print(&amp;#34;Number of Edges&amp;#34;,fb_G.number_of_edges()) Number of Nodes 4039 Number of Edges 176468  You can also compute the indegree and outdegree for each node.
In a directed graph this corresponds to no of followers and no of follows.
fb_G.degrees().head() Performance Benchmarks I won’t do any justice to this post if I don’t add certain benchmarks for the different algorithms.
In my benchmark study, I use three datasets in increasing order of scale from the Stanford Large Network Dataset Collection.
 ego-Facebook: Undirected graph with 4 K nodes and 88 K edges from Facebook
 ego-Twitter: Directed graph with 81 K nodes and 1.7 M edges from Twitter
 ego-Gplus: Directed graph with 107 K nodes and 13.6 M edges from Google&#43;
  Here are the results of the experiments I performed on NVIDIA Tesla V100 32 GB GPU. Thanks to Josh Patterson from NVIDIA and Richard Ulrich at Walmart Labs for arranging that for me. All the times are given in milliseconds:
I didn’t add Jaccard coefficients in the results as it didn’t run even for facebook using networkX. For cuGraph it had millisecond-level latencies.
Let us visualize these results:
Caveats Rapids cuGraph is an excellent library for graph analysis, but I feel some things are still missing. Maybe we will get them in the next version.
 A little bit of inconvenience that we have to use numbered nodes with data type int32 only. Renumbering helps with that. See my notebook for the benchmark for the exact code. Check the function cugraph.symmetrize_df too for creating undirected graphs.
 Some algorithms are still not implemented. For instance, I could not find MST, Centrality measures, etc.
 More example notebooks are needed to document best practices. I might be going to be work on some of those.
 No visualization component in the library. I have to go to networkx to plot graphs.
  But despite that, I would also like to add that the idea to provide graph analysis with GPU is so great that I can live with these small problems. And the way they have made the API so similar to pandas and networkx adds to its value.
I remember how using GPU needed a lot of code in the past. RAPIDS has aimed to make GPU ubiquitous, and that is a fabulous initiative.
Conclusion In this post, I talked about some of the most powerful graph algorithms that have changed the way we live and how to scale them with GPUs.
I love the way Rapids AI has been working to make GPUs accessible to the typical developer/data scientist and to think that we hadn’t heard about it till a year back. They have come a long way.
Also, here are the newest version 0.9 documentation for cuDF and cuGraph.
You can get the running code in this Google Colab Notebook, and the code with benchmarks on my Github repository as Google Colab fell short on resources while benchmarking.
Continue Learning If you want to read up more on Graph Algorithms here is a Graph Analytics for Big Data course on Coursera by UCSanDiego, which I highly recommend to learn the basics of graph theory.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
Also, a small disclaimer — There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Automate Hyperparameter Tuning for your models</title>
      <link>https://mlwhiz.com/blog/2019/10/10/hyperopt2/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/10/10/hyperopt2/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/hyperopt2/1.jpg"></media:content>
      

      
      <description>When we create our machine learning models, a common task that falls on us is how to tune them.
People end up taking different manual approaches. Some of them work, and some don’t, and a lot of time is spent in anticipation and running the code again and again.
So that brings us to the quintessential question: Can we automate this process?
A while back, I was working on an in-class competition from the “How to win a data science competition” Coursera course.</description>

      <content:encoded>  
        
        <![CDATA[  When we create our machine learning models, a common task that falls on us is how to tune them.
People end up taking different manual approaches. Some of them work, and some don’t, and a lot of time is spent in anticipation and running the code again and again.
So that brings us to the quintessential question: Can we automate this process?
A while back, I was working on an in-class competition from the “How to win a data science competition” Coursera course. Learned a lot of new things, one among them being Hyperopt — A bayesian Parameter Tuning Framework.
And I was amazed. I left my Mac with hyperopt in the night. And in the morning I had my results. It was awesome, and I did avoid a lot of hit and trial.
This post is about automating hyperparameter tuning because our time is more important than the machine.
So, What is Hyperopt? From the Hyperopt site:
 Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions
 In simple terms, this means that we get an optimizer that could minimize/maximize any function for us. For example, we can use this to minimize the log loss or maximize accuracy.
All of us know how grid search or random-grid search works.
A grid search goes through the parameters one by one, while a random search goes through the parameters randomly.
Hyperopt takes as an input space of hyperparameters in which it will search and moves according to the result of past trials.
 Thus, Hyperopt aims to search the parameter space in an informed way.
 I won’t go in the details. But if you want to know more about how it works, take a look at this paper by J Bergstra. Here is the documentation from Github.
Our Dataset To explain how hyperopt works, I will be working on the heart dataset from UCI precisely because it is a simple dataset. And why not do some good using Data Science apart from just generating profits?
This dataset predicts the presence of a heart disease given some variables.
This is a snapshot of the dataset :
This is how the target distribution looks like:
Hyperopt Step by Step So, while trying to run hyperopt, we will need to create two Python objects:
 An Objective function: The objective function takes the hyperparameter space as the input and returns the loss. Here we call our objective function objective
 A dictionary of hyperparams: We will define a hyperparam space by using the variable space which is actually just a dictionary. We could choose different distributions for different hyperparameter values.
  In the end, we will use the fmin function from the hyperopt package to minimize our objective through the space.
You can follow along with the code in this Kaggle Kernel.
1. Create the objective function Here we create an objective function which takes as input a hyperparameter space:
 We first define a classifier, in this case, XGBoost. Just try to see how we access the parameters from the space. For example space[‘max_depth’]
 We fit the classifier to the train data and then predict on the cross-validation set.
 We calculate the required metric we want to maximize or minimize.
 Since we only minimize using fmin in hyperopt, if we want to minimize logloss we just send our metric as is. If we want to maximize accuracy we will try to minimize -accuracy
  from sklearn.metrics import accuracy_score from hyperopt import hp, fmin, tpe, STATUS_OK, Trials import numpy as np import xgboost as xgb def objective(space): # Instantiate the classifier clf = xgb.XGBClassifier(n_estimators =1000,colsample_bytree=space[&amp;#39;colsample_bytree&amp;#39;], learning_rate = .3, max_depth = int(space[&amp;#39;max_depth&amp;#39;]), min_child_weight = space[&amp;#39;min_child_weight&amp;#39;], subsample = space[&amp;#39;subsample&amp;#39;], gamma = space[&amp;#39;gamma&amp;#39;], reg_lambda = space[&amp;#39;reg_lambda&amp;#39;]) eval_set = [( X, y), ( Xcv, ycv)] # Fit the classsifier clf.fit(X, y, eval_set=eval_set, eval_metric=&amp;#34;rmse&amp;#34;, early_stopping_rounds=10,verbose=False) # Predict on Cross Validation data pred = clf.predict(Xcv) # Calculate our Metric - accuracy accuracy = accuracy_score(ycv, pred&amp;gt;0.5) # return needs to be in this below format. We use negative of accuracy since we want to maximize it. return {&amp;#39;loss&amp;#39;: -accuracy, &amp;#39;status&amp;#39;: STATUS_OK } 2. Create the Space for your classifier Now, we create the search space for hyperparameters for our classifier.
To do this, we end up using many of hyperopt built-in functions which define various distributions.
As you can see in the code below, we use uniform distribution between 0.7 and 1 for our subsample hyperparameter. We also give a label for the subsample parameter x_subsample. You need to provide different labels for each hyperparam you define. I generally add a x_ before my parameter name to create this label.
space ={&amp;#39;max_depth&amp;#39;: hp.quniform(&amp;#34;x_max_depth&amp;#34;, 4, 16, 1), &amp;#39;min_child_weight&amp;#39;: hp.quniform (&amp;#39;x_min_child&amp;#39;, 1, 10, 1), &amp;#39;subsample&amp;#39;: hp.uniform (&amp;#39;x_subsample&amp;#39;, 0.7, 1), &amp;#39;gamma&amp;#39; : hp.uniform (&amp;#39;x_gamma&amp;#39;, 0.1,0.5), &amp;#39;colsample_bytree&amp;#39; : hp.uniform (&amp;#39;x_colsample_bytree&amp;#39;, 0.7,1), &amp;#39;reg_lambda&amp;#39; : hp.uniform (&amp;#39;x_reg_lambda&amp;#39;, 0,1) } You can also define a lot of other distributions too. Some of the most useful stochastic expressions currently recognized by hyperopt’s optimization algorithms are:
 hp.choice(label, options) — Returns one of the options, which should be a list or tuple.
 hp.randint(label, upper) — Returns a random integer in the range [0, upper).
 hp.uniform(label, low, high) — Returns a value uniformly between low and high.
 hp.quniform(label, low, high, q) — Returns a value like round(uniform(low, high) / q) * q
 hp.normal(label, mu, sigma) — Returns a real value that’s normally-distributed with mean mu and standard deviation sigma.
  There are a lot of other distributions. You can check them out here.
3. And finally, Run Hyperopt Once we run this, we get the best parameters for our model. Turns out we achieved an accuracy of 90% by just doing this on the problem.
trials = Trials() best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials) print(best) Now we can retrain our XGboost algorithm with these best params, and we are done.
Conclusion Running the above gives us pretty good hyperparams for our learning algorithm. And that saves me a lot of time to think about various other hypotheses and testing them.
I tend to use this a lot while tuning my models. From my experience, the most crucial part in this whole procedure is setting up the hyperparameter space, and that comes by experience as well as knowledge about the models.
So, Hyperopt is an awesome tool to have in your repository but never neglect to understand what your models does. It will be very helpful in the long run.
You can get the full code in this Kaggle Kernel.
Continue Learning If you want to learn more about practical data science, do take a look at the “How to win a data science competition” Coursera course. Learned a lot of new things from this course taught by one of the most prolific Kaggler.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
Also, a small disclaimer - There might be some affiliate links in this post to relevant resources as sharing knowledge is never a bad idea.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>How did I learn Data Science?</title>
      <link>https://mlwhiz.com/blog/2019/08/12/resources/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/08/12/resources/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/resources/1.png"></media:content>
      

      
      <description>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
But I didn’t like it and so I left that.
I made it my goal to move into the analytics and data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of efforts to shift.
Now, people on social networks ask me how I got started in the data science field.</description>

      <content:encoded>  
        
        <![CDATA[  I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.
But I didn’t like it and so I left that.
I made it my goal to move into the analytics and data science space somewhere around in 2013. From then on, it has taken me a lot of failures and a lot of efforts to shift.
Now, people on social networks ask me how I got started in the data science field. So I thought of giving a definitive answer.
 It is not really impossible to do this but it will take a lot of time and effort. Fortunately, I had an ample supply of both.
 Given below is the way that I took, and any aspiring person could choose to become a self-trained data scientist.
Some of the courses are not the same I did since some of them don’t exist and some have been merged into bigger specializations. But I have tried to keep it as similar to my experience as possible.
Also, I hope that you don’t lose hope after seeing the long list. You have to start with one or two courses. The rest will follow with time. Remember we have ample time.
Follow in order. I have tried to include everything that comes to my mind, including some post links which I think could be beneficial.
Introduction to Probability and Statistics Stat 110: The quintessential Probability and Statistics course you gotta take. All the lectures and notes are available on Youtube and his site for free.
If not for the content then for Prof. Joseph Blitzstein sense of humor. The above picture is a testament to that.
I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that.
Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.
This is a challenging class for a beginner but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving the proofs quickly. Sometimes the same proof was done in different ways to facilitate the learning of a concept.
One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts.
The inclusion of Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox, etc. made this course much much more exciting and enjoyable than any ordinary Statistics Course.
It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions.
He has also got a textbook based on this course, which is an excellent text and a must for any bookshelf.
 
Introduction to Python and Data Science:  Do first, understand later
 We need to get a taste of machine learning before understanding it fully. This segment is made up of three parts. These are not the exact courses I took to learn Python and getting an intro to data science. But they are quite similar and they serve the purpose.
a) Introduction to Data Science in Python This course is about learning to use Python and creating things on your own. You will learn about Python Libraries like Numpy, Pandas for data science.
You might also like my posts on Minimal Pandas for Data Scientists and small shorts on advanced python while going through this course.
Course description from Website:
 This course will introduce the learner to the basics of the python programming environment, including fundamental python programming techniques such as lambdas, reading and manipulating csv files, and the numpy library. The course will introduce data manipulation and cleaning techniques using the popular python pandas data science library and introduce the abstraction of the Series and DataFrame as the central data structures for data analysis, along with tutorials on how to use functions such as groupby, merge, and pivot tables effectively. By the end of this course, students will be able to take tabular data, clean it, manipulate it, and run basic inferential statistical analyses.
 b) Applied Machine Learning in Python This course gives an intro to many modern machine learning methods that you should know about. Not a thorough grinding but you will get the tools to build your own models. You will learn scikit-learn, which is the python library to create all sorts of models.
The focus here is to start creating things as soon as possible. No one likes to wait too long to get something useful, and you will become useful after this course.
 This course will introduce the learner to applied machine learning, focusing more on the techniques and methods than on the statistics behind these methods. The course will start with a discussion of how machine learning is different than descriptive statistics, and introduce the scikit learn toolkit through a tutorial.
 c) Visualizations  A well made visualization is worth more than any PPT
 One thing you also need to learn about is Visualizations. This is an area which is constantly evolving with a lot of new libraries coming frequently. The libraries I use most are Seaborn and Plotly.
You could take a look at the below posts to get started with both basic and advanced visualizations.
Python’s One-Liner graph creation library with animations Hans Rosling Style
3 Awesome Visualization Techniques for every dataset
Machine Learning Fundamentals After doing these above courses, you will gain the status of what I would like to call a “Beginner.”
Congrats!!!. You know stuff; you know how to implement things.
Yet you do not fully understand all the math and grind that goes behind all these models.
You need to understand what goes behind the clf.fit
 If you don’t understand it you won’t be able to improve it
 Here comes the Game Changer Machine Learning course. Contains the maths behind many of the Machine Learning algorithms.
I will put this course as the one course you gotta take as this course motivated me into getting in this field, and Andrew Ng is a great instructor. Also, this was the first course that I took myself when I started.
This course has a little of everything — Regression, Classification, Anomaly Detection, Recommender systems, Neural networks, plus a lot of great advice.
After this one, you are done with the three musketeers of the trade.
You know Python, you understand Statistics, and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D’artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that add a new freshness to our data science journey.
Here comes Big Data for you.
Big Data Analytics Using Spark  Big Data is omnipresent. Deal with it.
 The whole big data ecosystem has changed a lot since the time I learned Hadoop. And Spark was the new kid on the block at that time. Those days…
The courses I took are pretty redundant as of now so I would try to recommend something suitable for this era. The best course I could find that embodies most of what I learned through scattered sources is Big Data Analytics Using Spark.
From the course website, after doing this course, you will learn:
 Programming Spark using Pyspark Identifying the computational tradeoffs in a Spark application Performing data loading and cleaning using Spark and Parquet Modeling data through statistical and machine learning methods  You could also take a look at my recent post on Spark.
The Hitchhikers guide to handle Big Data using Spark
Understand Linux Shell Not a hard requirement but a good to have skill. Shell is a big friend of data scientists. It allows you to do simple data-related tasks in the terminal itself. I couldn’t emphasize how much time shell saves for me every day.
You can read the below post by me to know about this: Impress Onlookers with your newly acquired Shell Skills
If you would like to take a course, you can look at The UNIX workbench course on Coursera.
Congrats you are a “Hacker” now.
 You have got all the main tools in your belt to be a data scientist.
 On to more advanced topics. From here, it depends on you what you want to learn.
You may want to take a totally different approach than what I took going from here. There is no particular order. “All Roads Lead to Rome” as long as you are moving.
Learn Statistical Inference Mine Çetinkaya-Rundel teaches this course on Inferential Statistics. And it cannot get simpler than this one.
She is a great instructor and explains the fundamentals of Statistical inference nicely — a must-take course.
You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
Deep Learning  It is all about layers
 Intro — Making neural nets uncool again. This is a code-first class for neural nets. An excellent Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.
Advanced — You can try out this Deep Learning Specialization by Andrew Ng again. Pure Gold.
Advanced Math Book — A math-intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow
Take a look at below post if you want to learn Pytorch.
Moving from Keras to Pytorch
Learn NLP, Use Deep Learning with Text and create Chatbots  Reading is overrated. Let the machine do it.
 Natural Language Processing is something which captured my attention a while back.
I wrote a series of 6 posts on it. If you want, you can take a look.
NLP Learning Series — Towards Data Science
Algorithms, Graph Algorithms, and More  Algorithms. Yes, you need them.
 Apart from that if you want to learn about Python and the underlying intricacies of the language you can take the Computer Science Mini Specialization from RICE university too.
This is a series of 6 short but good courses.
I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn to program is by doing it.
The lectures are good, but the problems and assignments are awesome. If you work on this, you will learn Object-Oriented Programming, Graph algorithms, and creating games in Python. Pretty cool stuff.
You could also take a look at:
The 5 Feature Selection Algorithms every Data Scientist should know
The 5 Sampling Algorithms every Data Scientist need to know
Some Advanced Math Topics  Math — The power behind it all
 I am writing it last here but don’t underestimate the importance of Math in Data Science. You might want to look a little into these courses if you want to refresh your concepts.
Linear Algebra By Gilbert Strang— A Great Class by a great Teacher. I would definitely recommend this class to anyone who wants to learn Linear Algebra.
Multivariate Calculus — MIT Open Courseware
Convex Optimization — a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.
Conclusion The Machine learning field is evolving, and new advancements are made every day. That’s why I didn’t put the third tier.
 The maximum I can call myself is a “Hacker,” and my learning continues.
 Everyone has their own path, and here I provided mine to become a data scientist. And this is in no way perfect as obviously, a lot of things can be added to it.
Though I did not complete any professional training, I consider myself more of a Computer science engineer than a mechanical engineer now due to the above courses.
I hope they help you too.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Feature Selection Algorithms every Data Scientist should know</title>
      <link>https://mlwhiz.com/blog/2019/08/07/feature_selection/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/08/07/feature_selection/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/fs/1.png"></media:content>
      

      
      <description>Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.</description>

      <content:encoded>  
        
        <![CDATA[  Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
How many times it has happened when you create a lot of features and then you need to come up with ways to reduce the number of features.
We sometimes end up using correlation or tree-based methods to find out the important features.
Can we add some structure to it?
This post is about some of the most common feature selection techniques one can use while working with data.
Why Feature Selection? Before we proceed, we need to answer this question. Why don’t we give all the features to the ML algorithm and let it decide which feature is important?
So there are three reasons why we don’t:
1. Curse of dimensionality — Overfitting If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing.
2. Occam’s Razor: We want our models to be simple and explainable. We lose explainability when we have a lot of features.
3. Garbage In Garbage out: Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.
Also, a large number of features make a model bulky, time-taking, and harder to implement in production.
So What do we do? We select only useful features.
Fortunately, Scikit-learn has made it pretty much easy for us to make the feature selection. There are a lot of ways in which we can think of feature selection, but most feature selection methods can be divided into three major buckets
 Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.
 Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination
 Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.
  So enough of theory let us start with our five feature selection methods.
We will try to do this using a dataset to understand it better.
I am going to be using a football player dataset to find out what makes a good player great?
Don’t worry if you don’t understand football terminologies. I will try to keep it at a minimum.
Here is the Kaggle Kernel with the code to try out yourself.
Some Simple Data Preprocessing We have done some basic preprocessing such as removing Nulls and one hot encoding. And converting the problem to a classification problem using:
y = traindf[&#39;Overall&#39;]&amp;gt;=87  Here we use High Overall as a proxy for a great player.
Our dataset(X) looks like below and has 223 columns.
1. Pearson Correlation This is a filter-based method.
We check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion.
def cor_selector(X, y,num_feats): cor_list = [] feature_name = X.columns.tolist() # calculate the correlation with y for each feature for i in X.columns.tolist(): cor = np.corrcoef(X[i], y)[0, 1] cor_list.append(cor) # replace NaN with 0 cor_list = [0 if np.isnan(i) else i for i in cor_list] # feature name cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist() # feature selection? 0 for not select, 1 for select cor_support = [True if i in cor_feature else False for i in feature_name] return cor_support, cor_feature cor_support, cor_feature = cor_selector(X, y,num_feats) print(str(len(cor_feature)), &amp;#39;selected features&amp;#39;) 2. Chi-Squared This is another filter-based method.
In this method, we calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values.
Let us create a small example of how we calculate the chi-squared statistic for a sample.
So let’s say we have 75 Right-Forwards in our dataset and 25 Non-Right-Forwards. We observe that 40 of the Right-Forwards are good, and 35 are not good. Does this signify that the player being right forward affects the overall performance?
We calculate the chi-squared value:
To do this, we first find out the values we would expect to be falling in each bucket if there was indeed independence between the two categorical variables.
This is simple. We multiply the row sum and the column sum for each cell and divide it by total observations.
so Good and NotRightforward Bucket Expected value= 25(Row Sum)*60(Column Sum)/100(Total Observations)
Why is this expected? Since there are 25% notRightforwards in the data, we would expect 25% of the 60 good players we observed in that cell. Thus 15 players.
Then we could just use the below formula to sum over all the 4 cells:
I won’t show it here, but the chi-squared statistic also works in a hand-wavy way with non-negative numerical and categorical features.
We can get chi-squared features from our dataset as:
from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 from sklearn.preprocessing import MinMaxScaler X_norm = MinMaxScaler().fit_transform(X) chi_selector = SelectKBest(chi2, k=num_feats) chi_selector.fit(X_norm, y) chi_support = chi_selector.get_support() chi_feature = X.loc[:,chi_support].columns.tolist() print(str(len(chi_feature)), &amp;#39;selected features&amp;#39;) 3. Recursive Feature Elimination This is a wrapper based method. As I said before, wrapper methods consider the selection of a set of features as a search problem.
From sklearn Documentation:
 The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.
 As you would have guessed, we could use any estimator with the method. In this case, we use LogisticRegression, and the RFE observes the coef_ attribute of the LogisticRegression object
from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5) rfe_selector.fit(X_norm, y) rfe_support = rfe_selector.get_support() rfe_feature = X.loc[:,rfe_support].columns.tolist() print(str(len(rfe_feature)), &amp;#39;selected features&amp;#39;) 4. Lasso: SelectFromModel This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.
For example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero.
Here we use Lasso to select variables.
from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=&amp;#34;l1&amp;#34;), max_features=num_feats) embeded_lr_selector.fit(X_norm, y) embeded_lr_support = embeded_lr_selector.get_support() embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist() print(str(len(embeded_lr_feature)), &amp;#39;selected features&amp;#39;) 5. Tree-based: SelectFromModel This is an Embedded method. As said before, Embedded methods use algorithms that have built-in feature selection methods.
We can also use RandomForest to select features based on feature importance.
We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance.
from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats) embeded_rf_selector.fit(X, y) embeded_rf_support = embeded_rf_selector.get_support() embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist() print(str(len(embeded_rf_feature)), &amp;#39;selected features&amp;#39;) We could also have used a LightGBM. Or an XGBoost object as long it has a feature_importances_ attribute.
from sklearn.feature_selection import SelectFromModel from lightgbm import LGBMClassifier lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40) embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats) embeded_lgb_selector.fit(X, y) embeded_lgb_support = embeded_lgb_selector.get_support() embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist() print(str(len(embeded_lgb_feature)), &amp;#39;selected features&amp;#39;) Bonus Why use one, when we can have all?
The answer is sometimes it won’t be possible with a lot of data and time crunch.
But whenever possible, why not do this?
# put all selection together feature_selection_df = pd.DataFrame({&amp;#39;Feature&amp;#39;:feature_name, &amp;#39;Pearson&amp;#39;:cor_support, &amp;#39;Chi-2&amp;#39;:chi_support, &amp;#39;RFE&amp;#39;:rfe_support, &amp;#39;Logistics&amp;#39;:embeded_lr_support, &amp;#39;Random Forest&amp;#39;:embeded_rf_support, &amp;#39;LightGBM&amp;#39;:embeded_lgb_support}) # count the selected times for each feature feature_selection_df[&amp;#39;Total&amp;#39;] = np.sum(feature_selection_df, axis=1) # display the top 100 feature_selection_df = feature_selection_df.sort_values([&amp;#39;Total&amp;#39;,&amp;#39;Feature&amp;#39;] , ascending=False) feature_selection_df.index = range(1, len(feature_selection_df)&#43;1) We check if we get a feature based on all the methods. In this case, as we can see Reactions and LongPassing are excellent attributes to have in a high rated player. And as expected Ballcontrol and Finishing occupy the top spot too.
Conclusion Feature engineering and feature selection are critical parts of any machine learning pipeline.
We strive for accuracy in our models, and one cannot get to a good accuracy without revisiting these pieces again and again.
In this article, I tried to explain some of the most used feature selection techniques as well as my workflow when it comes to feature selection.
I also tried to provide some intuition into these methods, but you should probably try to see more into it and try to incorporate these methods into your work.
Do read my post on feature engineering too if you are interested.
If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The 5 Sampling Algorithms every Data Scientist need to know</title>
      <link>https://mlwhiz.com/blog/2019/07/30/sampling/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/30/sampling/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/sampling/1.jpg"></media:content>
      

      
      <description>Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.</description>

      <content:encoded>  
        
        <![CDATA[  Data Science is the study of algorithms.
I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.
This post is about some of the most common sampling techniques one can use while working with data.
Simple Random Sampling Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.
Below we select 100 sample points from a dataset.
sample_df = df.sample(100) Stratified Sampling Assume that we need to estimate the average number of votes for each candidate in an election. Assume that the country has 3 towns:
Town A has 1 million factory workers,
Town B has 2 million workers, and
Town C has 3 million retirees.
We can choose to get a random sample of size 60 over the entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation.
Instead, if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of the sample.
You can do something like this pretty easily with Python:
from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25) Reservoir Sampling I love this problem statement:
Say you have a stream of items of large and unknown length that we can only iterate over once.
Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.
How can we do that?
Let us assume we have to sample 5 objects out of an infinite stream such that each element has an equal probability of getting selected.
import random def generator(max): number = 1 while number &amp;lt; max: number &#43;= 1 yield number # Create as stream generator stream = generator(10000) # Doing Reservoir Sampling from the stream k=5 reservoir = [] for i, element in enumerate(stream): if i&#43;1&amp;lt;= k: reservoir.append(element) else: probability = k/(i&#43;1) if random.random() &amp;lt; probability: # Select item in stream and remove one of the k items already selected reservoir[random.choice(range(0,k))] = element print(reservoir) [1369, 4108, 9986, 828, 5589]  It can be mathematically proved that in the sample each element has the same probability of getting selected from the stream.
How?
It always helps to think of a smaller problem when it comes to mathematics.
So, let us think of a stream of only 3 items and we have to keep 2 of them.
We see the first item, we hold it in the list as our reservoir has space. We see the second item, we hold it in the list as our reservoir has space.
We see the third item. Here is where things get interesting. We choose the third item to be in the list with probability 2&amp;frasl;3.
Let us now see the probability of first item getting selected:
The probability of removing the first item is the probability of element 3 getting selected multiplied by the probability of Element 1 getting randomly chosen as the replacement candidate from the 2 elements in the reservoir. That probability is:
2&amp;frasl;3*1&amp;frasl;2 = 1&amp;frasl;3
Thus the probability of 1 getting selected is:
1–1/3 = 2&amp;frasl;3
We can have the exact same argument for the Second Element and we can extend it for many elements.
Thus each item has the same probability of getting selected: 2&amp;frasl;3 or in general k/n
Random Undersampling and Oversampling It is too often that we encounter an imbalanced dataset.
A widely adopted technique for dealing with highly imbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling).
Let us first create some example imbalanced data.
from sklearn.datasets import make_classification X, y = make_classification( n_classes=2, class_sep=1.5, weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=100, random_state=10 ) X = pd.DataFrame(X) X[&amp;#39;target&amp;#39;] = y We can now do random oversampling and undersampling using:
num_0 = len(X[X[&amp;#39;target&amp;#39;]==0]) num_1 = len(X[X[&amp;#39;target&amp;#39;]==1]) print(num_0,num_1) # random undersample undersampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0].sample(num_1) , X[X[&amp;#39;target&amp;#39;]==1] ]) print(len(undersampled_data)) # random oversample oversampled_data = pd.concat([ X[X[&amp;#39;target&amp;#39;]==0] , X[X[&amp;#39;target&amp;#39;]==1].sample(num_0, replace=True) ]) print(len(oversampled_data)) OUTPUT: 90 10 20 180  Undersampling and Oversampling using imbalanced-learn imbalanced-learn(imblearn) is a Python Package to tackle the curse of imbalanced datasets.
It provides a variety of methods to undersample and oversample.
a. Undersampling using Tomek Links: One of such methods it provides is called Tomek Links. Tomek links are pairs of examples of opposite classes in close vicinity.
In this algorithm, we end up removing the majority element from the Tomek link which provides a better decision boundary for a classifier.
from imblearn.under_sampling import TomekLinks tl = TomekLinks(return_indices=True, ratio=&amp;#39;majority&amp;#39;) X_tl, y_tl, id_tl = tl.fit_sample(X, y) b. Oversampling using SMOTE: In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.
from imblearn.over_sampling import SMOTE smote = SMOTE(ratio=&amp;#39;minority&amp;#39;) X_sm, y_sm = smote.fit_sample(X, y) There are a variety of other methods in the imblearn package for both undersampling(Cluster Centroids, NearMiss, etc.) and oversampling(ADASYN and bSMOTE) that you can check out.
Conclusion Algorithms are the lifeblood of data science.
Sampling is an important topic in data science and we really don’t talk about it as much as we should.
A good sampling strategy sometimes could pull the whole project forward. A bad sampling strategy could give us incorrect results. So one should be careful while selecting a sampling strategy.
So use sampling, be it at work or at bars.
If you want to learn more about Data Science, I would like to call out this excellent course by Andrew Ng. This was the one that got me started. Do check it out.
Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Bayesian Bandits explained simply</title>
      <link>https://mlwhiz.com/blog/2019/07/21/bandits/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/07/21/bandits/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.com/images/bandits/1.png"></media:content>
      

      
      <description>Exploration and Exploitation play a key role in any business.
And any good business will try to “explore” various opportunities where it can make a profit.
Any good business at the same time also tries to focus on a particular opportunity it has found already and tries to “exploits” it.
Let me explain this further with a thought experiment.
Thought Experiment: Assume that we have infinite slot machines. Every slot machine has some win probability.</description>

      <content:encoded>  
        
        <![CDATA[  Exploration and Exploitation play a key role in any business.
And any good business will try to “explore” various opportunities where it can make a profit.
Any good business at the same time also tries to focus on a particular opportunity it has found already and tries to “exploits” it.
Let me explain this further with a thought experiment.
Thought Experiment: Assume that we have infinite slot machines. Every slot machine has some win probability. But we don’t know these probability values.
You have to operate these slot machines one by one. How do you come up with a strategy to maximize your outcome from these slot machines in minimum time.
You will most probably start by trying out some machines.
Would you stick to a particular machine that has an okayish probability(exploitation) or would you keep searching for better machines(exploration)?
It is the exploration-exploitation tradeoff.
And the question is how do we balance this tradeoff such that we get maximum profits?
The answer is Bayesian Bandits.
Why? Business Use-cases: There are a lot of places where such a thought experiment could fit.
 AB Testing: You have a variety of assets that you can show at the website. Each asset has a particular probability of success(getting clicked by the user).
 Ad Clicks: You have a variety of ads that you can show to the user. Each advert has a particular probability of clickthrough
 Finance: which stock pick gives the highest return.
 We as human beings are faced with the exact same problem — Explore or exploit and we handle it quite brilliantly mostly. Should we go find a new job or should we earn money doing the thing we know would give us money?
  In this post, we will focus on AB Testing but this experiment will work for any of the above problems.
Problem Statement: The problem we have is that we have different assets which we want to show on our awesome website but we really don’t know which one to show.
One asset is blue(B), another red&amp;reg; and the third one green(G).
Our UX team say they like the blue one. But you like the green one.
Which one to show on our website?
Bayes Everywhere: Before we delve down into the algorithm we will use to solve this, let us revisit the Bayes theorem.
Just remember that Bayes theorem says that Posterior ~ likelihood*Prior
Beta Distribution The beta distribution is a continuous probability distribution defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by α and β.The PDF of the beta distribution is:
And, the pdf looks like below for different values of α and β:
The beta distribution is frequently applied to model the behavior of probabilities as it lies in the range [0,1]. The beta distribution is a suitable model for the random behavior of percentages and proportions too.
Bayesian Bandits So after knowing the above concepts let us come back to our present problem.
We have three assets.
For the sake of this problem, let&amp;rsquo;s assume that we know the click probabilities of these assets also.
The win probability of blue is 0.3, red is 0.8 and green is 0.4. Please note that in real life, we won’t know this.
These probabilities are going to be hidden from our algorithm and we will see how our algorithm will still converge to these real probabilities.
So, what are our priors(beliefs) about the probability of each of these assets?
Since we have not observed any data we cannot have prior beliefs about any of our three assets.
We need to model our prior probabilities and we will use beta distribution to do that. See the curve for beta distribution above for α = 1 and β=1.
It is actually just a uniform distribution over the range [0,1]. And that is what we want for our prior probabilities for our assets. We don’t have any information yet so we start with a uniform probability distribution over our probability values.
So we can denote the prior probabilities of each of our asset using a beta distribution.
Strategy:  We will sample a random variable from each of the 3 distributions for assets.
 We will find out which random variable is maximum and will show the one asset which gave the maximum random variable.
 We will get to know if that asset is clicked or not.
 We will update the prior for the asset using the information in step 3.
 Repeat.
  Updating the Prior: The reason we took beta distribution to model our probabilities is because of its great mathematical properties.
If the prior is f(α,β), then the posterior distribution is again beta, given by f(α&#43;#success, β&#43;#failures)
where #success is the number of clicks and #failures are the number of views minus the number of clicks.
Let us Code We have every bit of knowledge we require for writing some code now. I will be using pretty much simple and standard Python functionality to do this but there exist tools like pyMC and such for this sort of problem formulations.
Let us work through this problem step by step.
We have three assets with different probabilities.
real_probs_dict = {&amp;#39;R&amp;#39;:0.8,&amp;#39;G&amp;#39;:0.4,&amp;#39;B&amp;#39;:0.3} assets = [&amp;#39;R&amp;#39;,&amp;#39;G&amp;#39;,&amp;#39;B&amp;#39;] We will be trying to see if our strategy given above works or not.
&amp;#39;&amp;#39;&amp;#39; This function takes as input three tuples for alpha,beta that specify priorR,priorG,priorB And returns R,G,B along with the maximum value sampled from these three distributions. We can sample from a beta distribution using scipy. &amp;#39;&amp;#39;&amp;#39; def find_asset(priorR,priorG,priorB): red_rv = scipy.stats.beta.rvs(priorR[0],priorR[1]) green_rv = scipy.stats.beta.rvs(priorG[0],priorG[1]) blue_rv = scipy.stats.beta.rvs(priorB[0],priorB[1]) return assets[np.argmax([red_rv,green_rv,blue_rv])] &amp;#39;&amp;#39;&amp;#39; This is a helper function that simulates the real world using the actual probability value of the assets. In real life we won&amp;#39;t have this function and our user click input will be the proxy for this function. &amp;#39;&amp;#39;&amp;#39; def simulate_real_website(asset, real_probs_dict): #simulate a coin toss with probability. Asset clicked or not. if real_probs_dict[asset]&amp;gt; scipy.stats.uniform.rvs(0,1): return 1 else: return 0 &amp;#39;&amp;#39;&amp;#39; This function takes as input the selected asset and returns the posteriors for the selected asset. &amp;#39;&amp;#39;&amp;#39; def update_posterior(asset,priorR,priorG,priorB,outcome): if asset==&amp;#39;R&amp;#39;: priorR=(priorR[0]&#43;outcome,priorR[1]&#43;1-outcome) elif asset==&amp;#39;G&amp;#39;: priorG=(priorG[0]&#43;outcome,priorG[1]&#43;1-outcome) elif asset==&amp;#39;B&amp;#39;: priorB=(priorB[0]&#43;outcome,priorB[1]&#43;1-outcome) return priorR,priorG,priorB &amp;#39;&amp;#39;&amp;#39; This function runs the strategy once. &amp;#39;&amp;#39;&amp;#39; def run_strategy_once(priorR,priorG,priorB): # 1. get the asset asset = find_asset(priorR,priorG,priorB) # 2. get the outcome from the website/users outcome = simulate_real_website(asset, real_probs_dict) # 3. update prior based on outcome priorR,priorG,priorB = update_posterior(asset,priorR,priorG,priorB,outcome) return asset,priorR,priorG,priorB Let us run this strategy multiple times and collect the data.
priorR,priorG,priorB = (1,1),(1,1),(1,1) data = [(&amp;#34;_&amp;#34;,priorR,priorG,priorB)] for i in range(50): asset,priorR,priorG,priorB = run_strategy_once(priorR,priorG,priorB) data.append((asset,priorR,priorG,priorB)) This is the result of our runs. You can see the functions I used to visualize the posterior distributions here at kaggle. As you can see below, we have pretty much converged to the best asset by the end of 20 runs. And the probabilities are also estimated roughly what they should be.
At the start, we have a uniform prior. As we go through with the runs we see that the “red” asset’s posterior distribution converges towards a higher mean and as such a higher probability of pick. But remember that doesn’t mean that the green asset and blue asset are not going to be picked ever.
Let us also see how many times each asset is picked in the 50 runs we did.
Pick 1 : G ,Pick 2 : R ,Pick 3 : R ,Pick 4 : B ,Pick 5 : R ,Pick 6 : R ,Pick 7 : R ,Pick 8 : R ,Pick 9 : R ,Pick 10 : R ,Pick 11 : R ,Pick 12 : R ,Pick 13 : R ,Pick 14 : R ,Pick 15 : R ,Pick 16 : R ,Pick 17 : R ,Pick 18 : R ,Pick 19 : R ,Pick 20 : R ,Pick 21 : R ,Pick 22 : G ,Pick 23 : R ,Pick 24 : R ,Pick 25 : G ,Pick 26 : G ,Pick 27 : R ,Pick 28 : R ,Pick 29 : R ,Pick 30 : R ,Pick 31 : R ,Pick 32 : R ,Pick 33 : R ,Pick 34 : R ,Pick 35 : R ,Pick 36 : R ,Pick 37 : R ,Pick 38 : R ,Pick 39 : G ,Pick 40 : B ,Pick 41 : R ,Pick 42 : R ,Pick 43 : R ,Pick 44 : R ,Pick 45 : B ,Pick 46 : R ,Pick 47 : R ,Pick 48 : R ,Pick 49 : R ,Pick 50 : R  We can see that although we are mostly picking R we still end up sometimes picking B(Pick 45) and G(Pick 44) in the later runs too. Overall we can see that in the first few runs, we are focussing on exploration and as we go towards later runs we focus on exploitation.
End Notes: We saw how solving this problem using the Bayesian approach could help us converge to a good solution while maximizing our profit and not discarding any asset.
An added advantage of such an approach is that it is self-learning and could self-correct by itself if the probability of click on the red decreases and blue asset increases. This case might happen when the user preferences change for example.
The whole code is posted in the Kaggle Kernel.
Also, if you want to learn more about Bayesian Statistics, one of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization
I am going to be writing more of such posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. As always, I welcome feedback and constructive criticism and can be reached on Twitter @mlwhiz.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>The story of every distribution - Discrete Distributions</title>
      <link>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://mlwhiz.comimages/output_14_0.png"></media:content>
      

      
      <description>Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track. One of the most helpful way to learn more about these is the STAT110 course by Joe Blitzstein and his book.</description>

      <content:encoded>  
        
        <![CDATA[  Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track. One of the most helpful way to learn more about these is the STAT110 course by Joe Blitzstein and his book. You can check out this Coursera course too. Hope it could be useful to someone else too. So here goes:
1. Bernoulli Distribution: Perhaps the most simple discrete distribution of all.
Story: A Coin is tossed with probability p of heads.
PMF of Bernoulli Distribution is given by:
$$P(X=k) = \begin{cases}1-p &amp; k = 0\\p &amp; k = 1\end{cases}$$ CDF of Bernoulli Distribution is given by:
$$P(X \leq k) = \begin{cases}0 &amp; k \lt 0\\1-p &amp; 0 \leq k \lt 1 \\1 &amp; k \geq 1\end{cases}$$ Expected Value:
$$E[X] = \sum kP(X=k)$$ $$E[X] = 0*P(X=0)&#43;1*P(X=1) = p$$
Variance:
$$Var[X] = E[X^2] - E[X]^2$$ Now we find, $$E[X]^2 = p^2$$ and $$E[X^2] = \sum k^2P(X=k)$$ $$E[X^2] = 0^2P(X=0) &#43; 1^2P(X=1) = p $$ Thus, $$Var[X] = p(1-p)$$
2. Binomial Distribution:   One of the most basic distribution in the Statistician toolkit. The parameters of this distribution is n(number of trials) and p(probability of success).
Story: Probability of getting exactly k successes in n trials
PMF of binomial Distribution is given by:
$$P(X=k) = \left(\begin{array}{c}n\ k\end{array}\right) p^{k}(1-p)^{n-k}$$
CDF of binomial Distribution is given by:
$$ P(X\leq k) = \sum_{i=0}^k \left(\begin{array}{c}n\ i\end{array}\right) p^i(1-p)^{n-i} $$
Expected Value:
$$E[X] = \sum kP(X=k)$$ $$E[X] = \sum_{k=0}^n k \left(\begin{array}{c}n\ k\end{array}\right) * p^{k}(1-p)^{n-k} = np $$
A better way to solve this:
$$ X = I_{1} &#43; I_{2} &#43; ....&#43; I_{n-1}&#43; I_{n} $$ X is the sum on n Indicator Bernoulli random variables.
Thus,
 $$E[X] = E[I_{1} &#43; I_{2} &#43; ....&#43; I_{n-1}&#43; I_{n}]$$ $$E[X] = E[I_{1}] &#43; E[I_{2}] &#43; ....&#43; E[I_{n-1}]&#43; E[I_{n}]$$ $$E[X] = \underbrace{p &#43; p &#43; ....&#43; p &#43; p}_{n} = np$$ Variance:
$$ X = I_{1} &#43; I_{2} &#43; ....&#43; I_{n-1}&#43; I_{n} $$ X is the sum on n Indicator Bernoulli random variables. $$Var[X] = Var[I_{1} &#43; I_{2} &#43; ....&#43; I_{n-1}&#43; I_{n}]$$ $$Var[X] = Var[I_{1}] &#43; Var[I_{2}] &#43; ....&#43; Var[I_{n-1}]&#43; Var[I_{n}]$$ $$Var[X] = \underbrace{p(1-p) &#43; p(1-p) &#43; ....&#43; p(1-p) &#43; p(1-p)}_{n} = np(1-p)$$ 3. Geometric Distribution: The parameters of this distribution is p(probability of success).
Story: The number of failures before the first success(Heads) when a coin with probability p is tossed
PMF of Geometric Distribution is given by:
$$P(X=k) = (1-p)^kp$$
CDF of Geometric Distribution is given by:
$$ P(X\leq k) = \sum_{i=0}^k (1-p)^{i}p$$ $$ P(X\leq k) = p(1&#43;q&#43;q^2&amp;hellip;&#43;q^k)= p(1-q^k)/(1-q) = 1-(1-p)^k $$
Expected Value:
$$E[X] = \sum kP(X=k)$$ $$E[X] = \sum_{k=0}^{inf} k (1-p)^kp$$ $$E[X] = qp &#43;2q^2p &#43;3q^3p &#43;4q^4p &amp;hellip;. $$ $$E[X] = qp(1&#43;2q&#43;3q^2&#43;4q^3&#43;&amp;hellip;.)$$ $$E[X] = qp/(1-q)^2 = q/p $$
Variance:
$$Var[X] = E[X^2] - E[X]^2$$ Now we find, $$E[X]^2 = q^2/p^2$$ and $$E[X^2] = \sum_{k=0}^{inf} k^2q^kp= qp &#43; 4q^2p &#43; 9q^3p &#43;16q^4p &amp;hellip; = qp(1&#43;4q&#43;9q^2&#43;16q^3&amp;hellip;.)$$ $$E[X^2] = qp^{-2}(1&#43;q)$$
Thus, $$Var[X] =q/p^2$$
Check Math appendix at bottom of this post for Geometric Series Proofs.
Example:
Q. A doctor is seeking an anti-depressant for a newly diagnosed patient. Suppose that, of the available anti-depressant drugs, the probability that any particular drug will be effective for a particular patient is p=0.6. What is the probability that the first drug found to be effective for this patient is the first drug tried, the second drug tried, and so on? What is the expected number of drugs that will be tried to find one that is effective?
A. Expected number of drugs that will be tried to find one that is effective = q/p = .4/.6 =.67
4. Negative Binomial Distribution: The parameters of this distribution is p(probability of success) and r(number of success).
Story: The number of failures of independent Bernoulli(p) trials before the rth success.
PMF of Negative Binomial Distribution is given by:
r successes , k failures , last attempt needs to be a success: $$P(X=k) = \left(\begin{array}{c}k&#43;r-1\ k\end{array}\right) p^r(1-p)^k$$
Expected Value:
The negative binomial RV could be stated as the sum of r Geometric RVs $$X = X^1&#43;X^2&amp;hellip;. X^{r-1} &#43;X^r$$ Thus, $$E[X] = E[X^1]&#43;E[X^2]&amp;hellip;. E[X^{r-1}] &#43;E[X^r]$$
$$E[X] = rq/p$$
Variance:
The negative binomial RV could be stated as the sum of r independent Geometric RVs $$X = X^1&#43;X^2&amp;hellip;. X^{r-1} &#43;X^r$$ Thus, $$Var[X] = Var[X^1]&#43;Var[X^2]&amp;hellip;. Var[X^{r-1}] &#43;Var[X^r]$$
$$Var[X] = rq/p^2$$
Example:
Q. Pat is required to sell candy bars to raise money for the 6th grade field trip. There are thirty houses in the neighborhood, and Pat is not supposed to return home until five candy bars have been sold. So the child goes door to door, selling candy bars. At each house, there is a 0.4 probability of selling one candy bar and a 0.6 probability of selling nothing. What&amp;rsquo;s the probability of selling the last candy bar at the nth house?
A. r = 5 ; k = n - r
Probability of selling the last candy bar at the nth house = $$P(X=k) = \left(\begin{array}{c}k&#43;r-1\ k\end{array}\right) p^r(1-p)^k$$ $$P(X=k) = \left(\begin{array}{c}n-1\ n-5\end{array}\right) .4^5(.6)^{n-5}$$
5. Poisson Distribution: The parameters of this distribution is $\lambda$ the rate parameter.
Motivation: There is as such no story to this distribution but only motivation for using this distribution. The Poisson distribution is often used for applications where we count the successes of a large number of trials where the per-trial success rate is small. For example, the Poisson distribution is a good starting point for counting the number of people who email you over the course of an hour.The number of chocolate chips in a chocolate chip cookie is another good candidate for a Poisson distribution, or the number of earthquakes in a year in some particular region
PMF of Poisson Distribution is given by: $$ P(X=k) = \frac{e^{-\lambda}\lambda^k} {k!}$$
Expected Value:
$$E[X] = \sum kP(X=k)$$ $$ E[X] = \sum_{k=0}^{inf} k \frac{e^{-\lambda}\lambda^k} {k!}$$ $$ E[X] = \lambda e^{-\lambda}\sum_{k=0}^{inf} \frac{\lambda^{k-1}} {(k-1)!}$$ $$ E[X] = \lambda e^{-\lambda} e^{\lambda} = \lambda $$ Variance:
$$Var[X] = E[X^2] - E[X]^2$$ Now we find, $$E[X^2] = \lambda &#43; \lambda^2$$ Thus, $$Var[X] = \lambda$$
Example:
Q. If electricity power failures occur according to a Poisson distribution with an average of 3 failures every twenty weeks, calculate the probability that there will not be more than one failure during a particular week?
A. Probability = P(X=0)&#43;P(X=1) =
$$e^{-3/20} &#43; e^{-3/20}3/20 = 23/20*e^{-3/20} $$ Probability of selling the last candy bar at the nth house = $$P(X=k) = \left(\begin{array}{c}k&#43;r-1\ k\end{array}\right) p^r(1-p)^k$$ $$P(X=k) = \left(\begin{array}{c}n-1\ n-5\end{array}\right) .4^5(.6)^{n-5}$$
Math Appendix: Some Math (For Geometric Distribution) :
$$a&#43;ar&#43;ar^2&#43;ar^3&#43;⋯=a/(1−r)=a(1−r)^{−1}$$ Taking the derivatives of both sides, the first derivative with respect to r must be: $$a&#43;2ar&#43;3ar^2&#43;4ar^3⋯=a(1−r)^{−2}$$ Multiplying above with r: $$ar&#43;2ar^2&#43;3ar^3&#43;4ar^4⋯=ar(1−r)^{−2}$$ Taking the derivatives of both sides, the first derivative with respect to r must be: $$a&#43;4ar&#43;9ar^2&#43;16ar^3⋯=a(1−r)^{-3}(1&#43;r)$$
Bonus - Python Graphs and Functions: # Useful Function to create graph def chart_creator(x,y,title): import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes %matplotlib inline # Create a list of 100 Normal RVs data = pd.DataFrame(zip(x,y)) data.columns = [&amp;#39;x&amp;#39;,&amp;#39;y&amp;#39;] # We dont Probably need the Gridlines. Do we? If yes comment this line sns.set(style=&amp;#34;ticks&amp;#34;) # Here we create a matplotlib axes object. The extra parameters we use # &amp;#34;ci&amp;#34; to remove confidence interval # &amp;#34;marker&amp;#34; to have a x as marker. # &amp;#34;scatter_kws&amp;#34; to provide style info for the points.[s for size] # &amp;#34;line_kws&amp;#34; to provide style info for the line.[lw for line width] g = sns.regplot(x=&amp;#39;x&amp;#39;, y=&amp;#39;y&amp;#39;, data=data, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, line_kws={&amp;#34;color&amp;#34;:&amp;#34;g&amp;#34;,&amp;#34;alpha&amp;#34;:0.5,&amp;#34;lw&amp;#34;:0},marker=&amp;#34;x&amp;#34;) # remove the top and right line in graph sns.despine() # Set the size of the graph from here g.figure.set_size_inches(12,8) # Set the Title of the graph from here g.axes.set_title(title, fontsize=34,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the xlabel of the graph from here g.set_xlabel(&amp;#34;k&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ylabel of the graph from here g.set_ylabel(&amp;#34;pmf&amp;#34;,size = 67,color=&amp;#34;r&amp;#34;,alpha=0.5) # Set the ticklabel size and color of the graph from here g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;) And here I will generate the PMFs of the discrete distributions we just discussed above using Pythons built in functions. For more details on the upper function, please see my previous post - Create basic graph visualizations with SeaBorn. Also take a look at the documentation guide for the below functions
# Binomial : from scipy.stats import binom n=30 p=0.5 k = range(0,n) pmf = binom.pmf(k, n, p) chart_creator(k,pmf,&amp;#34;Binomial PMF&amp;#34;)   # Geometric : from scipy.stats import geom n=30 p=0.5 k = range(0,n) # -1 here is the location parameter for generating the PMF we want. pmf = geom.pmf(k, p,-1) chart_creator(k,pmf,&amp;#34;Geometric PMF&amp;#34;)   # Negative Binomial : from scipy.stats import nbinom r=5 # number of successes p=0.5 # probability of Success k = range(0,25) # number of failures # -1 here is the location parameter for generating the PMF we want. pmf = nbinom.pmf(k, r, p) chart_creator(k,pmf,&amp;#34;Nbinom PMF&amp;#34;)   #Poisson from scipy.stats import poisson lamb = .3 # Rate k = range(0,5) pmf = poisson.pmf(k, lamb) chart_creator(k,pmf,&amp;#34;Poisson PMF&amp;#34;)   References:  Introduction to Probability by Joe Blitzstein Wikipedia  Next thing I want to come up with is a same sort of post for continuous distributions too. Keep checking for the same. Till then Ciao.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Maths Beats Intuition probably every damn time</title>
      <link>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</guid>
      
      

      
      <description>Newton once said that &amp;amp;ldquo;God does not play dice with the universe&amp;amp;rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.
One of such ways intuition plays with us is with the Birthday problem.
Problem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?</description>

      <content:encoded>  
        
        <![CDATA[  Newton once said that &amp;ldquo;God does not play dice with the universe&amp;rdquo;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.
One of such ways intuition plays with us is with the Birthday problem.
Problem Statement: In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?
By the pigeonhole principle, the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).
However, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.
Mathematical Proof: Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event. We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement) Since there are k ways in which birthdays can be chosen with replacement.
$P(birthday Match) = 1 - \dfrac{(365).364&amp;hellip;(365−k&#43;1)}{365^k}$
Simulation: Lets try to build around this result some more by trying to simulate this result:
%matplotlib inline import matplotlib import numpy as np import matplotlib.pyplot as plt import matplotlib.pyplot as plt #sets up plotting under plt import seaborn as sns #sets up styles and gives us more plotting options import pandas as pd #lets us handle data as dataframes import random def sim_bithday_problem(num_people_room, trials =1000): &amp;#39;&amp;#39;&amp;#39;This function takes as input the number of people in the room. Runs 1000 trials by default and returns (number of times same brthday found)/(no of trials) &amp;#39;&amp;#39;&amp;#39; same_birthdays_found = 0 for i in range(trials): # randomly sample from the birthday space which could be any of a number from 1 to 365 birthdays = [random.randint(1,365) for x in range(num_people_room)] if len(birthdays) - len(set(birthdays))&amp;gt;0: same_birthdays_found&#43;=1 return same_birthdays_found/float(trials) num_people = range(2,100) probs = [sim_bithday_problem(i) for i in num_people] data = pd.DataFrame() data[&amp;#39;num_peeps&amp;#39;] = num_people data[&amp;#39;probs&amp;#39;] = probs sns.set(style=&amp;#34;ticks&amp;#34;) g = sns.regplot(x=&amp;#34;num_peeps&amp;#34;, y=&amp;#34;probs&amp;#34;, data=data, ci = False, scatter_kws={&amp;#34;color&amp;#34;:&amp;#34;darkred&amp;#34;,&amp;#34;alpha&amp;#34;:0.3,&amp;#34;s&amp;#34;:90}, marker=&amp;#34;x&amp;#34;,fit_reg=False) sns.despine() g.figure.set_size_inches(10,6) g.axes.set_title(&amp;#39;As the Number of people in room reaches 23 the probability reaches ~0.5\nAt more than 50 people the probability is reaching 1&amp;#39;, fontsize=15,color=&amp;#34;g&amp;#34;,alpha=0.5) g.set_xlabel(&amp;#34;# of people in room&amp;#34;,size = 30,color=&amp;#34;r&amp;#34;,alpha=0.5) g.set_ylabel(&amp;#34;Probability&amp;#34;,size = 30,color=&amp;#34;r&amp;#34;,alpha=0.5) g.tick_params(labelsize=14,labelcolor=&amp;#34;black&amp;#34;)   We can see from the graph that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.
Intuition: To understand it we need to think of this problem in terms of pairs. There are ${{23}\choose{2}} = 253$ pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at ${{70}\choose{2}} = 2450$ pairs.
So thats it for now. To learn more about this go to Wikipedia which has an awesome page on this topic.
References:  Introduction to Probability by Joseph K. Blitzstein Birthday Problem on Wikipedia  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      
      

      
      <description>I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;amp;rsquo;t cease to amaze me.</description>

      <content:encoded>  
        
        <![CDATA[  I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.
To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?
But it is not magic. And this is a way an aspiring guy could take to become a self-trained data scientist. Follow in order. I have tried to include everything that comes to my mind. So here goes:
1. Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University The one stat course you gotta take. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.
This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.
One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.
It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.
He got a textbook out based on this course which is clearly a great text:
 2. Data Science CS109: - Again by Professor Blitzstein. Again an awesome course. Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts. You will learn about Python Libraries like Numpy,Pandas for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.  3. CS229: Andrew Ng After doing these two above courses you will gain the status of what I would like to call a &amp;ldquo;Beginner&amp;rdquo;. Congrats!!!. You know stuff, you know how to implement stuff. Yet you do not fully understand all the math and grind that goes behind all this.
Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms. I will put this course as the one course you gotta take as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.
Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website here.
You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D&amp;rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.
4. Intro to Hadoop &amp;amp; Mapreduce - Udacity Let us first focus on the literal elephant in the room - Hadoop. Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.
Once you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem. Read this amazing Blog Post from Michael Noll: Writing An Hadoop MapReduce Program In Python - Michael G. Noll. Just read the basic mapreduce codes. Don&amp;rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.
Now try to solve these two problems from the CS109 Harvard course from 2013:
A. First, grab the file word_list.txt from here. This contains a list of six-letter words. To keep things simple, all of the words consist of lower-case letters only.Write a mapreduce job that finds all anagrams in word_list.txt.
B. For the next problem, download the file baseball_friends.csv. Each row of this csv file contains the following:
 A person&amp;rsquo;s name The team that person is rooting for &amp;ndash; either &amp;ldquo;Cardinals&amp;rdquo; or &amp;ldquo;Red Sox&amp;rdquo; A list of that person&amp;rsquo;s friends, which could have arbitrary length  For example: The first line tells us that Aaden is a Red Sox friend and he has 65 friends, who are all listed here. For this problem, it&amp;rsquo;s safe to assume that all of the names are unique and that the friendship structure is symmetric (i.e. if Alannah shows up in Aaden&amp;rsquo;s friends list, then Aaden will show up in Alannah&amp;rsquo;s friends list). Write an mr job that lists each person&amp;rsquo;s name, their favorite team, the number of Red Sox fans they are friends with, and the number of Cardinals fans they are friends with.
Try to do this yourself. Don&amp;rsquo;t use the mrjob (pronounced Mr. Job) way that they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.
If you are done with these, you can safely call yourself as someone who could &amp;ldquo;think in Mapreduce&amp;rdquo; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:
Hadoop Mapreduce Streaming Tricks and Techniques
If you are someone who likes learning from a book you can get: 
5. Spark - In memory Big Data tool. Now comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn&amp;rsquo;t.
Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.
Data Science and Engineering with Apache® Spark™
I have written a little bit about Basic data processing with Spark here. Take a look: Learning Spark using Python: Basics and Applications
Also take a look at some of the projects I did as part of course at github
If you would like a book to read: 
If you don&amp;rsquo;t go through the courses, try solving the same two problems above that you solved by Hadoop using Spark too. Otherwise the problem sets in the courses are more than enough.
6. Understand Linux Shell: Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn&amp;rsquo;t emphasize how much time shell saves for me everyday.
Read these tutorials by me for doing that:
Shell Basics every Data Scientist Should know -Part I Shell Basics every Data Scientist Should know - Part II(AWK)
If you would like a course you can go for this course on edX.
If you want a book, go for:
 Congrats you are an &amp;ldquo;Hacker&amp;rdquo; now. You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &amp;ldquo;All Roads lead to Rome&amp;rdquo; as long as you are running.
7. Learn Statistical Inference and Bayesian Statistics I took the previous version of the specialization which was a single course taught by Mine Çetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data. You can also use these books:
  8. Deep Learning Intro - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.
Advanced - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.
Bonus - A free online book by Michael Nielsen.
Advanced Math Book - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow
9. Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More This course used to be there on Coursera but now only video links on youtube available. You can learn from this book too: 
Apart from that if you want to learn about Python and the basic intricacies of the language you can take the Computer Science Mini Specialization from RICE university too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.
10. Advanced Maths: Couldn&amp;rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.
Linear Algebra By Gilbert Strang - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.
Multivariate Calculus - MIT OCW
Convex Optimization - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.
The Machine learning field is evolving and new advancements are made every day. That&amp;rsquo;s why I didn&amp;rsquo;t put a third tier. The maximum I can call myself is a &amp;ldquo;Hacker&amp;rdquo; and my learning continues. Hope you do the same.
Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.
Till then. Ciao!!!
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Top advice for a Data Scientist</title>
      <link>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</guid>
      
      

      
      <description>A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:
1. Beware of the Clean Data Syndrome You need to ask yourself questions even before you start working on the data. Does this data make sense? Falsely assuming that the data is clean could lead you towards wrong Hypotheses.</description>

      <content:encoded>  
        
        <![CDATA[    A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:
1. Beware of the Clean Data Syndrome You need to ask yourself questions even before you start working on the data. Does this data make sense? Falsely assuming that the data is clean could lead you towards wrong Hypotheses. Apart from that, you can discern a lot of important patterns by looking at discrepancies in the data. For example, if you notice that a particular column has more than 50% values missing, you might think about not using the column. Or you may think that some of the data collection instrument has some error.
Or let&amp;rsquo;s say you have a distribution of Male vs Female as 90:10 in a Female Cosmetic business. You may assume clean data and show the results as it is or you can use common sense and ask if the labels are switched.
2. Manage Outliers wisely Outliers can help you understand more about the people who are using your website/product 24 hours a day. But including them while building models will skew the models a lot.
3. Keep an eye out for the Abnormal Be on the lookout for something out of the obvious. If you find something you may have hit gold.
For example, Flickr started up as a Multiplayer game. Only when the founders noticed that people were using it as a photo upload service, did they pivot.
Another example: fab.com started up as fabulis.com, a site to help gay men meet people. One of the site&amp;rsquo;s popular features was the &amp;ldquo;Gay deal of the Day&amp;rdquo;. One day the deal was for Hamburgers - and half of the buyers were women. This caused the team to realize that there was a market for selling goods to women. So Fabulis pivoted to fab as a flash sale site for designer products.
4. Start Focussing on the right metrics  Beware of Vanity metrics For example, # of active users by itself doesn&amp;rsquo;t divulge a lot of information. I would rather say &amp;ldquo;5% MoM increase in active users&amp;rdquo; rather than saying &amp;ldquo; 10000 active users&amp;rdquo;. Even that is a vanity metric as active users would always increase. I would rather keep a track of percentage of users that are active to know how my product is performing. Try to find out a metric that ties with the business goal. For example, Average Sales/User for a particular month.   5. Statistics may lie too Be critical of everything that gets quoted to you. Statistics has been used to lie in advertisements, in workplaces and a lot of other marketing venues in the past. People will do anything to get sales or promotions.
For example: Do you remember Colgate’s claim that 80% of dentists recommended their brand?
This statistic seems pretty good at first. It turns out that at the time of surveying the dentists, they could choose several brands — not just one. So other brands could be just as popular as Colgate.
Another Example: &amp;ldquo;99 percent Accurate&amp;rdquo; doesn&amp;rsquo;t mean shit. Ask me to create a cancer prediction model and I could give you a 99 percent accurate model in a single line of code. How? Just predict &amp;ldquo;No Cancer&amp;rdquo; for each one. I will be accurate may be more than 99% of the time as Cancer is a pretty rare disease. Yet I have achieved nothing.
6. Understand how probability works It happened during the summer of 1913 in a Casino in Monaco. Gamblers watched in amazement as a casino&amp;rsquo;s roulette wheel landed on black 26 times in a row. And since the probability of a Red vs Black is exactly half, they were certain that red was &amp;ldquo;due&amp;rdquo;. It was a field day for the Casino. A perfect example of Gambler&amp;rsquo;s fallacy, aka the Monte Carlo fallacy.
And This happens in real life. People tend to avoid long strings of the same answer. Sometimes sacrificing accuracy of judgment for the sake of getting a pattern of decisions that looks fairer or probable.
For example, An admissions officer may reject the next application if he has approved three applications in a row, even if the application should have been accepted on merit.
7. Correlation Does Not Equal Causation   The Holy Grail of a Data scientist toolbox. To see something for what it is. Just because two variables move together in tandem doesn&amp;rsquo;t necessarily mean that one causes the another. There have been hilarious examples for this in the past. Some of my favorites are:
 Looking at the firehouse department data you infer that the more firemen are sent to a fire, the more damage is done.
 When investigating the cause of crime in New York City in the 80s, an academic found a strong correlation between the amount of serious crime committed and the amount of ice cream sold by street vendors! Obviously, there was an unobserved variable causing both. Summers are when the crime is the greatest and when the most ice cream is sold. So Ice cream sales don&amp;rsquo;t cause crime. Neither crime increases ice cream sales.
  8. More data may help Sometimes getting extra data may work wonders. You might be able to model the real world more closely by looking at the problem from all angles. Look for extra data sources.
For example, Crime data in a city might help banks provide a better credit line to a person living in a troubled neighborhood and in turn increase the bottom line.
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</guid>
      
      

      
      <description>As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.</description>

      <content:encoded>  
        
        <![CDATA[    As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:
1. Sampling Algorithms. In case you want to work with a sample of data.  Simple Random Sampling : Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen. Stratified Sampling: Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns : Town A has 1 million factory workers, Town B has 2 million workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample. Reservoir Sampling :Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.  2. Map-Reduce. If you want to work with the whole data. Can be used for feature creation. For Example: I had a use case where I had a graph of 60 Million customers and 130 Million accounts. Each account was connected to other account if they had the Same SSN or Same Name&#43;DOB&#43;Address. I had to find customer ID’s for each of the accounts. On a single node parsing such a graph took more than 2 days. On a Hadoop cluster of 80 nodes running a Connected Component Algorithm took less than 24 minutes. On Spark it is even faster.
3. Graph Algorithms. Recently I was working on an optimization problem which was focussed on finding shortest distance and routes between two points in a store layout. Routes which don’t pass through different aisles, so we cannot use euclidean distances. We solved this problem by considering turning points in the store layout and the djikstra’s Algorithm.
 4. Feature Selection.  Univariate Selection. Statistical tests can be used to select those features that have the strongest relationship with the output variable. VarianceThreshold. Feature selector that removes all low-variance features. Recursive Feature Elimination. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. Feature Importance: Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.  5. Algorithms to work efficiently. Apart from these above algorithms sometimes you may need to write your own algorithms. Now I think of big algorithms as a combination of small but powerful algorithms. You just need to have idea of these algorithms to make a more better/efficient product. So some of these powerful algorithms which can help you are:
 Recursive Algorithms:Binary search algorithm. Divide and Conquer Algorithms: Merge-Sort. Dynamic Programming:Solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.  6. Classification/Regression Algorithms. The usual suspects. Minimum you must know:  Linear Regression - Ridge Regression, Lasso Regression, ElasticNet Logistic Regression From there you can build upon:  Decision Trees - ID3, CART, C4.5, C5.0 KNN SVM ANN - Back Propogation, CNN  And then on to Ensemble based algorithms:  Boosting: Gradient Boosted Trees Bagging: Random Forests Blending: Prediction outputs of different learning algorithms are fed into another learning algorithm.   7 . Clustering Methods.For unsupervised learning.  k-Means k-Medians Expectation Maximisation (EM) Hierarchical Clustering  8. Other algorithms you can learn about:  Apriori algorithm- Association Rule Mining Eclat algorithm - Association Rule Mining Item/User Based Similarity - Recommender Systems Reinforcement learning - Build your own robot. Graphical Models Bayesian Algorithms NLP - For language based models. Chatbots.  Hope this has been helpful&amp;hellip;..
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Things to see while buying a Mutual Fund</title>
      <link>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</guid>
      
      

      
      <description>This is a post which deviates from my pattern fo blogs that I have wrote till now but I found that Finance also uses up a lot of Statistics. So it won&amp;amp;rsquo;t be a far cry to put this on my blog here. I recently started investing in Mutual funds so thought of rersearching the area before going all in. Here is the result of some of my research.</description>

      <content:encoded>  
        
        <![CDATA[  This is a post which deviates from my pattern fo blogs that I have wrote till now but I found that Finance also uses up a lot of Statistics. So it won&amp;rsquo;t be a far cry to put this on my blog here. I recently started investing in Mutual funds so thought of rersearching the area before going all in. Here is the result of some of my research.
1. Load/No-Load: Always Buy No Load Mutual Funds
2. Regular/Direct: There are many differenct sites from where you can buy Mutual funds. Most of these sites take a commision to let you the investor buy and sell from their platform. To overcome this commision you can buy direct Mutual funds from the fund houses themselves. But that would be difficult as their are a lot of fund houses and mmanaging all of that could be quite painful. But with the advent of MFUtility you can buy direct plans from the same platform.
3. Expense Ratios: The expense ratio is a measure of what it costs an investment company to operate a mutual fund. To see how expense ratios can affect your investments over time, let’s compare the returns of several hypothetical investments that differ only in expense ratio. The following table depicts the returns on a 10,000 initial investment, assuming an average annualized gain of 10%, with different expense ratios (0.5%, 1%, 1.5%, 2% and 2.5%):
  As the table illustrates, even a small difference in expense ratio can cost you a lot of money in the long run. If you had invested 10,000 in the fund with a 2.5% expense ratio, the value of your fund would be 46,022 after 20 years. Had you instead invested your 10,000 in the fund with a lower, 0.5% expense ratio, your investment would be worth $61,159 after two decades, a 0.33% improvement over the more expensive fund. Keep in mind, this hypothetical example examines funds whose only differences are the expense ratios: all other variables, including initial investment and annualized gains, remain constant (for the example, we must assume identical taxation as well). While two funds are not likely to have the exact same performance over a 20-year period, the table illustrates the effects that small changes in expense ratio can have on your long-term returns.
 4. Avoid Mutual Funds With High Turnover Ratios: Mutual fund turnover is calculated as the value of all transactions (buying, selling) divided by two, then divided by a fund&amp;rsquo;s total holdings. In simpler terms, mutual fund turnover typically measures the replacement of holdings in a mutual fund, and is commonly presented to investors as a percentage over a one year period. If a fund has 100% turnover, the fund replaces all of its holdings over a 12-month period and that bears cost to the investment company in terms of brokerage etc.
5. Look for Ample Diversification of Assets: Simply owning four different mutual funds specializing in the financial sector (shares of banks, insurance companies, etc.) is not diversification. Don’t own funds that make heavy sector or industry bets. If you choose to despite this warning, make sure that you don’t have a huge portion of your funds invested in them. If it’s a bond fund, you typically want to avoid bets on the direction of interest rates as this is rank speculation.
6. Not Same Fund Family: Don’t keep all of your funds within the same fund family. Witness the mutual fund scandal of a few years ago where portfolio management at many firms allowed big traders to market time the funds, essentially stealing money from smaller investors. By spreading your assets out at different companies, you can mitigate the risk of internal turmoil, ethics breaches, and other localized problems.
7. Keep Track of various Risk Ratios: a. Standard deviation: Standard deviation (SD) measures the volatility the fund&amp;rsquo;s returns in relation to its average. It tells you how much the fund&amp;rsquo;s return can deviate from the historical mean return of the scheme. If a fund has a 12% average rate of return and a standard deviation of 4%, its return will range from 8-16%
Computation:
Standard Deviation (SD) = Square root of Variance (V)
Variance = (Sum of squared difference between each monthly return and its mean / number of monthly return data – 1)
b. R-Squared: R-Squared measures the relationship between a portfolio and its benchmark. It can be thought of as a percentage from 1 to 100. R-squared is not a measure of the performance of a portfolio. A great portfolio can have a very low R-squared. It is simply a measure of the correlation of the portfolio&amp;rsquo;s returns to the benchmark&amp;rsquo;s returns.
Computation:
R-Squared = Square of Correlation
Correlation(xy)= Covariance between index and portfolio/(Standard deviation of portfolio * standard deviation of index)
Significance:
 If you want a portfolio that moves like the benchmark, you&amp;rsquo;d want a portfolio with a high Rsquared.
 If you want a portfolio that doesn&amp;rsquo;t move at all like the benchmark, you&amp;rsquo;d want a low R-squared.
  General Range for R-Squared:
 70-100% = good correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns
 40-70% = average correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns
 1-40% = low correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns
 Index funds will have an R-squared very close to 100.
 R-squared can be used to ascertain the significance of a particular beta or alpha. Generally, a higher R-squared will indicate a more useful beta figure. If the R-squared is lower, then the beta is less relevant to the fund&amp;rsquo;s performance
 Values range from 1 (returns are explained 100% by the market) to 0 (returns bear no association with the market)
  c. Beta: A beta of 1.0 indicates that the investment&amp;rsquo;s price will move in lock-step with the market.
A beta of less than 1.0 indicates that the investment will be less volatile than the market, and, correspondingly, a beta of more than 1.0 indicates that the investment&amp;rsquo;s price will be more volatile than the market.
For example, if a fund portfolio&amp;rsquo;s beta is 1.2, it&amp;rsquo;s theoretically 20% more volatile than the market. Conservative investors looking to preserve capital should focus on securities and fund portfolios with low betas, whereas those investors willing to take on more risk in search of higher returns should look for high beta investments.
Computation:
Beta = (Standard Deviation of Fund x R-Square) / Standard Deviation of Benchmark
If a fund has a beta of 1.5, it means that for every 10% upside or downside, the fund&amp;rsquo;s NAV would be 15% in the respective direction.
d. Jensens Alpha: Alpha is a measure of an investment&amp;rsquo;s performance on a risk-adjusted basis.
Simply stated, alpha is often considered to represent the value that a portfolio manager adds or subtracts from a fund portfolio&amp;rsquo;s return.
A positive alpha of 1.0 means the fund has outperformed its benchmark index by 1%. Correspondingly, a similar negative alpha would indicate an underperformance of 1%.
Computation:
Alpha = {(Fund return-Risk free return) – (Funds beta) *(Benchmark return- risk free return)}
For example, assume a mutual fund realized a return of 15% last year. The appropriate market index for this fund returned 12%. The beta of the fund versus that same index is 1.2 and the risk-free rate is 3%. The fund&amp;rsquo;s alpha is calculated as:
Alpha = {(15 -3) – (1.2) *(12- 3)} = 12 - 9 x 1.2 = 12-10.8 = 1.2
Given a beta of 1.2, the mutual fund is expected to be riskier than the index, and thus earn more. A positive alpha in this example shows that the mutual fund manager earned more than enough return to be compensated for the risk he took over the course of the year. If the mutual fund only returned 13%, the calculated alpha would be -0.8. With a negative alpha, the mutual fund manager would not have earned enough return given the amount of risk he was taking.
e. Sharpe Ratio: Sharpe Ratio measures how well the fund has performed vis-a vis the risk taken by it. It is the excess return over risk-free return (usually return from treasury bills or government securities) divided by the standard deviation. The higher the Sharpe Ratio, the better the fund has performed in proportion to the risk taken by it. The Sharpe ratio is also known as Reward-to-Variability ratio and it is named after William Forsyth Sharpe.
Computation:
SR = (Total Return – Risk Free Rate) / SD Of Fund
For example: Your investor gets 7 per cent return on her investment in a scheme with a standard deviation/volatility of 0.5. We assume risk free rate is 5 per cent. Sharpe Ratio is 7-5&amp;frasl;0.5 = 4 in this case
8. And Finally Always Dollar-Cost Average: Dollar cost averaging is a technique designed to reduce market risk through the systematic purchase of securities at predetermined intervals and set amounts.Instead of investing assets in a lump sum, the investor works his way into a position by slowly buying smaller amounts over a longer period of time. This spreads the cost basis out over several years, providing insulation against changes in market price.
Every investor investment strategy differs. These are just some common guidelines to work your way through the market and making informed decisions while buying Mutual Funds. Normally I work through points 1-6 and get my list to a few mutual funds after which I generally use risk ratios to determine which of the funds I selected might be a winner. I have a bias towards long term investing when it comes to investing so whatever I wrote here must be taken with a grain of salt just as everything related to investment must be. Some of you who are doing this for a longer time than I can also tell me about the various other things I can do. I will try to include those ideas in this post as well.
To Learn more about Mutual funds and investing in general, take a look at the following two gems:
  The Editorial review of The intelligent Investor says &amp;ldquo;Among the library of investment books promising no-fail strategies for riches, Benjamin Graham&amp;rsquo;s classic, The Intelligent Investor, offers no guarantees or gimmicks but overflows with the wisdom at the core of all good portfolio management&amp;rdquo; and it rings true in every sense. A must read for everyone looking to invest seriously.
Common Sense on Mutual Funds focusses on Mutual funds exclusively. Lets you understand that investing is not difficult. For the not so involved reader.
Till than Ciao!!!
References:  https://www.thebalance.com/picking-winning-mutual-funds-357957 http://www.miraeassetmf.co.in/uploads/TermofWeek/Sharpe_Ratio.pdf http://www.miraeassetmf.co.in/uploads/TermofWeek/Beta_SD_RSquared.pdf http://www.investopedia.com  ]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>Behold the power of MCMC</title>
      <link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link>
      <pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid>
      
      

      
      <description>Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;amp;rsquo;t know the normalizing constant.
So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.</description>

      <content:encoded>  
        
        <![CDATA[    Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.
So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.
So here goes. This time we will cover some applications of MCMC in various areas of Computer Science using Python. If you feel the problems difficult to follow with, I would advice you to go back and read the previous post, which tries to explain MCMC Methods. We Will try to solve the following two problems:
 Breaking the Code - This problem has got somewhat of a great pedigree as this method was suggested by Persi Diaconis- The Mathemagician. So Someone comes to you with the below text. This text looks like gibberish but this is a code, Could you decrypyt it?
XZ STAVRK HXVR MYAZ OAKZM JKSSO SO MYR OKRR XDP JKSJRK XBMASD SO YAZ TWDHZ MYR JXMBYNSKF BSVRKTRM NYABY NXZ BXKRTRZZTQ OTWDH SVRK MYR AKSD ERPZMRXP KWZMTRP MYR JXTR OXBR SO X QSWDH NSIXD NXZ KXAZRP ORRETQ OKSI MYR JATTSN XDP X OXADM VSABR AIJRKORBMTQ XKMABWTXMRP MYR NSKPZ TRM IR ZRR MYR BYATP XDP PAR MYR ZWKHRSD YXP ERRD ZAMMADH NAMY YAZ OXBR MWKDRP MSNXKPZ MYR OAKR HAVADH MYR JXTIZ SO YAZ YXDPZ X NXKI XDP X KWE XTMRKDXMRTQ XZ MYR QSWDH NSIXD ZJSFR YR KSZR XDP XPVXDBADH MS MYR ERP Z YRXP ZXAP NAMY ISKR FADPDRZZ MYXD IAHYM YXVR ERRD RGJRBMRP SO YAI
 The Knapsack Problem - This problem comes from Introduction to Probability by Joseph Blitzstein. You should check out his courses STAT110 and CS109 as they are awesome. Also as it turns out Diaconis was the advisor of Joseph. So you have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution? This is known as the Knapsack Problem in Computer Science.
  Breaking the Code   So we look at the data and form a hypothesis that the data has been scrambled using a Substitution Cipher. We don&amp;rsquo;t know the encryption key, and we would like to know the Decryption Key so that we can decrypt the data and read the code.
To create this example, this data has actually been taken from Oliver Twist. We scrambled the data using a random encryption key, which we forgot after encrypting and we would like to decrypt this encrypted text using MCMC Chains. The real decryption key actually is &amp;ldquo;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;rdquo;
So lets think about this problem for a little bit. The decryption key could be any 26 letter string with all alphabets appearing exactly once. How many string permutations are there like that? That number would come out to be $26! \approx 10^{26}$ permutations. That is a pretty large number. If we go for using a brute force approach we are screwed. So what could we do? MCMC Chains come to rescue.
We will devise a Chain whose states theoritically could be any of these permutations. Then we will:
 Start by picking up a random current state. Create a proposal for a new state by swapping two random letters in the current state. Use a Scoring Function which calculates the score of the current state $Score_C$ and the proposed State $Score_P$. If the score of the proposed state is more than current state, Move to Proposed State. Else flip a coin which has a probability of Heads $Score_P/Score_C$. If it comes heads move to proposed State. Repeat from 2nd State.  If we get lucky we may reach a steady state where the chain has the stationary distribution of the needed states and the state that the chain is at could be used as a solution.
So the Question is what is the scoring function that we will want to use. We want to use a scoring function for each state(Decryption key) which assigns a positive score to each decryption key. This score intuitively should be more if the encrypted text looks more like actual english if decrypted using this decryption key.
So how can we quantify such a function. We will check a long text and calculate some statistics. See how many times one alphabet comes after another in a legitimate long text like War and Peace. For example we want to find out how many times does &amp;lsquo;BA&amp;rsquo; appears in the text or how many times &amp;lsquo;TH&amp;rsquo; occurs in the text.
For each pair of characters $\beta_1$ and $\beta_2$ (e.g. $\beta_1$ = T and $\beta_2$ =H), we let $R(\beta_1,\beta_2)$ record the number of times that specific pair(e.g. &amp;ldquo;TH&amp;rdquo;) appears consecutively in the reference text.
Similarly, for a putative decryption key x, we let $F_x(\beta_1,\beta_2)$ record the number of times that pair appears when the cipher text is decrypted using the decryption key x.
We then Score a particular decryption key x using:
$$Score(x) = \prod R(\beta_1,\beta_2)^{F_x(\beta_1,\beta_2)}$$ This function can be thought of as multiplying, for each consecutive pair of letters in the decrypted text, the number of times that pair occurred in the reference text. Intuitively, the score function is higher when the pair frequencies in the decrypted text most closely match those of the reference text, and the decryption key is thus most likely to be correct.
To make life easier with calculations we will calculate $log(Score(x))$
So lets start working through the problem step by step.
# AIM: To Decrypt a text using MCMC approach. i.e. find decryption key which we will call cipher from now on. import string import math import random # This function takes as input a decryption key and creates a dict for key where each letter in the decryption key # maps to a alphabet For example if the decryption key is &amp;#34;DGHJKL....&amp;#34; this function will create a dict like {D:A,G:B,H:C....}  def create_cipher_dict(cipher): cipher_dict = {} alphabet_list = list(string.ascii_uppercase) for i in range(len(cipher)): cipher_dict[alphabet_list[i]] = cipher[i] return cipher_dict # This function takes a text and applies the cipher/key on the text and returns text. def apply_cipher_on_text(text,cipher): cipher_dict = create_cipher_dict(cipher) text = list(text) newtext = &amp;#34;&amp;#34; for elem in text: if elem.upper() in cipher_dict: newtext&#43;=cipher_dict[elem.upper()] else: newtext&#43;=&amp;#34; &amp;#34; return newtext # This function takes as input a path to a long text and creates scoring_params dict which contains the  # number of time each pair of alphabet appears together # Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..} def create_scoring_params_dict(longtext_path): scoring_params = {} alphabet_list = list(string.ascii_uppercase) with open(longtext_path) as fp: for line in fp: data = list(line.strip()) for i in range(len(data)-1): alpha_i = data[i].upper() alpha_j = data[i&#43;1].upper() if alpha_i not in alphabet_list and alpha_i != &amp;#34; &amp;#34;: alpha_i = &amp;#34; &amp;#34; if alpha_j not in alphabet_list and alpha_j != &amp;#34; &amp;#34;: alpha_j = &amp;#34; &amp;#34; key = alpha_i&#43;alpha_j if key in scoring_params: scoring_params[key]&#43;=1 else: scoring_params[key]=1 return scoring_params # This function takes as input a text and creates scoring_params dict which contains the  # number of time each pair of alphabet appears together # Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..} def score_params_on_cipher(text): scoring_params = {} alphabet_list = list(string.ascii_uppercase) data = list(text.strip()) for i in range(len(data)-1): alpha_i =data[i].upper() alpha_j = data[i&#43;1].upper() if alpha_i not in alphabet_list and alpha_i != &amp;#34; &amp;#34;: alpha_i = &amp;#34; &amp;#34; if alpha_j not in alphabet_list and alpha_j != &amp;#34; &amp;#34;: alpha_j = &amp;#34; &amp;#34; key = alpha_i&#43;alpha_j if key in scoring_params: scoring_params[key]&#43;=1 else: scoring_params[key]=1 return scoring_params # This function takes the text to be decrypted and a cipher to score the cipher. # This function returns the log(score) metric def get_cipher_score(text,cipher,scoring_params): cipher_dict = create_cipher_dict(cipher) decrypted_text = apply_cipher_on_text(text,cipher) scored_f = score_params_on_cipher(decrypted_text) cipher_score = 0 for k,v in scored_f.iteritems(): if k in scoring_params: cipher_score &#43;= v*math.log(scoring_params[k]) return cipher_score # Generate a proposal cipher by swapping letters at two random location def generate_cipher(cipher): pos1 = random.randint(0, len(list(cipher))-1) pos2 = random.randint(0, len(list(cipher))-1) if pos1 == pos2: return generate_cipher(cipher) else: cipher = list(cipher) pos1_alpha = cipher[pos1] pos2_alpha = cipher[pos2] cipher[pos1] = pos2_alpha cipher[pos2] = pos1_alpha return &amp;#34;&amp;#34;.join(cipher) # Toss a random coin with robability of head p. If coin comes head return true else false. def random_coin(p): unif = random.uniform(0,1) if unif&amp;gt;=p: return False else: return True # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states  def MCMC_decrypt(n_iter,cipher_text,scoring_params): current_cipher = string.ascii_uppercase # Generate a random cipher to start state_keeper = set() best_state = &amp;#39;&amp;#39; score = 0 for i in range(n_iter): state_keeper.add(current_cipher) proposed_cipher = generate_cipher(current_cipher) score_current_cipher = get_cipher_score(cipher_text,current_cipher,scoring_params) score_proposed_cipher = get_cipher_score(cipher_text,proposed_cipher,scoring_params) acceptance_probability = min(1,math.exp(score_proposed_cipher-score_current_cipher)) if score_current_cipher&amp;gt;score: best_state = current_cipher if random_coin(acceptance_probability): current_cipher = proposed_cipher if i%500==0: print &amp;#34;iter&amp;#34;,i,&amp;#34;:&amp;#34;,apply_cipher_on_text(cipher_text,current_cipher)[0:99] return state_keeper,best_state ## Run the Main Program: scoring_params = create_scoring_params_dict(&amp;#39;war_and_peace.txt&amp;#39;) plain_text = &amp;#34;As Oliver gave this first proof of the free and proper action of his lungs, \ the patchwork coverlet which was carelessly flung over the iron bedstead, rustled; \ the pale face of a young woman was raised feebly from the pillow; and a faint voice imperfectly \ articulated the words, Let me see the child, and die. \ The surgeon had been sitting with his face turned towards the fire: giving the palms of his hands a warm \ and a rub alternately. As the young woman spoke, he rose, and advancing to the bed&amp;#39;s head, said, with more kindness \ than might have been expected of him: &amp;#34; encryption_key = &amp;#34;XEBPROHYAUFTIDSJLKZMWVNGQC&amp;#34; cipher_text = apply_cipher_on_text(plain_text,encryption_key) decryption_key = &amp;#34;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;#34; print&amp;#34;Text To Decode:&amp;#34;, cipher_text print &amp;#34;\n&amp;#34; states,best_state = MCMC_decrypt(10000,cipher_text,scoring_params) print &amp;#34;\n&amp;#34; print &amp;#34;Decoded Text:&amp;#34;,apply_cipher_on_text(cipher_text,best_state) print &amp;#34;\n&amp;#34; print &amp;#34;MCMC KEY FOUND:&amp;#34;,best_state print &amp;#34;ACTUAL DECRYPTION KEY:&amp;#34;,decryption_key   This chain converges around the 2000th iteration and we are able to unscramble the code. That&amp;rsquo;s awesome!!! Now as you see the MCMC Key found is not exactly the encryption key. So the solution is not a deterministic one, but we can see that it does not actually decrease any of the value that the MCMC Methods provide. Now Lets Help Bilbo :)
The Knapsack Problem Restating, we have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution?
So in this problem we have an $1$x$M$ array of Weight Values W, Gold Values G and a value for the maximum weight $w_{MAX}$ that Bilbo can carry. We want to find out an $1$x$M$ array $X$ of 1&amp;rsquo;s and 0&amp;rsquo;s, which holds weather Bilbo Carries a particular treasure or not. This array needs to follow the constraint $WX^T &amp;lt; w_{MAX}$ and we want to maximize $GX^T$ for a particular state X.(Here the T means transpose)
So lets first discuss as to how we will create a proposal from a previous state.
 Pick a random index from the state and toggle the index value. Check if we satisfy our constraint. If yes this state is the proposal state. Else pick up another random index and repeat.  We also need to think about the Scoring Function. We need to give high values to states with high gold value. We will use: $$Score(X)=e^{\beta GX^T}$$ We give exponentially more value to higher score. The Beta here is a &#43;ve constant. But how to choose it? If $\beta$ is big we will give very high score to good solutions and the chain will not be able to try new solutions as it can get stuck in local optimas. If we give a small value the chain will not converge to very good solutions. So weuse an Optimization Technique called Simulated Annealing i.e. we will start with a small value of $\beta$ and increase as no of iterations go up. That way the chain will explore in the starting stages and stay at the best solution in the later stages.
So now we have everything we need to get started
import numpy as np W = [20,40,60,12,34,45,67,33,23,12,34,56,23,56] G = [120,420,610,112,341,435,657,363,273,812,534,356,223,516] W_max = 150 # This function takes a state X , The gold vector G and a Beta Value and return the Log of score def score_state_log(X,G,Beta): return Beta*np.dot(X,G) # This function takes as input a state X and the number of treasures M, The weight vector W and the maximum weight W_max # and returns a proposal state def create_proposal(X,W,W_max): M = len(W) random_index = random.randint(0,M-1) #print random_index proposal = list(X) proposal[random_index] = 1 - proposal[random_index] #Toggle #print proposal if np.dot(proposal,W)&amp;lt;=W_max: return proposal else: return create_proposal(X,W,W_max) # Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also # the last few states  def MCMC_Golddigger(n_iter,W,G,W_max, Beta_start = 0.05, Beta_increments=.02): M = len(W) Beta = Beta_start current_X = [0]*M # We start with all 0&amp;#39;s state_keeper = [] best_state = &amp;#39;&amp;#39; score = 0 for i in range(n_iter): state_keeper.append(current_X) proposed_X = create_proposal(current_X,W,W_max) score_current_X = score_state_log(current_X,G,Beta) score_proposed_X = score_state_log(proposed_X,G,Beta) acceptance_probability = min(1,math.exp(score_proposed_X-score_current_X)) if score_current_X&amp;gt;score: best_state = current_X if random_coin(acceptance_probability): current_X = proposed_X if i%500==0: Beta &#43;= Beta_increments # You can use these below two lines to tune value of Beta #if i%20==0: # print &amp;#34;iter:&amp;#34;,i,&amp;#34; |Beta=&amp;#34;,Beta,&amp;#34; |Gold Value=&amp;#34;,np.dot(current_X,G) return state_keeper,best_state Running the Main program:
max_state_value =0 Solution_MCMC = [0] for i in range(10): state_keeper,best_state = MCMC_Golddigger(50000,W,G,W_max,0.0005, .0005) state_value=np.dot(best_state,G) if state_value&amp;gt;max_state_value: max_state_value = state_value Solution_MCMC = best_state print &amp;#34;MCMC Solution is :&amp;#34; , str(Solution_MCMC) , &amp;#34;with Gold Value:&amp;#34;, str(max_state_value) MCMC Solution is : [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0] with Gold Value: 2435  Now I won&amp;rsquo;t say that this is the best solution. The deterministic solution using DP will be the best for such use case but sometimes when the problems gets large, having such techniques at disposal becomes invaluable.
So tell me What do you think about MCMC Methods?
Also, If you find any good applications or would like to apply these techniques to some area, I would really be glad to know about them and help if possible.
The codes for both examples are sourced at Github
References and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia The Markov Chain Monte Carlo Revolution, Persi Diaconis Decrypting Classical Cipher Text Using Markov Chain Monte Carlo, Jian Chen and Jeffrey S. Rosenthal  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)
Apart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.
Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:
  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)
]]>
        
      </content:encoded>
      
      
      
    </item>
    

    <item>
      <title>My Tryst With MCMC Algorithms</title>
      <link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link>
      <pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid>
      
      

      
      <description>The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.
  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.</description>

      <content:encoded>  
        
        <![CDATA[  The things that I find hard to understand push me to my limits. One of the things that I have always found hard is Markov Chain Monte Carlo Methods. When I first encountered them, I read a lot about them but mostly it ended like this.
  The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher. This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.
So what really are MCMC Methods? First of all we have to understand what are Monte Carlo Methods!!!
Monte Carlo methods derive their name from Monte Carlo Casino in Monaco. There are many card games that need probability of winning against the dealer. Sometimes calculating this probability can be mathematically complex or highly intractable. But we can always run a computer simulation to simulate the whole game many times and see the probability as the number of wins divided by the number of games played.
So that is all you need to know about Monte carlo Methods. Yes it is just a simple simulation technique with a Fancy Name.
So as we have got the first part of MCMC, we also need to understand what are Markov Chains. Before Jumping onto Markov Chains let us learn a little bit about Markov Property.
Suppose you have a system of $M$ possible states, and you are hopping from one state to another. Markov Property says that given a process which is at a state $X_n$ at a particular point of time, the probability of $X_{n&#43;1} = k$, where $k$ is any of the $M$ states the process can hop to, will only be dependent on which state it is at the given moment of time. And not on how it reached the current state.
Mathematically speaking:
 $$P(X_{n&#43;1}=k | X_n=k_n,X_{n-1}=k_{n-1},....,X_1=k_1) = P(X_{n&#43;1}=k|X_n=k_n)$$ If a process exhibits the Markov Property than it is known as a Markov Process.
Now Why is a Markov Chain important? It is important because of its stationary distribution.
So what is a Stationary Distribution?
Assume you have a markov process like below. You start from any state $X_i$ and want to find out the state Probability distribution at $X_{i&#43;1}$.
  You have a matrix of transition probability  
which defines the probability of going from a state $X_i$ to $X_j$. You start calculating the Probability distribution for the next state. If you are at Bull Market State at time $i$ , you have a state Probability distribution as [0,1,0]
you want to get the state pdf at $X_{i&#43;1}$. That is given by
$$s_{i&#43;1} = s_{i}Q$$ $$ s_{i&#43;1}=\left[ {\begin{array}{cc} .15 &amp; .8 &amp; .05 \end{array} } \right]$$ And the next state distribution could be found out by $$s_{i&#43;1} = s_iQ^2$$div and so on. Eventually you will reach a stationary state s where: $$sQ=s$$ For this transition matrix Q the Stationary distribution $s$ is $$ s_{i&#43;1}=\left[ {\begin{array}{cc} .625 &amp; .3125 &amp; .0625 \end{array} } \right]$$ The stationary state distribution is important because it lets you define the probability for every state of a system at a random time. That is for this particular example we can say that 62.5% of the times market will be in a bull market state, 31.25% of weeks it will be a bear market and 6.25% of weeks it will be stagnant
Intuitively you can think of it as an random walk on a chain. You might visit some nodes more often than others based on node probabilities. In the Google Pagerank problem you might think of a node as a page, and the probability of a page in the stationary distribution as its relative importance.
Woah! That was a lot of information and we have yet not started talking about the MCMC Methods. Well if you are with me till now, we can now get on to the real topic now.
So What is MCMC? According to Wikipedia:
 **Markov Chain Monte Carlo** (MCMC) methods are a class of algorithms for **sampling from a probability distribution** based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.  So let&amp;rsquo;s explain this with an example: Assume that we want to sample from a Beta distribution. The PDF is:
$$f(x) = Cx^{\alpha -1}(1-x)^{\beta -1}$$ where $C$ is the normalizing constant (which we actually don&amp;rsquo;t need to Sample from the distribution as we will see later).
This is a fairly difficult problem with the Beta Distribution if not intractable. In reality you might need to work with a lot harder Distribution Functions and sometimes you won&amp;rsquo;t actually know the normalizing constants.
MCMC methods make life easier for us by providing us with algorithms that could create a Markov Chain which has the Beta distribution as its stationary distribution given that we can sample from a uniform distribution(which is fairly easy).
If we start from a random state and traverse to the next state based on some algorithm repeatedly, we will end up creating a Markov Chain which has the Beta distribution as its stationary distribution and the states we are at after a long time could be used as sample from the Beta Distribution.
One such MCMC Algorithm is the Metropolis Hastings Algorithm
Metropolis Hastings Algorithm Let $s=(s_1,s_2,&amp;hellip;.,s_M)$ be the desired stationary distribution. We want to create a Markov Chain that has this stationary distribution. We start with an arbitrary Markov Chain $P$ with $M$ states with transition matrix $Q$, so that $Q_{ij}$ represents the probability of going from state $i$ to $j$. Intuitively we know how to wander around this Markov Chain but this Markov Chain does not have the required Stationary Distribution. This chain does have some stationary distribution(which is not of our use)
Our Goal is to change the way we wander on the this Markov Chain $P$ so that this chain has the desired Stationary distribution.
To do this we:
 Start at a random initial State $i$. Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Compute an measure called the Acceptance Probability which is defined as: $a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$ Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  After a long time this chain will converge and will have a stationary distribution $s$. We can then use the states of the chain as the sample from any distribution.
While doing this to sample the Beta Distribution, the only time we are using the PDF is to find the acceptance probability and in that we divide $s_j$ by $s_i$, i.e. the normalizing constant $C$ gets cancelled.
Now Let&amp;rsquo;s Talk about the intuition. For the Intuition I am quoting an Answer from the site Stack Exchange,as this was the best intuitive explanation that I could find:  I think there&amp;rsquo;s a nice and simple intuition to be gained from the (independence-chain) Metropolis-Hastings algorithm. First, what&amp;rsquo;s the goal? The goal of MCMC is to draw samples from some probability distribution without having to know its exact height at any point(We don&amp;rsquo;t need to know C). The way MCMC achieves this is to &amp;ldquo;wander around&amp;rdquo; on that distribution in such a way that the amount of time spent in each location is proportional to the height of the distribution. If the &amp;ldquo;wandering around&amp;rdquo; process is set up correctly, you can make sure that this proportionality (between time spent and height of the distribution) is achieved. Intuitively, what we want to do is to to walk around on some (lumpy) surface in such a way that the amount of time we spend (or # samples drawn) in each location is proportional to the height of the surface at that location. So, e.g., we&amp;rsquo;d like to spend twice as much time on a hilltop that&amp;rsquo;s at an altitude of 100m as we do on a nearby hill that&amp;rsquo;s at an altitude of 50m. The nice thing is that we can do this even if we don&amp;rsquo;t know the absolute heights of points on the surface: all we have to know are the relative heights. e.g., if one hilltop A is twice as high as hilltop B, then we&amp;rsquo;d like to spend twice as much time at A as we spend at B. The simplest variant of the Metropolis-Hastings algorithm (independence chain sampling) achieves this as follows: assume that in every (discrete) time-step, we pick a random new &amp;ldquo;proposed&amp;rdquo; location (selected uniformly across the entire surface). If the proposed location is higher than where we&amp;rsquo;re standing now, move to it. If the proposed location is lower, then move to the new location with probability p, where p is the ratio of the height of that point to the height of the current location. (i.e., flip a coin with a probability p of getting heads; if it comes up heads, move to the new location; if it comes up tails, stay where we are). Keep a list of the locations you&amp;rsquo;ve been at on every time step, and that list will (asyptotically) have the right proportion of time spent in each part of the surface. (And for the A and B hills described above, you&amp;rsquo;ll end up with twice the probability of moving from B to A as you have of moving from A to B). There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: (1) pick a new &amp;ldquo;proposed&amp;rdquo; location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location. 
Sampling from Beta Distribution Now Let&amp;rsquo;s Move on to the problem of Simulating from Beta Distribution. Now Beta Distribution is a continuous Distribution on [0,1] and it can have infinite states on [0,1].
Lets Assume an arbitrary Markov Chain P with infinite states on [0,1] having transition Matrix Q such that $Q_{ij} = Q_{ji} = $ All entries in Matrix. We don&amp;rsquo;t really need the Matrix Q as we will see later, But I want to keep the problem description as close to the algorihm we suggested.
 Start at a random initial State $i$ given by Unif(0,1). Randomly pick a new Proposal State by looking at the transition probabilities in the ith row of the transition matrix Q. Lets say we pick up another Unif(0,1) state as a proposal state $j$. Compute an measure called the Acceptance Probability :  $$a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$$ which is, $$a_{ij} = min(s_j/s_i,1)$$ where, $$s_i = Ci^{\alpha -1}(1-i)^{\beta -1}$$ and, $$s_j = Cj^{\alpha -1}(1-j)^{\beta -1}$$  Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state. Repeat for a long time  So enough with theory, Let&amp;rsquo;s Move on to python to create our Beta Simulations Now&amp;hellip;.
import random # Lets define our Beta Function to generate s for any particular state. We don&amp;#39;t care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&amp;gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain Let us check our results of the MCMC Sampled Beta distribution against the actual beta distribution.
import numpy as np import pylab as pl import scipy.special as ss %matplotlib inline pl.rcParams[&amp;#39;figure.figsize&amp;#39;] = (17.0, 4.0) # Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a &#43; b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label=&amp;#34;Real Distribution: a=&amp;#34;&#43;str(a)&#43;&amp;#34;, b=&amp;#34;&#43;str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype=&amp;#39;step&amp;#39;,label=&amp;#34;Simulated_MCMC: a=&amp;#34;&#43;str(a)&#43;&amp;#34;, b=&amp;#34;&#43;str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)  As we can see our sampled beta values closely resemble the beta distribution.
So MCMC Methods are useful for the following basic problems.
 Simulating from a Random Variable PDF. Example: Simulate from a Beta(0.5,0.5) or from a Normal(0,1). Solve problems with a large state space.For Example: Knapsack Problem, Encrytion Cipher etc. We will work on this in the Next Blog Post as this one has already gotten bigger than what I expected.  Till Then Ciao!!!!!!
References and Sources:  Introduction to Probability Joseph K Blitzstein, Jessica Hwang Wikipedia StackExchange  One of the newest and best resources that you can keep an eye on is the Bayesian Methods for Machine Learning course in the Advanced machine learning specialization created jointly by Kazanova(Number 3 Kaggler at the time of writing)
Apart from that I also found a course on Bayesian Statistics on Coursera. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on Inferential Statistics taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.
Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:
  Both these books are pretty high level and hard on math. But these are the best texts out there too. :)
]]>
        
      </content:encoded>
      
      
      
    </item>
    
  </channel>
</rss>