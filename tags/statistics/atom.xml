<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on MLWhiz</title>
    <link>https://mlwhiz.com/tags/statistics/</link>
    <description>Recent content in Statistics on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Sep 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/statistics/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>The story of every distribution - Discrete Distributions</title>
      <link>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</link>
      <pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/09/14/discrete_distributions/</guid>
      <description>

&lt;p&gt;Distributions play an important role in the life of every Statistician. I coming from a non-statistic background am not so well versed in these and keep forgetting about the properties of these famous distributions. That is why I chose to write my own understanding in an intuitive way to keep a track.
One of the most helpful way to learn more about these is the &lt;a href=&#34;https://projects.iq.harvard.edu/stat110/home&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;STAT110&lt;/a&gt; course by Joe Blitzstein and his &lt;a href=&#34;http://amzn.to/2xAsYzE&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;book&lt;/a&gt;. You can check out this &lt;a href=&#34;https://www.coursera.org/specializations/statistics?siteID=lVarvwc5BD0-1nQtJg8.ENATqSUIufAaaw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Coursera&lt;/a&gt; course too. Hope it could be useful to someone else too. So here goes:&lt;/p&gt;

&lt;h2 id=&#34;1-bernoulli-distribution&#34;&gt;1. Bernoulli Distribution:&lt;/h2&gt;

&lt;p&gt;Perhaps the most simple discrete distribution of all.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; A Coin is tossed with probability p of heads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PMF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$P(X=k) = \begin{cases}1-p &amp; k = 0\\p &amp; k = 1\end{cases}$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;CDF of Bernoulli Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$P(X \leq k) = \begin{cases}0 &amp; k \lt 0\\1-p &amp; 0 \leq k \lt 1 \\1 &amp; k \geq 1\end{cases}$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Expected Value:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$E[X] = \sum kP(X=k)$$
$$E[X] = 0*P(X=0)+1*P(X=1) = p$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$Var[X] = E[X^2] - E[X]^2$$
Now we find,
$$E[X]^2 = p^2$$
and
$$E[X^2] = \sum k^2P(X=k)$$
$$E[X^2] =  0^2P(X=0) + 1^2P(X=1) = p $$
Thus,
$$Var[X] = p(1-p)$$&lt;/p&gt;

&lt;h2 id=&#34;2-binomial-distribution&#34;&gt;2. Binomial Distribution:&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/maxresdefault.jpg&#34;  height=&#34;400&#34; width=&#34;500&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;One of the most basic distribution in the Statistician toolkit. The parameters of this distribution is n(number of trials) and p(probability of success).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
Probability of getting exactly k successes in n trials&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PMF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$P(X=k) = \left(\begin{array}{c}n\ k\end{array}\right) p^{k}(1-p)^{n-k}$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CDF of binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$ P(X\leq k) = \sum_{i=0}^k  \left(\begin{array}{c}n\ i\end{array}\right)  p^i(1-p)^{n-i} $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected Value:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$E[X] = \sum kP(X=k)$$
$$E[X] = \sum_{k=0}^n k \left(\begin{array}{c}n\ k\end{array}\right) * p^{k}(1-p)^{n-k} = np $$&lt;/p&gt;

&lt;p&gt;A better way to solve this:&lt;/p&gt;

&lt;div&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;

&lt;p&gt;X is the sum on n Indicator Bernoulli random variables.&lt;/p&gt;

&lt;p&gt;Thus,&lt;/p&gt;

&lt;div&gt;
$$E[X] = E[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;

&lt;div&gt;$$E[X] = E[I_{1}] + E[I_{2}] + ....+ E[I_{n-1}]+ E[I_{n}]$$&lt;/div&gt;

&lt;div&gt;$$E[X] = \underbrace{p + p + ....+ p + p}_{n} = np$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$ X = I_{1} + I_{2} + ....+ I_{n-1}+ I_{n} $$&lt;/div&gt;
X is the sum on n Indicator Bernoulli random variables.
&lt;div&gt;$$Var[X] = Var[I_{1} + I_{2} + ....+ I_{n-1}+ I_{n}]$$&lt;/div&gt;
&lt;div&gt;$$Var[X] = Var[I_{1}] + Var[I_{2}] + ....+ Var[I_{n-1}]+ Var[I_{n}]$$&lt;/div&gt;
&lt;div&gt;$$Var[X] = \underbrace{p(1-p) + p(1-p) + ....+ p(1-p) + p(1-p)}_{n} = np(1-p)$$&lt;/div&gt;

&lt;h2 id=&#34;3-geometric-distribution&#34;&gt;3. Geometric Distribution:&lt;/h2&gt;

&lt;p&gt;The parameters of this distribution is p(probability of success).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The number of failures before the first success(Heads) when a coin with probability p is tossed&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PMF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$P(X=k) = (1-p)^kp$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CDF of Geometric Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$ P(X\leq k) = \sum_{i=0}^k (1-p)^{i}p$$
$$ P(X\leq k) = p(1+q+q^2&amp;hellip;+q^k)= p(1-q^k)/(1-q) = 1-(1-p)^k $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected Value:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$E[X] = \sum kP(X=k)$$
$$E[X] = \sum_{k=0}^{inf} k (1-p)^kp$$
$$E[X] = qp +2q^2p +3q^3p +4q^4p &amp;hellip;. $$
$$E[X] = qp(1+2q+3q^2+4q^3+&amp;hellip;.)$$
$$E[X] = qp/(1-q)^2 = q/p $$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$$Var[X] = E[X^2] - E[X]^2$$
Now we find,
$$E[X]^2 = q^2/p^2$$
and
$$E[X^2] = \sum_0^k k^2q^kp= qp + 4q^2p + 9q^3p +16q^4p &amp;hellip; = qp(1+4q+9q^2+16q^3&amp;hellip;.)$$
$$E[X^2] = qp^{-2}(1+q)$$&lt;/p&gt;

&lt;p&gt;Thus,
$$Var[X] =q/p^2$$&lt;/p&gt;

&lt;p&gt;Check Math appendix at bottom of this post for Geometric Series Proofs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Q. A doctor is seeking an anti-depressant for a newly diagnosed patient. Suppose that, of the available anti-depressant drugs, the probability that any particular drug will be effective for a particular patient is p=0.6. What is the probability that the first drug found to be effective for this patient is the first drug tried, the second drug tried, and so on? What is the expected number of drugs that will be tried to find one that is effective?&lt;/p&gt;

&lt;p&gt;A. Expected number of drugs that will be tried to find one that is effective = q/p = .4/.6 =.67&lt;/p&gt;

&lt;h2 id=&#34;4-negative-binomial-distribution&#34;&gt;4. Negative Binomial Distribution:&lt;/h2&gt;

&lt;p&gt;The parameters of this distribution is p(probability of success) and r(number of success).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt;
The &lt;strong&gt;number of failures&lt;/strong&gt; of independent Bernoulli(p) trials before the rth success.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PMF of Negative Binomial Distribution is given by:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;r successes , k failures , last attempt needs to be a success:
$$P(X=k) = \left(\begin{array}{c}k+r-1\ k\end{array}\right) p^r(1-p)^k$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected Value:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The negative binomial RV could be stated as the sum of r Geometric RVs
$$X = X^1+X^2&amp;hellip;. X^{r-1} +X^r$$
Thus,
$$E[X] = E[X^1]+E[X^2]&amp;hellip;. E[X^{r-1}] +E[X^r]$$&lt;/p&gt;

&lt;p&gt;$$E[X] = rq/p$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The negative binomial RV could be stated as the sum of r independent Geometric RVs
$$X = X^1+X^2&amp;hellip;. X^{r-1} +X^r$$
Thus,
$$Var[X] = Var[X^1]+Var[X^2]&amp;hellip;. Var[X^{r-1}] +Var[X^r]$$&lt;/p&gt;

&lt;p&gt;$$E[X] = rq/p^2$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Q. Pat is required to sell candy bars to raise money for the 6th grade field trip. There are thirty houses in the neighborhood, and Pat is not supposed to return home until five candy bars have been sold. So the child goes door to door, selling candy bars. At each house, there is a 0.4 probability of selling one candy bar and a 0.6 probability of selling nothing.
What&amp;rsquo;s the probability of selling the last candy bar at the nth house?&lt;/p&gt;

&lt;p&gt;A. r = 5 ; k = n - r&lt;/p&gt;

&lt;p&gt;Probability of selling the last candy bar at the nth house =
$$P(X=k) = \left(\begin{array}{c}k+r-1\ k\end{array}\right) p^r(1-p)^k$$
$$P(X=k) = \left(\begin{array}{c}n-1\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/p&gt;

&lt;h2 id=&#34;5-poisson-distribution&#34;&gt;5. Poisson Distribution:&lt;/h2&gt;

&lt;p&gt;The parameters of this distribution is $\lambda$ the rate parameter.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt;
There is as such no story to this distribution but only motivation for using this distribution. The Poisson distribution is often used for applications where we count the successes of a large number of trials where the per-trial success rate is small. For example, the Poisson distribution is a good starting point for counting the number of people who email you over the course of an hour.The number of chocolate chips in a chocolate chip cookie is another good candidate for a Poisson distribution, or the number of earthquakes in a year in some particular region&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PMF of Poisson Distribution is given by:&lt;/strong&gt;
$$ P(X=k) = \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected Value:&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$E[X] = \sum kP(X=k)$$&lt;/div&gt;

&lt;div&gt;$$ E[X] = \sum_{k=0}^{inf} k \frac{e^{-\lambda}\lambda^k} {k!}$$&lt;/div&gt;
&lt;div&gt;$$ E[X] = \lambda e^{-\lambda}\sum_{k=0}^{inf}  \frac{\lambda^{k-1}} {(k-1)!}$$&lt;/div&gt;
&lt;div&gt;$$ E[X] = \lambda e^{-\lambda} e^{\lambda} = \lambda $$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt;&lt;/p&gt;

&lt;div&gt;$$Var[X] = E[X^2] - E[X]^2$$&lt;/div&gt;

&lt;p&gt;Now we find,
&lt;div&gt;$$E[X]^2 = \lambda + \lambda^2$$&lt;/div&gt;
Thus,
$$Var[X] = \lambda$$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Q. If electricity power failures occur according to a Poisson distribution with an average of 3 failures every twenty weeks, calculate the probability that there will not be more than one failure during a particular week?&lt;/p&gt;

&lt;p&gt;A. Probability = P(X=0)+P(X=1) =&lt;/p&gt;

&lt;div&gt;$$e^{-3/20} + e^{-3/20}3/20 = 23/20*e^{-3/20} $$&lt;/div&gt;

&lt;p&gt;Probability of selling the last candy bar at the nth house =
$$P(X=k) = \left(\begin{array}{c}k+r-1\ k\end{array}\right) p^r(1-p)^k$$
$$P(X=k) = \left(\begin{array}{c}n-1\ n-5\end{array}\right) .4^5(.6)^{n-5}$$&lt;/p&gt;

&lt;h2 id=&#34;math-appendix&#34;&gt;Math Appendix:&lt;/h2&gt;

&lt;p&gt;Some Math (For Geometric Distribution) :&lt;/p&gt;

&lt;p&gt;$$a+ar+ar^2+ar^3+⋯=a/(1−r)=a(1−r)^{−1}$$
Taking the derivatives of both sides, the first derivative with respect to r must be:
$$a+2ar+3ar^2+4ar^3⋯=a(1−r)^{−2}$$
Multiplying above with r:
$$ar+2ar^2+3ar^3+4ar^4⋯=ar(1−r)^{−2}$$
Taking the derivatives of both sides, the first derivative with respect to r must be:
$$a+4ar+9ar^2+16ar^3⋯=a(1−r)^{-3}(1+r)$$&lt;/p&gt;

&lt;h2 id=&#34;bonus-python-graphs-and-functions&#34;&gt;Bonus - Python Graphs and Functions:&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Useful Function to create graph&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;chart_creator&lt;/span&gt;(x,y,title):
    &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt  &lt;span style=&#34;color:#75715e&#34;&gt;#sets up plotting under plt&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns           &lt;span style=&#34;color:#75715e&#34;&gt;#sets up styles and gives us more plotting options&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd             &lt;span style=&#34;color:#75715e&#34;&gt;#lets us handle data as dataframes&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
    &lt;span style=&#34;color:#75715e&#34;&gt;# Create a list of 100 Normal RVs&lt;/span&gt;
    data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(zip(x,y))
    data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;]
    &lt;span style=&#34;color:#75715e&#34;&gt;# We dont Probably need the Gridlines. Do we? If yes comment this line&lt;/span&gt;
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Here we create a matplotlib axes object. The extra parameters we use&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#34;ci&amp;#34; to remove confidence interval&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#34;marker&amp;#34; to have a x as marker.&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#34;scatter_kws&amp;#34; to provide style info for the points.[s for size]&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#34;line_kws&amp;#34; to provide style info for the line.[lw for line width]&lt;/span&gt;

    g &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;regplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data, ci &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False,
        scatter_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;color&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;darkred&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;alpha&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;},
        line_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;color&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;g&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;alpha&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lw&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;},marker&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# remove the top and right line in graph&lt;/span&gt;
    sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;despine()

    &lt;span style=&#34;color:#75715e&#34;&gt;# Set the size of the graph from here&lt;/span&gt;
    g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_size_inches(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Set the Title of the graph from here&lt;/span&gt;
    g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(title, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Set the xlabel of the graph from here&lt;/span&gt;
    g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;67&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Set the ylabel of the graph from here&lt;/span&gt;
    g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pmf&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;67&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Set the ticklabel size and color of the graph from here&lt;/span&gt;
    g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tick_params(labelsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;,labelcolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here I will generate the PMFs of the discrete distributions we just discussed above using Pythons built in functions. For more details on the upper function, please see my previous post - &lt;a href=&#34;http://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Create basic graph visualizations with SeaBorn&lt;/a&gt;. Also take a look at the &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/stats.html&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; guide for the below functions&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Binomial :&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; binom
n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
p&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;
k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,n)
pmf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; binom&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pmf(k, n, p)
chart_creator(k,pmf,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Binomial PMF&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/output_12_0.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Geometric :&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; geom
n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
p&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;
k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,n)
&lt;span style=&#34;color:#75715e&#34;&gt;# -1 here is the location parameter for generating the PMF we want.&lt;/span&gt;
pmf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; geom&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pmf(k, p,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
chart_creator(k,pmf,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Geometric PMF&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/output_13_0.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Negative Binomial :&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nbinom
r&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# number of successes&lt;/span&gt;
p&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# probability of Success&lt;/span&gt;
k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# number of failures&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# -1 here is the location parameter for generating the PMF we want.&lt;/span&gt;
pmf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nbinom&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pmf(k, r, p)
chart_creator(k,pmf,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Nbinom PMF&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/output_14_0.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Poisson&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; poisson
lamb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Rate&lt;/span&gt;
k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
pmf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; poisson&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pmf(k, lamb)
chart_creator(k,pmf,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Poisson PMF&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/output_15_0.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://amzn.to/2xAsYzE&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Introduction to Probability by Joe Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Negative_binomial_distribution&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Next thing I want to come up with is a same sort of post for continuous distributions too. Keep checking for the same. Till then Ciao.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maths Beats Intuition probably every damn time</title>
      <link>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/04/16/maths_beats_intuition/</guid>
      <description>

&lt;p&gt;Newton once said that &lt;strong&gt;&amp;ldquo;God does not play dice with the universe&amp;rdquo;&lt;/strong&gt;. But actually he does. Everything happening around us could be explained in terms of probabilities. We repeatedly watch things around us happen due to chances, yet we never learn. We always get dumbfounded by the playfulness of nature.&lt;/p&gt;

&lt;p&gt;One of such ways intuition plays with us is with the Birthday problem.&lt;/p&gt;

&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement:&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;In a room full of N people, what is the probability that 2 or more people share the same birthday(Assumption: 365 days in year)?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pigeonhole_principle&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;pigeonhole principle&lt;/a&gt;, the probability reaches 100% when the number of people reaches 366 (since there are only 365 possible birthdays).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However, the paradox is that 99.9% probability is reached with just 70 people, and 50% probability is reached with just 23 people.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;mathematical-proof&#34;&gt;Mathematical Proof:&lt;/h2&gt;

&lt;p&gt;Sometimes a good strategy when trying to find out probability of an event is to look at the probability of the complement event.Here it is easier to find the probability of the complement event.
We just need to count the number of cases in which no person has the same birthday.(Sampling without replacement)
Since there are k ways in which birthdays can be chosen with replacement.&lt;/p&gt;

&lt;p&gt;$P(birthday Match) = 1 - \dfrac{(365).364&amp;hellip;(365−k+1)}{365^k}$&lt;/p&gt;

&lt;h2 id=&#34;simulation&#34;&gt;Simulation:&lt;/h2&gt;

&lt;p&gt;Lets try to build around this result some more by trying to simulate this result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt  &lt;span style=&#34;color:#75715e&#34;&gt;#sets up plotting under plt&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; sns           &lt;span style=&#34;color:#75715e&#34;&gt;#sets up styles and gives us more plotting options&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd             &lt;span style=&#34;color:#75715e&#34;&gt;#lets us handle data as dataframes&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sim_bithday_problem&lt;/span&gt;(num_people_room, trials &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39;This function takes as input the number of people in the room.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Runs 1000 trials by default and returns
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    (number of times same brthday found)/(no of trials)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    same_birthdays_found &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(trials):
        &lt;span style=&#34;color:#75715e&#34;&gt;# randomly sample from the birthday space which could be any of a number from 1 to 365&lt;/span&gt;
        birthdays &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_people_room)]
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(birthdays) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; len(set(birthdays))&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
            same_birthdays_found&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; same_birthdays_found&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;float(trials)

num_people &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)
probs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [sim_bithday_problem(i) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; num_people]
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame()
data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_peeps&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; num_people
data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;probs&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; probs
sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set(style&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;)

g &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;regplot(x&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;num_peeps&amp;#34;&lt;/span&gt;, y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;probs&amp;#34;&lt;/span&gt;, data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data, ci &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False,
    scatter_kws&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;color&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;darkred&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;alpha&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;s&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;},
    marker&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;,fit_reg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)

sns&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;despine()
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_size_inches(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;axes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_title(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;As the Number of people in room reaches 23 the probability reaches ~0.5&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;At more than 50 people the probability is reaching 1&amp;#39;&lt;/span&gt;, fontsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;g&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_xlabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;# of people in room&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylabel(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability&amp;#34;&lt;/span&gt;,size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,alpha&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tick_params(labelsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;,labelcolor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;black&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/bithdayproblem.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can see from the &lt;a href=&#34;https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/&#34;&gt;graph&lt;/a&gt; that as the Number of people in room reaches 23 the probability reaches ~ 0.5. So we have proved this fact Mathematically as well as with simulation.&lt;/p&gt;

&lt;h2 id=&#34;intuition&#34;&gt;Intuition:&lt;/h2&gt;

&lt;p&gt;To understand it we need to think of this problem in terms of pairs. There are ${{23}\choose{2}} = 253$ pairs of people in the room when only 23 people are present. Now with that big number you should not find the probability of 0.5 too much. In the case of 70 people we are looking at ${{70}\choose{2}} = 2450$ pairs.&lt;/p&gt;

&lt;p&gt;So thats it for now. To learn more about this go to &lt;a href=&#34;https://en.wikipedia.org/wiki/Birthday_problem&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt; which has an awesome page on this topic.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://amzn.to/2nIUkxq&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Introduction to Probability by Joseph K. Blitzstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Birthday_problem&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Birthday Problem on Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Top Data Science Resources on the Internet right now</title>
      <link>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/26/top_data_science_resources_on_the_internet_right_now/</guid>
      <description>

&lt;p&gt;I have been looking to create this list for a while now. There are many people on quora who ask me how I started in the data science field. And so I wanted to create this reference.&lt;/p&gt;

&lt;p&gt;To be frank, when I first started learning it all looked very utopian and out of the world. The Andrew Ng course felt like black magic. And it still doesn&amp;rsquo;t cease to amaze me. After all, we are predicting the future. Take the case of Nate Silver - What else can you call his success if not Black Magic?&lt;/p&gt;

&lt;p&gt;But it is not magic. And this is a way an aspiring guy could take to become a &lt;b&gt;&lt;u&gt;self-trained data scientist&lt;/u&gt;&lt;/b&gt;. Follow in order. I have tried to include everything that comes to my mind. So here goes:&lt;/p&gt;

&lt;h2 id=&#34;1-stat-110-introduction-to-probability-joe-blitzstein-harvard-university-http-projects-iq-harvard-edu-stat110-youtube&#34;&gt;1. &lt;a href=&#34;http://projects.iq.harvard.edu/stat110/youtube&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Stat 110: Introduction to Probability: Joe Blitzstein - Harvard University&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The one stat course you gotta take&lt;/em&gt;. If not for the content then for Prof. Blitzstein sense of humor. I took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.&lt;/p&gt;

&lt;p&gt;This was a Hard Class but most definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily. Sometimes the same proof was done in different ways to facilitate learning of a concept.&lt;/p&gt;

&lt;p&gt;One of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of ** Gambler’s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox ** etc. made this course much much more exciting than a normal Statistics Course.&lt;/p&gt;

&lt;p&gt;It will help you understand Discrete (Bernoulli, Binomial, Hypergeometric, Geometric, Negative Binomial, FS, Poisson) and Continuous (Uniform, Normal, expo, Beta, Gamma) Distributions and the stories behind them. Something that I was always afraid of.&lt;/p&gt;

&lt;p&gt;He got a textbook out based on this course which is clearly a great text:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li3&amp;tag=mlwhizcon-20&amp;linkId=7254baef925507e0d8dfd07cca2f519d&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=B00MMOJ19I&amp;Format=_SL250_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li3&amp;o=1&amp;a=B00MMOJ19I&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;h2 id=&#34;2-data-science-cs109-http-cs109-github-io-2015&#34;&gt;2. &lt;a href=&#34;http://cs109.github.io/2015/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Data Science CS109&lt;/a&gt;: -&lt;/h2&gt;

&lt;p&gt;Again by Professor Blitzstein. Again an awesome course. &lt;em&gt;Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts&lt;/em&gt;. You will learn about &lt;em&gt;Python Libraries&lt;/em&gt; like &lt;strong&gt;Numpy,Pandas&lt;/strong&gt; for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:&lt;/p&gt;

&lt;div style=&#34;color:black; background-color: #E9DAEE;&#34;&gt;
Learning from data in order to gain useful predictions and insights. This course introduces methods for five key facets of an investigation: data wrangling, cleaning, and sampling to get a suitable data set; data management to be able to access big data quickly and reliably; exploratory data analysis to generate hypotheses and intuition; prediction based on statistical methods such as regression and classification; and communication of results through visualization, stories, and interpretable summaries.
&lt;/div&gt;

&lt;h2 id=&#34;3-cs229-andrew-ng-https-click-linksynergy-com-fs-bin-click-id-lvarvwc5bd0-offerid-495576-248-type-3-subid-0&#34;&gt;3. &lt;a href=&#34;https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=495576.248&amp;amp;type=3&amp;amp;subid=0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;CS229: Andrew Ng&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;After doing these two above courses you will gain the status of what I would like to call a &lt;strong&gt;&amp;ldquo;Beginner&amp;rdquo;&lt;/strong&gt;. Congrats!!!. &lt;em&gt;You know stuff, you know how to implement stuff&lt;/em&gt;. Yet you do not fully understand all the math and grind that goes behind all this.&lt;/p&gt;

&lt;p&gt;Here comes the Game Changer machine learning course. Contains the maths behind many of the Machine Learning algorithms.  I will put this course as the &lt;em&gt;one course you gotta take&lt;/em&gt; as this course motivated me into getting in this field and Andrew Ng is a great instructor. Also this was the first course that I took.&lt;/p&gt;

&lt;p&gt;Also recently Andrew Ng Released a new Book. You can get the Draft chapters by subcribing on his website &lt;a href=&#34;http://www.mlyearning.org/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You are done with the three musketeers of the trade. You know Python, you understand Statistics and you have gotten the taste of the math behind ML approaches. Now it is time for the new kid on the block. D&amp;rsquo;artagnan. This kid has skills. While the three musketeers are masters in their trade, this guy brings qualities that adds a new freshness to our data science journey. Here comes Big Data for you.&lt;/p&gt;

&lt;h2 id=&#34;4-intro-to-hadoop-mapreduce-udacity-https-www-udacity-com-course-intro-to-hadoop-and-mapreduce-ud617&#34;&gt;4. &lt;a href=&#34;https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Intro to Hadoop &amp;amp; Mapreduce - Udacity&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Let us first focus on the literal elephant in the room - Hadoop.&lt;/em&gt; Short and Easy Course. Taught the Fundamentals of Hadoop streaming with Python. Taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.&lt;/p&gt;

&lt;p&gt;Once  you are done through this course you would have gained quite a basic understanding of concepts and you would have installed a Hadoop VM in your own machine. You would also have solved the Basic Wordcount Problem.
Read this amazing Blog Post from Michael Noll: &lt;a href=&#34;http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Writing An Hadoop MapReduce Program In Python - Michael G. Noll&lt;/a&gt;.  Just read the basic mapreduce codes. Don&amp;rsquo;t use Iterators and Generators yet. This has been a starting point for many of us Hadoop developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now try to solve these two problems from the CS109 Harvard course from 2013:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; First, grab the file word_list.txt from &lt;a href=&#34;https://raw.github.com/cs109/content/master/labs/lab8/word_list.txt&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.  This contains a list of six-letter words. To keep things simple, all of  the words consist of lower-case letters only.Write a mapreduce job that  finds all anagrams in word_list.txt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;B.&lt;/strong&gt; For the next problem, download the file &lt;a href=&#34;https://raw.github.com/cs109/content/master/labs/lab8/baseball_friends.csv&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;baseball_friends.csv&lt;/a&gt;. Each row of this csv file contains the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A person&amp;rsquo;s name&lt;/li&gt;
&lt;li&gt;The team that person is rooting for &amp;ndash; either &amp;ldquo;Cardinals&amp;rdquo; or &amp;ldquo;Red Sox&amp;rdquo;&lt;/li&gt;
&lt;li&gt;A list of that person&amp;rsquo;s friends, which could have arbitrary length&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;For  example:&lt;/em&gt; The first line tells us that Aaden is a Red Sox friend and he  has 65  friends, who are all listed here. For this problem, it&amp;rsquo;s safe to  assume  that all of the names are unique and that the friendship  structure is  symmetric (i.e. if Alannah shows up in Aaden&amp;rsquo;s friends list, then Aaden will show up in Alannah&amp;rsquo;s friends list).
Write  an mr job that lists each person&amp;rsquo;s name, their favorite  team, the  number of Red Sox fans they are friends with, and the number  of  Cardinals fans they are friends with.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Try to do this yourself.&lt;/strong&gt; Don&amp;rsquo;t use the mrjob (pronounced Mr. Job) way that  they use in the CS109 2013 class. Use the proper Hadoop Streaming way as taught in the Udacity class as it is much more customizable in the long run.&lt;/p&gt;

&lt;p&gt;If you are done with these, you can safely call yourself as someone who could &lt;strong&gt;&amp;ldquo;think in Mapreduce&amp;rdquo;&lt;/strong&gt; as how people like to call it.Try to do groupby, filter and joins using Hadoop. You can read up some good tricks from my blog:&lt;br&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/05/09/hadoop_mapreduce_streaming_tricks_and_techniques/&#34;&gt;Hadoop Mapreduce Streaming Tricks and Techniques&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are someone who likes learning from a book you can get:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Hadoop-Definitive-Storage-Analysis-Internet/dp/1491901632/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543345&amp;sr=1-1&amp;keywords=hadoop+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=e0a6c64497866b874326afa08a069654&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491901632&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491901632&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;5-spark-in-memory-big-data-tool&#34;&gt;5. Spark - In memory Big Data tool.&lt;/h2&gt;

&lt;p&gt;Now  comes the next part of your learning process. This should be undertaken after a little bit of experience with Hadoop. Spark will provide you with the speed and tools that Hadoop couldn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;Now Spark is used for data preparation as well as Machine learning purposes. I would encourage you to take a look at the series of courses on edX provided by Berkeley instructors. This course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.edx.org/xseries/data-science-engineering-apacher-sparktm&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Data Science and Engineering with Apache® Spark™&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have written a little bit about Basic data processing with Spark here. Take a look:
&lt;a href=&#34;https://mlwhiz.com/blog/2015/09/07/spark_basics_explained/&#34;&gt;Learning Spark using Python: Basics and Applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also take a look at some of the projects I did as part of course at &lt;a href=&#34;http://www.github.com/MLWhiz/Spark_Projects/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you would like a book to read:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491912766/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490543902&amp;sr=1-1&amp;keywords=spark+python&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=85591cf408de278e23e8570b7e9c284b&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1491912766&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1491912766&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t go through the courses, &lt;strong&gt;try solving the same two problems above that you solved by Hadoop using Spark too&lt;/strong&gt;. Otherwise the problem sets in the courses are more than enough.&lt;/p&gt;

&lt;h2 id=&#34;6-understand-linux-shell&#34;&gt;6. Understand Linux Shell:&lt;/h2&gt;

&lt;p&gt;Shell is a big friend for data scientists. It allows you to do simple data related tasks in the terminal itself. I couldn&amp;rsquo;t emphasize how much time shell saves for me everyday.&lt;/p&gt;

&lt;p&gt;Read these tutorials by me for doing that:&lt;br&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/10/09/shell_basics_for_data_science/&#34;&gt;Shell Basics every Data Scientist Should know -Part I&lt;/a&gt;
&lt;a href=&#34;https://mlwhiz.com/blog/2015/10/11/shell_basics_for_data_science_2/&#34;&gt;Shell Basics every Data Scientist Should know - Part II(AWK)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you would like a course you can go for &lt;a href=&#34;https://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1#!&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;this course on edX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want a book, go for:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Linux-Command-Line-Complete-Introduction/dp/1593273894/ref=as_li_ss_il?s=books&amp;ie=UTF8&amp;qid=1490544715&amp;sr=1-1&amp;keywords=the+linux+command+line&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=9f155a16f16c7ae34e682e0e0312ee8f&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1593273894&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1593273894&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Congrats you are an &amp;ldquo;Hacker&amp;rdquo; now.&lt;/strong&gt; You have got all the main tools in your belt to be a data scientist. On to more advanced topics. From here it depends on you what you want to learn. You may want to take a totally different approach than what I took going from here. There is no particular order. &lt;strong&gt;&amp;ldquo;All Roads lead to Rome&amp;rdquo;&lt;/strong&gt; as long as you are running.&lt;/p&gt;

&lt;h2 id=&#34;7-learn-statistical-inference-and-bayesian-statistics-https-click-linksynergy-com-fs-bin-click-id-lvarvwc5bd0-offerid-467035-204-type-3-subid-0&#34;&gt;7. &lt;a href=&#34;https://click.linksynergy.com/fs-bin/click?id=lVarvwc5BD0&amp;amp;offerid=467035.204&amp;amp;type=3&amp;amp;subid=0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Learn Statistical Inference and Bayesian Statistics&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I took the previous version of the specialization which was a single course taught by Mine Çetinkaya-Rundel. She is a great instrucor and explains the fundamentals of Statistical inference nicely. A must take course. You will learn about hypothesis testing, confidence intervals, and statistical inference methods for numerical and categorical data.
You can also use these books:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/dp/1943450056/ref=as_li_ss_il?m=A3EEBE82C3HYRD&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=cfd246ebddfde379bc01dcb2c467c199&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1943450056&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1943450056&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a href=&#34;https://www.amazon.com/Probability-Statistics-4th-Morris-DeGroot/dp/0321500466/ref=as_li_ss_il?ie=UTF8&amp;qid=1490547535&amp;sr=8-1&amp;keywords=degroot+statistics&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=fc106a3b8c56be8baf34793816762ec8&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=0321500466&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=0321500466&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;h2 id=&#34;8-deep-learning&#34;&gt;8. Deep Learning&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.fast.ai/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Intro&lt;/a&gt; - Making neural nets uncool again. An awesome Deep learning class from Kaggle Master Jeremy Howard. Entertaining and enlightening at the same time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://cs231n.github.io/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced&lt;/a&gt; - A series of notes from the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Bonus&lt;/a&gt; - A free online book by Michael Nielsen.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://amzn.to/2npItnM&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced Math Book&lt;/a&gt; - A math intensive book by Yoshua Bengio &amp;amp; Ian Goodfellow&lt;/p&gt;

&lt;h2 id=&#34;9-algorithms-graph-algorithms-recommendation-systems-pagerank-and-more-https-www-youtube-com-watch-v-xoa5v9ao7s0-list-pllsst5z-dsk9jdlct8t62vtzwyw9lnepv&#34;&gt;9. &lt;a href=&#34;https://www.youtube.com/watch?v=xoA5v9AO7S0&amp;amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Algorithms, Graph Algorithms, Recommendation Systems, Pagerank and More&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This course used to be there on Coursera but now only video links on youtube available.
You can learn from this book too:
&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a href=&#34;https://www.amazon.com/Mining-Massive-Datasets-Jure-Leskovec/dp/1107077230/ref=as_li_ss_il?ie=UTF8&amp;linkCode=li2&amp;tag=mlwhizcon-20&amp;linkId=ba893a022640a279d427fd0c5ea44c1a&#34; target=&#34;_blank&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=1107077230&amp;Format=_SL160_&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;https://ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=li2&amp;o=1&amp;a=1107077230&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Apart from that if you want to learn about Python and the basic intricacies of the language you can take the &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=495576.1921197134&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fcomputer-fundamentals&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Computer Science Mini Specialization from RICE university&lt;/a&gt;&lt;/strong&gt; too. This is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. If you work on this you will learn Object Oriented Programming,Graph algorithms and games in Python. Pretty cool stuff.&lt;/p&gt;

&lt;h2 id=&#34;10-advanced-maths&#34;&gt;10. Advanced Maths:&lt;/h2&gt;

&lt;p&gt;Couldn&amp;rsquo;t write enough of the importance of Math. But here are a few awesome resources that you can go for.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Linear Algebra By Gilbert Strang&lt;/a&gt; - A Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Multivariate Calculus - MIT OCW&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Convex Optimization&lt;/a&gt; - a MOOC on optimization from Stanford, by Steven Boyd, an authority on the subject.&lt;/p&gt;

&lt;p&gt;The Machine learning field is evolving and new advancements are made every day. That&amp;rsquo;s why I didn&amp;rsquo;t put a third tier. The maximum I can call myself is a &amp;ldquo;Hacker&amp;rdquo; and my learning continues. Hope you do the same.&lt;/p&gt;

&lt;p&gt;Hope you like this list. Please provide your inputs in comments on more learning resources as you see fit.&lt;/p&gt;

&lt;p&gt;Till then. Ciao!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top advice for a Data Scientist</title>
      <link>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/03/05/think_like_a_data_scientist/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/thinklikeds.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;A data scientist needs to be Critical and always on a lookout of something that misses others. So here are some advices that one can include in day to day data science work to be better at their work:&lt;/p&gt;

&lt;h2 id=&#34;1-beware-of-the-clean-data-syndrome&#34;&gt;1. Beware of the Clean Data Syndrome&lt;/h2&gt;

&lt;p&gt;You need to ask yourself questions even before you start working on the data. &lt;strong&gt;Does this data make sense?&lt;/strong&gt; Falsely assuming that the data is clean could lead you towards wrong Hypotheses. Apart from that, you can discern a lot of important patterns by looking at discrepancies in the data. For example, if you notice that a particular column has more than 50% values missing, you might think about not using the column. Or you may think that some of the data collection instrument has some error.&lt;/p&gt;

&lt;p&gt;Or let&amp;rsquo;s say you have a distribution of Male vs Female as 90:10 in a Female Cosmetic business. You may assume clean data and show the results as it is or you can use common sense and ask if the labels are switched.&lt;/p&gt;

&lt;h2 id=&#34;2-manage-outliers-wisely&#34;&gt;2. Manage Outliers wisely&lt;/h2&gt;

&lt;p&gt;Outliers can help you understand more about the people who are using your website/product 24 hours a day. But including them while building models will skew the models a lot.&lt;/p&gt;

&lt;h2 id=&#34;3-keep-an-eye-out-for-the-abnormal&#34;&gt;3. Keep an eye out for the Abnormal&lt;/h2&gt;

&lt;p&gt;Be on the &lt;strong&gt;lookout for something out of the obvious&lt;/strong&gt;. If you find something you may have hit gold.&lt;/p&gt;

&lt;p&gt;For example, &lt;a href=&#34;https://www.fastcompany.com/1783127/flickr-founders-glitch-can-game-wants-you-play-nice-be-blockbuster&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Flickr started up as a Multiplayer game&lt;/a&gt;. Only when the founders noticed that people were using it as a photo upload service, did they pivot.&lt;/p&gt;

&lt;p&gt;Another example: fab.com started up as fabulis.com, a site to help gay men meet people. One of the site&amp;rsquo;s popular features was the &amp;ldquo;Gay deal of the Day&amp;rdquo;. One day the deal was for Hamburgers - and half of the buyers were women. This caused the team to realize that there was a market for selling goods to women. So Fabulis pivoted to fab as a flash sale site for designer products.&lt;/p&gt;

&lt;h2 id=&#34;4-start-focussing-on-the-right-metrics&#34;&gt;4. Start Focussing on the right metrics&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beware of Vanity metrics&lt;/strong&gt; For example, # of active users by itself doesn&amp;rsquo;t divulge a lot of information. I would rather say &amp;ldquo;5% MoM increase in active users&amp;rdquo; rather than saying &amp;ldquo; 10000 active users&amp;rdquo;. Even that is a vanity metric as active users would always increase. I would rather keep a track of percentage of users that are active to know how my product is performing.&lt;/li&gt;
&lt;li&gt;Try to find out a &lt;strong&gt;metric that ties with the business goal&lt;/strong&gt;. For example, Average Sales/User for a particular month.&lt;/li&gt;
&lt;/ul&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;5-statistics-may-lie-too&#34;&gt;5. Statistics may lie too&lt;/h2&gt;

&lt;p&gt;Be critical of everything that gets quoted to you. Statistics has been used to lie in advertisements, in workplaces and a lot of other marketing venues in the past. People will do anything to get sales or promotions.&lt;/p&gt;

&lt;p&gt;For example: &lt;a href=&#34;http://marketinglaw.osborneclarke.com/retailing/colgates-80-of-dentists-recommend-claim-under-fire/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Do you remember Colgate’s claim that 80% of dentists recommended their brand?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This statistic seems pretty good at first. It turns out that at the time of surveying the dentists, they could choose several brands — not just one. So other brands could be just as popular as Colgate.&lt;/p&gt;

&lt;p&gt;Another Example: &lt;strong&gt;&amp;ldquo;99 percent Accurate&amp;rdquo; doesn&amp;rsquo;t mean shit&lt;/strong&gt;. Ask me to create a cancer prediction model and I could give you a 99 percent accurate model in a single line of code. How? Just predict &amp;ldquo;No Cancer&amp;rdquo; for each one. I will be accurate may be more than 99% of the time as Cancer is a pretty rare disease. Yet I have achieved nothing.&lt;/p&gt;

&lt;h2 id=&#34;6-understand-how-probability-works&#34;&gt;6. Understand how probability works&lt;/h2&gt;

&lt;p&gt;It happened during the summer of 1913 in a Casino in Monaco. Gamblers watched in amazement as a casino&amp;rsquo;s roulette wheel landed on black 26 times in a row. And since the probability of a Red vs Black is exactly half, they were certain that red was &amp;ldquo;due&amp;rdquo;. It was a field day for the Casino. A perfect example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gambler&#39;s_fallacy&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Gambler&amp;rsquo;s fallacy&lt;/a&gt;, aka the Monte Carlo fallacy.&lt;/p&gt;

&lt;p&gt;And This happens in real life. &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2538147&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;People tend to avoid long strings of the same answer&lt;/a&gt;. Sometimes sacrificing accuracy of judgment for the sake of getting a pattern of decisions that looks fairer or probable.&lt;/p&gt;

&lt;p&gt;For example, An admissions officer may reject the next application if he has approved three applications in a row, even if the application should have been accepted on merit.&lt;/p&gt;

&lt;h2 id=&#34;7-correlation-does-not-equal-causation&#34;&gt;7. Correlation Does Not Equal Causation&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/corr_caus.png&#34;  height=&#34;400&#34; width=&#34;500&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The Holy Grail of a Data scientist toolbox. To see something for what it is. Just because two variables move together in tandem doesn&amp;rsquo;t necessarily mean that one causes the another. There have been hilarious examples for this in the past. Some of my favorites are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Looking at the firehouse department data you infer that the more firemen are sent to a fire, the more damage is done.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When investigating the cause of crime in New York City in the 80s, an academic found a strong correlation between the amount of serious crime committed and the amount of ice cream sold by street vendors! Obviously, there was an unobserved variable causing both. Summers are when the crime is the greatest and when the most ice cream is sold. So Ice cream sales don&amp;rsquo;t cause crime. Neither crime increases ice cream sales.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;8-more-data-may-help&#34;&gt;8. More data may help&lt;/h2&gt;

&lt;p&gt;Sometimes getting extra data may work wonders. You might be able to model the real world more closely by looking at the problem from all angles. Look for extra data sources.&lt;/p&gt;

&lt;p&gt;For example, Crime data in a city might help banks provide a better credit line to a person living in a troubled neighborhood and in turn increase the bottom line.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning Algorithms for Data Scientists</title>
      <link>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2017/02/05/ml_algorithms_for_data_scientist/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/mlago_fords.png&#34;  height=&#34;400&#34; width=&#34;600&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;As a data scientist I believe that a lot of work has to be done before Classification/Regression/Clustering methods are applied to the data you get. The data which may be messy, unwieldy and big. So here are the list of algorithms that helps a data scientist to make better models using the data they have:&lt;/p&gt;

&lt;h2 id=&#34;1-sampling-algorithms-in-case-you-want-to-work-with-a-sample-of-data&#34;&gt;1. Sampling Algorithms. In case you want to work with a sample of data.&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Random Sampling :&lt;/strong&gt; &lt;em&gt;Say you want to select a subset of a population in which each member of the subset has an equal probability of being chosen.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stratified Sampling:&lt;/strong&gt; Assume that we need to estimate average number of votes for each candidate in an election. Assume that country has 3 towns : Town A has 1 million factory workers, Town B has 2 million workers and Town C has 3 million retirees. We can choose to get a random sample of size 60 over entire population but there is some chance that the random sample turns out to be not well balanced across these towns and hence is biased causing a significant error in estimation. Instead if we choose to take a random sample of 10, 20 and 30 from Town A, B and C respectively then we can produce a smaller error in estimation for the same total size of sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reservoir Sampling&lt;/strong&gt; :&lt;em&gt;Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from this stream such that each item is equally likely to be selected.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-map-reduce-if-you-want-to-work-with-the-whole-data&#34;&gt;2. &lt;strong&gt;Map-Reduce. If you want to work with the whole data&lt;/strong&gt;.&lt;/h2&gt;

&lt;p&gt;Can be used for feature creation. For Example: I had a use case where I had a graph of 60 Million customers and 130 Million accounts. Each account was connected to other account if they had the Same SSN or Same Name+DOB+Address. I had to find customer ID’s for each of the accounts. On a single node parsing such a graph took more than 2 days. On a Hadoop cluster of 80 nodes running a &lt;em&gt;Connected Component Algorithm&lt;/em&gt; took less than 24 minutes. On Spark it is even faster.&lt;/p&gt;

&lt;h2 id=&#34;3-graph-algorithms&#34;&gt;3. &lt;strong&gt;Graph Algorithms.&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Recently I was working on an optimization problem which was focussed on finding shortest distance and routes between two points in a store layout. Routes which don’t pass through different aisles, so we cannot use euclidean distances. We solved this problem by considering turning points in the store layout and the &lt;em&gt;djikstra’s Algorithm.&lt;/em&gt;&lt;/p&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;4-feature-selection&#34;&gt;4. &lt;strong&gt;Feature Selection.&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Univariate Selection.&lt;/strong&gt; Statistical tests can be used to select those features that have the strongest relationship with the output variable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VarianceThreshold.&lt;/strong&gt; Feature selector that removes all low-variance features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive Feature Elimination.&lt;/strong&gt; The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature Importance:&lt;/strong&gt; Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;5-algorithms-to-work-efficiently&#34;&gt;5. &lt;strong&gt;Algorithms to work efficiently.&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Apart from these above algorithms sometimes you may need to write your own algorithms. Now I think of big algorithms as a combination of small but powerful algorithms. You just need to have idea of these algorithms to make a more better/efficient product. So some of these powerful algorithms which can help you are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recursive Algorithms:&lt;/strong&gt;Binary search algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Divide and Conquer Algorithms:&lt;/strong&gt; Merge-Sort.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Programming:&lt;/strong&gt;Solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;6-classification-regression-algorithms-the-usual-suspects-minimum-you-must-know&#34;&gt;6. &lt;strong&gt;Classification/Regression Algorithms.&lt;/strong&gt; The usual suspects. Minimum you must know:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression -&lt;/strong&gt; Ridge Regression, Lasso Regression, ElasticNet&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;From there you can build upon:

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Decision Trees -&lt;/strong&gt; ID3, CART, C4.5, C5.0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KNN&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVM&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ANN&lt;/strong&gt; - Back Propogation, CNN&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;And then on to Ensemble based algorithms:

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Boosting&lt;/strong&gt;: Gradient Boosted Trees&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bagging&lt;/strong&gt;: Random Forests&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blending&lt;/strong&gt;: Prediction outputs of different learning algorithms are fed into another learning algorithm.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;7-clustering-methods-for-unsupervised-learning&#34;&gt;7 . &lt;strong&gt;Clustering Methods.&lt;/strong&gt;For unsupervised learning.&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;k-Means&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;k-Medians&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expectation Maximisation (EM)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical Clustering&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;8-other-algorithms-you-can-learn-about&#34;&gt;8. &lt;strong&gt;Other algorithms you can learn about:&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Apriori algorithm&lt;/strong&gt;- Association Rule Mining&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eclat algorithm -&lt;/strong&gt; Association Rule Mining&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Item/User Based Similarity -&lt;/strong&gt; Recommender Systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reinforcement learning -&lt;/strong&gt; Build your own robot.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Graphical Models&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian Algorithms&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP -&lt;/strong&gt; For language based models. Chatbots.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hope this has been helpful&amp;hellip;..&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Things to see while buying a Mutual Fund</title>
      <link>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2016/12/24/mutual_fund_ratios/</guid>
      <description>

&lt;p&gt;This is a post which deviates from my pattern fo blogs that I have wrote till now but I found that Finance also uses up a lot of Statistics. So it won&amp;rsquo;t be a far cry to put this on my blog here. I recently started investing in Mutual funds so thought of rersearching the area before going all in. Here is the result of some of my research.&lt;/p&gt;

&lt;h2 id=&#34;1-load-no-load&#34;&gt;1. Load/No-Load:&lt;/h2&gt;

&lt;p&gt;Always Buy No Load Mutual Funds&lt;/p&gt;

&lt;h2 id=&#34;2-regular-direct&#34;&gt;2. Regular/Direct:&lt;/h2&gt;

&lt;p&gt;There are many differenct sites from where you can buy Mutual funds. Most of these sites take a commision to let you the investor buy and sell from their platform. To overcome this commision you can buy direct Mutual funds from the fund houses themselves. But that would be difficult as their are a lot of fund houses and mmanaging all of that could be quite painful. But with the advent of MFUtility you can buy direct plans from the same platform.&lt;/p&gt;

&lt;h2 id=&#34;3-expense-ratios&#34;&gt;3. Expense Ratios:&lt;/h2&gt;

&lt;p&gt;The expense ratio is a measure of what it costs an investment company to operate a mutual fund.
To see how expense ratios can affect your investments over time, let’s compare the returns of several hypothetical investments that differ only in expense ratio. The following table depicts the returns on a 10,000 initial investment, assuming an average annualized gain of 10%, with different expense ratios (0.5%, 1%, 1.5%, 2% and 2.5%):&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/fund_expense_ratio.jpg&#34;  height=&#34;400&#34; width=&#34;500&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;As the table illustrates, even a small difference in expense ratio can cost you a lot of money in the long run. If you had invested 10,000 in the fund with a 2.5% expense ratio, the value of your fund would be 46,022 after 20 years. Had you instead invested your 10,000 in the fund with a lower, 0.5% expense ratio, your investment would be worth $61,159 after two decades, a 0.33% improvement over the more expensive fund. Keep in mind, this hypothetical example examines funds whose only differences are the expense ratios: all other variables, including initial investment and annualized gains, remain constant (for the example, we must assume identical taxation as well). While two funds are not likely to have the exact same performance over a 20-year period, the table illustrates the effects that small changes in expense ratio can have on your long-term returns.&lt;/p&gt;

&lt;script src=&#34;//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=c4ca54df-6d53-4362-92c0-13cb9977639e&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;4-avoid-mutual-funds-with-high-turnover-ratios&#34;&gt;4. Avoid Mutual Funds With High Turnover Ratios:&lt;/h2&gt;

&lt;p&gt;Mutual fund turnover is calculated as the value of all transactions (buying, selling) divided by two, then divided by a fund&amp;rsquo;s total holdings. In simpler terms, mutual fund turnover typically measures the replacement of holdings in a mutual fund, and is commonly presented to investors as a percentage over a one year period. If a fund has 100% turnover, the fund replaces all of its holdings over a 12-month period and that bears cost to the investment company in terms of brokerage etc.&lt;/p&gt;

&lt;h2 id=&#34;5-look-for-ample-diversification-of-assets&#34;&gt;5. Look for Ample Diversification of Assets:&lt;/h2&gt;

&lt;p&gt;Simply owning four different mutual funds specializing in the financial sector (shares of banks, insurance companies, etc.) is not diversification. Don’t own funds that make heavy sector or industry bets. If you choose to despite this warning, make sure that you don’t have a huge portion of your funds invested in them. If it’s a bond fund, you typically want to avoid bets on the direction of interest rates as this is rank speculation.&lt;/p&gt;

&lt;h2 id=&#34;6-not-same-fund-family&#34;&gt;6. Not Same Fund Family:&lt;/h2&gt;

&lt;p&gt;Don’t keep all of your funds within the same fund family. Witness the mutual fund scandal of a few years ago where portfolio management at many firms allowed big traders to market time the funds, essentially stealing money from smaller investors. By spreading your assets out at different companies, you can mitigate the risk of internal turmoil, ethics breaches, and other localized problems.&lt;/p&gt;

&lt;h2 id=&#34;7-keep-track-of-various-risk-ratios&#34;&gt;7. Keep Track of various Risk Ratios:&lt;/h2&gt;

&lt;h3 id=&#34;a-standard-deviation&#34;&gt;a. &lt;strong&gt;&lt;strong&gt;Standard deviation:&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Standard deviation (SD) measures the volatility the fund&amp;rsquo;s returns in relation to its average. It tells
you how much the fund&amp;rsquo;s return can deviate from the historical mean return of the scheme. If a fund
has a 12% average rate of return and a standard deviation of 4%, its return will range from 8-16%&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Standard Deviation (SD) = Square root of Variance (V)&lt;/p&gt;

&lt;p&gt;Variance = (Sum of squared difference between each monthly return and its mean / number of monthly return data – 1)&lt;/p&gt;

&lt;h3 id=&#34;b-r-squared&#34;&gt;b. &lt;strong&gt;&lt;strong&gt;R-Squared:&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;R-Squared measures the relationship between a portfolio and its benchmark. It can be thought of as a percentage from 1 to 100. R-squared is not a measure of the performance of a portfolio. A great portfolio can have a very low R-squared. It is simply a measure of the correlation of the portfolio&amp;rsquo;s returns to the benchmark&amp;rsquo;s returns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;R-Squared = Square of Correlation&lt;/p&gt;

&lt;p&gt;Correlation(xy)= Covariance between index and portfolio/(Standard deviation of portfolio * standard deviation of index)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Significance:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you want a portfolio that moves like the benchmark, you&amp;rsquo;d want a portfolio with a high Rsquared.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you want a portfolio that doesn&amp;rsquo;t move at all like the benchmark, you&amp;rsquo;d want a low R-squared.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;General Range for R-Squared:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;70-100% = good correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;40-70% = average correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1-40% = low correlation between the portfolio&amp;rsquo;s returns and the benchmark&amp;rsquo;s returns&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Index funds will have an R-squared very close to 100.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;R-squared can be used to ascertain the significance of a particular beta or alpha. Generally, a higher R-squared will indicate a more useful beta figure. If the R-squared is lower, then the beta is less relevant to the fund&amp;rsquo;s performance&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Values range from 1 (returns are explained 100% by the market) to 0 (returns bear no association with the market)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;c-beta&#34;&gt;c. &lt;strong&gt;&lt;strong&gt;Beta:&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;A beta of 1.0 indicates that the investment&amp;rsquo;s price will move in lock-step with the market.&lt;/p&gt;

&lt;p&gt;A beta of less than 1.0 indicates that the investment will be less volatile than the market, and, correspondingly, a beta of more than 1.0 indicates that the investment&amp;rsquo;s price will be more volatile than the market.&lt;/p&gt;

&lt;p&gt;For example, if a fund portfolio&amp;rsquo;s beta is 1.2, it&amp;rsquo;s theoretically 20% more volatile than the market. Conservative investors looking to preserve capital should focus on securities and fund portfolios with low betas, whereas those investors willing to take on more risk in search of higher returns should look for high beta investments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Beta = (Standard Deviation of Fund x R-Square) / Standard Deviation of Benchmark&lt;/p&gt;

&lt;p&gt;If a fund has a beta of 1.5, it means that for every 10% upside or downside, the fund&amp;rsquo;s NAV would be 15% in the respective direction.&lt;/p&gt;

&lt;h3 id=&#34;d-jensens-alpha&#34;&gt;d. &lt;strong&gt;&lt;strong&gt;Jensens Alpha:&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Alpha is a measure of an investment&amp;rsquo;s performance on a risk-adjusted basis.&lt;/p&gt;

&lt;p&gt;Simply stated, alpha is often considered to represent the value that a portfolio manager adds or subtracts from a fund portfolio&amp;rsquo;s return.&lt;/p&gt;

&lt;p&gt;A positive alpha of 1.0 means the fund has outperformed its benchmark index by 1%. Correspondingly, a similar negative alpha would indicate an underperformance of 1%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Alpha = {(Fund return-Risk free return) – (Funds beta) *(Benchmark return- risk free return)}&lt;/p&gt;

&lt;p&gt;For example, assume a mutual fund realized a return of 15% last year. The appropriate market index for this fund returned 12%. The beta of the fund versus that same index is 1.2 and the risk-free rate is 3%. The fund&amp;rsquo;s alpha is calculated as:&lt;/p&gt;

&lt;p&gt;Alpha = {(15 -3) – (1.2) *(12- 3)} = 12 - 9 x 1.2 = 12-10.8 = 1.2&lt;/p&gt;

&lt;p&gt;Given a beta of 1.2, the mutual fund is expected to be riskier than the index, and thus earn more. A positive alpha in this example shows that the mutual fund manager earned more than enough return to be compensated for the risk he took over the course of the year. If the mutual fund only returned 13%, the calculated alpha would be -0.8. With a negative alpha, the mutual fund manager would not have earned enough return given the amount of risk he was taking.&lt;/p&gt;

&lt;h3 id=&#34;e-sharpe-ratio&#34;&gt;e. &lt;strong&gt;&lt;strong&gt;Sharpe Ratio:&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Sharpe Ratio measures how well the fund has performed vis-a vis the risk taken by it. It is the excess return over risk-free return (usually return from treasury bills or government securities) divided by the standard deviation. The higher the Sharpe Ratio, the better the fund has performed in proportion to the risk taken by it.
The Sharpe ratio is also known as Reward-to-Variability ratio and it is named after William Forsyth Sharpe.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SR = (Total Return – Risk Free Rate) / SD Of Fund&lt;/p&gt;

&lt;p&gt;For example: Your investor gets 7 per cent return on her investment in a scheme with a standard deviation/volatility of 0.5. We assume risk free rate is 5 per cent.
Sharpe Ratio is 7-&lt;sup&gt;5&lt;/sup&gt;&amp;frasl;&lt;sub&gt;0&lt;/sub&gt;.5 = 4 in this case&lt;/p&gt;

&lt;h2 id=&#34;8-and-finally-always-dollar-cost-average&#34;&gt;8. And Finally Always Dollar-Cost Average:&lt;/h2&gt;

&lt;p&gt;Dollar cost averaging is a technique designed to reduce market risk through the systematic purchase of securities at predetermined intervals and set amounts.Instead of investing assets in a lump sum, the investor works his way into a position by slowly buying smaller amounts over a longer period of time. This spreads the cost basis out over several years, providing insulation against changes in market price.&lt;/p&gt;

&lt;p&gt;Every investor investment strategy differs. These are just some common guidelines to work your way through the market and making informed decisions while buying Mutual Funds.
Normally I work through points 1-6 and get my list to a few mutual funds after which I generally use risk ratios to determine which of the funds I selected might be a winner.
I have a bias towards long term investing when it comes to investing so whatever I wrote here must be taken with a grain of salt just as everything related to investment must be.  Some of you who are doing this for a longer time than I can also tell me about the various other things I can do.
I will try to include those ideas in this post as well.&lt;/p&gt;

&lt;p&gt;To Learn more about Mutual funds and investing in general, take a look at the following two gems:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;
&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/0060555661/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0060555661&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=59d5b0af035ad4ba7eda55548194a638&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=0060555661&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=0060555661&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a target=&#34;_blank&#34;  href=&#34;https://www.amazon.com/gp/product/0470138130/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0470138130&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=2f78b02ff1a38e3383c5a8cff52f2a9a&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=0470138130&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=0470138130&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;The Editorial review of The intelligent Investor says &amp;ldquo;Among the library of investment books promising no-fail strategies for riches, Benjamin Graham&amp;rsquo;s classic, The Intelligent Investor, offers no guarantees or gimmicks but overflows with the wisdom at the core of all good portfolio management&amp;rdquo; and it rings true in every sense. A must read for everyone looking to invest seriously.&lt;/p&gt;

&lt;p&gt;Common Sense on Mutual Funds focusses on Mutual funds exclusively. Lets you understand that investing is not difficult. For the not so involved reader.&lt;/p&gt;

&lt;p&gt;Till than Ciao!!!&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.thebalance.com/picking-winning-mutual-funds-357957&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;https://www.thebalance.com/picking-winning-mutual-funds-357957&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.miraeassetmf.co.in/uploads/TermofWeek/Sharpe_Ratio.pdf&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;http://www.miraeassetmf.co.in/uploads/TermofWeek/Sharpe_Ratio.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.miraeassetmf.co.in/uploads/TermofWeek/Beta_SD_RSquared.pdf&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;http://www.miraeassetmf.co.in/uploads/TermofWeek/Beta_SD_RSquared.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.investopedia.com&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;http://www.investopedia.com&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Behold the power of MCMC</title>
      <link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link>
      <pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/mcmc.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.&lt;/p&gt;

&lt;p&gt;So MCMC Methods may sound interesting to some (for these what follows is a treat) and for those who don&amp;rsquo;t really appreciate MCMC till now, I hope I will be able to pique your interest by the end of this blog post.&lt;/p&gt;

&lt;p&gt;So here goes. This time we will cover some applications of MCMC in various areas of Computer Science using Python. If you feel the problems difficult to follow with, I would advice you to go back and read the &lt;a href=&#34;https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_beta_distribution/&#34;&gt;previous post&lt;/a&gt;, which tries to explain MCMC Methods. We Will try to solve the following two problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Breaking the Code&lt;/strong&gt; - This problem has got somewhat of a great pedigree as this method was suggested by Persi Diaconis- The Mathemagician. So Someone comes to you with the below text. This text looks like gibberish but this is a code, Could you decrypyt it?&lt;br&gt;&lt;br&gt;
&lt;em&gt;XZ STAVRK HXVR MYAZ OAKZM JKSSO SO MYR OKRR XDP JKSJRK XBMASD SO YAZ TWDHZ  MYR JXMBYNSKF BSVRKTRM NYABY NXZ BXKRTRZZTQ OTWDH SVRK MYR AKSD ERPZMRXP  KWZMTRP  MYR JXTR OXBR SO X QSWDH NSIXD NXZ KXAZRP ORRETQ OKSI MYR JATTSN  XDP X OXADM VSABR AIJRKORBMTQ XKMABWTXMRP MYR NSKPZ  TRM IR ZRR MYR BYATP  XDP PAR  MYR ZWKHRSD YXP ERRD ZAMMADH NAMY YAZ OXBR MWKDRP MSNXKPZ MYR OAKR  HAVADH MYR JXTIZ SO YAZ YXDPZ X NXKI XDP X KWE XTMRKDXMRTQ  XZ MYR QSWDH NSIXD ZJSFR  YR KSZR  XDP XPVXDBADH MS MYR ERP Z YRXP  ZXAP  NAMY ISKR FADPDRZZ MYXD IAHYM YXVR ERRD RGJRBMRP SO YAI&lt;/em&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;The Knapsack Problem&lt;/strong&gt; - This problem comes from &lt;a href=&#34;http://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Probability&lt;/a&gt; by Joseph Blitzstein. You should check out his courses &lt;a href=&#34;http://projects.iq.harvard.edu/stat110/handouts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;STAT110&lt;/a&gt; and &lt;a href=&#34;http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CS109&lt;/a&gt; as they are awesome. Also as it turns out Diaconis was the advisor of Joseph. So you have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution? This is known as the Knapsack Problem in Computer Science.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;breaking-the-code&#34;&gt;Breaking the Code&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/security.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;So we look at the data and form a hypothesis that the data has been scrambled using a Substitution Cipher. We don&amp;rsquo;t know the encryption key, and we would like to know the Decryption Key so that we can decrypt the data and read the code.&lt;/p&gt;

&lt;p&gt;To create this example, this data has actually been taken from Oliver Twist. We scrambled the data using a random encryption key, which we forgot after encrypting and we would like to decrypt this encrypted text using MCMC Chains. The real decryption key actually is &amp;ldquo;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;rdquo;&lt;/p&gt;

&lt;p&gt;So lets think about this problem for a little bit. The decryption key could be any 26 letter string with all alphabets appearing exactly once. How many string permutations are there like that? That number would come out to be $26! \approx 10^{26}$ permutations. That is a pretty large number. If we go for using a brute force approach we are screwed.
So what could we do? MCMC Chains come to rescue.&lt;/p&gt;

&lt;p&gt;We will devise a Chain whose states theoritically could be any of these permutations. Then we will:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start by picking up a random current state.&lt;/li&gt;
&lt;li&gt;Create a proposal for a new state by swapping two random letters in the current state.&lt;/li&gt;
&lt;li&gt;Use a Scoring Function which calculates the score of the current state $Score_C$ and the proposed State $Score_P$.&lt;/li&gt;
&lt;li&gt;If the score of the proposed state is more than current state, Move to Proposed State.&lt;/li&gt;
&lt;li&gt;Else flip a coin which has a probability of Heads $Score_P/Score_C$. If it comes heads move to proposed State.&lt;/li&gt;
&lt;li&gt;Repeat from 2nd State.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we get lucky we may reach a steady state where the chain has the stationary distribution of the needed states and the state that the chain is at could be used as a solution.&lt;/p&gt;

&lt;p&gt;So the Question is what is the scoring function that we will want to use. We want to use a scoring function for each state(Decryption key) which assigns a positive score to each decryption key. This score intuitively should be more if the encrypted text looks more like actual english if decrypted using this decryption key.&lt;/p&gt;

&lt;p&gt;So how can we quantify such a function. We will check a long text and calculate some statistics. See how many times one alphabet comes after another in a legitimate long text like War and Peace. For example we want to find out how many times does &amp;lsquo;BA&amp;rsquo; appears in the text or how many times &amp;lsquo;TH&amp;rsquo; occurs in the text.&lt;/p&gt;

&lt;p&gt;For each pair of characters $\beta_1$ and $\beta_2$ (e.g. $\beta_1$ = T and $\beta_2$ =H), we let $R(\beta_1,\beta_2)$ record the number of times that specific pair(e.g. &amp;ldquo;TH&amp;rdquo;) appears consecutively in the reference text.&lt;/p&gt;

&lt;p&gt;Similarly, for a putative decryption key x, we let $F_x(\beta_1,\beta_2)$ record the number of times that
pair appears when the cipher text is decrypted using the decryption key x.&lt;/p&gt;

&lt;p&gt;We then Score a particular decryption key x using:&lt;/p&gt;

&lt;div&gt;$$Score(x) = \prod R(\beta_1,\beta_2)^{F_x(\beta_1,\beta_2)}$$&lt;/div&gt;

&lt;p&gt;This function can be thought of as multiplying, for each consecutive pair of letters in the decrypted
text, the number of times that pair occurred in the reference text.  Intuitively, the score function
is higher when the pair frequencies in the decrypted text most closely match those of the reference
text,  and  the  decryption  key  is  thus  most  likely  to  be  correct.&lt;/p&gt;

&lt;p&gt;To make life easier with calculations we will calculate $log(Score(x))$&lt;/p&gt;

&lt;p&gt;So lets start working through the problem step by step.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# AIM: To Decrypt a text using MCMC approach. i.e. find decryption key which we will call cipher from now on.&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; string
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; math
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes as input a decryption key and creates a dict for key where each letter in the decryption key&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# maps to a alphabet For example if the decryption key is &amp;#34;DGHJKL....&amp;#34; this function will create a dict like {D:A,G:B,H:C....} &lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_cipher_dict&lt;/span&gt;(cipher):
    cipher_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
    alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_uppercase)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(cipher)):
        cipher_dict[alphabet_list[i]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cipher[i]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; cipher_dict

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes a text and applies the cipher/key on the text and returns text.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;apply_cipher_on_text&lt;/span&gt;(text,cipher):
    cipher_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_cipher_dict(cipher) 
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(text)
    newtext &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; elem &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; text:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; elem&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper() &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cipher_dict:
            newtext&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;cipher_dict[elem&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()]
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            newtext&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; newtext

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes as input a path to a long text and creates scoring_params dict which contains the &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# number of time each pair of alphabet appears together&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..}&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_scoring_params_dict&lt;/span&gt;(longtext_path):
    scoring_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
    alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_uppercase)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(longtext_path) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; fp:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; line &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; fp:
            data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(line&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip())
            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(data)&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
                alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()
                alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;:
                    alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;:
                    alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;
                key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; alpha_i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;alpha_j
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; scoring_params:
                    scoring_params[key]&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
                &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                    scoring_params[key]&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; scoring_params

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes as input a text and creates scoring_params dict which contains the &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# number of time each pair of alphabet appears together&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Ex. {&amp;#39;AB&amp;#39;:234,&amp;#39;TH&amp;#39;:2343,&amp;#39;CD&amp;#39;:23 ..}&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;score_params_on_cipher&lt;/span&gt;(text):
    scoring_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
    alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_uppercase)
    data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(data)&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
        alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data[i]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()
        alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;:
            alpha_i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; alphabet_list &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;:
            alpha_j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;
        key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; alpha_i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;alpha_j
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; scoring_params:
            scoring_params[key]&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            scoring_params[key]&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; scoring_params

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes the text to be decrypted and a cipher to score the cipher.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# This function returns the log(score) metric&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_cipher_score&lt;/span&gt;(text,cipher,scoring_params):
    cipher_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_cipher_dict(cipher)
    decrypted_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; apply_cipher_on_text(text,cipher)
    scored_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; score_params_on_cipher(decrypted_text)
    cipher_score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k,v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; scored_f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iteritems():
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; k &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; scoring_params:
            cipher_score &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log(scoring_params[k])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; cipher_score

&lt;span style=&#34;color:#75715e&#34;&gt;# Generate a proposal cipher by swapping letters at two random location&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;generate_cipher&lt;/span&gt;(cipher):
    pos1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(list(cipher))&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    pos2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(list(cipher))&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; pos1 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; pos2:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; generate_cipher(cipher)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(cipher)
        pos1_alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cipher[pos1]
        pos2_alpha &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cipher[pos2]
        cipher[pos1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pos2_alpha
        cipher[pos2] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pos1_alpha
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(cipher)

&lt;span style=&#34;color:#75715e&#34;&gt;# Toss a random coin with robability of head p. If coin comes head return true else false.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;random_coin&lt;/span&gt;(p):
    unif &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; unif&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt;p:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; False
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; True
    
&lt;span style=&#34;color:#75715e&#34;&gt;# Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the last few states &lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MCMC_decrypt&lt;/span&gt;(n_iter,cipher_text,scoring_params):
    current_cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_uppercase &lt;span style=&#34;color:#75715e&#34;&gt;# Generate a random cipher to start&lt;/span&gt;
    state_keeper &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; set()
    best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n_iter):
        state_keeper&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add(current_cipher)
        proposed_cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; generate_cipher(current_cipher)
        score_current_cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_cipher_score(cipher_text,current_cipher,scoring_params)
        score_proposed_cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_cipher_score(cipher_text,proposed_cipher,scoring_params)
        acceptance_probability &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(score_proposed_cipher&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;score_current_cipher))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; score_current_cipher&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;score:
            best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_cipher
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; random_coin(acceptance_probability):
            current_cipher &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; proposed_cipher
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iter&amp;#34;&lt;/span&gt;,i,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;,apply_cipher_on_text(cipher_text,current_cipher)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; state_keeper,best_state

&lt;span style=&#34;color:#75715e&#34;&gt;## Run the Main Program:&lt;/span&gt;

scoring_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_scoring_params_dict(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;war_and_peace.txt&amp;#39;&lt;/span&gt;)

plain_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;As Oliver gave this first proof of the free and proper action of his lungs, &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;the patchwork coverlet which was carelessly flung over the iron bedstead, rustled; &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;the pale face of a young woman was raised feebly from the pillow; and a faint voice imperfectly &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;articulated the words, Let me see the child, and die. &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;The surgeon had been sitting with his face turned towards the fire: giving the palms of his hands a warm &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;and a rub alternately. As the young woman spoke, he rose, and advancing to the bed&amp;#39;s head, said, with more kindness &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;than might have been expected of him: &amp;#34;&lt;/span&gt;

encryption_key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;XEBPROHYAUFTIDSJLKZMWVNGQC&amp;#34;&lt;/span&gt;
cipher_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; apply_cipher_on_text(plain_text,encryption_key)
decryption_key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ICZNBKXGMPRQTWFDYEOLJVUAHS&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Text To Decode:&amp;#34;&lt;/span&gt;, cipher_text
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
states,best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MCMC_decrypt(&lt;span style=&#34;color:#ae81ff&#34;&gt;10000&lt;/span&gt;,cipher_text,scoring_params)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Decoded Text:&amp;#34;&lt;/span&gt;,apply_cipher_on_text(cipher_text,best_state)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MCMC KEY FOUND:&amp;#34;&lt;/span&gt;,best_state
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ACTUAL DECRYPTION KEY:&amp;#34;&lt;/span&gt;,decryption_key&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/result1_MCMC.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;This chain converges around the 2000th iteration and we are able to unscramble the code. That&amp;rsquo;s awesome!!!
Now as you see the MCMC Key found is not exactly the encryption key. So the solution is not a deterministic one, but we can see that it does not actually decrease any of the value that the MCMC Methods provide. Now Lets Help Bilbo :)&lt;/p&gt;

&lt;h2 id=&#34;the-knapsack-problem&#34;&gt;The Knapsack Problem&lt;/h2&gt;

&lt;p&gt;Restating, we have Bilbo a Thief who goes to Smaug&amp;rsquo;s Lair. He finds M treasures. Each treasure has some Weight and some Gold value. But Bilbo cannot really take all of that. He could only carry a certain Maximum Weight. But being a smart hobbit, he wants to Maximize the value of the treasures he takes. Given the values for weights and value of the treasures and the maximum weight that Bilbo could carry, could you find a good solution?&lt;/p&gt;

&lt;p&gt;So in this problem we have an $1$x$M$ array of Weight Values W, Gold Values G and a value for the maximum weight $w_{MAX}$ that Bilbo can carry.
We want to find out an $1$x$M$ array $X$ of 1&amp;rsquo;s and 0&amp;rsquo;s, which holds weather Bilbo Carries a particular treasure or not.
This array needs to follow the constraint $WX^T &amp;lt; w_{MAX}$ and we want to maximize $GX^T$ for a particular state X.(Here the T means transpose)&lt;/p&gt;

&lt;p&gt;So lets first discuss as to how we will create a proposal from a previous state.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Pick a random index from the state and toggle the index value.&lt;/li&gt;
&lt;li&gt;Check if we satisfy our constraint. If yes this state is the proposal state.&lt;/li&gt;
&lt;li&gt;Else pick up another random index and repeat.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also need to think about the Scoring Function.
We need to give high values to states with high gold value. We will use:
&lt;br&gt;&lt;/p&gt;

&lt;div&gt;$$Score(X)=e^{\beta GX^T}$$&lt;/div&gt;

&lt;p&gt;We give exponentially more value to higher score. The Beta here is a +ve constant. But how to choose it? If $\beta$ is big we will give very high score to good solutions and the chain will not be able to try new solutions as it can get stuck in local optimas. If we give a small value the chain will not converge to very good solutions. So weuse an Optimization Technique called &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Simulated_annealing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Simulated Annealing&lt;/a&gt;&lt;/strong&gt; i.e. we will start with a small value of $\beta$ and increase as no of iterations go up.
That way the chain will explore in the starting stages and stay at the best solution in the later stages.&lt;/p&gt;

&lt;p&gt;So now we have everything we need to get started&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

W &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;67&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;33&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;23&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt;]
G &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;420&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;610&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;112&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;341&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;435&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;657&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;363&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;273&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;812&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;534&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;356&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;223&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;516&lt;/span&gt;]
W_max &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;150&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes a state X , The gold vector G and a Beta Value and return the Log of score&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;score_state_log&lt;/span&gt;(X,G,Beta):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Beta&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(X,G)

&lt;span style=&#34;color:#75715e&#34;&gt;# This function takes as input a state X and the number of treasures M, The weight vector W and the maximum weight W_max&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# and returns a proposal state&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_proposal&lt;/span&gt;(X,W,W_max):
    M &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
    random_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randint(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,M&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;#print random_index&lt;/span&gt;
    proposal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(X)
    proposal[random_index] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; proposal[random_index]  &lt;span style=&#34;color:#75715e&#34;&gt;#Toggle&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;#print proposal&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(proposal,W)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;W_max:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; proposal
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; create_proposal(X,W,W_max)
    
&lt;span style=&#34;color:#75715e&#34;&gt;# Takes as input a text to decrypt and runs a MCMC algorithm for n_iter. Returns the state having maximum score and also&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the last few states &lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MCMC_Golddigger&lt;/span&gt;(n_iter,W,G,W_max, Beta_start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.05&lt;/span&gt;, Beta_increments&lt;span style=&#34;color:#f92672&#34;&gt;=.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt;):
    M &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(W)
    Beta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Beta_start
    current_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;M &lt;span style=&#34;color:#75715e&#34;&gt;# We start with all 0&amp;#39;s&lt;/span&gt;
    state_keeper &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    score &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n_iter):
        state_keeper&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(current_X)
        proposed_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; create_proposal(current_X,W,W_max)

        score_current_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; score_state_log(current_X,G,Beta)
        score_proposed_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; score_state_log(proposed_X,G,Beta)
        acceptance_probability &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(score_proposed_X&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;score_current_X))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; score_current_X&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;score:
            best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; current_X
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; random_coin(acceptance_probability):
            current_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; proposed_X
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
            Beta &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; Beta_increments 
        &lt;span style=&#34;color:#75715e&#34;&gt;# You can use these below two lines to tune value of Beta&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;#if i%20==0:&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;#    print &amp;#34;iter:&amp;#34;,i,&amp;#34; |Beta=&amp;#34;,Beta,&amp;#34; |Gold Value=&amp;#34;,np.dot(current_X,G)&lt;/span&gt;
            
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; state_keeper,best_state&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running the Main program:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;max_state_value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; 
Solution_MCMC &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;):
    state_keeper,best_state &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MCMC_Golddigger(&lt;span style=&#34;color:#ae81ff&#34;&gt;50000&lt;/span&gt;,W,G,W_max,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0005&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0005&lt;/span&gt;)
    state_value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(best_state,G)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; state_value&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;max_state_value:
        max_state_value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; state_value
        Solution_MCMC &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; best_state

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MCMC Solution is :&amp;#34;&lt;/span&gt; , str(Solution_MCMC) , &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;with Gold Value:&amp;#34;&lt;/span&gt;, str(max_state_value)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre style=&#34;font-family:courier new,monospace; background-color:#f6c6529c; color:#000000&#34;&gt;
MCMC Solution is : [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0] with Gold Value: 2435
&lt;/pre&gt;

&lt;p&gt;Now I won&amp;rsquo;t say that this is the best solution. The deterministic solution using DP will be the best for such use case but sometimes when the problems gets large, having such techniques at disposal becomes invaluable.&lt;/p&gt;

&lt;p&gt;So tell me What do you think about MCMC Methods?&lt;/p&gt;

&lt;p&gt;Also, If you find any good applications or would like to apply these techniques to some area, I would really be glad to know about them and help if possible.&lt;/p&gt;

&lt;p&gt;The codes for both examples are sourced at &lt;a href=&#34;https://github.com/MLWhiz/MCMC_Project&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;references-and-sources&#34;&gt;References and Sources:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Probability Joseph K Blitzstein, Jessica Hwang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://statweb.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;The Markov Chain Monte Carlo Revolution, Persi Diaconis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.utstat.toronto.edu/wordpress/WSFiles/technicalreports/1005.pdf&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Decrypting Classical Cipher Text Using Markov Chain Monte Carlo, Jian Chen and Jeffrey S. Rosenthal&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One of the newest and best resources that you can keep an eye on is the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bayesian Methods for Machine Learning&lt;/a&gt; course in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Advanced machine learning specialization&lt;/a&gt; created jointly by Kazanova(Number 3 Kaggler at the time of writing)&lt;/p&gt;

&lt;p&gt;Apart from that I also found a course on &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=495576.8910375858&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbayesian&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bayesian Statistics on Coursera&lt;/a&gt;&lt;/strong&gt;. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=495576.8839843074&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Finferential-statistics-intro&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Inferential Statistics&lt;/a&gt;&lt;/strong&gt; taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.&lt;/p&gt;

&lt;p&gt;Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;

&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34; href=&#34;https://www.amazon.com/gp/product/1439840954/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1439840954&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=d55979088adc0aabeaed88f4f14b48b6&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1439840954&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1439840954&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34;  href=&#34;https://www.amazon.com/gp/product/1584885874/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1584885874&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=ee3e2a0bc99359d6c5db0463ab1abb13&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1584885874&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1584885874&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;Both these books are pretty high level and hard on math. But these are the best texts out there too. :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Tryst With MCMC Algorithms</title>
      <link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link>
      <pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid>
      <description>

&lt;p&gt;The things that I find hard to understand push me to my limits. One of the things that I have always found hard is &lt;strong&gt;Markov Chain Monte Carlo Methods&lt;/strong&gt;.
When I first encountered them, I read a lot about them but mostly it ended like this.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/flabbergasted.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The meaning is normally hidden in deep layers of Mathematical noise and not easy to decipher.
This blog post is intended to clear up the confusion around MCMC methods, Know what they are actually useful for and Get hands on with some applications.&lt;/p&gt;

&lt;h2 id=&#34;so-what-really-are-mcmc-methods&#34;&gt;&lt;strong&gt;So what really are MCMC Methods?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;First of all we have to understand what are &lt;strong&gt;&lt;em&gt;Monte Carlo&lt;/em&gt;&lt;/strong&gt; Methods!!!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Monte_Carlo_method&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Monte Carlo&lt;/a&gt; methods derive their name from Monte Carlo Casino in Monaco. There are many card games that need probability of winning against the dealer. Sometimes calculating this probability can be mathematically complex or highly intractable. But we can always run a computer simulation to simulate the whole game many times and see the probability as the number of wins divided by the number of games played.&lt;/p&gt;

&lt;p&gt;So that is all you need to know about Monte carlo Methods. Yes it is just a simple simulation technique with a Fancy Name.&lt;/p&gt;

&lt;p&gt;So as we have got the first part of MCMC, we also need to understand what are &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Markov Chains&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;.
Before Jumping onto Markov Chains let us learn a little bit about &lt;strong&gt;Markov Property&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Suppose you have a system of $M$ possible states, and you are hopping from one state to another.
&lt;em&gt;Markov Property&lt;/em&gt; says that given a process which is at a state $X_n$ at a particular point of time, the probability of $X_{n+1} = k$, where $k$ is any of the $M$ states the process can hop to, will only be dependent on which state it is at the given moment of time.
And not on how it reached the current state.&lt;/p&gt;

&lt;p&gt;Mathematically speaking:&lt;/p&gt;

&lt;div&gt; $$P(X_{n+1}=k | X_n=k_n,X_{n-1}=k_{n-1},....,X_1=k_1) = P(X_{n+1}=k|X_n=k_n)$$&lt;/div&gt;

&lt;p&gt;If a process exhibits the Markov Property than it is known as a Markov Process.&lt;/p&gt;

&lt;p&gt;Now Why is a Markov Chain important?
It is important because of its &lt;strong&gt;stationary distribution&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So what is a &lt;strong&gt;Stationary Distribution&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Assume you have a markov process like below. You start from any state $X_i$ and want to find out the state Probability distribution at $X_{i+1}$.&lt;/p&gt;

&lt;div style=&#34;margin-top: 10px; margin-bottom: -10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/Finance_Markov_chain_example_state_space.svg&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;You have a matrix of transition probability
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/transition_matrix.png&#34;&gt;&lt;/center&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;which defines the probability of going from a state $X_i$ to $X_j$.
You start calculating the Probability distribution for the next state. If you are at Bull Market State at time $i$ , you have a state Probability distribution as [0,1,0]&lt;/p&gt;

&lt;p&gt;you want to get the state pdf at $X_{i+1}$. That is given by&lt;/p&gt;

&lt;div&gt;&lt;center&gt;$$s_{i+1} = s_{i}Q$$&lt;/center&gt;&lt;/div&gt;

&lt;div&gt;&lt;center&gt;$$ s_{i+1}=\left[ {\begin{array}{cc}   .15 &amp; .8 &amp; .05      \end{array} } \right]$$&lt;/center&gt;&lt;/div&gt;
And the next state distribution could be found out by

&lt;div&gt;&lt;center&gt;$$s_{i+1} = s_iQ^2$$&lt;/center&gt;&lt;/div&gt;div&gt;

and so on. 
Eventually you will reach a stationary state s where:
&lt;center&gt;$$sQ=s$$&lt;/center&gt;
For this transition matrix Q the Stationary distribution $s$ is
&lt;div&gt;&lt;center&gt;$$ s_{i+1}=\left[ {\begin{array}{cc}   .625 &amp; .3125 &amp; .0625      \end{array} } \right]$$&lt;/center&gt;&lt;/div&gt;

&lt;p&gt;The stationary state distribution is important because it lets you define the probability for every state of a system at a random time. That is for this particular example we can say that 62.5% of the times market will be in a bull market state, 31.25% of weeks it will be a bear market and 6.25% of weeks it will be stagnant&lt;/p&gt;

&lt;p&gt;Intuitively you can think of it as an random walk on a chain. You might visit some nodes more often than others based on node probabilities. In the &lt;em&gt;Google Pagerank&lt;/em&gt; problem you might think of a node as a page, and the probability of a page in the stationary distribution as its relative importance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Woah!&lt;/em&gt;&lt;/strong&gt; That was a lot of information and we have yet not started talking about the MCMC Methods. Well if you are with me till now, we can now get on to the real topic now.&lt;/p&gt;

&lt;h2 id=&#34;so-what-is-mcmc&#34;&gt;So What is MCMC?&lt;/h2&gt;

&lt;p&gt;According to
&lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
**Markov Chain Monte Carlo** (MCMC) methods are a class of algorithms for **sampling from a probability distribution** based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.
&lt;/blockquote&gt;

&lt;p&gt;So let&amp;rsquo;s explain this with an example: Assume that &lt;strong&gt;we want to sample from a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Beta distribution&lt;/a&gt;&lt;/strong&gt;. The &lt;em&gt;PDF&lt;/em&gt; is:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$$f(x) = Cx^{\alpha -1}(1-x)^{\beta -1}$$&lt;/center&gt;
where $C$ is the normalizing constant &lt;em&gt;(which we actually don&amp;rsquo;t need to Sample from the distribution as we will see later)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is a &lt;strong&gt;fairly difficult problem&lt;/strong&gt; with the Beta Distribution if not intractable. In reality you might need to work with a lot harder Distribution Functions and sometimes you won&amp;rsquo;t actually know the normalizing constants.&lt;/p&gt;

&lt;p&gt;MCMC methods make life easier for us by providing us with algorithms that could create a Markov Chain which has the Beta distribution as its &lt;strong&gt;stationary distribution&lt;/strong&gt; given that we can sample from a uniform distribution(which is &lt;em&gt;fairly&lt;/em&gt; easy).&lt;/p&gt;

&lt;p&gt;If we start from a random state and traverse to the next state based on some algorithm repeatedly, we will end up creating a Markov Chain which has the Beta distribution as its &lt;strong&gt;stationary distribution&lt;/strong&gt; and the states we are at after a long time could be used as sample from the Beta Distribution.&lt;/p&gt;

&lt;p&gt;One such MCMC Algorithm is the
&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Metropolis Hastings Algorithm&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;metropolis-hastings-algorithm&#34;&gt;Metropolis Hastings Algorithm&lt;/h2&gt;

&lt;p&gt;Let $s=(s_1,s_2,&amp;hellip;.,s_M)$ be the desired stationary distribution. We want to create a Markov Chain that has this stationary distribution. We start with an arbitrary Markov Chain $P$ with $M$ states with transition matrix $Q$, so that $Q_{ij}$ represents the probability of going from state $i$ to $j$. Intuitively we know how to wander around this Markov Chain but this Markov Chain does not have the required Stationary Distribution. This chain does have some stationary distribution(which is not of our use)&lt;/p&gt;

&lt;p&gt;Our Goal is to change the way we wander on the this Markov Chain $P$ so that this chain has the desired Stationary distribution.&lt;/p&gt;

&lt;p&gt;To do this we:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start at a random initial State $i$.&lt;/li&gt;
&lt;li&gt;Randomly pick a new &lt;em&gt;Proposal State&lt;/em&gt; by looking at the transition probabilities in the ith row of the transition matrix Q.&lt;/li&gt;
&lt;li&gt;Compute an measure called the &lt;em&gt;Acceptance Probability&lt;/em&gt; which is defined as: $a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$&lt;/li&gt;
&lt;li&gt;Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state.&lt;/li&gt;
&lt;li&gt;Repeat for a long time&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After a long time this chain will converge and will have a stationary distribution $s$. &lt;strong&gt;We can then use the states of the chain as the sample from any distribution.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While doing this to sample the Beta Distribution, the only time we are using the PDF is to find the acceptance probability and in that we divide $s_j$ by $s_i$, i.e. the &lt;strong&gt;normalizing constant $C$ gets cancelled&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now Let&amp;rsquo;s Talk about the intuition. For the Intuition I am quoting an &lt;a href=&#34;http://stats.stackexchange.com/a/12657&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Answer&lt;/a&gt; from the site Stack Exchange,as this was the best intuitive explanation that I could find:
&lt;blockquote&gt;
I think there&amp;rsquo;s a nice and simple intuition to be gained from the (independence-chain) Metropolis-Hastings algorithm.
&lt;br&gt;
&lt;br&gt;
First, what&amp;rsquo;s the goal? The goal of MCMC is to &lt;strong&gt;draw samples from some probability distribution&lt;/strong&gt; without having to know its exact height at any point(We don&amp;rsquo;t need to know C). The way MCMC achieves this is to &lt;strong&gt;&amp;ldquo;wander around&amp;rdquo; on that distribution in such a way that the amount of time spent in each location is proportional to the height of the distribution&lt;/strong&gt;. If the &amp;ldquo;wandering around&amp;rdquo; process is set up correctly, you can make sure that this proportionality (between time spent and height of the distribution) is achieved.
&lt;br&gt;
&lt;br&gt;
Intuitively, what we want to do is to to walk around on some (lumpy) surface in such a way that the amount of time we spend (or # samples drawn) in each location is proportional to the height of the surface at that location. So, e.g., we&amp;rsquo;d like to spend twice as much time on a hilltop that&amp;rsquo;s at an altitude of 100m as we do on a nearby hill that&amp;rsquo;s at an altitude of 50m. The nice thing is that we can do this even if we don&amp;rsquo;t know the absolute heights of points on the surface: all we have to know are the relative heights. e.g., if one hilltop A is twice as high as hilltop B, then we&amp;rsquo;d like to spend twice as much time at A as we spend at B.
&lt;br&gt;
&lt;br&gt;
The simplest variant of the Metropolis-Hastings algorithm (independence chain sampling) achieves this as follows: assume that in every (discrete) time-step, we pick a random new &amp;ldquo;proposed&amp;rdquo; location (selected uniformly across the entire surface). If the proposed location is higher than where we&amp;rsquo;re standing now, move to it. If the proposed location is lower, then move to the new location with probability p, where p is the ratio of the height of that point to the height of the current location. (i.e., flip a coin with a probability p of getting heads; if it comes up heads, move to the new location; if it comes up tails, stay where we are). Keep a list of the locations you&amp;rsquo;ve been at on every time step, and that list will (asyptotically) have the right proportion of time spent in each part of the surface. (And for the A and B hills described above, you&amp;rsquo;ll end up with twice the probability of moving from B to A as you have of moving from A to B).
&lt;br&gt;
&lt;br&gt;
There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: &lt;strong&gt;(1) pick a new &amp;ldquo;proposed&amp;rdquo; location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location.&lt;/strong&gt;
&lt;/blockquote&gt;&lt;/p&gt;

&lt;h2 id=&#34;sampling-from-beta-distribution&#34;&gt;Sampling from Beta Distribution&lt;/h2&gt;

&lt;p&gt;Now Let&amp;rsquo;s Move on to the problem of Simulating from Beta Distribution. Now Beta Distribution is a continuous Distribution on [0,1] and it can have infinite states on [0,1].&lt;/p&gt;

&lt;p&gt;Lets Assume an arbitrary Markov Chain P with infinite states on [0,1] having transition Matrix Q such that $Q_{ij} = Q_{ji} = $ All entries in Matrix. We don&amp;rsquo;t really need the Matrix Q as we will see later, But I want to keep the problem description as close to the algorihm we suggested.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Start at a random &lt;strong&gt;initial State $i$&lt;/strong&gt; given by Unif(0,1).&lt;/li&gt;
&lt;li&gt;Randomly pick a new &lt;strong&gt;Proposal State&lt;/strong&gt; by looking at the transition probabilities in the ith row of the transition matrix Q. Lets say we pick up another Unif(0,1) state as a proposal state $j$.&lt;/li&gt;
&lt;li&gt;Compute an measure called the &lt;strong&gt;Acceptance Probability&lt;/strong&gt; :&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;&lt;center&gt;$$a_{ij} = min(s_jp_{ji}/s_{i}p_{ij},1)$$&lt;/center&gt;&lt;/div&gt; which is, &lt;div&gt;&lt;center&gt;$$a_{ij} = min(s_j/s_i,1)$$&lt;/center&gt;&lt;/div&gt; where, &lt;div&gt;&lt;center&gt;$$s_i = Ci^{\alpha -1}(1-i)^{\beta -1}$$&lt;/center&gt;&lt;/div&gt; and, &lt;div&gt;&lt;center&gt;$$s_j = Cj^{\alpha -1}(1-j)^{\beta -1}$$&lt;/center&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Now Flip a coin that lands head with probability $a_{ij}$. If the coin comes up heads, accept the proposal i.e move to next state else reject the proposal i.e. stay at the current state.&lt;/li&gt;
&lt;li&gt;Repeat for a long time&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So enough with theory, Let&amp;rsquo;s Move on to python to create our Beta Simulations Now&amp;hellip;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random
&lt;span style=&#34;color:#75715e&#34;&gt;# Lets define our Beta Function to generate s for any particular state. We don&amp;#39;t care for the normalizing constant here.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;beta_s&lt;/span&gt;(w,a,b):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;(a&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;w)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;(b&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# This Function returns True if the coin with probability P of heads comes heads when flipped.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;random_coin&lt;/span&gt;(p):
    unif &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; unif&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt;p:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; False
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; True

&lt;span style=&#34;color:#75715e&#34;&gt;# This Function runs the MCMC chain for Beta Distribution.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;beta_mcmc&lt;/span&gt;(N_hops,a,b):
    states &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    cur &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,N_hops):
        states&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(cur)
        next &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uniform(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        ap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(beta_s(next,a,b)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;beta_s(cur,a,b),&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the acceptance probability&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; random_coin(ap):
            cur &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; next
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; states[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;:] &lt;span style=&#34;color:#75715e&#34;&gt;# Returns the last 100 states of the chain&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let us check our results of the MCMC Sampled Beta distribution against the actual beta distribution.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pylab &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pl
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; scipy.special &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; ss
&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline
pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rcParams[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;figure.figsize&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;17.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Actual Beta PDF.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;beta&lt;/span&gt;(a, b, i):
    e1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gamma(a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b)
    e2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gamma(a)
    e3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gamma(b)
    e4 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; (a &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    e5 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; i) &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; (b &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (e1&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(e2&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;e3)) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; e4 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; e5

&lt;span style=&#34;color:#75715e&#34;&gt;# Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;plot_beta&lt;/span&gt;(a, b):
    Ly &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    Lx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    i_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mgrid[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;100j&lt;/span&gt;]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; i_list:
        Lx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(i)
        Ly&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(beta(a, b, i))
    pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(Lx, Ly, label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Real Distribution: a=&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;str(a)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, b=&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;str(b))
    pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hist(beta_mcmc(&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;,a,b),normed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True,bins &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;, histtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;step&amp;#39;&lt;/span&gt;,label&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Simulated_MCMC: a=&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;str(a)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, b=&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;str(b))
    pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;legend()
    pl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show()
    
plot_beta(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
plot_beta(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
plot_beta(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: -9px; margin-bottom: 30px;&#34;&gt;
&lt;img src=&#34;https://mlwhiz.com/images/graphs.png&#34;&gt;
&lt;/div&gt;

&lt;p&gt;As we can see our sampled beta values closely resemble the beta distribution.&lt;/p&gt;

&lt;p&gt;So MCMC Methods are useful for the following basic problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Simulating from a Random Variable PDF. Example: Simulate from a Beta(0.5,0.5) or from a Normal(0,1).&lt;/li&gt;
&lt;li&gt;Solve problems with a large state space.For Example: Knapsack Problem, Encrytion Cipher etc. We will work on this in the &lt;a href=&#34;https://mlwhiz.com/blog/2015/08/21/mcmc_algorithms_cryptography/&#34;&gt;Next Blog Post&lt;/a&gt; as this one has already gotten bigger than what I expected.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Till Then Ciao!!!!!!&lt;/p&gt;

&lt;h2 id=&#34;references-and-sources&#34;&gt;References and Sources:&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science-ebook/dp/B00MMOJ19I&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Probability Joseph K Blitzstein, Jessica Hwang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stats.stackexchange.com/a/12657&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;StackExchange&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One of the newest and best resources that you can keep an eye on is the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bayesian Methods for Machine Learning&lt;/a&gt; course in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Advanced machine learning specialization&lt;/a&gt; created jointly by Kazanova(Number 3 Kaggler at the time of writing)&lt;/p&gt;

&lt;p&gt;Apart from that I also found a course on &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=495576.8910375858&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fbayesian&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bayesian Statistics on Coursera&lt;/a&gt;&lt;/strong&gt;. In the process of doing it right now so couldn&amp;rsquo;t really comment on it. But since I had done an course on &lt;strong&gt;&lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=495576.8839843074&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Finferential-statistics-intro&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Inferential Statistics&lt;/a&gt;&lt;/strong&gt; taught by the same professor before(Mine Çetinkaya-Rundel), I am very hopeful for this course. Let&amp;rsquo;s see.&lt;/p&gt;

&lt;p&gt;Also look out for these two books to learn more about MCMC. I have not yet read them whole but still I liked whatever I read:&lt;/p&gt;

&lt;div style=&#34;margin-left:1em ; text-align: center;&#34;&gt;

&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34; href=&#34;https://www.amazon.com/gp/product/1439840954/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1439840954&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=d55979088adc0aabeaed88f4f14b48b6&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1439840954&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1439840954&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/t&gt;&lt;/t&gt;
&lt;a target=&#34;_blank&#34; rel=&#34;nofollow&#34;  href=&#34;https://www.amazon.com/gp/product/1584885874/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1584885874&amp;linkCode=as2&amp;tag=mlwhizcon-20&amp;linkId=ee3e2a0bc99359d6c5db0463ab1abb13&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1584885874&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=mlwhizcon-20&#34; &gt;&lt;/a&gt;&lt;img src=&#34;//ir-na.amazon-adsystem.com/e/ir?t=mlwhizcon-20&amp;l=am2&amp;o=1&amp;a=1584885874&#34; width=&#34;1&#34; height=&#34;1&#34; border=&#34;0&#34; alt=&#34;&#34; style=&#34;border:none !important; margin:0px !important;&#34; /&gt;
&lt;/div&gt;

&lt;p&gt;Both these books are pretty high level and hard on math. But these are the best texts out there too. :)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>