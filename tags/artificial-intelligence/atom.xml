<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:content="http://purl.org/rss/1.0/modules/content" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Artificial Intelligence on MLWhiz</title>
    <link>https://mlwhiz.com/tags/artificial-intelligence/</link>
    <description>Recent content in Artificial Intelligence on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://mlwhiz.com/tags/artificial-intelligence/atom.xml" rel="self" type="application/rss+xml" />
    

    

    <item>
      <title>Chatbots  aren&#39;t as difficult to make as You Think</title>
      <link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/chatbot/dvader.jpeg"></media:content>
      

      
      <description>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them. Are they hard to create? What technologies should I know before attempting to work on them?</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid>
      
      
      <media:content type="image/jpeg" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/nlp_tl/spiderman.jpeg"></media:content>
      

      
      <description>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/birnn.png"></media:content>
      

      
      <description>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc.</description>
      
      
    </item>
    

    <item>
      <title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title>
      <link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/silver/CV_vs_LB.png"></media:content>
      

      
      <description>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.
As a side note: if you want to know more about NLP, I would like to recommend this awesome course on Natural Language Processing in the Advanced machine learning specialization.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 2 - Conventional Methods for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/tfidf.png"></media:content>
      

      
      <description>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the various preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In this post, I will try to take you through some basic conventional models like TFIDF, Count Vectorizer, Hashing etc.</description>
      
      
    </item>
    

    <item>
      <title>NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title>
      <link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid>
      
      
      <media:content type="image/png" medium="image" width="700" height="400"
      url="https://www.mlwhiz.com/images/text_processing_flow_1.png"></media:content>
      

      
      <description>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.
Since we have a large amount of material to cover, I am splitting this post into a series of posts.</description>
      
      
    </item>
    
  </channel>
</rss>