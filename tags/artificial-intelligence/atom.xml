<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>
    Artificial Intelligence on 
    MLWhiz
    </title>
    <link>https://mlwhiz.com/tags/artificial-intelligence/</link>
    <description>Recent content in Artificial Intelligence 
    on MLWhiz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
    
    <lastBuildDate>Mon, 15 Apr 2019 00:00:00 +0000</lastBuildDate>
    
    
        <atom:link href="https://mlwhiz.com/tags/artificial-intelligence/atom.xml" rel="self" type="application/rss" />
    
    
    <item>
      <title>Chatbots  aren&#39;t as difficult to make as You Think</title>
      <link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid>
      <description>

&lt;p&gt;Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/dvader.jpeg&#34;  style=&#34;height:60%;width:60%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Let me assure you this is not going to be &lt;em&gt;&amp;ldquo;that kind of a post&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I will try to distill some of the knowledge I acquired while working through a project in the &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; course in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So before I start, let me first say it for once that don&amp;rsquo;t be intimidated by the hype and the enigma surrounding Chatbots. They are pretty much using pretty simple NLP techniques which most of us already know. If you don&amp;rsquo;t, you are welcome to check out my &lt;a href=&#34;https://towardsdatascience.com/tagged/nlp-learning-series&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;NLP Learning Series&lt;/a&gt;, where I go through the problem of text classification in fair detail using &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/&#34;&gt;Conventional&lt;/a&gt;, &lt;a href=&#34;https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/&#34;&gt;Deep Learning&lt;/a&gt; and &lt;a href=&#34;https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/&#34;&gt;Transfer Learning&lt;/a&gt; methods.&lt;/p&gt;

&lt;h2 id=&#34;a-very-brief-intro-to-chatbots&#34;&gt;A Very brief Intro to Chatbots&lt;/h2&gt;

&lt;p&gt;We can logically divide of Chatbots in the following two categories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Database/FAQ based&lt;/strong&gt; - We have a database with some questions and answers, and we would like that a user can query that using Natural Language. This is the sort of Chatbots you find at most of the Banking websites for answering FAQs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chit-Chat Based&lt;/strong&gt; - Simulate dialogue with the user. These are the kind of chatbots that bring the cool in chatbots. We can use Seq-2-Seq models to create such bots.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;the-chatbot-we-will-be-creating&#34;&gt;The Chatbot we will be creating&lt;/h2&gt;

&lt;p&gt;We will be creating a &lt;strong&gt;dialogue chat bot&lt;/strong&gt;, which will be able to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Answer programming-related questions&lt;/strong&gt; (using StackOverflow dataset)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chit-Chat&lt;/strong&gt; and simulate dialogue on all non-programming related questions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once we will have it up and running our final chatbot should look like this.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/telegram_final.png&#34;  style=&#34;height:60%;width:60%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Seems quite fun.&lt;/p&gt;

&lt;p&gt;We will be taking help of resources like Telegram and Chatterbot to build our Chatbot. So before we start, I think I should get you up and running with these two tools.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;1-telegram&#34;&gt;1. Telegram:&lt;/h2&gt;

&lt;p&gt;From the &lt;a href=&#34;https://telegram.org/faq#q-what-is-telegram-what-do-i-do-here&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;website&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Telegram is a messaging app with a focus on speed and security, it’s super-fast, simple and free. You can use Telegram on all your devices at the same time — your messages sync seamlessly across any number of your phones, tablets or computers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For us, Telegram provides us with an easy way to create a Chatbot UI. It provides us with an access token which we will use to connect to the Telegram App backend and run our chatbot logic. Naturally, we need to have a window where we will write our questions to the chatbot, for us that is provided by Telegram. Also, telegram powers the chatbot by communicating with our chatbot logic. The above screenshot is taken from the telegram app only.&lt;/p&gt;

&lt;h3 id=&#34;set-up-telegram&#34;&gt;Set up Telegram:&lt;/h3&gt;

&lt;p&gt;Don&amp;rsquo;t worry if you don&amp;rsquo;t understand how it works yet; I will try to give step by step instructions as we go forward.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1: &lt;a href=&#34;https://macos.telegram.org/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Download and Install&lt;/a&gt; Telegram App on your Laptop.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Step 2: Talk with BotFather by opening this &lt;a href=&#34;https://telegram.me/BotFather&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; in Chrome and subsequently your Telegram App.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Step 3: The above steps will take you to a Chatbot called Botfather which can help you create a new bot. Inception Anyone? It will look something like this.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up a new bot using command &amp;ldquo;/newbot&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Create a name for Your bot.&lt;/li&gt;
&lt;li&gt;Create a username for your bot.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/telegram_botfather.png&#34;  style=&#34;height:70%;width:70%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Step 4: You will get an access token for the bot. Copy the Token at a safe place.&lt;/li&gt;
&lt;li&gt;Step 5: Click on the &amp;ldquo;t.me/MLWhizbot&amp;rdquo; link to open Chat with your chatbot in a new window.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Right now if you try to communicate with the chatbot, you won&amp;rsquo;t receive any answers. And that is how it should be.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/telegram_unresponsive.png&#34;  style=&#34;height:20%;width:20%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;But that&amp;rsquo;s not at all fun. Is it? Let&amp;rsquo;s do some python magic to make it responsive.&lt;/p&gt;

&lt;h3 id=&#34;making-our-telegram-chatbot-responsive&#34;&gt;Making our Telegram Chatbot responsive&lt;/h3&gt;

&lt;p&gt;Create a file &lt;code&gt;main.py&lt;/code&gt; and put the following code in it. Don&amp;rsquo;t worry most of the code here is Boilerplate code to make our Chatbot communicate with Telegram using the Access token. We need to worry about implementing the class &lt;code&gt;SimpleDialogueManager&lt;/code&gt;. This class contains a function called &lt;code&gt;generate_answer&lt;/code&gt; which is where we will write our bot logic.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env python3&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; time
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; argparse
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; requests.compat &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; urljoin

&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;BotHandler&lt;/span&gt;(object):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        BotHandler is a class which implements all back-end of the bot.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        It has three main functions:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#39;get_updates&amp;#39; — checks for new messages
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#39;send_message&amp;#39; – posts new message to user
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#39;get_answer&amp;#39; — computes the most relevant on a user&amp;#39;s question
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, token, dialogue_manager):
        
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;token &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; token
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;api_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://api.telegram.org/bot{}/&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(token)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dialogue_manager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dialogue_manager

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_updates&lt;/span&gt;(self, offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, timeout&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;):
        params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;timeout&amp;#34;&lt;/span&gt;: timeout, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;offset&amp;#34;&lt;/span&gt;: offset}
        raw_resp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(urljoin(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;api_url, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;getUpdates&amp;#34;&lt;/span&gt;), params)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
            resp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_resp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decoder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;JSONDecodeError &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Failed to parse response {}: {}.&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(raw_resp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;content, e))
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; []

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;result&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; resp:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; []
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; resp[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;result&amp;#34;&lt;/span&gt;]

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;send_message&lt;/span&gt;(self, chat_id, text):
        params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chat_id&amp;#34;&lt;/span&gt;: chat_id, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;: text}
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;post(urljoin(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;api_url, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sendMessage&amp;#34;&lt;/span&gt;), params)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_answer&lt;/span&gt;(self, question):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; question &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/start&amp;#39;&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hi, I am your project bot. How can I help you today?&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dialogue_manager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;generate_answer(question)


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_unicode&lt;/span&gt;(text):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(text) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; len(text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;encode())


&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SimpleDialogueManager&lt;/span&gt;(object):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    This is a simple dialogue manager to test the telegram bot.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The main part of our bot will be written here.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;generate_answer&lt;/span&gt;(self, question): 
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hi&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; question:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello, You&amp;#34;&lt;/span&gt; 
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Don&amp;#39;t be rude. Say Hi first.&amp;#34;&lt;/span&gt;
        

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;():
    &lt;span style=&#34;color:#75715e&#34;&gt;# Put your own Telegram Access token here...&lt;/span&gt;
    token &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;839585958:AAEfTDo2X6PgHb9IEdb62ueS4SmdpCkhtmc&amp;#39;&lt;/span&gt;
    simple_manager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SimpleDialogueManager()
    bot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BotHandler(token, simple_manager)
    &lt;span style=&#34;color:#75715e&#34;&gt;###############################################################&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ready to talk!&amp;#34;&lt;/span&gt;)
    offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; True:
        updates &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_updates(offset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;offset)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; update &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; updates:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;An update received.&amp;#34;&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; update:
                chat_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; update[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chat&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;]
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; update[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;]:
                    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; update[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;]
                    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_unicode(text):
                        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Update content: {}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(update))
                        bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send_message(chat_id, bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_answer(update[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;]))
                    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                        bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send_message(chat_id, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hmm, you are sending some weird characters to me...&amp;#34;&lt;/span&gt;)
            offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(offset, update[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;update_id&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sleep(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
    main()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run the file &lt;code&gt;main.py&lt;/code&gt; in the terminal window to make your bot responsive.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python main.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/telegram_naive.png&#34;  style=&#34;height:100%;width:100%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Nice. It is following simple logic. But the good thing is that our bot now does something.&lt;/p&gt;

&lt;p&gt;Also, take a look at the terminal window where we have run our &lt;code&gt;main.py&lt;/code&gt; File. Whenever a user asks a question, we get the sort of dictionary below containing Unique Chat ID, Chat Text, User Information, etc.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Update content: {&#39;update_id&#39;: 484689748, &#39;message&#39;: {&#39;message_id&#39;: 115, &#39;from&#39;: {&#39;id&#39;: 844474950, &#39;is_bot&#39;: False, &#39;first_name&#39;: &#39;Rahul&#39;, &#39;last_name&#39;: &#39;Agarwal&#39;, &#39;language_code&#39;: &#39;en&#39;}, &#39;chat&#39;: {&#39;id&#39;: 844474950, &#39;first_name&#39;: &#39;Rahul&#39;, &#39;last_name&#39;: &#39;Agarwal&#39;, &#39;type&#39;: &#39;private&#39;}, &#39;date&#39;: 1555266010, &#39;text&#39;: &#39;What is 2+2&#39;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Until now whatever we had done was sort of setting up and engineering sort of work.&lt;/p&gt;

&lt;p&gt;Only if we can write some sound Data Science logic in the &lt;code&gt;generate_answer&lt;/code&gt; function in our &lt;code&gt;main.py&lt;/code&gt; we should have a decent chatbot.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-chatterbot&#34;&gt;2. ChatterBot&lt;/h2&gt;

&lt;p&gt;From the Documentation:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ChatterBot is a Python library that makes it easy to generate automated responses to a user’s input. ChatterBot uses a selection of machine learning algorithms to produce different types of reactions. This makes it easy for developers to create chat bots and automate conversations with users.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Simply. It is a Blackbox system which can provide us with responses for Chitchat type questions for our Chatbot. And the best part about it is that it is pretty easy to integrate with our current flow. We could also have trained a SeqtoSeq model to do the same thing. Might be I will do it in a later post. I digress.&lt;/p&gt;

&lt;p&gt;So, install it with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ pip install chatterbot&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And change the &lt;code&gt;SimpleDialogueManager&lt;/code&gt; Class in main.py to the following. We can have a bot that can talk to the user and answer random queries.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SimpleDialogueManager&lt;/span&gt;(object):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    This is a simple dialogue manager to test the telegram bot.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The main part of our bot will be written here.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
        &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; chatterbot &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatBot
        &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; chatterbot.trainers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatterBotCorpusTrainer
        chatbot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatBot(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;MLWhizChatterbot&amp;#39;&lt;/span&gt;)
        trainer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatterBotCorpusTrainer(chatbot)
        trainer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;chatterbot.corpus.english&amp;#39;&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chitchat_bot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; chatbot

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;generate_answer&lt;/span&gt;(self, question): 
        response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chitchat_bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_response(question)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; response&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The code in &lt;code&gt;init&lt;/code&gt; instantiates a chatbot using chatterbot and trains it on the &lt;a href=&#34;https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/english&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;provided english corpus&lt;/a&gt; data. The data is pretty small, but you can always train it on your dataset too. Just see the &lt;a href=&#34;https://chatterbot.readthedocs.io/en/stable/training.html&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;. We can then give our responses using the Chatterbot chatbot in the &lt;code&gt;generate_answer&lt;/code&gt; function.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/telegram_chatterbot.png&#34;  style=&#34;height:80%;width:80%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Not too &amp;ldquo;ba a a a a a d&amp;rdquo; , I must say.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;creating-our-stackoverflow-chatbot&#34;&gt;Creating our StackOverFlow ChatBot&lt;/h2&gt;

&lt;p&gt;Ok, so finally we are at a stage where we can do something we love. Use Data Science to power our Application/Chatbot. Let us start with creating a rough architecture of what we are going to do next.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/chatbot_architecture.png&#34;  style=&#34;height:80%;width:80%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We will need to create two classifiers and save them as &lt;code&gt;.pkl&lt;/code&gt; files.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Intent-Classifier&lt;/strong&gt;: This classifier will predict if it a question is a Stack-Overflow question or not. If it is not a Stack-overflow question, we let Chatterbot handle it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Programming-Language(Tag) Classifier&lt;/strong&gt;: This classifier will predict which language a question belongs to if the question is a Stack-Overflow question. We do this so that we can search for those language questions in our database only.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To keep it simple we will create simple TFIDF models. We will need to save these TFIDF vectorizers.&lt;/p&gt;

&lt;p&gt;We will also need to store word vectors for every question for similarity calculations later.&lt;/p&gt;

&lt;p&gt;Let us go through the process step by step. You can get the full code in this &lt;a href=&#34;https://github.com/MLWhiz/chatbot/blob/master/Model%20Creation.ipynb&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;jupyter notebook&lt;/a&gt; in my &lt;a href=&#34;https://github.com/MLWhiz/chatbot&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;project repository&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;step-1-reading-and-visualizing-the-data&#34;&gt;Step 1. Reading and Visualizing the Data&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dialogues &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/dialogues.tsv&amp;#34;&lt;/span&gt;,sep&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
posts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/tagged_posts.tsv&amp;#34;&lt;/span&gt;,sep&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dialogues&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: left;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;tag&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Okay -- you&#39;re gonna need to learn how to lie.&lt;/td&gt;
      &lt;td&gt;dialogue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;I&#39;m kidding.  You know how sometimes you just ...&lt;/td&gt;
      &lt;td&gt;dialogue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Like my fear of wearing pastels?&lt;/td&gt;
      &lt;td&gt;dialogue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;I figured you&#39;d get to the good stuff eventually.&lt;/td&gt;
      &lt;td&gt;dialogue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Thank God!  If I had to hear one more story ab...&lt;/td&gt;
      &lt;td&gt;dialogue&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;posts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;post_id&lt;/th&gt;
      &lt;th&gt;title&lt;/th&gt;
      &lt;th&gt;tag&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Calculate age in C#&lt;/td&gt;
      &lt;td&gt;c#&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Filling a DataSet or DataTable from a LINQ que...&lt;/td&gt;
      &lt;td&gt;c#&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;Reliable timer in a console application&lt;/td&gt;
      &lt;td&gt;c#&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;Best way to allow plugins for a PHP application&lt;/td&gt;
      &lt;td&gt;php&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;59&lt;/td&gt;
      &lt;td&gt;How do I get a distinct, ordered list of names...&lt;/td&gt;
      &lt;td&gt;c#&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Num Posts:&amp;#34;&lt;/span&gt;,len(posts))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Num Dialogues:&amp;#34;&lt;/span&gt;,len(dialogues))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Num Posts: 2171575
Num Dialogues: 218609
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-create-training-data-for-intent-classifier-chitchat-stackoverflow-question&#34;&gt;Step 2: Create training data for intent classifier - Chitchat/StackOverflow Question&lt;/h4&gt;

&lt;p&gt;We will be creating a TFIDF model with Logistic regression to do this. If you want to know about the TFIDF model you can read it here.&lt;/p&gt;

&lt;p&gt;We could also have used one of the Deep Learning models or transfer learning approaches to do this, but since the main objective of this post is to get a chatbot up and running and not worry too much about the accuracy we sort of work with the TFIDF based model only.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;texts  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  list(dialogues[:&lt;span style=&#34;color:#ae81ff&#34;&gt;200000&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; list(posts[:&lt;span style=&#34;color:#ae81ff&#34;&gt;200000&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)
labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dialogue&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;200000&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stackoverflow&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;200000&lt;/span&gt;
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;:texts,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;target&amp;#39;&lt;/span&gt;:labels})

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;text_prepare&lt;/span&gt;(text):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Performs tokenization and simple preprocessing.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    
    replace_by_space_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[/(){}\[\]\|@,;]&amp;#39;&lt;/span&gt;)
    bad_symbols_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[^0-9a-z #+_]&amp;#39;&lt;/span&gt;)
    stopwords_set &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; set(stopwords&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;words(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;))

    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; replace_by_space_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;, text)
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bad_symbols_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;, text)
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join([x &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split() &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; stopwords_set])

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()

&lt;span style=&#34;color:#75715e&#34;&gt;# Doing some data cleaning&lt;/span&gt;
data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x : text_prepare(x))

X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;],data[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;target&amp;#39;&lt;/span&gt;],test_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; , random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Train size = {}, test size = {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(X_train), len(X_test)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Train size = 360000, test size = 40000
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-3-create-intent-classifier&#34;&gt;Step 3. Create Intent classifier&lt;/h4&gt;

&lt;p&gt;Here we Create a TFIDF Vectorizer to create features and also train a Logistic regression model to create the intent_classifier. Please note how we are saving TFIDF Vectorizer to &lt;code&gt;resources/tfidf.pkl&lt;/code&gt; and intent_classifier to &lt;code&gt;resources/intent_clf.pkl&lt;/code&gt;. We will need these files once we are going to write the &lt;code&gt;SimpleDialogueManager&lt;/code&gt; class for our final Chatbot.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We will keep our models and vectorizers in this folder&lt;/span&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;mkdir resources

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tfidf_features&lt;/span&gt;(X_train, X_test, vectorizer_path):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Performs TF-IDF transformation and dumps the model.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    tfv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TfidfVectorizer(dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32, min_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,  max_features&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, 
            strip_accents&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;unicode&amp;#39;&lt;/span&gt;, analyzer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;,token_pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\w{1,}&amp;#39;&lt;/span&gt;,
            ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), use_idf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,smooth_idf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,sublinear_tf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,
            stop_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;)
    
    X_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tfv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(X_train)
    X_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tfv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(X_test)
    
    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(tfv,vectorizer_path)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; X_train, X_test

X_train_tfidf, X_test_tfidf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tfidf_features(X_train, X_test, open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;resources/tfidf.pkl&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;))

intent_recognizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LogisticRegression(C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
intent_recognizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train_tfidf,y_train)
pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(intent_recognizer, open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;resources/intent_clf.pkl&amp;#34;&lt;/span&gt; , &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;))

&lt;span style=&#34;color:#75715e&#34;&gt;# Check test accuracy.&lt;/span&gt;
y_test_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; intent_recognizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test_tfidf)
test_accuracy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; accuracy_score(y_test, y_test_pred)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Test accuracy = {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(test_accuracy))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Test accuracy = 0.989825
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Intent Classifier has a pretty good test accuracy of 98%. TFIDF is not so bad.&lt;/p&gt;

&lt;h4 id=&#34;step-4-create-programming-language-classifier&#34;&gt;Step 4: Create Programming Language classifier&lt;/h4&gt;

&lt;p&gt;Let us first create the data for Programming Language classifier and then train a Logistic Regression model using TFIDF features. We save this tag Classifier at the location &lt;code&gt;resources/tag_clf.pkl&lt;/code&gt;. We do this step mostly because we don&amp;rsquo;t want to do similarity calculations over the whole database of questions but only on the subset of questions by the language tag.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# creating the data for Programming Language classifier &lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; posts[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values
y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; posts[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tag&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values

X_train, X_test, y_train, y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(X, y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Train size = {}, test size = {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(len(X_train), len(X_test)))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Train size = 1737260, test size = 434315
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;resources/tfidf.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
X_train_tfidf, X_test_tfidf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(X_train), vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(X_test)
tag_classifier &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; OneVsRestClassifier(LogisticRegression(C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;))
tag_classifier&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train_tfidf,y_train)
pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump(tag_classifier, open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;resources/tag_clf.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;))

&lt;span style=&#34;color:#75715e&#34;&gt;# Check test accuracy.&lt;/span&gt;
y_test_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tag_classifier&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test_tfidf)
test_accuracy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; accuracy_score(y_test, y_test_pred)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Test accuracy = {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(test_accuracy))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Test accuracy = 0.8043816124241622
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not Bad again.&lt;/p&gt;

&lt;h4 id=&#34;step-5-store-question-database-embeddings&#34;&gt;Step 5: Store Question database Embeddings&lt;/h4&gt;

&lt;p&gt;One can use &lt;a href=&#34;https://code.google.com/archive/p/word2vec/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;pre-trained word vectors&lt;/a&gt; from Google or get a better result by training their embeddings using their data. Since again accuracy and precision is not the primary goal of this post, we will use pretrained vectors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Load Google&amp;#39;s pre-trained Word2Vec model.&lt;/span&gt;
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gensim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;KeyedVectors&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_word2vec_format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;GoogleNews-vectors-negative300.bin&amp;#39;&lt;/span&gt;, binary&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We want to convert every question to an embedding and store them so that we don&amp;rsquo;t calculate the embeddings for the whole dataset every time. In essence, whenever the user asks a Stack Overflow question, we want to use some distance similarity measure to get the most similar question.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;question_to_vec&lt;/span&gt;(question, embeddings, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        question: a string
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        embeddings: dict where the key is a word and a value is its&amp;#39; embedding
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        dim: size of the representation
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        result: vector representation for the question
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    word_tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;)
    question_len &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(word_tokens)
    question_mat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((question_len,dim), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx, word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(word_tokens):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; embeddings:
            question_mat[idx,:] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings[word]
            
    &lt;span style=&#34;color:#75715e&#34;&gt;# remove zero-rows which stand for OOV words       &lt;/span&gt;
    question_mat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question_mat[&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;all(question_mat &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, axis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
    
    &lt;span style=&#34;color:#75715e&#34;&gt;# Compute the mean of each word along the sentence&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; question_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
        vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(question_mat, axis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape((&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,dim))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,dim), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
        
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vec

counts_by_tag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; posts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;groupby(by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tag&amp;#39;&lt;/span&gt;])[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tag&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_index(name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;count&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort_values([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;count&amp;#39;&lt;/span&gt;], ascending &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False)
counts_by_tag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(zip(counts_by_tag[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tag&amp;#39;&lt;/span&gt;],counts_by_tag[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;count&amp;#39;&lt;/span&gt;]))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(counts_by_tag)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;[(&#39;c#&#39;, 394451), (&#39;java&#39;, 383456), (&#39;javascript&#39;, 375867), (&#39;php&#39;, 321752), (&#39;c_cpp&#39;, 281300), (&#39;python&#39;, 208607), (&#39;ruby&#39;, 99930), (&#39;r&#39;, 36359), (&#39;vb&#39;, 35044), (&#39;swift&#39;, 34809)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We save the embeddings in a folder aptly named &lt;code&gt;resources/embeddings_folder&lt;/code&gt;. This folder will contain a .pkl file for every tag. For example one of the files will be &lt;code&gt;python.pkl&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt; mkdir resources&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;embeddings_folder

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; tag, count &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; counts_by_tag:
    tag_posts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; posts[posts[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tag&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; tag]
    tag_post_ids &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tag_posts[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;post_id&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values
    tag_vectors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((count, &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i, title &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(tag_posts[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;]):
        tag_vectors[i, :] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question_to_vec(title, model, &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# Dump post ids and vectors to a file.&lt;/span&gt;
    filename &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/embeddings_folder/&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tag &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.pkl&amp;#39;&lt;/span&gt;
    pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dump((tag_post_ids, tag_vectors), open(filename, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We are nearing the end now. We need to have a function to get most similar question&amp;rsquo;s &lt;em&gt;post id&lt;/em&gt; in the dataset given we know the programming Language of the question. Here it is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_similar_question&lt;/span&gt;(question,tag):
    &lt;span style=&#34;color:#75715e&#34;&gt;# get the path where all question embeddings are kept and load the post_ids and post_embeddings&lt;/span&gt;
    embeddings_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/embeddings_folder/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tag &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.pkl&amp;#34;&lt;/span&gt;
    post_ids, post_embeddings &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(embeddings_path, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
    &lt;span style=&#34;color:#75715e&#34;&gt;# Get the embeddings for the question&lt;/span&gt;
    question_vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question_to_vec(question, model, &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# find index of most similar post&lt;/span&gt;
    best_post_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pairwise_distances_argmin(question_vec,
                                                post_embeddings)
    &lt;span style=&#34;color:#75715e&#34;&gt;# return best post id&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; post_ids[best_post_index]

get_similar_question(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how to use list comprehension in python?&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;array([5947137])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we can use this post ID and find this question at &lt;a href=&#34;https://stackoverflow.com/questions/5947137&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/5947137&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The question the similarity checker suggested has the actual text: &amp;ldquo;How can I use a list comprehension to extend a list in python? [duplicate]&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Not too bad. It could have been better if we train our embeddings or use &lt;a href=&#34;https://github.com/facebookresearch/StarSpace&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;starspace&lt;/a&gt; embeddings.&lt;/p&gt;

&lt;h2 id=&#34;assemble-the-puzzle-simpledialoguemanager-class&#34;&gt;Assemble the Puzzle - SimpleDialogueManager Class&lt;/h2&gt;

&lt;p&gt;Finally, we have reached the end of the whole exercise, and we have to fit all the pieces in the puzzle in our &lt;code&gt;SimpleDialogueManager&lt;/code&gt; Class. Here is the code for that. Go in the &lt;code&gt;main.py&lt;/code&gt; file again to paste this code and see if it works or not.&lt;/p&gt;

&lt;p&gt;Go through the comments to understand how the pieces are fitting together to build one wholesome logic.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; gensim
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pickle
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; re
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; nltk
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.corpus &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; stopwords
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics.pairwise &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pairwise_distances_argmin

&lt;span style=&#34;color:#75715e&#34;&gt;# We will need this function to prepare text at prediction time&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;text_prepare&lt;/span&gt;(text):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Performs tokenization and simple preprocessing.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    
    replace_by_space_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[/(){}\[\]\|@,;]&amp;#39;&lt;/span&gt;)
    bad_symbols_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[^0-9a-z #+_]&amp;#39;&lt;/span&gt;)
    stopwords_set &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; set(stopwords&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;words(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;))

    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; replace_by_space_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;, text)
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bad_symbols_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;, text)
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join([x &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split() &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; stopwords_set])

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()

&lt;span style=&#34;color:#75715e&#34;&gt;# need this to convert questions asked by user to vectors&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;question_to_vec&lt;/span&gt;(question, embeddings, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        question: a string
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        embeddings: dict where the key is a word and a value is its&amp;#39; embedding
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        dim: size of the representation
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        result: vector representation for the question
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    word_tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;)
    question_len &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(word_tokens)
    question_mat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((question_len,dim), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx, word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(word_tokens):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; embeddings:
            question_mat[idx,:] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings[word]
            
    &lt;span style=&#34;color:#75715e&#34;&gt;# remove zero-rows which stand for OOV words       &lt;/span&gt;
    question_mat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question_mat[&lt;span style=&#34;color:#f92672&#34;&gt;~&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;all(question_mat &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, axis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
    
    &lt;span style=&#34;color:#75715e&#34;&gt;# Compute the mean of each word along the sentence&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; question_mat&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
        vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(question_mat, axis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape((&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,dim))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,dim), dtype &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
        
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vec

&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SimpleDialogueManager&lt;/span&gt;(object):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    This is a simple dialogue manager to test the telegram bot.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The main part of our bot will be written here.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):

        &lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate all the models and TFIDF Objects.&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Loading resources...&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate a Chatterbot for Chitchat type questions&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; chatterbot &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatBot
        &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; chatterbot.trainers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatterBotCorpusTrainer
        chatbot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatBot(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;MLWhizChatterbot&amp;#39;&lt;/span&gt;)
        trainer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatterBotCorpusTrainer(chatbot)
        trainer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;chatterbot.corpus.english&amp;#39;&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chitchat_bot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; chatbot
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Loading Word2vec model...&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# Instantiate the Google&amp;#39;s pre-trained Word2Vec model.&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gensim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;KeyedVectors&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_word2vec_format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;GoogleNews-vectors-negative300.bin&amp;#39;&lt;/span&gt;, binary&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) 
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Loading Classifier objects...&amp;#34;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# Load the intent classifier and tag classifier&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;intent_recognizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/intent_clf.pkl&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tag_classifier &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/tag_clf.pkl&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
        &lt;span style=&#34;color:#75715e&#34;&gt;# Load the TFIDF vectorizer object&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tfidf_vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/tfidf.pkl&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Finished Loading Resources&amp;#34;&lt;/span&gt;)

    &lt;span style=&#34;color:#75715e&#34;&gt;# We created this function just above. We just need to have a function to get most similar question&amp;#39;s *post id* in the dataset given we know the programming Language of the question. Here it is:&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_similar_question&lt;/span&gt;(self,question,tag):
        &lt;span style=&#34;color:#75715e&#34;&gt;# get the path where all question embeddings are kept and load the post_ids and post_embeddings&lt;/span&gt;
        embeddings_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resources/embeddings_folder/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tag &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.pkl&amp;#34;&lt;/span&gt;
        post_ids, post_embeddings &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(embeddings_path, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;))
        &lt;span style=&#34;color:#75715e&#34;&gt;# Get the embeddings for the question&lt;/span&gt;
        question_vec &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; question_to_vec(question, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;model, &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# find index of most similar post&lt;/span&gt;
        best_post_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pairwise_distances_argmin(question_vec,
                                                    post_embeddings)
        &lt;span style=&#34;color:#75715e&#34;&gt;# return best post id&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; post_ids[best_post_index]

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;generate_answer&lt;/span&gt;(self, question): 
        prepared_question &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; text_prepare(question)
        features &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tfidf_vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform([prepared_question])
        &lt;span style=&#34;color:#75715e&#34;&gt;# find intent&lt;/span&gt;
        intent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;intent_recognizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(features)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
        &lt;span style=&#34;color:#75715e&#34;&gt;# Chit-chat part:   &lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; intent &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dialogue&amp;#39;&lt;/span&gt;:
            response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;chitchat_bot&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_response(question)
        &lt;span style=&#34;color:#75715e&#34;&gt;# Stack Overflow Question&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#75715e&#34;&gt;# find programming language&lt;/span&gt;
            tag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tag_classifier&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(features)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
            &lt;span style=&#34;color:#75715e&#34;&gt;# find most similar question post id&lt;/span&gt;
            post_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_similar_question(question,tag)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
            &lt;span style=&#34;color:#75715e&#34;&gt;# respond with &lt;/span&gt;
            response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;I think its about &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;This thread might help you: https://stackoverflow.com/questions/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; (tag, post_id)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; response&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here is the code for the whole &lt;a href=&#34;https://github.com/MLWhiz/chatbot/blob/master/main.py&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt; for you to use and see. Just run the whole &lt;code&gt;main.py&lt;/code&gt; using&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python main.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And we will have our bot up and running.&lt;/p&gt;

&lt;p&gt;Again, here is the link to the github &lt;a href=&#34;https://github.com/MLWhiz/chatbot&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;repository&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-possibilities-are-really-endless&#34;&gt;The possibilities are really endless&lt;/h2&gt;

&lt;p&gt;This is just a small demo project of what you can do with the chatbots. You can do a whole lot more once you recognize that the backend is just python.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One idea is to run a chatbot script on all the servers I have to run system commands straight from telegram. We can use &lt;code&gt;os.system&lt;/code&gt; to run any system command. Bye Bye SSH.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can &lt;strong&gt;make chatbots to do some daily tasks by using simple keyword-based intents&lt;/strong&gt;. It is just simple logic. Find out the weather, find out cricket scores or maybe newly released movies. Whatever floats your boat.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Or maybe try to integrate Telegram based Chatbot in your website. See &lt;a href=&#34;https://livechatbot.net/#&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;livechatbot&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Or maybe just try to have fun with it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/chatbot/dilbert_chatbot.jpg&#34;  style=&#34;height:80%;width:80%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Here we learned how to create a simple chatbot. And it works well. We can improve a whole lot on this present chatbot by increasing classifier accuracy, handling edge cases, making it respond faster or maybe adding more logic to handle more use cases. But the fact remains the same. The AI in chatbots is just simple human logic and nothing magic.&lt;/p&gt;

&lt;p&gt;In this post, I closely followed one of the projects from this &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; to create this chatbot. Do check out this course if you get confused, or tell me your problems in the comments I will certainly try to help.&lt;/p&gt;

&lt;p&gt;Follow me up at &lt;a href=&#34;https://medium.com/@rahul_agarwal&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Medium&lt;/a&gt; or Subscribe to my &lt;a href=&#34;https://mlwhiz.com&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;blog&lt;/a&gt; to be informed about my next posts.&lt;/p&gt;

&lt;p&gt;Till then Ciao!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NLP  Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/spiderman.jpeg&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/&#34;&gt;first post&lt;/a&gt; talked about the different &lt;strong&gt;preprocessing techniques that work with Deep learning models&lt;/strong&gt; and &lt;strong&gt;increasing embeddings coverage&lt;/strong&gt;. In the &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/&#34;&gt;second post&lt;/a&gt;, I talked through some &lt;strong&gt;basic conventional models&lt;/strong&gt; like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the &lt;a href=&#34;https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/&#34;&gt;third post&lt;/a&gt;, I delved deeper into &lt;strong&gt;Deep learning models and the various architectures&lt;/strong&gt; we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As a side note&lt;/strong&gt;: if you want to know more about NLP, I would like to &lt;strong&gt;recommend this excellent course&lt;/strong&gt; on &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;Before introducing the notion of transfer learning to NLP applications, we will first need to understand a little bit about Language models.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;language-models-and-nlp-transfer-learning-intuition&#34;&gt;Language Models And NLP Transfer Learning Intuition:&lt;/h2&gt;

&lt;p&gt;In very basic terms the objective of the language model is to &lt;strong&gt;predict the next word given a stream of input words.&lt;/strong&gt; In the past, many different approaches have been used to solve this particular problem. Probabilistic models using Markov assumption is one example of this sort of models.&lt;/p&gt;

&lt;p&gt;$$ P(W_n) = P(W_n|W_{n-1}) $$&lt;/p&gt;

&lt;p&gt;In the recent era, people have been using &lt;em&gt;RNNs/LSTMs&lt;/em&gt; to create such language models. They take as input a word embedding and at each time state return the probability distribution of next word probability over the dictionary words. An example of this is shown below in which the below Neural Network uses multiple stacked layers of RNN cells to learn a language model to predict the next word.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/language_model.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Now why do we need the concept of Language Modeling? Or How does predicting the next word tie with the current task of text classification?&lt;/em&gt; The intuition ties to the way that the neural network gets trained. The neural network that can predict the next word after being trained on a massive corpus like Wikipedia already has learned a lot of structure in a particular language. Can we use this knowledge in the weights of the network for our advantage? Yes, we can, and that is where the idea of Transfer Learning in NLP stems from. So to make this intuition more concrete, Let us think that our neural network is divided into two parts -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Language Specific&lt;/strong&gt;: The lower part of the neural network is language specific. That is it learns the features of the language. This part could be used to transfer our knowledge from a language corpus to our current task&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task Specific&lt;/strong&gt;: I will call the upper part of our network as task specific. The weights in these layers are trained so that it learns to predict the next word.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/language_model_2.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Now as it goes in a lot of transfer learning models for Image, we stack the Language Specific part with some dense and softmax layers(Our new task) and train on our new task to achieve what we want to do.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;ulmfit&#34;&gt;ULMFit:&lt;/h2&gt;

&lt;p&gt;Now the concept of Transfer learning in NLP is not entirely new and people already used Language models for transfer learning back in 2015-16 without good result. So what has changed now?&lt;/p&gt;

&lt;p&gt;The thing that has changed is that people like Jeremy Howard and Sebastian Ruder have done a lot of research on how to train these networks. And so we have achieved state of the art results on many text datasets with Transfer Learning approaches.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s follow up with the key research findings in the &lt;a href=&#34;https://arxiv.org/pdf/1801.06146.pdf&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;ULMFit paper&lt;/a&gt; written by them along with the code.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;change-in-the-way-transfer-learning-networks-are-trained&#34;&gt;Change in the way Transfer Learning networks are trained:&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/ulmfit_training.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Training a model as per ULMFiT we need to take these three steps:&lt;/p&gt;

&lt;p&gt;a) &lt;strong&gt;Create a Base Language Model:&lt;/strong&gt; Training the language model on a general-domain corpus that captures high-level natural language features&lt;br /&gt;
b) &lt;strong&gt;Finetune Base Language Model on Task Specific Data:&lt;/strong&gt; Fine-tuning the pre-trained language model on target task data&lt;br /&gt;
c) &lt;strong&gt;Finetune Base Language Model Layers + Task Specific Layers on Task Specific Data:&lt;/strong&gt; Fine-tuning the classifier on target task data&lt;/p&gt;

&lt;p&gt;So let us go through these three steps one by one along with the code that is provided to us with the FastAI library.&lt;/p&gt;

&lt;h4 id=&#34;a-create-a-base-language-model&#34;&gt;a) Create a Base Language Model:&lt;/h4&gt;

&lt;p&gt;This task might be the most time-consuming task. This model is analogous to resnet50 or Inception for the vision task. In the paper, they use the language model AWD-LSTM, a regular LSTM architecture trained with various tuned dropout hyperparameters. This model was trained on Wikitext-103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words. We won&amp;rsquo;t perform this task ourselves and will use the fabulous FastAI library to use this model as below. The code below will take our data and preprocess it for usage in the AWD_LSTM model as well as load the model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Language model data : We use test_df as validation for language model&lt;/span&gt;
data_lm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TextLMDataBunch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_df(path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,train_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_df ,valid_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; test_df)
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; language_model_learner(data_lm, AWD_LSTM, drop_mult&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is also where we preprocess the data as per the required usage for the FastAI models. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(train_df)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/train_df.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(data_lm)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;TextLMDataBunch;

Train: LabelList (1306122 items)
x: LMTextList
xxbos xxmaj how did xxmaj quebec nationalists see their province as a nation in the 1960s ?,xxbos xxmaj do you have an adopted dog , how would you encourage people to adopt and not shop ?,xxbos xxmaj why does velocity affect time ? xxmaj does velocity affect space geometry ?,xxbos xxmaj how did xxmaj otto von xxmaj guericke used the xxmaj magdeburg hemispheres ?,xxbos xxmaj can i convert montra xxunk d to a mountain bike by just changing the tyres ?
y: LMLabelList
,,,,
Path: .;

Valid: LabelList (375806 items)
x: LMTextList
xxbos xxmaj why do so many women become so rude and arrogant when they get just a little bit of wealth and power ?,xxbos xxmaj when should i apply for xxup rv college of engineering and xxup bms college of engineering ? xxmaj should i wait for the xxup comedk result or am i supposed to apply before the result ?,xxbos xxmaj what is it really like to be a nurse practitioner ?,xxbos xxmaj who are entrepreneurs ?,xxbos xxmaj is education really making good people nowadays ?
y: LMLabelList
,,,,
Path: .;

Test: None
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tokenized prepared data is based on a lot of research from the FastAI developers. To make this post a little bit complete, I am sharing some of the tokens definition as well.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;xxunk&lt;/em&gt; is for an unknown word (one that isn&amp;rsquo;t present in the current vocabulary)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;xxpad&lt;/em&gt; is the token used for padding, if we need to regroup several texts of different lengths in a batch&lt;/li&gt;
&lt;li&gt;&lt;em&gt;xxbos&lt;/em&gt; represents the beginning of a text in your dataset&lt;/li&gt;
&lt;li&gt;&lt;em&gt;xxmaj&lt;/em&gt; is used to indicate the next word begins with a capital in the original text&lt;/li&gt;
&lt;li&gt;&lt;em&gt;xxup&lt;/em&gt; is used to indicate the next word is written in all caps in the original text&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;b-finetune-base-language-model-on-task-specific-data&#34;&gt;b) Finetune Base Language Model on Task Specific Data&lt;/h4&gt;

&lt;p&gt;This task is also pretty easy when we look at the code. The specific details of how we do the training is what holds the essence.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Learning with Discriminative fine tuning&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-2&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Save encoder Object&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save_encoder(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ft_enc&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The paper introduced two general concepts for this learning stage:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Discriminative fine-tuning:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Main Idea is: As different layers capture different types of information, they should be fine-tuned to different extents.
Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning
rates. In the paper, the authors suggest first to finetune only the last layer, and then unfreeze all the layers with a learning rate lowered by a factor of 2.6.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slanted triangular learning rates:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/Stlr.png&#34;  height=&#34;200&#34; width=&#34;400&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;According to the authors: &lt;em&gt;&amp;ldquo;For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters&amp;rdquo;&lt;/em&gt;
The Main Idea is to use a high learning rate at the starting stage for increased learning and low learning rates to finetune at later stages in an epoch.&lt;/p&gt;

&lt;p&gt;After training our Language model on the Quora dataset, we should be able to see how our model performs on the Language Model task itself. FastAI library provides us with a simple function to do that.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# check how the language model performs&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;What should&amp;#34;&lt;/span&gt;, n_words&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;&#39;What should be the likelihood of a tourist visiting Mumbai for&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;c-finetune-base-language-model-layers-task-specific-layers-on-task-specific-data&#34;&gt;c) Finetune Base Language Model Layers + Task Specific Layers on Task Specific Data&lt;/h4&gt;

&lt;p&gt;This is the stage where task-specific learning takes place that is we add the classification layers and fine tune them.&lt;/p&gt;

&lt;p&gt;The authors augment the pretrained language model with two additional
linear blocks. Each block uses batch normalization (Ioffe and Szegedy, 2015) and dropout, with ReLU activations for the intermediate layer and a
softmax activation that outputs a probability distribution over target classes at the last layer. The params of these task-specific layers are the only ones that are learned from scratch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Creating Classification Data&lt;/span&gt;
data_clas &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TextClasDataBunch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_df(path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;, train_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;train, valid_df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;valid,  test_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;test_df, vocab&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data_lm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train_ds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;vocab, bs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;,label_cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;target&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Creating Classifier Object&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; text_classifier_learner(data_clas, AWD_LSTM, drop_mult&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Add weights of finetuned Language model &lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_encoder(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ft_enc&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting Classifier Object&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-2&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting Classifier Object after freezing all but last 2 layers&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;freeze_to(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;5e-3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2.&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5e-3&lt;/span&gt;))
&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting Classifier Object - discriminative learning&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;2e-3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2e-3&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here also the Authors have derived a few novel methods:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Concat Pooling:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors use not only the concatenation of all the hidden state but also the Maxpool and Meanpool representation of all hidden states as input to the linear layers.&lt;/p&gt;

&lt;p&gt;$$ H = [h_1, &amp;hellip; , h_T ] $$&lt;/p&gt;

&lt;p&gt;$$ h_c = [h_T , maxpool(H), meanpool(H)] $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gradual Unfreezing:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rather than fine-tuning all layers at once, which risks catastrophic forgetting(Forgetting everything we have learned so far from language models), the authors propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge. The Authors first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. They then unfreeze the next lower frozen layer and repeat, until they finetune all layers until convergence at the last iteration. The function &lt;code&gt;slice(2e-3/100, 2e-3)&lt;/code&gt; means that we train every layer with different learning rates ranging from max to min value.&lt;/p&gt;

&lt;p&gt;One can get the predictions for the test data at once using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;test_preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_preds(DatasetType&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Test, ordered&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I am a big fan of Kaggle Kernels. One could not have imagined having all that compute for free. You can find a running version of the above code in this &lt;a href=&#34;https://www.kaggle.com/mlwhiz/ulmfit&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;kaggle kernel&lt;/a&gt;. Do try to experiment with it after forking and running the code. Also please upvote the kernel if you find it helpful.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;results&#34;&gt;Results:&lt;/h2&gt;

&lt;p&gt;Here are the final results of all the different approaches I have tried on the Kaggle Dataset. I ran a 5 fold Stratified CV.&lt;/p&gt;

&lt;h3 id=&#34;a-conventional-methods&#34;&gt;a. Conventional Methods:&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/results_conv.png&#34;  style=&#34;height:40%;width:40%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3 id=&#34;b-deep-learning-methods&#34;&gt;b. Deep Learning Methods:&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/results_deep_learning.png&#34;  style=&#34;height:50%;width:50%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3 id=&#34;c-transfer-learning-methods-ulmfit&#34;&gt;c. Transfer Learning Methods(ULMFIT):&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/nlp_tl/results_ulm.png&#34;  style=&#34;height:30%;width:30%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;The results achieved were not very good compared to deep learning methods, but I still liked the idea of the transfer learning approach, and it was so easy to implement it using fastAI. Also running the code took a lot of time at 9 hours, compared to other methods which got over in 2 hours.&lt;/p&gt;

&lt;p&gt;Even if this approach didn&amp;rsquo;t work well for this dataset, it is a valid approach for other datasets, as the Authors of the paper have achieved pretty good results on different datasets — definitely a genuine method to try out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Note that I didn&amp;rsquo;t work on tuning the above models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning &lt;a href=&#34;https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/&#34;&gt;using hyperopt&lt;/a&gt; or just old fashioned Grid-search.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h2&gt;

&lt;p&gt;Finally, this post concludes my NLP Learning series. It took a lot of time to write, but the effort was well worth it. I hope you found it helpful in your work. I will try to write some more on this topic when I get some time. Follow me up at &lt;a href=&#34;https://medium.com/@rahul_agarwal&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Medium&lt;/a&gt; or Subscribe to my blog to be informed about my next posts.&lt;/p&gt;

&lt;p&gt;Also if you want to &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;learn more about NLP&lt;/strong&gt; here&lt;/a&gt; is an excellent course. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;Let me know if you think I can add something more to the post; I will try to incorporate it.&lt;/p&gt;

&lt;p&gt;Cheers!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NLP  Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid>
      <description>

&lt;p&gt;This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/&#34;&gt;first post&lt;/a&gt; talked about the different &lt;strong&gt;preprocessing techniques that work with Deep learning models&lt;/strong&gt; and &lt;strong&gt;increasing embeddings coverage&lt;/strong&gt;. In the &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/&#34;&gt;second post&lt;/a&gt;, I talked through some &lt;strong&gt;basic conventional models&lt;/strong&gt; like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong&gt;Deep learning models and the various architectures&lt;/strong&gt; we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong&gt;ULMFit transfer learning&lt;/strong&gt; approaches in the fourth post in the series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As a side note&lt;/strong&gt;: if you want to know more about NLP, I would like to &lt;strong&gt;recommend this excellent course&lt;/strong&gt; on &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;So let me try to go through some of the models which people are using to perform text classification and try to provide a brief intuition for them — also, some code in Keras and Pytorch. So you can try them out for yourself.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;1-textcnn&#34;&gt;1. TextCNN&lt;/h2&gt;

&lt;p&gt;The idea of using a CNN to classify text was first presented in the paper &lt;a href=&#34;https://www.aclweb.org/anthology/D14-1181&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Convolutional Neural Networks for Sentence Classification&lt;/a&gt; by Yoon Kim.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation:&lt;/strong&gt; The central intuition about this idea is to &lt;strong&gt;see our documents as images&lt;/strong&gt;. How? Let us say we have a sentence and we have maxlen = 70 and embedding size = 300. We can create a matrix of numbers with the shape 70x300 to represent this sentence. For images, we also have a matrix where individual elements are pixel values. Instead of image pixels, the input to the tasks is sentences or documents represented as a matrix. Each row of the matrix corresponds to one-word vector.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/text_convolution.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Convolution Idea:&lt;/strong&gt; While for an image we move our conv filter horizontally as well as vertically, for text we fix kernel size to filter_size x embed_size, i.e. (3,300) we are just going to move vertically down for the convolution taking look at three words at once since our filter size is 3 in this case. This idea seems right since our convolution filter is not splitting word embedding. It gets to look at the full embedding of each word. Also one can think of filter sizes as unigrams, bigrams, trigrams, etc. Since we are looking at a context window of 1,2,3, and 5 words respectively.&lt;/p&gt;

&lt;p&gt;Here is the text classification network coded in Pytorch:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; nn
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn.functional &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; F
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch.autograd &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Variable


&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CNN_Text&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
        super(CNN_Text, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
        filter_sizes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
        num_filters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;36&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(max_features, embed_size)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Parameter(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(embedding_matrix, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32))
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;convs1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ModuleList([nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Conv2d(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, num_filters, (K, embed_size)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; K &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; filter_sizes])
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dropout(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(len(Ks)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;num_filters, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)


    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding(x)  
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)  
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu(conv(x))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; conv &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;convs1] 
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_pool1d(i, i&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; x]  
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropout(x)  
        logit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc1(x)  
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; logit&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And for the Keras enthusiasts:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# https://www.kaggle.com/yekenot/2dcnn-textclassifier&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model_cnn&lt;/span&gt;(embedding_matrix):
    filter_sizes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
    num_filters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;36&lt;/span&gt;

    inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Input(shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(maxlen,))
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Embedding(max_features, embed_size, weights&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[embedding_matrix])(inp)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Reshape((maxlen, embed_size, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))(x)

    maxpool_pool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(filter_sizes)):
        conv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Conv2D(num_filters, kernel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(filter_sizes[i], embed_size),
                                     kernel_initializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;he_normal&amp;#39;&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;)(x)
        maxpool_pool&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(MaxPool2D(pool_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(maxlen &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; filter_sizes[i] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))(conv))

    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Concatenate(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)(maxpool_pool)   
    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Flatten()(z)
    z &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dropout(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)(z)

    outp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sigmoid&amp;#34;&lt;/span&gt;)(z)

    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Model(inputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;inp, outputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;outp)
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;binary_crossentropy&amp;#39;&lt;/span&gt;, optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I am a big fan of Kaggle Kernels. One could not have imagined having all that compute for free. You can find a running version of the above two code snippets in this &lt;a href=&#34;https://www.kaggle.com/mlwhiz/textcnn-pytorch-and-keras&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;kaggle kernel&lt;/a&gt;. Do try to experiment with it after forking and running the code. Also please upvote the kernel if you find it helpful.&lt;/p&gt;

&lt;p&gt;The Keras model and Pytorch model performed similarly with Pytorch model beating the keras model by a small margin. The Out-Of-Fold CV F1 score for the Pytorch model came out to be 0.6609 while for Keras model the same score came out to be 0.6559. I used the same preprocessing in both the models to be better able to compare the platforms.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2-bidirectional-rnn-lstm-gru&#34;&gt;2. BiDirectional RNN(LSTM/GRU):&lt;/h2&gt;

&lt;p&gt;TextCNN works well for Text Classification. It takes care of words in close range. It can see &amp;ldquo;new york&amp;rdquo; together. However, it still can&amp;rsquo;t take care of all the context provided in a particular text sequence. It still does not learn the sequential structure of the data, where every word is dependent on the previous word. Or a word in the previous sentence.&lt;/p&gt;

&lt;p&gt;RNN help us with that. &lt;em&gt;They can remember previous information using hidden states and connect it to the current task.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Long Short Term Memory networks (LSTM) are a subclass of RNN, specialized in remembering information for an extended period. Moreover, the Bidirectional LSTM keeps the contextual information in both directions which is pretty useful in text classification task (But won&amp;rsquo;t work for a time series prediction task as we don&amp;rsquo;t have visibility into the future in this case).&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/birnn.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;For a most simplistic explanation of Bidirectional RNN, think of RNN cell as a black box taking as input a hidden state(a vector) and a word vector and giving out an output vector and the next hidden state. This box has some weights which are to be tuned using Backpropagation of the losses. Also, the same cell is applied to all the words so that the weights are shared across the words in the sentence. This phenomenon is called weight-sharing.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/singlernn.png&#34;  height=&#34;30%&#34; width=&#34;30%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;        Hidden state, Word vector -&amp;gt;(RNN Cell) -&amp;gt; Output Vector , Next Hidden state
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a sequence of length 4 like &lt;strong&gt;&amp;ldquo;you will never believe&amp;rdquo;&lt;/strong&gt;, The RNN cell gives 4 output vectors, which can be concatenated and then used as part of a dense feedforward architecture.&lt;/p&gt;

&lt;p&gt;In the Bidirectional RNN, the only change is that we read the text in the usual fashion as well in reverse. So we stack two RNNs in parallel, and hence we get 8 output vectors to append.&lt;/p&gt;

&lt;p&gt;Once we get the output vectors, we send them through a series of dense layers and finally a softmax layer to build a text classifier.&lt;/p&gt;

&lt;p&gt;In most cases, you need to understand how to stack some layers in a neural network to get the best results. We can try out multiple bidirectional GRU/LSTM layers in the network if it performs better.&lt;/p&gt;

&lt;p&gt;Due to the limitations of RNNs like not remembering long term dependencies, in practice, we almost always use LSTM/GRU to model long term dependencies. In such a case you can think of the RNN cell being replaced by an LSTM cell or a GRU cell in the above figure. An example model is provided below. You can use CuDNNGRU interchangeably with CuDNNLSTM when you build models. (CuDNNGRU/LSTM are just implementations of LSTM/GRU that are created to run faster on GPUs. In most cases always use them instead of the vanilla LSTM/GRU implementations)&lt;/p&gt;

&lt;p&gt;So here is some code in Pytorch for this network.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;BiLSTM&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
        super(BiLSTM, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hidden_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;
        drp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(max_features, embed_size)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Parameter(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(embedding_matrix, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32))
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LSTM(embed_size, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hidden_size, bidirectional&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, batch_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hidden_size&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; , &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ReLU()
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dropout(drp)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)


    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
        h_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding(x)
        h_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(h_embedding, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;))
        
        h_lstm, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm(h_embedding)
        avg_pool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(h_lstm, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        max_pool, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(h_lstm, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat(( avg_pool, max_pool), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear(conc))
        conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dropout(conc)
        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out(conc)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, here is the same code in Keras.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# BiDirectional LSTM&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model_lstm_du&lt;/span&gt;(embedding_matrix):
    inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Input(shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(maxlen,))
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Embedding(max_features, embed_size, weights&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[embedding_matrix])(inp)
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        64*70(maxlen)*2(bidirection concat)
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Bidirectional(CuDNNLSTM(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, return_sequences&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True))(x)
    avg_pool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GlobalAveragePooling1D()(x)
    max_pool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GlobalMaxPooling1D()(x)
    conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; concatenate([avg_pool, max_pool])
    conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;relu&amp;#34;&lt;/span&gt;)(conc)
    conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dropout(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)(conc)
    outp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sigmoid&amp;#34;&lt;/span&gt;)(conc)
    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Model(inputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;inp, outputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;outp)
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;binary_crossentropy&amp;#39;&lt;/span&gt;, optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run this code in my &lt;a href=&#34;https://www.kaggle.com/mlwhiz/bilstm-pytorch-and-keras&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;BiLSTM with Pytorch and Keras kaggle kernel&lt;/a&gt; for this competition. Please do upvote the kernel if you find it helpful.&lt;/p&gt;

&lt;p&gt;In the BiLSTM case also, Pytorch model beats the keras model by a small margin. The Out-Of-Fold CV F1 score for the Pytorch model came out to be 0.6741 while for Keras model the same score came out to be 0.6727. This score is around a 1-2% increase from the TextCNN performance which is pretty good. Also, note that it is around 6-7% better than conventional methods.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;3-attention-models&#34;&gt;3. Attention Models&lt;/h2&gt;

&lt;p&gt;Dzmitry Bahdanau et al first presented attention in their paper &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt; but I find that the paper on &lt;a href=&#34;https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Hierarchical Attention Networks for Document Classification&lt;/a&gt; written jointly by CMU and Microsoft in 2016 is a much easier read and provides more intuition.&lt;/p&gt;

&lt;p&gt;So let us talk about the intuition first. In the past conventional methods like TFIDF/CountVectorizer etc. we used to find features from the text by doing a keyword extraction. Some word is more helpful in determining the category of a text than others. However, in this method we sort of lost the sequential structure of the text. With LSTM and deep learning methods, while we can take care of the sequence structure, we lose the ability to give higher weight to more important words.
&lt;strong&gt;Can we have the best of both worlds?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The answer is Yes. Actually, &lt;strong&gt;Attention is all you need&lt;/strong&gt;. In the author&amp;rsquo;s words:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/birnn attention.png&#34;  height=&#34;400&#34; width=&#34;700&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;In essence, we want to create scores for every word in the text, which is the attention similarity score for a word.&lt;/p&gt;

&lt;p&gt;To do this, we start with a weight matrix(W), a bias vector(b) and a context vector u. The optimization algorithm learns all of these weights. On this note I would like to highlight something I like a lot about neural networks - If you don&amp;rsquo;t know some params, let the network learn them. We only have to worry about creating architectures and params to tune.&lt;/p&gt;

&lt;p&gt;Then there are a series of mathematical operations. See the figure for more clarification. We can think of u1 as nonlinearity on RNN word output. After that v1 is a dot product of u1 with a context vector u raised to exponentiation. From an intuition viewpoint, the value of v1 will be high if u and u1 are similar. Since we want the sum of scores to be 1, we divide v by the sum of v’s to get the Final Scores,s&lt;/p&gt;

&lt;p&gt;These final scores are then multiplied by RNN output for words to weight them according to their importance. After which the outputs are summed and sent through dense layers and softmax for the task of text classification.&lt;/p&gt;

&lt;p&gt;Here is the code in Pytorch. &lt;strong&gt;Do try to read through the pytorch code for attention layer.&lt;/strong&gt; It just does what I have explained above.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Attention&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, feature_dim, step_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        super(Attention, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)
        
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;supports_masking &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bias
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; feature_dim
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; step_dim
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;features_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        
        weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(feature_dim, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;kaiming_uniform_(weight)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Parameter(weight)
        
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; bias:
            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Parameter(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(step_dim))
        
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x, mask&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
        feature_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;feature_dim 
        step_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step_dim

        eij &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mm(
            x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;contiguous()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, feature_dim), 
            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight
        )&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, step_dim)
        
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias:
            eij &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; eij &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b
            
        eij &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tanh(eij)
        a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(eij)
        
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; mask &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mask

        a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(a, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, keepdim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-10&lt;/span&gt;)

        weighted_input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(a, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(weighted_input, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Attention_Net&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self):
        super(Attention_Net, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()
        drp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Embedding(max_features, embed_size)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Parameter(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tensor(embedding_matrix, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32))
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding_dropout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Dropout2d(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LSTM(embed_size, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, bidirectional&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, batch_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;GRU(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, bidirectional&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, batch_first&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;attention_layer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Attention(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, maxlen)
        
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; , &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ReLU()
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
        h_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding(x)
        h_embedding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(h_embedding, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;))
        h_lstm, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm(h_embedding)
        h_lstm, _ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lstm2(h_lstm)
        h_lstm_atten &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;attention_layer(h_lstm)
        conc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear(h_lstm_atten))
        out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;out(conc)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; out&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Same code for Keras.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dot_product&lt;/span&gt;(x, kernel):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Wrapper for dot product operation, in order to be compatible with both
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Theano and Tensorflow
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Args:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        x (): input
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kernel (): weights
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Returns:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backend() &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tensorflow&amp;#39;&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(x, K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;expand_dims(kernel)), axis&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(x, kernel)
    

&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;AttentionWithContext&lt;/span&gt;(Layer):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Attention operation, with a context/query vector, for temporal data.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Supports Masking.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;Hierarchical Attention Networks for Document Classification&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    by using a context vector to assist the attention
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    # Input shape
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        3D tensor with shape: `(samples, steps, features)`.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    # Output shape
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        2D tensor with shape: `(samples, features)`.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    How to use:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    The dimensions are inferred based on the output shape of the RNN.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Note: The layer has been tested with Keras 2.0.6
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    Example:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        model.add(LSTM(64, return_sequences=True))
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        model.add(AttentionWithContext())
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        # next add a Dense layer (for classification/regression) or whatever...
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self,
                 W_regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, u_regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, b_regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None,
                 W_constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, u_constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, b_constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None,
                 bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;supports_masking &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; initializers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;glorot_uniform&amp;#39;&lt;/span&gt;)

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_regularizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; regularizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(W_regularizer)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u_regularizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; regularizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(u_regularizer)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b_regularizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; regularizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(b_regularizer)

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_constraint &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; constraints&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(W_constraint)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u_constraint &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; constraints&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(u_constraint)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b_constraint &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; constraints&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(b_constraint)

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bias
        super(AttentionWithContext, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;build&lt;/span&gt;(self, input_shape):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(input_shape) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_weight((input_shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], input_shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],),
                                 initializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init,
                                 name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{}_W&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name),
                                 regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_regularizer,
                                 constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_constraint)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias:
            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_weight((input_shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],),
                                     initializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;zero&amp;#39;&lt;/span&gt;,
                                     name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{}_b&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name),
                                     regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b_regularizer,
                                     constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b_constraint)

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_weight((input_shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],),
                                 initializer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;init,
                                 name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{}_u&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name),
                                 regularizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u_regularizer,
                                 constraint&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u_constraint)

        super(AttentionWithContext, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;build(input_shape)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_mask&lt;/span&gt;(self, input, input_mask&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
        &lt;span style=&#34;color:#75715e&#34;&gt;# do not pass the mask to the next layers&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; None

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;(self, x, mask&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
        uit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dot_product(x, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W)

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bias:
            uit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b

        uit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tanh(uit)
        ait &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dot_product(uit, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;u)

        a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(ait)

        &lt;span style=&#34;color:#75715e&#34;&gt;# apply mask after the exp. will be re-normalized next&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; mask &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            &lt;span style=&#34;color:#75715e&#34;&gt;# Cast the mask to floatX to avoid float64 upcasting in theano&lt;/span&gt;
            a &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cast(mask, K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floatx())

        &lt;span style=&#34;color:#75715e&#34;&gt;# in some cases especially in the early stages of training the sum may be almost zero&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# and this results in NaN&amp;#39;s. A workaround is to add a very small positive number ε to the sum.&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())&lt;/span&gt;
        a &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cast(K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(a, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, keepdims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;epsilon(), K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;floatx())

        a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;expand_dims(a)
        weighted_input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; a
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(weighted_input, axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_output_shape&lt;/span&gt;(self, input_shape):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; input_shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], input_shape[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;model_lstm_atten&lt;/span&gt;(embedding_matrix):
    inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Input(shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(maxlen,))
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Embedding(max_features, embed_size, weights&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[embedding_matrix], trainable&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)(inp)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Bidirectional(CuDNNLSTM(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, return_sequences&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True))(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Bidirectional(CuDNNLSTM(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, return_sequences&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True))(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AttentionWithContext()(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;relu&amp;#34;&lt;/span&gt;)(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Dense(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, activation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sigmoid&amp;#34;&lt;/span&gt;)(x)
    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Model(inputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;inp, outputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;x)
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;binary_crossentropy&amp;#39;&lt;/span&gt;, optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, my &lt;a href=&#34;https://www.kaggle.com/mlwhiz/attention-pytorch-and-keras&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Attention with Pytorch and Keras Kaggle kernel&lt;/a&gt; contains the working versions for this code. Please do upvote the kernel if you find it useful.&lt;/p&gt;

&lt;p&gt;This method performed well with Pytorch CV scores reaching around 0.6758 and Keras CV scores reaching around 0.678. &lt;strong&gt;This score is more than what we were able to achieve with BiLSTM and TextCNN.&lt;/strong&gt; However, please note that we didn&amp;rsquo;t work on tuning any of the given methods yet and so the scores might be different.&lt;/p&gt;

&lt;p&gt;With this, I leave you to experiment with new architectures and playing around with stacking multiple GRU/LSTM layers to improve your network performance. You can also look at including more techniques in these network like Bucketing, handmade features, etc. Some of the tips and new techniques are mentioned here on my blog post: &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/&#34;&gt;What my first Silver Medal taught me about Text Classification and Kaggle in general?&lt;/a&gt;. Also, here is another Kaggle kernel which is &lt;a href=&#34;https://www.kaggle.com/mlwhiz/multimodel-ensemble-clean-kernel?scriptVersionId=10279838&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;my silver-winning entry&lt;/a&gt; for this competition.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;Here are the final results of all the different approaches I have tried on the Kaggle Dataset. I ran a 5 fold Stratified CV.&lt;/p&gt;

&lt;h3 id=&#34;a-conventional-methods&#34;&gt;a. Conventional Methods:&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/results_conv.png&#34;  style=&#34;height:40%;width:40%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3 id=&#34;b-deep-learning-methods&#34;&gt;b. Deep Learning Methods:&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/results_deep_learning.png&#34;  style=&#34;height:50%;width:50%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Note that I didn&amp;rsquo;t work on tuning the above models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning &lt;a href=&#34;https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/&#34;&gt;using hyperopt&lt;/a&gt; or just old fashioned Grid-search.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, I went through with the explanations of various deep learning architectures people are using for Text classification tasks. In the next post, we will delve further into the next new phenomenon in NLP space - Transfer Learning with BERT and ULMFit. Follow me up at &lt;a href=&#34;https://medium.com/@rahul_agarwal&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Medium&lt;/a&gt; or Subscribe to my blog to be informed about my next post.&lt;/p&gt;

&lt;p&gt;Also if you want to &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;learn more about NLP&lt;/strong&gt; here&lt;/a&gt; is an excellent course. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;Let me know if you think I can add something more to the post; I will try to incorporate it.&lt;/p&gt;

&lt;p&gt;Cheers!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title>
      <link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid>
      <description>

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/silver/leaderboard.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong&gt;Quora Insincere questions classification&lt;/strong&gt; in which I got a rank of &lt;strong&gt;&lt;code&gt;182/4037&lt;/code&gt;&lt;/strong&gt;. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As a side note&lt;/strong&gt;: if you want to know more about NLP, I would like to &lt;strong&gt;recommend this awesome course&lt;/strong&gt; on &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;So first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. &lt;strong&gt;&lt;em&gt;The challenge was not only a test for performance but also a test of efficient code writing skills.&lt;/em&gt;&lt;/strong&gt; As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.&lt;/p&gt;

&lt;h2 id=&#34;some-kaggle-learnings&#34;&gt;Some Kaggle Learnings:&lt;/h2&gt;

&lt;p&gt;There were a couple of &lt;strong&gt;learnings about kaggle as a whole&lt;/strong&gt; that I would like to share before jumping into my final solution:&lt;/p&gt;

&lt;h3 id=&#34;1-always-trust-your-cv&#34;&gt;1. Always trust your CV&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/silver/CV_vs_LB.png&#34;  style=&#34;height:50%;width:50%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;One of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was &lt;strong&gt;small test dataset&lt;/strong&gt;(only 65k rows) in the first stage(around 15% of total test data).&lt;/p&gt;

&lt;p&gt;A common theme on discussion forums was focussing on which submissions we should select as the final submission:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The one having the best local CV? or&lt;/li&gt;
&lt;li&gt;The one having the best LB?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.&lt;/p&gt;

&lt;p&gt;Luckily I didn&amp;rsquo;t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, &lt;strong&gt;&lt;em&gt;I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around &amp;gt;1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score &amp;gt;= 0.70&lt;/p&gt;

&lt;h3 id=&#34;2-use-the-code-from-public-kernels-but-check-for-errors&#34;&gt;2. Use the code from public kernels but check for errors&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;This&lt;/a&gt; Pytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn&amp;rsquo;t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/&#34;&gt;here&lt;/a&gt; or on my &lt;a href=&#34;https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;kernel&lt;/a&gt;. Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.&lt;/p&gt;

&lt;p&gt;Nonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.&lt;/p&gt;

&lt;h3 id=&#34;3-don-t-trust-everything-that-goes-on-the-discussion-forums&#34;&gt;3. Don&amp;rsquo;t trust everything that goes on the discussion forums&lt;/h3&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/silver/read-what-the-smart-people-are-saying.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;I will talk about two things here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Seed tuning&lt;/strong&gt;: While in the middle of the competition, everyone was trying to get the best possible rank on the public LB. It is just human nature. A lot of discussions was around good seeds and bad seeds for neural network initialization. While it seems okay in the first look, the conversation went a stage further where &lt;strong&gt;people started tuning seeds in the kernel as a hyper param&lt;/strong&gt;. Some discussions even went on to say that it was a valid strategy. And that is where a large amount of overfitting to public LB started happening. The same submission would score 0.704 from 0.699 just by changing the seed. For a reference, that meant you could go from anywhere near 400-500 rank to top 50 only by changing seed in a public kernel. And that spelled disaster. Some people did that. They went up the public LB. Went crashing out at the private stage.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;CV score disclosure on discussion forums&lt;/strong&gt;: We always try to gauge our performance against other people. In a lot of discussions, people provided their CV scores and corresponding Public LB scores. The scores were all over the place and not comparable due to Different CV schemes, No of folds in CV, Metric reported, Overfitting or just plain Wrong implementation of Cross-Validation. But they ended up influencing a lot of starters and newcomers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;4-on-that-note-be-active-on-discussion-forums-and-check-public-kernels-regularly&#34;&gt;4. On that note, be active on Discussion forums and check public kernels regularly&lt;/h3&gt;

&lt;p&gt;You can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by &lt;a href=&#34;https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;SRK&lt;/a&gt;, Models by &lt;a href=&#34;https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Shujian&lt;/a&gt;, and Preprocessing by &lt;a href=&#34;https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Theo Viel&lt;/a&gt; which gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.&lt;/p&gt;

&lt;p&gt;Even after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very ** vital to check out the winning solutions.**&lt;/p&gt;

&lt;h3 id=&#34;5-share-a-lot&#34;&gt;5. Share a lot&lt;/h3&gt;

&lt;p&gt;Sharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/&#34;&gt;first post&lt;/a&gt; talked about the different &lt;strong&gt;preprocessing techniques that work with Deep learning models&lt;/strong&gt; and &lt;strong&gt;increasing embeddings coverage&lt;/strong&gt;. In the &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/&#34;&gt;second post&lt;/a&gt;, I talked through some &lt;strong&gt;basic conventional models&lt;/strong&gt; like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into &lt;strong&gt;Deep learning models and the various architectures&lt;/strong&gt; we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like &lt;strong&gt;ULMFit transfer learning&lt;/strong&gt; approaches in the fourth post in the series.&lt;/p&gt;

&lt;p&gt;It might take me a little time to write the whole series. Till then you can take a look at my other posts too: &lt;a href=&#34;https://mlwhiz.com/blog/2018/12/17/text_classification/&#34;&gt;What Kagglers are using for Text Classification&lt;/a&gt;, which talks about various deep learning models in use in NLP and &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/&#34;&gt;how to switch from Keras to Pytorch&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;6-beware-of-trolls&#34;&gt;6. Beware of trolls :)&lt;/h3&gt;

&lt;p&gt;We were going along happily towards the end of the competition with two weeks left. Scores were increasing slowly. The top players were somewhat stagnant. &lt;strong&gt;&lt;em&gt;And then came Pavel and team with a Public LB score of 0.782.&lt;/em&gt;&lt;/strong&gt; The next group had an LB score of 0.713. Such a huge difference. I was so sure that there was some leakage in the data which nobody has caught yet except for Pavel. I spent nearly half a day to do EDA again.&lt;/p&gt;

&lt;p&gt;In the end, it turned out that what they did was &lt;a href=&#34;https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80665&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;scraping&lt;/a&gt; — nicely played!&lt;/p&gt;

&lt;p&gt;They also have some pretty awesome ideas around including additional data, which could have worked but did not in this competition.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;my-final-solution&#34;&gt;My Final Solution:&lt;/h2&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/silver/lb2.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;My main focus was on &lt;em&gt;meta-feature engineering&lt;/em&gt; and on &lt;em&gt;increasing embedding coverage and quality&lt;/em&gt;. That means I did not play much with various Neural Net architectures. Here are the things that I included in my final submission:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I noticed that Glove embeddings were doing good on the local CV but not on LB, while meta embeddings(mean of glove and paragram) were doing good on LB but not that good on the CV. I took a mixed approach so &lt;strong&gt;some of my models are trained with only glove embedding and some on meta embeddings&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Included four more features in embedding&lt;/strong&gt;. Thus my embedding was a 304-dimensional vector. The four new values corresponded to title case flag, uppercase flag, Textblob word polarity, textblob word subjectivity&lt;/li&gt;
&lt;li&gt;Found out &lt;strong&gt;NER tokens from the whole train and test data using spacy&lt;/strong&gt; and kept the tokens and the entities in a dict. I used this dict to create extra features like counts of &lt;code&gt;GPE&lt;/code&gt;, &lt;code&gt;PERSON&lt;/code&gt;, &lt;code&gt;ORG&lt;/code&gt;, &lt;code&gt;NORP&lt;/code&gt;, &lt;code&gt;WORK_OF_ART&lt;/code&gt;.Added some value and were highly correlated with the target.&lt;/li&gt;
&lt;li&gt;Other features that I used include &lt;code&gt;total_length&lt;/code&gt;,&lt;code&gt;capitals&lt;/code&gt;,&lt;code&gt;words_vs_unique&lt;/code&gt; as well as some engineered features like &lt;code&gt;sum_feat&lt;/code&gt;(sum of expletives), &lt;code&gt;question_start_with_why&lt;/code&gt;, &lt;code&gt;question_start_with_how_or_what&lt;/code&gt;, &lt;code&gt;question_start_with_do_or_are&lt;/code&gt;. Might not have added much value but still kept them.&lt;/li&gt;
&lt;li&gt;My final solution consisted of a &lt;strong&gt;stacked ensemble for four models&lt;/strong&gt;. I stacked the four models using Logistic regression(with positive weights and 0 intercept) and gave the weights as a list in the final kernel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find the kernel for my final submission &lt;a href=&#34;https://www.kaggle.com/mlwhiz/multimodel-ensemble-clean-kernel?scriptVersionId=10279838&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;tips-and-tricks-used-in-other-solutions&#34;&gt;Tips and Tricks used in other solutions:&lt;/h2&gt;

&lt;h3 id=&#34;1-increasing-embeddings-coverage&#34;&gt;1. Increasing Embeddings Coverage:&lt;/h3&gt;

&lt;p&gt;In the third place solution &lt;a href=&#34;https://www.kaggle.com/wowfattie/3rd-place&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;kernel&lt;/a&gt;, wowfattie uses stemming, lemmatization, capitalize, lower, uppercase, as well as embedding of the nearest word using a spell checker to get embeddings for all words in his vocab. Such a great idea. &lt;strong&gt;I liked this solution the best as it can do what I was trying to do and finished at a pretty good place.&lt;/strong&gt; Also, the code is very clean.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.stem &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; PorterStemmer
ps &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PorterStemmer()
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.stem.lancaster &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LancasterStemmer
lc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LancasterStemmer()
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.stem &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SnowballStemmer
sb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SnowballStemmer(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;english&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_glove&lt;/span&gt;(word_dict, lemma_dict):
    EMBEDDING_FILE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_coefs&lt;/span&gt;(word,&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;arr): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; word, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;asarray(arr, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;)
    embeddings_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dict(get_coefs(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; open(EMBEDDING_FILE))
    embed_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;
    nb_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(word_dict)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    embedding_matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((nb_words, embed_size), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
    unknown_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros((embed_size,), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(unknown_vector[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;])
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(word_dict):
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;upper()
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; key&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;capitalize()
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ps&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stem(key)
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stem(key)
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stem(key)
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lemma_dict[key]
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(key) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
            word &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; correction(key)
            embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
                embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        embedding_matrix[word_dict[key]] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; unknown_vector                    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embedding_matrix, nb_words &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;2-checkpoint-ensembling&#34;&gt;2. Checkpoint Ensembling:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Get a lot of models at no cost&lt;/strong&gt;. Most of the winning solutions have some version of checkpoint ensembling. For the third place solution, the predictions are a weighted average of predictions after the 4th epoch and predictions after the 5th epoch. I got this idea but forgot to implement it in my ensemble based kernel submission.&lt;/p&gt;

&lt;h3 id=&#34;3-meta-embeddings&#34;&gt;3. Meta Embeddings:&lt;/h3&gt;

&lt;p&gt;A lot of winning solutions ended up using &lt;strong&gt;weighted meta embeddings&lt;/strong&gt; where they provided a higher weight to the Glove embedding. Some solutions also used &lt;strong&gt;concatenated embeddings&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;4-model-architecture&#34;&gt;4. Model Architecture:&lt;/h3&gt;

&lt;p&gt;One surprising thing I saw people doing was to use a &lt;strong&gt;1Dconv layer just after the Bidirectional layer&lt;/strong&gt;. For example, This is the &lt;a href=&#34;https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;architecture&lt;/a&gt; used by the team that placed first in the competition.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/silver/arch_1_place.png&#34;  style=&#34;height:50%;width:50%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;h3 id=&#34;5-bucketing-variable-sequence-length-and-increased-hidden-units&#34;&gt;5. Bucketing/Variable Sequence Length and increased hidden units:&lt;/h3&gt;

&lt;p&gt;Another thing I noticed is the increased number of hidden units as compared to many public kernels. Most of the public kernels used a hidden unit size of 60 due to time constraints. I used 80 units at the cost of training one less network. A lot of high scoring kernels were able to use a higher number of hidden units owing to variable sequence length idea or bucketing. From the 1st place kernel discussion:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct &lt;strong&gt;padding and truncation on the data generator level for each batch separately&lt;/strong&gt;, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also from 7th place &lt;a href=&#34;https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80561&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;discussion&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Bucketing is to make a minibatch from instances that have similar lengths to alleviate the cost of padding. This makes the training speed more than &lt;strong&gt;3x faster, and thus I can run 9 epochs for each split of 5-fold.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus the use of this technique also allowed some competitors to fit many more epochs in less time and run more models at the same time. Pretty Neat!&lt;/p&gt;

&lt;h3 id=&#34;6-for-those-winners-who-didn-t-use-bucketing-maxlen-72-was-too-large&#34;&gt;6. For those winners who didn&amp;rsquo;t use bucketing, Maxlen = 72 was too large:&lt;/h3&gt;

&lt;p&gt;Most of us saw a distribution of question length and took the length that covered maximum questions fully as the maxlen parameter. I never tried to tune it, but it seems like it could have been tuned. &lt;strong&gt;One of the tricks was to use maxlen ranging from 35 to 60.&lt;/strong&gt; This made the kernels run a lot faster.&lt;/p&gt;

&lt;h3 id=&#34;7-time-taking-models-complex-architectures-like-capsule-were-mostly-not-used&#34;&gt;7. Time taking models/complex architectures like Capsule were mostly not used:&lt;/h3&gt;

&lt;p&gt;Most of the winning solutions didn&amp;rsquo;t use capsule networks as they took a lot of time to train.&lt;/p&gt;

&lt;h3 id=&#34;8-backprop-errors-on-embeddings-weights-in-last-few-epochs&#34;&gt;8. Backprop errors on embeddings weights in last few epochs:&lt;/h3&gt;

&lt;p&gt;Another thing I saw was in the &lt;a href=&#34;https://www.kaggle.com/kentaronakanishi/18th-place-solution&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;18th place kernel&lt;/a&gt; which uses a single model&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;:
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embedding&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;embeddings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;weight&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h2&gt;

&lt;p&gt;It was a good and long 2-month competition, and I learned a lot about Text and NLP during this time. I want to emphasize here is that &lt;strong&gt;I ended up trying a lot of things that didn&amp;rsquo;t work before reaching my final solution&lt;/strong&gt;. It was a little frustrating at times, but in the end, I was happy that I ended up with the best data science practices. Would also like to thank Kaggle master Kazanova who along with some of his friends released a &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-BShznKdc3CUauhfsM7_8xw&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;“How to win a data science competition”&lt;/a&gt; Coursera course. I learned a lot from this course.&lt;/p&gt;

&lt;p&gt;Let me know in the comments if you think something is missing/wrong or if I could add more tips/tricks for this competition.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NLP  Learning Series: Part 2 - Conventional Methods for Text Classification</title>
      <link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid>
      <description>

&lt;p&gt;This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/&#34;&gt;first post&lt;/a&gt; talked about the various &lt;strong&gt;preprocessing techniques that work with Deep learning models&lt;/strong&gt; and &lt;strong&gt;increasing embeddings coverage&lt;/strong&gt;. In this post, I will try to take you through some &lt;strong&gt;basic conventional models&lt;/strong&gt; like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong&gt;Deep learning models&lt;/strong&gt; in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong&gt;ULMFit transfer learning&lt;/strong&gt; approaches in the fourth post in the series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As a side note&lt;/strong&gt;: if you want to know more about NLP, I would like to &lt;strong&gt;recommend this awesome course&lt;/strong&gt; on &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;It might take me a little time to write the whole series. Till then you can take a look at my other posts too: &lt;a href=&#34;https://mlwhiz.com/blog/2018/12/17/text_classification/&#34;&gt;What Kagglers are using for Text Classification&lt;/a&gt;, which talks about various deep learning models in use in NLP and &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/&#34;&gt;how to switch from Keras to Pytorch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So again we start with the first step: Preprocessing.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;basic-preprocessing-techniques-for-text-data-continued&#34;&gt;Basic Preprocessing Techniques for text data(Continued)&lt;/h2&gt;

&lt;p&gt;So in the last post, we talked about various preprocessing methods for text for deep learning purpose. Most of the preprocessing for conventional methods remains the same. &lt;strong&gt;We will still remove special characters, punctuations, and contractions&lt;/strong&gt;. But We also may want to do stemming/lemmatization when it comes to conventional methods. Let us talk about them.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since we are going to create features for words in the feature creation step, it makes sense to reduce words to a common denominator so that &amp;lsquo;organize&amp;rsquo;,&amp;lsquo;organizes&amp;rsquo; and &amp;lsquo;organizing&amp;rsquo; could be referred to by a single word &amp;lsquo;organize&amp;rsquo;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;a-stemming&#34;&gt;a) Stemming&lt;/h3&gt;

&lt;p&gt;Stemming is the process of converting words to their base forms using crude Heuristic rules. For example, one rule could be to remove &amp;rsquo;s&amp;rsquo; from the end of any word, so that &amp;lsquo;cats&amp;rsquo; becomes &amp;lsquo;cat&amp;rsquo;. or another rule could be to replace &amp;lsquo;ies&amp;rsquo; with &amp;lsquo;i&amp;rsquo; so that &amp;lsquo;ponies becomes &amp;lsquo;poni&amp;rsquo;. One of the main point to note here is that when we stem the word we might get a nonsense word like &amp;lsquo;poni&amp;rsquo;. But it will still work for our use case as we count the number of occurrences of a particular word and not focus on the meanings of these words in conventional methods. It doesn&amp;rsquo;t work with deep learning for precisely the same reason.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/text_stemming.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can do this pretty simply by using this function in python.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.stem &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt;  SnowballStemmer
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.tokenize.toktok &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ToktokTokenizer
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;stem_text&lt;/span&gt;(text):
    tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ToktokTokenizer()
    stemmer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SnowballStemmer(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;)
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tokenize(text)
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [token&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip() &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tokens]
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [stemmer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stem(token) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tokens]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(tokens)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h3 id=&#34;b-lemmatization&#34;&gt;b) Lemmatization&lt;/h3&gt;

&lt;p&gt;Lemmatization is very similar to stemming but it aims to remove endings only if the base form is present in a dictionary.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.stem &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; WordNetLemmatizer
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.tokenize.toktok &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ToktokTokenizer
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lemma_text&lt;/span&gt;(text):
    tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ToktokTokenizer()
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tokenize(text)
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [token&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip() &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tokens]
    tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [wordnet_lemmatizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lemmatize(token) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; token &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tokens]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(tokens)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once we are done with processing a text, our text will necessarily go through these following steps.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;clean_sentence&lt;/span&gt;(x):
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clean_text(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clean_numbers(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; replace_typical_misspell(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; remove_stopwords(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; replace_contractions(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lemma_text(x)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#39;&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h2 id=&#34;text-representation&#34;&gt;Text Representation&lt;/h2&gt;

&lt;p&gt;In Conventional Machine learning methods, we ought to create features for a text. There are a lot of representations that are present to achieve this. Let us talk about them one by one.&lt;/p&gt;

&lt;h3 id=&#34;a-bag-of-words-countvectorizer-features&#34;&gt;a) Bag of Words - Countvectorizer Features&lt;/h3&gt;

&lt;p&gt;Suppose we have a series of sentences(documents)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;This is good&amp;#39;&lt;/span&gt;,
     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;This is bad&amp;#39;&lt;/span&gt;,
     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;This is awesome&amp;#39;&lt;/span&gt;
     ]  &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/countvectorizer.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Bag of words will create a dictionary of the most common words in all the sentences. For the example above the dictionary would look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;word_index
{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;this&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;is&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;good&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bad&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;awesome&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then encode the sentences using the above dict.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;This &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; good &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
This &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; bad &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
This &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; awesome &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We could do this pretty simply in Python by using the CountVectorizer class from Python. Don&amp;rsquo;t worry much about the heavy name, it just does what I explained above. It has a lot of parameters most significant of which are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ngram_range:&lt;/strong&gt; I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min_df:&lt;/strong&gt; Minimum no of time an ngram should appear in a corpus to be used as a feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;cnt_vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CountVectorizer(dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32,
            strip_accents&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;unicode&amp;#39;&lt;/span&gt;, analyzer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;,token_pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\w{1,}&amp;#39;&lt;/span&gt;,
            ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;),min_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)


&lt;span style=&#34;color:#75715e&#34;&gt;# we fit count vectorizer to get ngrams from both train and test data.&lt;/span&gt;
cnt_vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(list(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; list(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values))

xtrain_cntv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  cnt_vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) 
xtest_cntv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnt_vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We could then use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like.
For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting a simple Logistic Regression on CV Feats&lt;/span&gt;
clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LogisticRegression(C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;)
clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(xtrain_cntv,y_train)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&amp;rsquo;t forget to upvote.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;b-tfidf-features&#34;&gt;b) TFIDF Features&lt;/h3&gt;

&lt;p&gt;TFIDF is a simple technique to find features from sentences. While in Count features we take count of all the words/ngrams present in a document, with TFIDF we take features only for the significant words. How do we do that? If you think of a document in a corpus, we will consider two things about any word in that document:&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/tfidf.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Term Frequency:&lt;/strong&gt; How important is the word in the document?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$TF(word\ in\ a\ document) = \dfrac{No\ of\ occurances\ of\ that\ word\ in\ document}{No\ of\ words\ in\ document}$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inverse Document Frequency:&lt;/strong&gt; How important the term is in the whole corpus?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$IDF(word\ in\ a\ corpus) = -log(ratio\ of\ documents\ that\ include\ the\ word)$$&lt;/p&gt;

&lt;p&gt;TFIDF then is just multiplication of these two scores.&lt;/p&gt;

&lt;p&gt;Intuitively, One can understand that a word is important if it occurs many times in a document. But that creates a problem. Words like &amp;ldquo;a&amp;rdquo;, &amp;ldquo;the&amp;rdquo; occur many times in sentence. Their TF score will always be high. We solve that by using Inverse Document frequency, which is high if the word is rare, and low if the word is common across the corpus.&lt;/p&gt;

&lt;p&gt;In essence, we want to find important words in a document which are also not very common.&lt;/p&gt;

&lt;p&gt;We could do this pretty simply in Python by using the TFIDFVectorizer class from Python. It has a lot of parameters most significant of which are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ngram_range:&lt;/strong&gt; I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;min_df:&lt;/strong&gt; Minimum no of time an ngram should appear in a corpus to be used as a feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Always start with these features. They work (almost) everytime!&lt;/span&gt;
tfv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TfidfVectorizer(dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32, min_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,  max_features&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, 
            strip_accents&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;unicode&amp;#39;&lt;/span&gt;, analyzer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;,token_pattern&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\w{1,}&amp;#39;&lt;/span&gt;,
            ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), use_idf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,smooth_idf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,sublinear_tf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,
            stop_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting TF-IDF to both training and test sets (semi-supervised learning)&lt;/span&gt;
tfv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(list(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; list(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values))
xtrain_tfv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  tfv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) 
xvalid_tfv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tfv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, we could use these features with any machine learning classification model like Logistic Regression, Naive Bayes, SVM or LightGBM as we would like. &lt;a href=&#34;https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&amp;rsquo;t forget to upvote.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;c-hashing-features&#34;&gt;c) Hashing Features&lt;/h3&gt;

&lt;p&gt;Normally there will be a lot of ngrams in a document corpus. The number of features that our TFIDFVectorizer generated was in excess of 2,00,000 features. This might lead to a problem on very large datasets as we have to hold a very large vocabulary dictionary in memory. One way to counter this is to use the Hash Trick.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/hashfeats.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;One can think of hashing as a single function which maps any ngram to a number range for example between 0 to 1024. Now we don&amp;rsquo;t have to store our ngrams in a dictionary. We can just use the function to get the index of any word, rather than getting the index from a dictionary.&lt;/p&gt;

&lt;p&gt;Since there can be more than 1024 ngrams, different ngrams might map to the same number, and this is called collision. The larger the range we provide our Hashing function, the less is the chance of collisions.&lt;/p&gt;

&lt;p&gt;We could do this pretty simply in Python by using the HashingVectorizer class from Python. It has a lot of parameters most significant of which are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ngram_range:&lt;/strong&gt; I specify in the code (1,3). This means that unigrams, bigrams, and trigrams will be taken into account while creating features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n_features:&lt;/strong&gt; No of features you want to consider. The range I gave above.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Always start with these features. They work (almost) everytime!&lt;/span&gt;
hv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; HashingVectorizer(dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32,
            strip_accents&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;unicode&amp;#39;&lt;/span&gt;, analyzer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;,
            ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;),n_features&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,non_negative&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;span style=&#34;color:#75715e&#34;&gt;# Fitting Hash Vectorizer to both training and test sets (semi-supervised learning)&lt;/span&gt;
hv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(list(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; list(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values))
xtrain_hv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  hv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values) 
xvalid_hv &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hv&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transform(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)
y_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;target&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&amp;rsquo;t forget to upvote.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;d-word2vec-features&#34;&gt;d) Word2vec Features&lt;/h3&gt;

&lt;p&gt;We already talked a little about word2vec in the previous post. We can use the word to vec features to create sentence level feats also. We want to create a &lt;code&gt;d&lt;/code&gt; dimensional vector for sentence. For doing this, we will simply average the word embedding of all the words in a sentence.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/word2vec_feats.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We can do this in Python using the following functions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# load the GloVe vectors in a dictionary:&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_glove_index&lt;/span&gt;():
    EMBEDDING_FILE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_coefs&lt;/span&gt;(word,&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;arr): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; word, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;asarray(arr, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;)[:&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;]
    embeddings_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dict(get_coefs(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; open(EMBEDDING_FILE))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embeddings_index

embeddings_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_glove_index()

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Found &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; word vectors.&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; len(embeddings_index))

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.corpus &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; stopwords
stop_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stopwords&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;words(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sent2vec&lt;/span&gt;(s):
    words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; str(s)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower()
    words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; word_tokenize(words)
    words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [w &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; words &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; stop_words]
    words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [w &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; words &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isalpha()]
    M &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; words:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
            M&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(embeddings_index[w])
        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
    M &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(M)
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; M&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; type(v) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ndarray:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros(&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; v &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt((v &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum())

&lt;span style=&#34;color:#75715e&#34;&gt;# create glove features&lt;/span&gt;
xtrain_glove &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([sent2vec(x) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(train_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)])
xtest_glove &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([sent2vec(x) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(test_df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cleaned_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a link to a kernel where I tried these features on the Quora Dataset. If you like it please don&amp;rsquo;t forget to upvote.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;Here are the results of different approaches on the Kaggle Dataset. I ran a 5 fold Stratified CV.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/results_conv.png&#34;  style=&#34;height:40%;width:40%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is the code. If you like it please don&amp;rsquo;t forget to upvote.
Also note that I didn&amp;rsquo;t work on tuning the models, so these results are only cursory. You can try to squeeze more performance by performing hyperparams tuning &lt;a href=&#34;https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/&#34;&gt;using hyperopt&lt;/a&gt; or just old fashioned Grid-search and the performance of models may change after that substantially.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While Deep Learning works a lot better for NLP classification task, it still makes sense to have an understanding of how these problems were solved in the past, so that we can appreciate the nature of the problem. I have tried to provide a perspective on the conventional methods and one should experiment with them too to create baselines before moving to Deep Learning methods. If you want to &lt;strong&gt;learn more about NLP&lt;/strong&gt; &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is an awesome course. You can start for free with the 7-day Free Trial. If you think I can add something to the flow, do mention it in the comments.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;endnotes-and-references&#34;&gt;Endnotes and References&lt;/h2&gt;

&lt;p&gt;This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Approaching (Almost) Any NLP Problem on Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;How to: Preprocessing when using embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>NLP  Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title>
      <link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid>
      <description>

&lt;p&gt;Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p&gt;

&lt;p&gt;Since we have a large amount of material to cover, I am splitting this post into a series of posts. The first post i.e. this one will be based on &lt;strong&gt;preprocessing techniques that work with Deep learning models&lt;/strong&gt; and we will also talk about &lt;strong&gt;increasing embeddings coverage&lt;/strong&gt;. In the &lt;a href=&#34;https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/&#34;&gt;second post&lt;/a&gt;, I will try to take you through some &lt;strong&gt;basic conventional models&lt;/strong&gt; like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong&gt;Deep learning models&lt;/strong&gt; in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong&gt;ULMFit transfer learning&lt;/strong&gt; approaches in the fourth post in the series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As a side note&lt;/strong&gt;: if you want to know more about NLP, I would like to recommend this awesome course on &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Natural Language Processing&lt;/a&gt; in the &lt;a href=&#34;https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;amp;utm_content=2&amp;amp;utm_medium=partners&amp;amp;utm_source=linkshare&amp;amp;utm_campaign=lVarvwc5BD0&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Advanced machine learning specialization&lt;/a&gt;. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.&lt;/p&gt;

&lt;p&gt;It might take me a little time to write the whole series. Till then you can take a look at my other posts: &lt;a href=&#34;https://mlwhiz.com/blog/2018/12/17/text_classification/&#34;&gt;What Kagglers are using for Text Classification&lt;/a&gt;, which talks about various deep learning models in use in NLP and &lt;a href=&#34;https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/&#34;&gt;how to switch from Keras to Pytorch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So first let me start with explaining a little more about the text classification problem. &lt;strong&gt;Text classification&lt;/strong&gt; is a common task in natural language processing, which transforms a sequence of a text of indefinite length into a category of text. How could you use that?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To find the sentiment of a review.&lt;/li&gt;
&lt;li&gt;Find toxic comments on a platform like Facebook&lt;/li&gt;
&lt;li&gt;Find Insincere questions on Quora. A current ongoing competition on kaggle&lt;/li&gt;
&lt;li&gt;Find fake reviews on websites&lt;/li&gt;
&lt;li&gt;Will a text advert get clicked or not?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now each of these problems has something in common. From a Machine Learning perspective, these are essentially the same problem with just the target labels changing and nothing else. With that said, the addition of business knowledge can help make these models more robust and that is what we want to incorporate while preprocessing the data for test classification. While the preprocessing pipeline I am focussing on in this post is mainly centered around Deep Learning but most of it will also be applicable to conventional machine learning models too.&lt;/p&gt;

&lt;p&gt;But let me first go through the flow of a deep learning pipeline for text data before going through all the steps to get a higher level perspective about the whole process.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/text_processing_flow_1.png&#34;  style=&#34;height:90%;width:90%&#34;&gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;We normally start with cleaning up the text data and performing basic EDA. Here we try to improve our data quality by cleaning up the data. We also try to improve the quality of our word2vec embeddings by removing OOV(Out-of-Vocabulary) words. These first two steps normally don&amp;rsquo;t have much order between them and I generally go back and forth between these two steps. Next, we create a representation for text that could be fed into a deep learning model. We then start with creating our models and training them. Finally, we evaluate the models using appropriate metrics and get approval from respective shareholders to deploy our models. Don&amp;rsquo;t worry if these terms don&amp;rsquo;t make much sense now. I will try to explain them through the course of this article.&lt;/p&gt;

&lt;p&gt;Here at this junction, let us take a little detour to talk a little about word embeddings. We will have to think about them while preprocessing data for our Deep Learning models.&lt;/p&gt;

&lt;h2 id=&#34;a-primer-on-word2vec-embeddings&#34;&gt;A Primer on word2vec embeddings:&lt;/h2&gt;

&lt;p&gt;We need to have a way to represent words in a vocab. One way to do that could be to use One hot encoding of word vectors but that is not really a good choice. One of the major reasons is that the one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity.&lt;/p&gt;

&lt;p&gt;$$\frac{\boldsymbol{x}^\top \boldsymbol{y}}{|\boldsymbol{x}| |\boldsymbol{y}|} \in [-1, 1].$$&lt;/p&gt;

&lt;p&gt;Given the structure of one hot encoded vectors, the similarity is always going to come as 0 between different words. Another reason is that as the size of vocabulary increases these one hot encoded vectors become very large.&lt;/p&gt;

&lt;p&gt;Word2Vec overcomes the above difficulties by providing us with a fixed length vector representation of words and by capturing the similarity and analogy relationships between different words.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/word2vec.png&#34; style=&#34;height:80%;width:80%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;Word2vec vectors of words are learned in such a way that they allow us to learn different analogies. It enables us to do algebraic manipulations on words which were not possible before. For example: What is king - man + woman? It comes out to be Queen.&lt;/p&gt;

&lt;p&gt;Word2Vec vectors also help us to find out the similarity between words. If we try to find similar words to &amp;ldquo;good&amp;rdquo;, we will find awesome, great etc. It is this property of word2vec that makes it invaluable for text classification. Now our deep learning network understands that &amp;ldquo;good&amp;rdquo; and &amp;ldquo;great&amp;rdquo; are essentially words with similar meaning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thus in very simple terms, word2vec creates vectors for words. Thus we have a &lt;code&gt;d&lt;/code&gt; dimensional vector for every word(common bigrams too) in a dictionary.&lt;/strong&gt; We normally use pretrained word vectors which are provided to us by others after training on large corpora of texts like Wikipedia, twitter etc. The most commonly used pretrained word vectors are Glove and Fasttext with 300-dimensional word vectors. We are going to use Glove in this post.&lt;/p&gt;

&lt;h2 id=&#34;basic-preprocessing-techniques-for-text-data&#34;&gt;Basic Preprocessing Techniques for text data:&lt;/h2&gt;

&lt;p&gt;In most of the cases, we observe that text data is not entirely clean. Data coming from different sources have different characteristics and that makes Text Preprocessing as one of the most important steps in the classification pipeline. For example, Text data from Twitter is totally different from text data on Quora, or some news/blogging platform, and thus would need to be treated differently. Helpfully, the techniques I am going to talk about in this post are generic enough for any kind of data you might encounter in the jungles of NLP.&lt;/p&gt;

&lt;h4 id=&#34;a-cleaning-special-characters-and-removing-punctuations&#34;&gt;a) Cleaning Special Characters and Removing Punctuations:&lt;/h4&gt;

&lt;p&gt;Our preprocessing pipeline depends a lot on the word2vec embeddings we are going to use for our classification task. &lt;em&gt;In principle our preprocessing should match the preprocessing that was used before training the word embedding&lt;/em&gt;. Since most of the embeddings don&amp;rsquo;t provide vector values for punctuations and other special chars, the first thing you want to do is to get rid of is the special characters in your text data. These are some of the special chars that were there in the Quora Question data and we use &lt;code&gt;replace&lt;/code&gt; function to get rid of these special chars.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Some preprocesssing that will be common to all the text classification methods you will see. &lt;/span&gt;

puncts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#34;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;:&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;)&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;!&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;?&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#39;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;$&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;amp;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;]&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;=&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;+&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;•&amp;#39;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;~&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;@&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;£&amp;#39;&lt;/span&gt;, 
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;·&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;}&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;©&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;^&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;®&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;`&amp;#39;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;→&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;°&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;€&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;™&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;›&amp;#39;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;♥&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;←&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;×&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;§&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;″&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;′&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Â&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;█&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;½&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;à&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;…&amp;#39;&lt;/span&gt;, 
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;“&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;★&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;”&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;–&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;●&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;â&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;►&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;−&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¢&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;²&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¬&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;░&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¶&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;↑&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;±&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¿&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▾&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;═&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¦&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;║&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;―&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¥&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▓&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;—&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;‹&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;─&amp;#39;&lt;/span&gt;, 
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▒&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;：&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¼&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;⊕&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▼&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▪&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;†&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;■&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;’&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▀&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¨&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▄&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;♫&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;☆&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;é&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¯&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;♦&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¤&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▲&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;è&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¸&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¾&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ã&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;⋅&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;‘&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;∞&amp;#39;&lt;/span&gt;, 
 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;∙&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;）&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;↓&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;、&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;│&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;（&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;»&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;，&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;♪&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╩&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╚&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;³&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;・&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╦&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╣&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╔&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;╗&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▬&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;❤&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ï&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ø&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;¹&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;≤&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;‡&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;√&amp;#39;&lt;/span&gt;, ]

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;clean_text&lt;/span&gt;(x):
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; str(x)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; punct &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; puncts:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; punct &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; x:
            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;replace(punct, f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; {punct} &amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This could also have been done with the help of a simple regex. But I normally like the above way of doing things as it helps to understand the sort of characters we are removing from our data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;clean_text&lt;/span&gt;(x):
    pattern &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[^a-zA-z0-9\s]&amp;#39;&lt;/span&gt;
    text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(pattern, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;, x)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;b-cleaning-numbers&#34;&gt;b) Cleaning Numbers:&lt;/h4&gt;

&lt;p&gt;Why do we want to replace numbers with &lt;code&gt;#&lt;/code&gt;s? Because most embeddings have preprocessed their text like this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Small Python Trick:&lt;/strong&gt; We use an &lt;code&gt;if&lt;/code&gt; statement in the code below to check beforehand if a number exists in a text. It is as an &lt;code&gt;if&lt;/code&gt; is always fast than a &lt;code&gt;re.sub&lt;/code&gt; command and most of our text doesn&amp;rsquo;t contain numbers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;clean_numbers&lt;/span&gt;(x):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; bool(re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;search(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\d&amp;#39;&lt;/span&gt;, x)):
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[0-9]{5,}&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;#####&amp;#39;&lt;/span&gt;, x)
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[0-9]{4}&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;####&amp;#39;&lt;/span&gt;, x)
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[0-9]{3}&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;###&amp;#39;&lt;/span&gt;, x)
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[0-9]{2}&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;##&amp;#39;&lt;/span&gt;, x)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;c-removing-misspells&#34;&gt;c) Removing Misspells:&lt;/h4&gt;

&lt;p&gt;It always helps to find out misspells in the data. As those word embeddings are not present in the word2vec, we should replace words with their correct spellings to get better embedding coverage. The following code artifact is an adaptation of Peter Norvig&amp;rsquo;s spell checker. It uses word2vec ordering of words to approximate word probabilities. As Google word2vec apparently orders words in decreasing order of frequency in the training corpus. You can use this to find out some misspelled words in the data you have.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This comes from CPMP script in the Quora questions similarity challenge. &lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; re
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; collections &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Counter
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; gensim
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; heapq
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; operator &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; itemgetter
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; multiprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Pool

model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gensim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;KeyedVectors&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_word2vec_format(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin&amp;#39;&lt;/span&gt;, 
                                                        binary&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;index2word

w_rank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i,word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; enumerate(words):
    w_rank[word] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i

WORDS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; w_rank

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;words&lt;/span&gt;(text): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;findall(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\w+&amp;#39;&lt;/span&gt;, text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower())

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;P&lt;/span&gt;(word): 
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability of `word`.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# use inverse of rank as proxy&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# returns 0 if the word isn&amp;#39;t in the dictionary&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; WORDS&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;correction&lt;/span&gt;(word): 
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Most probable spelling correction for word.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; max(candidates(word), key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;P)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;candidates&lt;/span&gt;(word): 
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Generate possible spelling corrections for word.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (known([word]) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; known(edits1(word)) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; known(edits2(word)) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; [word])

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;known&lt;/span&gt;(words): 
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The subset of `words` that appear in the dictionary of WORDS.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; set(w &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; words &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; WORDS)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;edits1&lt;/span&gt;(word):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;All edits that are one edit away from `word`.&amp;#34;&lt;/span&gt;
    letters    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;abcdefghijklmnopqrstuvwxyz&amp;#39;&lt;/span&gt;
    splits     &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [(word[:i], word[i:])    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(word) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)]
    deletes    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [L &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:]               &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; L, R &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; splits &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; R]
    transposes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [L &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; L, R &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; splits &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(R)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    replaces   &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [L &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:]           &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; L, R &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; splits &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; R &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; letters]
    inserts    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [L &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; R               &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; L, R &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; splits &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; letters]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; set(deletes &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; transposes &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; replaces &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; inserts)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;edits2&lt;/span&gt;(word): 
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;All edits that are two edits away from `word`.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (e2 &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; e1 &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; edits1(word) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; e2 &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; edits1(e1))

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;build_vocab&lt;/span&gt;(texts):
    sentences &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; texts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values
    vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sentence &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sentences:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sentence:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
                vocab[word] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;KeyError&lt;/span&gt;:
                vocab[word] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vocab

vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; build_vocab(train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;question_text)

top_90k_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dict(heapq&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nlargest(&lt;span style=&#34;color:#ae81ff&#34;&gt;90000&lt;/span&gt;, vocab&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items(), key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;itemgetter(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)))

pool &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pool(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
corrected_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pool&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(correction,list(top_90k_words&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keys()))

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word,corrected_word &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(top_90k_words,corrected_words):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word&lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt;corrected_word:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(word,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;,corrected_word)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once we are through with finding misspelled data, the next thing remains to replace them using a misspell mapping and regex functions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;mispell_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;colour&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;color&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;centre&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;center&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;favourite&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;favorite&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;travelling&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;traveling&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;counselling&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;counseling&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;theatre&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;theater&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cancelled&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;canceled&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;labour&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;labor&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;organisation&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;organization&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wwii&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;world war 2&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;citicise&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;criticize&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;youtu &amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;youtube &amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Qoura&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Quora&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sallary&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;salary&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Whta&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;What&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;narcisist&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;narcissist&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;howdo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how do&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whatare&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;what are&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;howcan&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how can&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;howmuch&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how much&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;howmany&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how many&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whydo&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;why do&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;doI&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;do I&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;theBest&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;the best&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;howdoes&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how does&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mastrubation&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;masturbation&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mastrubate&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;masturbate&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mastrubating&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;masturbating&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pennis&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;penis&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Etherium&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ethereum&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;narcissit&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;narcissist&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bigdata&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;big data&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2k17&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2k18&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2018&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;qouta&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;quota&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;exboyfriend&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ex boyfriend&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;airhostess&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;air hostess&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;whst&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;what&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;watsapp&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;whatsapp&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonitisation&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonetization&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonitization&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonetization&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonetisation&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;demonetization&amp;#39;&lt;/span&gt;}

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_get_mispell&lt;/span&gt;(mispell_dict):
    mispell_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;)&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(mispell_dict&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keys()))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mispell_dict, mispell_re

mispellings, mispellings_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _get_mispell(mispell_dict)
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;replace_typical_misspell&lt;/span&gt;(text):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;replace&lt;/span&gt;(match):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mispellings[match&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;group(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mispellings_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(replace, text)

&lt;span style=&#34;color:#75715e&#34;&gt;# Usage&lt;/span&gt;
replace_typical_misspell(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Whta is demonitisation&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id=&#34;d-removing-contractions&#34;&gt;d) Removing Contractions:&lt;/h4&gt;

&lt;p&gt;Contractions are words that we write with an apostrophe. Examples of contractions are words like &amp;ldquo;ain&amp;rsquo;t&amp;rdquo; or &amp;ldquo;aren&amp;rsquo;t&amp;rdquo;. Since we want to standardize our text, it makes sense to expand these contractions. Below we have done this using a contraction mapping and regex functions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;contraction_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ain&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;is not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;aren&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;are not&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;can&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cannot&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#39;cause&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;because&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;could&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;could have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;couldn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;could not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;didn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;did not&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;doesn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;does not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;don&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;do not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hadn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;had not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hasn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;has not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;haven&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;have not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he would&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;he is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how did&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how&amp;#39;d&amp;#39;y&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how do you&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;how is&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I will have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;m&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I am&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;I have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i will&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i will have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;m&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i am&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;isn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;is not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it will have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;it is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;let&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;let us&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ma&amp;#39;am&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;madam&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mayn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;may not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;might&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;might have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mightn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;might not&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mightn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;might not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;must&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;must have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mustn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;must not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mustn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;must not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;needn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;need not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;needn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;need not have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;o&amp;#39;clock&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;of the clock&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;oughtn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ought not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;oughtn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ought not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shan&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shall not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sha&amp;#39;n&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shall not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shan&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shall not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;she is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shouldn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;shouldn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;so&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;so have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;so&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;so as&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;this&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;this is&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;that is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;there is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;here&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;here is&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;re&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they are&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;they have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wasn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;was not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;re&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we are&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;we have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;weren&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;were not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what&amp;#39;re&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what are&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;what have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;when&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;when is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;when&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;when have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where did&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;where have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;who have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;why&amp;#39;s&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;why is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;why&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;why have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;will&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;won&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;will not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;won&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;will not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;would&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wouldn&amp;#39;t&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;would not&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wouldn&amp;#39;t&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;would not have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;y&amp;#39;all&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you all&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;y&amp;#39;all&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you all would&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;y&amp;#39;all&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you all would have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;y&amp;#39;all&amp;#39;re&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you all are&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;y&amp;#39;all&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you all have&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;d&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you would&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;d&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you would have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;ll&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you will&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;ll&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you will have&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;re&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you are&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you&amp;#39;ve&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;you have&amp;#34;&lt;/span&gt;}

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_get_contractions&lt;/span&gt;(contraction_dict):
    contraction_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;)&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(contraction_dict&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keys()))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; contraction_dict, contraction_re

contractions, contractions_re &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _get_contractions(contraction_dict)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;replace_contractions&lt;/span&gt;(text):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;replace&lt;/span&gt;(match):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; contractions[match&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;group(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; contractions_re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(replace, text)

&lt;span style=&#34;color:#75715e&#34;&gt;# Usage&lt;/span&gt;
replace_contractions(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;this&amp;#39;s a text with contraction&amp;#34;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Apart from the above techniques, there are other preprocessing techniques of text like Stemming, Lemmatization and Stopword Removal. Since these techniques are not used along with Deep Learning NLP models, we won&amp;rsquo;t talk about them.&lt;/p&gt;

&lt;h2 id=&#34;representation-sequence-creation&#34;&gt;Representation: Sequence Creation&lt;/h2&gt;

&lt;p&gt;One of the things that have made Deep Learning the goto choice for NLP is the fact that we don&amp;rsquo;t really have to hand-engineer features from the text data. The deep learning algorithms take as input a sequence of text to learn the structure of text just like a human does. Since Machine cannot understand words they expect their data in numerical form. So we would like to represent out text data as a series of numbers. To understand how this is done we need to understand a little about the Keras Tokenizer function. One can use any other tokenizer also but keras tokenizer seems like a good choice for me.&lt;/p&gt;

&lt;h4 id=&#34;a-tokenizer&#34;&gt;a) Tokenizer:&lt;/h4&gt;

&lt;p&gt;In simple words, a tokenizer is a utility function to split a sentence into words.
&lt;code&gt;keras.preprocessing.text.Tokenizer&lt;/code&gt; tokenizes(splits) the texts into tokens(words) while keeping only the most occurring words in the text corpus.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#Signature:&lt;/span&gt;
Tokenizer(num_words&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, filters&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;!&amp;#34;#$%&amp;amp;()*+,-./:;&amp;lt;=&amp;gt;?@[&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;]^_`{|}~&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\t\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;, 
lower&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, split&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;, char_level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False, oov_token&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, document_count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The num_words parameter keeps a prespecified number of words in the text only. This is helpful as we don&amp;rsquo;t want our models to get a lot of noise by considering words that occur very infrequently. In real-world data, most of the words we leave using num_words param are normally misspells. The tokenizer also filters some non-wanted tokens by default and converts the text into lowercase.&lt;/p&gt;

&lt;p&gt;The tokenizer once fitted to the data also keeps an index of words(dictionary of words which we can use to assign a unique number to a word) which can be accessed by tokenizer.word_index. The words in the indexed dictionary are ranked in order of frequencies.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/tokenizer_working.png&#34; style=&#34;height:80%;width:80%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;

&lt;p&gt;So the whole code to use tokenizer is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; keras.preprocessing.text &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Tokenizer
&lt;span style=&#34;color:#75715e&#34;&gt;## Tokenize the sentences&lt;/span&gt;
tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Tokenizer(num_words&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;max_features)
tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_on_texts(list(train_X)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;list(test_X))
train_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;texts_to_sequences(train_X)
test_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;texts_to_sequences(test_X)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;train_X&lt;/code&gt; and &lt;code&gt;test_X&lt;/code&gt; are lists of documents in the corpus.&lt;/p&gt;

&lt;h4 id=&#34;b-pad-sequence&#34;&gt;b) Pad Sequence:&lt;/h4&gt;

&lt;p&gt;Normally our model expects that each sequence(each training example) will be of the same length(same number of words/tokens). We can control this using the &lt;code&gt;maxlen&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;For example:
&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/pad_seq.png&#34; style=&#34;height:40%;width:40%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;train_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences(train_X, maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;maxlen)
test_X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad_sequences(test_X, maxlen&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;maxlen)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now our train data contains a list of list of numbers. Each list has the same length. And we also have the &lt;code&gt;word_index&lt;/code&gt; which is a dictionary of most occuring words in the text corpus.&lt;/p&gt;

&lt;h2 id=&#34;embedding-enrichment&#34;&gt;Embedding Enrichment:&lt;/h2&gt;

&lt;p&gt;As I said I will be using GLoVE Word2Vec embeddings to explain the enrichment. GLoVE pretrained vectors are trained on the Wikipedia corpus. (You can &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;download them here&lt;/a&gt;). That means some of the words that might be present in your data might not be present in the embeddings. How could we deal with that? Let&amp;rsquo;s first load the Glove Embeddings first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_glove_index&lt;/span&gt;():
    EMBEDDING_FILE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;../input/embeddings/glove.840B.300d/glove.840B.300d.txt&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_coefs&lt;/span&gt;(word,&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;arr): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; word, np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;asarray(arr, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;float32&amp;#39;&lt;/span&gt;)[:&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;]
    embeddings_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dict(get_coefs(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; open(EMBEDDING_FILE))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embeddings_index

glove_embedding_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_glove_index()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be sure to put the path of the folder where you download these GLoVE vectors. What does this &lt;code&gt;glove_embedding_index&lt;/code&gt; contain? It is just a dictionary in which the key is the word and the value is the word vector, a &lt;code&gt;np.array&lt;/code&gt; of length 300. The length of this dictionary is somewhere around a billion. Since we only want the embeddings of words that are in our &lt;code&gt;word_index&lt;/code&gt;, we will create a matrix which just contains required embeddings.&lt;/p&gt;

&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/embedding_matrix_creation.png&#34; style=&#34;height:100%;width:100%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_glove&lt;/span&gt;(word_index,embeddings_index):
    emb_mean,emb_std &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.005838499&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.48782197&lt;/span&gt;
    all_embs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values())
    embed_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; all_embs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    nb_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(max_features, len(word_index))
    embedding_matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(emb_mean, emb_std, (nb_words, embed_size))
    count_found &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nb_words
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word, i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(word_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items()):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; max_features: &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None: 
            embedding_matrix[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  embedding_vector
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                count_found&lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Got embedding for &amp;#34;&lt;/span&gt;,count_found,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; words.&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embedding_matrix&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above code works fine but is there a way that we can use the preprocessing in GLoVE to our advantage? Yes. When preprocessing was done for glove, the creators didn&amp;rsquo;t convert the words to lowercase. That means that it contains multiple variations of a word like &amp;lsquo;USA&amp;rsquo;, &amp;lsquo;usa&amp;rsquo; and &amp;lsquo;Usa&amp;rsquo;. That also means that in some cases while a word like &amp;lsquo;Word&amp;rsquo; is present, its analog in lowercase i.e. &amp;lsquo;word&amp;rsquo; is not present. We can get through this situation by using the below code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_glove&lt;/span&gt;(word_index,embeddings_index):
    emb_mean,emb_std &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.005838499&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.48782197&lt;/span&gt;
    all_embs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values())
    embed_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; all_embs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    nb_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(max_features, len(word_index))
    embedding_matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(emb_mean, emb_std, (nb_words, embed_size))

    count_found &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nb_words
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word, i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(word_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items()):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; max_features: &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None: 
            embedding_matrix[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  embedding_vector
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;islower():
                &lt;span style=&#34;color:#75715e&#34;&gt;# try to get the embedding of word in titlecase if lowercase is not present&lt;/span&gt;
                embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;capitalize())
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None: 
                    embedding_matrix[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embedding_vector
                &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                    count_found&lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                count_found&lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Got embedding for &amp;#34;&lt;/span&gt;,count_found,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; words.&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embedding_matrix&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above was just an example of how we can use our knowledge of an embedding to get better coverage. Sometimes depending on the problem, one might also derive value by adding extra information to the embeddings using some domain knowledge and NLP skills. For example, we can add external knowledge to the embeddings themselves by adding polarity and subjectivity of a word from the TextBlob package in Python.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; textblob &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; TextBlob
word_sent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TextBlob(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;good&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sentiment
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(word_sent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;polarity,word_sent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subjectivity)
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.7 0.6&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can get the polarity and subjectivity of any word using TextBlob. Pretty neat. So let us try to add this extra information to our embeddings.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_glove&lt;/span&gt;(word_index,embeddings_index):
    emb_mean,emb_std &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.005838499&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.48782197&lt;/span&gt;
    all_embs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values())
    embed_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; all_embs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    nb_words &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min(max_features, len(word_index))
    embedding_matrix &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normal(emb_mean, emb_std, (nb_words, embed_size&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;))
    
    count_found &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nb_words
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; word, i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; tqdm(word_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items()):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; max_features: &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
        embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word)
        word_sent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TextBlob(word)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sentiment
        &lt;span style=&#34;color:#75715e&#34;&gt;# Extra information we are passing to our embeddings&lt;/span&gt;
        extra_embed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [word_sent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;polarity,word_sent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subjectivity]
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None: 
            embedding_matrix[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(embedding_vector,extra_embed)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; word&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;islower():
                embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embeddings_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(word&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;capitalize())
                &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; embedding_vector &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None: 
                    embedding_matrix[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(embedding_vector,extra_embed)
                &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                    embedding_matrix[i,&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;:] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extra_embed
                    count_found&lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                embedding_matrix[i,&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;:] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; extra_embed
                count_found&lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Got embedding for &amp;#34;&lt;/span&gt;,count_found,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; words.&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; embedding_matrix&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Engineering embeddings is an essential part of getting better performance from the Deep learning models at a later stage. Generally, I revisit this part of code multiple times during the stage of a project while trying to improve my models even further. You can show up a lot of creativity here to improve coverage over your &lt;code&gt;word_index&lt;/code&gt; and to include extra features in your embedding.&lt;/p&gt;

&lt;h2 id=&#34;more-engineered-features&#34;&gt;More Engineered Features&lt;/h2&gt;

&lt;p&gt;&lt;div style=&#34;margin-top: 9px; margin-bottom: 10px;&#34;&gt;
&lt;center&gt;&lt;img src=&#34;https://mlwhiz.com/images/example_nlp_network.png&#34; style=&#34;height:100%;width:100%&#34; &gt;&lt;/center&gt;
&lt;/div&gt;
One can always add sentence specific features like sentence length, number of unique words etc. as another input layer to give extra information to the Deep Neural Network. For example: I created these extra features as part of a feature engineering pipeline for Quora Insincerity Classification Challenge.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add_features&lt;/span&gt;(df):
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;question_text&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;question_text&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;progress_apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x:str(x))
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;lower_question_text&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;question_text&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower())
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;total_length&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;question_text&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;progress_apply(len)
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;capitals&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;question_text&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;progress_apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; comment: sum(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; comment &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; c&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isupper()))
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;caps_vs_length&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;progress_apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; row: float(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;capitals&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;float(row[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;total_length&amp;#39;&lt;/span&gt;]),
                                axis&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_words&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;question_text&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;str&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;count(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;\S+&amp;#39;&lt;/span&gt;)
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_unique_words&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;question_text&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;progress_apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; comment: len(set(w &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; w &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; comment&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split())))
    df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;words_vs_unique&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_unique_words&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;num_words&amp;#39;&lt;/span&gt;] 
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; df&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h2&gt;

&lt;p&gt;NLP is still a very interesting problem in Deep Learning space and thus I would encourage you to do a lot of experimentation to see what works and what doesn&amp;rsquo;t. I have tried to provide a wholesome perspective of the preprocessing steps for a Deep Learning Neural network for any NLP problem. But that doesn&amp;rsquo;t mean it is definitive. If you want to learn more about NLP &lt;a href=&#34;https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;amp;offerid=467035.11503135394&amp;amp;type=2&amp;amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; is an awesome course. You can start for free with the 7-day Free Trial. If you think we can add something to the flow, do mention it in the comments.&lt;/p&gt;

&lt;h2 id=&#34;endnotes-and-references&#34;&gt;Endnotes and References&lt;/h2&gt;

&lt;p&gt;This post is a result of an effort of a lot of excellent Kagglers and I will try to reference them in this section. If I leave out someone, do understand that it was not my intention to do so.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;How to: Preprocessing when using embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Improve your Score with some Text Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Pytorch baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/hengzheng/pytorch-starter&#34; rel=&#34;nofollow&#34; target=&#34;_blank&#34;&gt;Pytorch starter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>