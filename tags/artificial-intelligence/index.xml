<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Intelligence on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/tags/artificial-intelligence/</link><description>Recent content in Artificial Intelligence on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 07 Jul 2023 15:10:03 +0100</lastBuildDate><atom:link href="https://mlwhiz.com/tags/artificial-intelligence/index.xml" rel="self" type="application/rss+xml"/><item><title>How I Created a Dataset for Instance Segmentation from Scratch?</title><link>https://mlwhiz.com/blog/2020/10/04/custom-dataset-instance-segmentation-scratch/</link><pubDate>Sun, 04 Oct 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/10/04/custom-dataset-instance-segmentation-scratch/</guid><description>&lt;p>Recently, I was looking for a toy dataset for my new book’s chapter (you can subscribe to the updates 

&lt;a href="https://mlwhiz.ck.page/9a2ffe9e2c" target="_blank" rel="nofollow noopener">here&lt;/a>
) on instance segmentation. And, I really wanted to have something like the Iris Dataset for Instance Segmentation so that I would be able to explain the model without worrying about the dataset too much. But, alas, it is not always possible to get a dataset that you are looking for.&lt;/p></description></item><item><title>Understanding Transformers, the Data Science Way</title><link>https://mlwhiz.com/blog/2020/09/20/transformers/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/20/transformers/</guid><description>&lt;p>Transformers have become the defacto standard for NLP tasks nowadays.&lt;/p></description></item><item><title>The Most Complete Guide to PyTorch for Data Scientists</title><link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid><description>&lt;p>&lt;em>&lt;strong>PyTorch&lt;/strong>&lt;/em> has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.&lt;/p></description></item><item><title>A Layman’s Introduction to GANs for Data Scientists using PyTorch</title><link>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</guid><description>&lt;p>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and 

&lt;a href="https://lionbridge.ai/articles/a-look-at-deepfakes-in-2020/" target="_blank" rel="nofollow noopener">faces generated by artificial intelligence&lt;/a>
. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.&lt;/p></description></item><item><title>Creating my First Deep Learning + Data Science Workstation</title><link>https://mlwhiz.com/blog/2020/08/09/owndlrig/</link><pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/09/owndlrig/</guid><description>&lt;p>Creating my workstation has been a dream for me, if nothing else.&lt;/p></description></item><item><title>Accelerating Spark 3.0 Google DataProc Project with NVIDIA GPUs in 6 simple steps</title><link>https://mlwhiz.com/blog/2020/08/04/spark_dataproc/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/04/spark_dataproc/</guid><description>&lt;p>Data Exploration is a key part of Data Science. And does it take long? Ahh. Don’t even ask. Preparing a data set for ML not only requires understanding the data set, cleaning, and creating new features, it also involves doing these steps repeatedly until we have a fine-tuned system.&lt;/p></description></item><item><title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</title><link>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</guid><description>&lt;p>Just recently, I had written a simple 

&lt;a href="https://towardsdatascience.com/a-layman-guide-for-data-scientists-to-create-apis-in-minutes-31e6f451cd2f" target="_blank" rel="nofollow noopener">tutorial&lt;/a>
 on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.&lt;/p></description></item><item><title>How to Create an End to End Object Detector using Yolov5</title><link>https://mlwhiz.com/blog/2020/08/08/yolov5/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/08/yolov5/</guid><description>&lt;p>Ultralytics recently launched YOLOv5 amid controversy surrounding its name. For context, the first three versions of YOLO (You Only Look Once) were created by Joseph Redmon. Following this, Alexey Bochkovskiy created YOLOv4 on darknet, which boasted higher Average Precision (AP) and faster results than previous iterations.&lt;/p></description></item><item><title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title><link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid><description>&lt;p>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.&lt;/p></description></item><item><title>End to End Pipeline for setting up Multiclass Image Classification for Data Scientists</title><link>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</guid><description>&lt;p>&lt;em>Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?&lt;/em>&lt;/p></description></item><item><title>How to run your ML model Predictions 50 times faster?</title><link>https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/</guid><description>&lt;p>With the advent of so many computing and serving frameworks, it is getting stressful day by day for the developers to put a model into 

&lt;a href="https://towardsdatascience.com/take-your-machine-learning-models-to-production-with-these-5-simple-steps-35aa55e3a43c" target="_blank" rel="nofollow noopener">production&lt;/a>
. If the question of what model performs best on my data was not enough, now the question is what framework to choose for serving a model trained with Sklearn or LightGBM or 

&lt;a href="https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79" target="_blank" rel="nofollow noopener">PyTorch&lt;/a>
. And new frameworks are being added as each day passes.&lt;/p></description></item><item><title>Stop Worrying and Create your Deep Learning Server in 30 minutes</title><link>https://mlwhiz.com/blog/2020/05/25/dls/</link><pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/25/dls/</guid><description>&lt;p>I have found myself creating a Deep Learning Machine time and time again whenever I start a new project.&lt;/p></description></item><item><title>Using Deep Learning for End to End Multiclass Text Classification</title><link>https://mlwhiz.com/blog/2020/05/24/multitextclass/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/24/multitextclass/</guid><description>&lt;p>






 
 
 
 
 

 
 
 

 
 &lt;img
 sizes="(min-width: 35em) 1200px, 100vw"
 srcset='
 
 /images/multitextclass/main_hu5347609766070859065.png 500w
 
 
 , /images/multitextclass/main_hu15737587202097698753.png 800w
 
 
 , /images/multitextclass/main_hu7039699309994025813.png 1200w
 
 
 , /images/multitextclass/main_hu5618161544359752641.png 1500w 
 '
 src="https://mlwhiz.com/images/multitextclass/main.png"

 alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence">
 

&lt;/p></description></item><item><title>Can AI help in fighting against Corona?</title><link>https://mlwhiz.com/blog/2020/03/24/coronaai/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/03/24/coronaai/</guid><description>&lt;p>Feeling Helpless? I know I am.&lt;/p>
&lt;p>With the whole shutdown situation, what I thought was once a paradise for my introvert self doesn’t look so good when it is actually happening.&lt;/p></description></item><item><title>Lightning Fast XGBoost on Multiple GPUs</title><link>https://mlwhiz.com/blog/2020/02/23/xgbparallel/</link><pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/23/xgbparallel/</guid><description>&lt;p>XGBoost is one of the most used libraries fora data science.&lt;/p></description></item><item><title>Minimal Pandas Subset for Data Scientists on GPU</title><link>https://mlwhiz.com/blog/2020/02/22/pandas_gpu/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/22/pandas_gpu/</guid><description>&lt;p>Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.&lt;/p></description></item><item><title>Implementing Object Detection and Instance Segmentation for Data Scientists</title><link>https://mlwhiz.com/blog/2019/12/06/weapons/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/06/weapons/</guid><description>&lt;p>Object Detection is a helpful tool to have in your coding repository.&lt;/p></description></item><item><title>Demystifying Object Detection and Instance Segmentation for Data Scientists</title><link>https://mlwhiz.com/blog/2019/12/05/od/</link><pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/05/od/</guid><description>&lt;p>I like deep learning a lot but Object Detection is something that doesn’t come easily to me.&lt;/p></description></item><item><title>4 Graph Algorithms on Steroids for data Scientists with cuGraph</title><link>https://mlwhiz.com/blog/2019/10/20/cugraph/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/10/20/cugraph/</guid><description>&lt;p>We, as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.&lt;/p></description></item><item><title>An End to End Introduction to GANs using Keras</title><link>https://mlwhiz.com/blog/2019/06/17/gans/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/06/17/gans/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/gans/faces.png"">&lt;/center>
&lt;/div>
&lt;p>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.&lt;/p></description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>&lt;p>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p></description></item><item><title>NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid><description>&lt;p>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the 

&lt;a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/">third post&lt;/a>
, I delved deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.&lt;/p></description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>&lt;p>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>&lt;p>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong>Quora Insincere questions classification&lt;/strong> in which I got a rank of &lt;strong>&lt;code>182/4037&lt;/code>&lt;/strong>. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p></description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>&lt;p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the various &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In this post, I will try to take you through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong>Deep learning models&lt;/strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>&lt;p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p></description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/artificial-neural-network.png" height="350" width="700" >&lt;/center>
&lt;/div>
&lt;p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the &lt;strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic&lt;/strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.&lt;/p></description></item><item><title>What Kagglers are using for Text Classification</title><link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid><description>&lt;p>With the problem of Image Classification is more or less solved by Deep learning, &lt;em>Text Classification is the next new developing theme in deep learning&lt;/em>. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that?&lt;/p></description></item><item><title>Object Detection: An End to End Theoretical Perspective</title><link>https://mlwhiz.com/blog/2018/09/22/object_detection/</link><pubDate>Sat, 22 Sep 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/09/22/object_detection/</guid><description>&lt;p>We all know about the image classification problem. Given an image can you find out the class the image belongs to? We can solve any new image classification problem with ConvNets and 

&lt;a href="https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8" target="_blank" rel="nofollow noopener">Transfer Learning&lt;/a>
 using pre-trained nets.
&lt;br>&lt;/p></description></item><item><title>Today I Learned This Part 2: Pretrained Neural Networks What are they?</title><link>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</link><pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/04/17/deep_learning_pretrained_models/</guid><description>&lt;p>Deeplearning is the buzz word right now. I was working on the 

&lt;a href="http://www.fast.ai/" target="_blank" rel="nofollow noopener">course&lt;/a>
 for deep learning by Jeremy Howard and one thing I noticed were pretrained deep Neural Networks. In the first lesson he used the pretrained NN to predict on the 

&lt;a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="nofollow noopener">Dogs vs Cats&lt;/a>
 competition on Kaggle to achieve very good results.&lt;/p></description></item><item><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid><description>&lt;p>Recently Quora put out a 

&lt;a href="https://www.kaggle.com/c/quora-question-pairs" target="_blank" rel="nofollow noopener">Question similarity&lt;/a>
 competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p></description></item></channel></rss>