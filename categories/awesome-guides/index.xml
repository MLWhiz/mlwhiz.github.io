<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Awesome Guides on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/categories/awesome-guides/</link><description>Recent content in Awesome Guides on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 13 Aug 2023 22:35:47 +0100</lastBuildDate><atom:link href="https://mlwhiz.com/categories/awesome-guides/index.xml" rel="self" type="application/rss+xml"/><item><title>Everything Programmers need to learn about GPT — Using OpenAI and Understanding Prompting</title><link>https://mlwhiz.com/blog/2023/08/13/openai-api/</link><pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2023/08/13/openai-api/</guid><description>&lt;p>ChatGPT&amp;rsquo;s free conversational interface offers a tantalizing glimpse into the future of AI. But the full potential of generative models lies in integration with real-world systems.&lt;/p></description></item><item><title>Everything you need to learn about GPT — How Does ChatGPT Work?</title><link>https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/</link><pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/</guid><description>&lt;p>ChatGPT is what everyone is talking about nowadays. Would it take all the jobs? Or would it result in misinformation on the web? There are just so many posts and articles that fill my inbox daily when it comes to GPTs. Add to that so many versions of GPTs and tools to use these GPTs; it is getting increasingly frustrating to keep track of everything in the GPT Landscape.&lt;/p></description></item><item><title>The Primer on Asyncio that I Wish I Had</title><link>https://mlwhiz.com/blog/2022/11/26/asyncio/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2022/11/26/asyncio/</guid><description>&lt;h3 id="parallelism-and-concurrency-arent-the-same-things-in-some-cases-concurrency-is-much-more-powerful-here-is-a-guide-to-help-you-make-the-most-of-concurrency-with-asyncio">Parallelism and concurrency aren’t the same things. In some cases, concurrency is much more powerful. Here is a guide to help you make the most of concurrency with Asyncio.&lt;/h3>
&lt;p>Python is an easy language to pick up, but mastering it requires understanding a lot of concepts.&lt;/p></description></item><item><title>Explaining BERT Simply Using Sketches</title><link>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</guid><description>&lt;p>In my last series of posts on Transformers, I talked about how a 

&lt;a href="https://mlwhiz.com/blog/2020/09/20/transformers/">transformer &lt;/a>
works and how to 

&lt;a href="https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/">implement&lt;/a>
 one yourself for a translation task.&lt;/p></description></item><item><title>How Can Data Scientists Use Parallel Processing?</title><link>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</guid><description>&lt;p>Finally, my program is running! Should I go and get a coffee?&lt;/p></description></item><item><title>Solve almost every Binary Search Problem</title><link>https://mlwhiz.com/blog/2021/07/24/binary-search-best/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/binary-search-best/</guid><description>&lt;p>Algorithms are an integral part of data science. While most of us data scientists don’t take a proper algorithms course while studying, they are important all the same. Many companies ask data structures and algorithms as part of their interview process for hiring data scientists.&lt;/p></description></item><item><title>Understanding BERT with Huggingface</title><link>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</guid><description>&lt;p>In my last post on 

&lt;a href="https://mlwhiz.medium.com/explaining-bert-simply-using-sketches-ba30f6f0c8cb" target="_blank" rel="nofollow noopener">BERT&lt;/a>
, I talked in quite a detail about BERT transformers and how they work on a basic level. I went through the BERT Architecture, training data and training tasks.&lt;/p></description></item><item><title>Understanding Transformers, the Programming Way</title><link>https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/</link><pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/</guid><description>&lt;p>Transformers have become the defacto standard for NLP tasks nowadays. They started being used in NLP but they are now being used in Computer Vision and sometimes to generate music as well. I am sure you would all have heard about the GPT3 Transformer or the jokes thereof.&lt;/p></description></item><item><title>Understanding Transformers, the Data Science Way</title><link>https://mlwhiz.com/blog/2020/09/20/transformers/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/20/transformers/</guid><description>&lt;p>Transformers have become the defacto standard for NLP tasks nowadays.&lt;/p></description></item><item><title>The Most Complete Guide to PyTorch for Data Scientists</title><link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid><description>&lt;p>&lt;em>&lt;strong>PyTorch&lt;/strong>&lt;/em> has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.&lt;/p></description></item><item><title>Create an Awesome Development Setup for Data Science using Atom</title><link>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/02/atom_for_data_science/</guid><description>&lt;p>Before I even begin this article, let me just say that I love iPython Notebooks, and Atom is not an alternative to Jupyter in any way. Notebooks provide me an interface where I have to think of &lt;em>“Coding one code block at a time,”&lt;/em> as I like to call it, and it helps me to think more clearly while helping me make my code more modular.&lt;/p></description></item><item><title>A Layman’s Introduction to GANs for Data Scientists using PyTorch</title><link>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</link><pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/27/pyt_gan/</guid><description>&lt;p>Most of us in data science has seen a lot of AI-generated people in recent times, whether it be in papers, blogs, or videos. We’ve reached a stage where it’s becoming increasingly difficult to distinguish between actual human faces and 

&lt;a href="https://lionbridge.ai/articles/a-look-at-deepfakes-in-2020/" target="_blank" rel="nofollow noopener">faces generated by artificial intelligence&lt;/a>
. However, with the currently available machine learning toolkits, creating these images yourself is not as difficult as you might think.&lt;/p></description></item><item><title>Deployment could be easy — A Data Scientist’s Guide to deploy an Image detection FastAPI API using Amazon ec2</title><link>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/08/deployment_fastapi/</guid><description>&lt;p>Just recently, I had written a simple 

&lt;a href="https://towardsdatascience.com/a-layman-guide-for-data-scientists-to-create-apis-in-minutes-31e6f451cd2f" target="_blank" rel="nofollow noopener">tutorial&lt;/a>
 on FastAPI, which was about simplifying and understanding how APIs work, and creating a simple API using the framework.&lt;/p></description></item><item><title>How to Create an End to End Object Detector using Yolov5</title><link>https://mlwhiz.com/blog/2020/08/08/yolov5/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/08/08/yolov5/</guid><description>&lt;p>Ultralytics recently launched YOLOv5 amid controversy surrounding its name. For context, the first three versions of YOLO (You Only Look Once) were created by Joseph Redmon. Following this, Alexey Bochkovskiy created YOLOv4 on darknet, which boasted higher Average Precision (AP) and faster results than previous iterations.&lt;/p></description></item><item><title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title><link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid><description>&lt;p>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.&lt;/p></description></item><item><title>A Layman’s Guide for Data Scientists to create APIs in minutes</title><link>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/</guid><description>&lt;p>Have you ever been in a situation where you want to provide your model predictions to a frontend developer without them having access to model related code? Or has a developer ever asked you to create an API that they can use? I have faced this a lot.&lt;/p></description></item><item><title>End to End Pipeline for setting up Multiclass Image Classification for Data Scientists</title><link>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/</guid><description>&lt;p>&lt;em>Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images?&lt;/em>&lt;/p></description></item><item><title>The Most Complete Guide to pySpark DataFrames</title><link>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/spark_df_complete_guide/</guid><description>&lt;p>Big Data has become synonymous with Data engineering. But the line between Data Engineering and Data scientists is blurring day by day. At this point in time, I think that Big Data must be in the repertoire of all data scientists.&lt;/p></description></item><item><title>Using Deep Learning for End to End Multiclass Text Classification</title><link>https://mlwhiz.com/blog/2020/05/24/multitextclass/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/24/multitextclass/</guid><description>&lt;p>






 
 
 
 
 

 
 
 

 
 &lt;img
 sizes="(min-width: 35em) 1200px, 100vw"
 srcset='
 
 /images/multitextclass/main_hu5347609766070859065.png 500w
 
 
 , /images/multitextclass/main_hu15737587202097698753.png 800w
 
 
 , /images/multitextclass/main_hu7039699309994025813.png 1200w
 
 
 , /images/multitextclass/main_hu5618161544359752641.png 1500w 
 '
 src="https://mlwhiz.com/images/multitextclass/main.png"

 alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence">
 

&lt;/p></description></item><item><title>A Newspaper for COVID-19 — The CoronaTimes</title><link>https://mlwhiz.com/blog/2020/03/29/coronatimes/</link><pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/03/29/coronatimes/</guid><description>&lt;p>It seems that the way that I consume information has changed a lot. I have become quite a news junkie recently. One thing, in particular, is that I have been reading quite a lot of international news to determine the stages of Covid-19 in my country.&lt;/p></description></item><item><title>Practical Spark Tips for Data Scientists</title><link>https://mlwhiz.com/blog/2020/03/20/practicalspark/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/03/20/practicalspark/</guid><description>&lt;p>&lt;em>&lt;strong>I know — Spark is sometimes frustrating to work with.&lt;/strong>&lt;/em>&lt;/p></description></item><item><title>5 Ways to add a new column in a PySpark Dataframe</title><link>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/24/sparkcolumns/</guid><description>&lt;p>&lt;em>&lt;strong>Too much data is getting generated day by day.&lt;/strong>&lt;/em>&lt;/p>
&lt;p>Although sometimes we can manage our big data using tools like 

&lt;a href="https://towardsdatascience.com/minimal-pandas-subset-for-data-scientist-on-gpu-d9a6c7759c7f?source=---------5------------------" target="_blank" rel="nofollow noopener">Rapids&lt;/a>
 or 

&lt;a href="https://towardsdatascience.com/add-this-single-word-to-make-your-pandas-apply-faster-90ee2fffe9e8?source=---------11------------------" target="_blank" rel="nofollow noopener">Parallelization&lt;/a>
, Spark is an excellent tool to have in your repertoire if you are working with Terabytes of data.&lt;/p></description></item><item><title>Become a Data Scientist in 2023 with these 10 resources</title><link>https://mlwhiz.com/blog/2020/02/21/ds2020/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/21/ds2020/</guid><description>&lt;p>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.&lt;/p></description></item><item><title>Confidence Intervals Explained Simply for Data Scientists</title><link>https://mlwhiz.com/blog/2020/02/21/ci/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/21/ci/</guid><description>&lt;p>Recently, I got asked about how to explain confidence intervals in simple terms to a layperson. I found that it is hard to do that.&lt;/p></description></item><item><title>Learning SQL the Hard Way</title><link>https://mlwhiz.com/blog/2020/02/21/sql/</link><pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/02/21/sql/</guid><description>&lt;p>&lt;em>&lt;strong>A Data Scientist who doesn’t know SQL is not worth his salt&lt;/strong>&lt;/em>&lt;/p></description></item><item><title>The 5 most useful Techniques to Handle Imbalanced datasets</title><link>https://mlwhiz.com/blog/2020/01/28/imbal/</link><pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/01/28/imbal/</guid><description>&lt;p>Have you ever faced an issue where you have such a small sample for the positive class in your dataset that the model is unable to learn?&lt;/p></description></item><item><title>Take your Machine Learning Models to Production with these 5 simple steps</title><link>https://mlwhiz.com/blog/2019/12/25/prod/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/25/prod/</guid><description>&lt;blockquote>
&lt;p>Creating a great machine learning system is an art.&lt;/p>
&lt;/blockquote>
&lt;p>There are a lot of things to consider while building a great machine learning system. But often it happens that we as data scientists only worry about certain parts of the project.&lt;/p></description></item><item><title>How to write Web apps using simple Python for Data Scientists?</title><link>https://mlwhiz.com/blog/2019/12/07/streamlit/</link><pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/07/streamlit/</guid><description>&lt;p>A Machine Learning project is never really complete if we don’t have a good way to showcase it.&lt;/p></description></item><item><title>Implementing Object Detection and Instance Segmentation for Data Scientists</title><link>https://mlwhiz.com/blog/2019/12/06/weapons/</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/06/weapons/</guid><description>&lt;p>Object Detection is a helpful tool to have in your coding repository.&lt;/p></description></item><item><title>Demystifying Object Detection and Instance Segmentation for Data Scientists</title><link>https://mlwhiz.com/blog/2019/12/05/od/</link><pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/12/05/od/</guid><description>&lt;p>I like deep learning a lot but Object Detection is something that doesn’t come easily to me.&lt;/p></description></item><item><title>The Simple Math behind 3 Decision Tree Splitting criterions</title><link>https://mlwhiz.com/blog/2019/11/12/dtsplits/</link><pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/12/dtsplits/</guid><description>&lt;p>Decision Trees are great and are useful for a variety of tasks. They form the backbone of most of the best performing models in the industry like XGboost and Lightgbm.&lt;/p></description></item><item><title>P-value Explained Simply for Data Scientists</title><link>https://mlwhiz.com/blog/2019/11/11/pval/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/11/pval/</guid><description>&lt;p>Recently, I got asked about how to explain p-values in simple terms to a layperson. I found that it is hard to do that.&lt;/p></description></item><item><title>The 5 Classification Evaluation metrics every Data Scientist must know</title><link>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</link><pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/07/eval_metrics/</guid><description>&lt;p>&lt;em>&lt;strong>What do we want to optimize for?&lt;/strong>&lt;/em> Most of the businesses fail to answer this simple question.&lt;/p></description></item><item><title>Data Scientists, The 5 Graph Algorithms that you should know</title><link>https://mlwhiz.com/blog/2019/09/02/graph_algs/</link><pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/09/02/graph_algs/</guid><description>&lt;p>We as data scientists have gotten quite comfortable with Pandas or SQL or any other relational database.&lt;/p></description></item><item><title>The Ultimate Guide to using the Python regex module</title><link>https://mlwhiz.com/blog/2019/09/01/regex/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/09/01/regex/</guid><description>&lt;p>One of the main tasks while working with text data is to create a lot of text-based features.&lt;/p></description></item><item><title>How did I learn Data Science?</title><link>https://mlwhiz.com/blog/2019/08/12/resources/</link><pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/08/12/resources/</guid><description>&lt;p>I am a Mechanical engineer by education. And I started my career with a core job in the steel industry.&lt;/p></description></item><item><title>The 5 Feature Selection Algorithms every Data Scientist should know</title><link>https://mlwhiz.com/blog/2019/08/07/feature_selection/</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/08/07/feature_selection/</guid><description>&lt;p>Data Science is the study of algorithms.&lt;/p>
&lt;p>I grapple through with many algorithms on a day to day basis, so I thought of listing some of the most common and most used algorithms one will end up using in this new 

&lt;a href="https://towardsdatascience.com/tagged/ds-algorithms" target="_blank" rel="nofollow noopener">DS Algorithm series&lt;/a>
.&lt;/p></description></item><item><title>The 5 Sampling Algorithms every Data Scientist need to know</title><link>https://mlwhiz.com/blog/2019/07/30/sampling/</link><pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/30/sampling/</guid><description>&lt;p>Data Science is the study of algorithms.&lt;/p>
&lt;p>I grapple through with many algorithms on a day to day basis so I thought of listing some of the most common and most used algorithms one will end up using in this new DS Algorithm series.&lt;/p></description></item><item><title>Minimal Pandas Subset for Data Scientists</title><link>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/20/pandas_subset/</guid><description>&lt;p>Pandas is a vast library.&lt;/p>
&lt;p>Data manipulation is a breeze with pandas, and it has become such a standard for it that a lot of parallelization libraries like Rapids and Dask are being created in line with Pandas syntax.&lt;/p></description></item><item><title>The Hitchhikers guide to handle Big Data using Spark</title><link>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</link><pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/07/07/spark_hitchhiker/</guid><description>&lt;p>Big Data has become synonymous with Data engineering.&lt;/p>
&lt;p>But the line between Data Engineering and Data scientists is blurring day by day.&lt;/p></description></item><item><title>An End to End Introduction to GANs using Keras</title><link>https://mlwhiz.com/blog/2019/06/17/gans/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/06/17/gans/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/gans/faces.png"">&lt;/center>
&lt;/div>
&lt;p>I bet most of us have seen a lot of AI-generated people faces in recent times, be it in papers or blogs. We have reached a stage where it is becoming increasingly difficult to distinguish between actual human faces and faces that are generated by Artificial Intelligence.&lt;/p></description></item><item><title>The Hitchhiker’s Guide to Feature Extraction</title><link>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/05/19/feature_extraction/</guid><description>&lt;p>Good Features are the backbone of any machine learning model.&lt;/p></description></item><item><title>3 Awesome Visualization Techniques for every dataset</title><link>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/19/awesome_seaborn_visuals/</guid><description>&lt;p>Visualizations are awesome. &lt;strong>However, a good visualization is annoyingly hard to make.&lt;/strong>&lt;/p></description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>&lt;p>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p></description></item><item><title>NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid><description>&lt;p>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the 

&lt;a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/">third post&lt;/a>
, I delved deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.&lt;/p></description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>&lt;p>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>&lt;p>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong>Quora Insincere questions classification&lt;/strong> in which I got a rank of &lt;strong>&lt;code>182/4037&lt;/code>&lt;/strong>. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p></description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>&lt;p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the various &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In this post, I will try to take you through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong>Deep learning models&lt;/strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>&lt;p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p></description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/artificial-neural-network.png" height="350" width="700" >&lt;/center>
&lt;/div>
&lt;p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the &lt;strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic&lt;/strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.&lt;/p></description></item><item><title>What Kagglers are using for Text Classification</title><link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid><description>&lt;p>With the problem of Image Classification is more or less solved by Deep learning, &lt;em>Text Classification is the next new developing theme in deep learning&lt;/em>. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that?&lt;/p></description></item><item><title>Using XGBoost for time series prediction tasks</title><link>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</link><pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/12/26/win_a_data_science_competition/</guid><description>&lt;p>Recently Kaggle master Kazanova along with some of his friends released a 

&lt;a href="https://coursera.pxf.io/yRPoZB" target="_blank" rel="nofollow noopener">&amp;ldquo;How to win a data science competition&amp;rdquo;&lt;/a>
 Coursera course. You can start for free with the 7-day Free Trial. The Course involved a final project which itself was a time series prediction problem. Here I will describe how I got a top 10 position as of writing this article.&lt;/p></description></item><item><title>Create basic graph visualizations with SeaBorn- The Most Awesome Python Library For Visualization yet</title><link>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/09/13/seaborn_visualizations/</guid><description>&lt;p>When it comes to data preparation and getting acquainted with data, the &lt;strong>one step we normally skip is the data visualization&lt;/strong>.
While a part of it could be attributed to the &lt;strong>lack of good visualization tools&lt;/strong> for the platforms we use, most of us also &lt;strong>get lazy&lt;/strong> at times.&lt;/p></description></item><item><title>Learning Spark using Python: Basics and Applications</title><link>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</link><pubDate>Mon, 07 Sep 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/09/07/spark_basics_explain/</guid><description>&lt;p>I generally have a use case for &lt;a href="https://hadoop.apache.org/" target="_blank" rel="nofollow">Hadoop&lt;/a> in my daily job. It has made my life easier in a sense that I am able to get results which I was not able to see with SQL queries. But still I find it painfully slow.
I have to write procedural programs while I work. As in merge these two datasets and then filter and then merge another dataset and then filter using some condition and yada-yada.
You get the gist. And in hadoop its painstakingly boring to do this. You have to write more than maybe 3 Mapreduce Jobs. One job will read the data line by line and write to the disk.&lt;/p></description></item><item><title>Behold the power of MCMC</title><link>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</link><pubDate>Fri, 21 Aug 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/08/21/mcmc_algorithm_cryptography/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/mcmc.png">&lt;/center>
&lt;/div>
&lt;p>Last time I wrote an article on MCMC and how they could be useful. We learned how MCMC chains could be used to simulate from a random variable whose distribution is partially known i.e. we don&amp;rsquo;t know the normalizing constant.&lt;/p></description></item><item><title>My Tryst With MCMC Algorithms</title><link>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</link><pubDate>Wed, 19 Aug 2015 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2015/08/19/mcmc_algorithms_b_distribution/</guid><description>&lt;p>The things that I find hard to understand push me to my limits. One of the things that I have always found hard is &lt;strong>Markov Chain Monte Carlo Methods&lt;/strong>.
When I first encountered them, I read a lot about them but mostly it ended like this.&lt;/p></description></item></channel></rss>