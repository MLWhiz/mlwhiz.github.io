<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on MLWhiz - Your Home for DS, ML, AI!</title><link>https://mlwhiz.com/categories/natural-language-processing/</link><description>Recent content in Natural Language Processing on MLWhiz - Your Home for DS, ML, AI!</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://mlwhiz.com/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Everything Programmers need to learn about GPT — Using OpenAI and Understanding Prompting</title><link>https://mlwhiz.com/blog/2023/08/13/openai-api/</link><pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2023/08/13/openai-api/</guid><description>&lt;p>ChatGPT&amp;rsquo;s free conversational interface offers a tantalizing glimpse into the future of AI. But the full potential of generative models lies in integration with real-world systems.&lt;/p></description></item><item><title>Everything you need to learn about GPT — How Does ChatGPT Work?</title><link>https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/</link><pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2023/07/07/how_chatgpt_works/</guid><description>&lt;p>ChatGPT is what everyone is talking about nowadays. Would it take all the jobs? Or would it result in misinformation on the web? There are just so many posts and articles that fill my inbox daily when it comes to GPTs. Add to that so many versions of GPTs and tools to use these GPTs; it is getting increasingly frustrating to keep track of everything in the GPT Landscape.&lt;/p></description></item><item><title>Explaining BERT Simply Using Sketches</title><link>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</guid><description>&lt;p>In my last series of posts on Transformers, I talked about how a 

&lt;a href="https://mlwhiz.com/blog/2020/09/20/transformers/">transformer &lt;/a>
works and how to 

&lt;a href="https://mlwhiz.com/blog/2020/10/10/create-transformer-from-scratch/">implement&lt;/a>
 one yourself for a translation task.&lt;/p></description></item><item><title>How Can Data Scientists Use Parallel Processing?</title><link>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</guid><description>&lt;p>Finally, my program is running! Should I go and get a coffee?&lt;/p></description></item><item><title>Understanding BERT with Huggingface</title><link>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</guid><description>&lt;p>In my last post on 

&lt;a href="https://mlwhiz.medium.com/explaining-bert-simply-using-sketches-ba30f6f0c8cb" target="_blank" rel="nofollow noopener">BERT&lt;/a>
, I talked in quite a detail about BERT transformers and how they work on a basic level. I went through the BERT Architecture, training data and training tasks.&lt;/p></description></item><item><title>Understanding Transformers, the Data Science Way</title><link>https://mlwhiz.com/blog/2020/09/20/transformers/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/20/transformers/</guid><description>&lt;p>Transformers have become the defacto standard for NLP tasks nowadays.&lt;/p></description></item><item><title>The Most Complete Guide to PyTorch for Data Scientists</title><link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid><description>&lt;p>&lt;em>&lt;strong>PyTorch&lt;/strong>&lt;/em> has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface. Yet, it is somehow a little difficult for beginners to get a hold of.&lt;/p></description></item><item><title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title><link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid><description>&lt;p>Creating my own workstation has been a dream for me if nothing else. I knew the process involved, yet I somehow never got to it.&lt;/p></description></item><item><title>Using Deep Learning for End to End Multiclass Text Classification</title><link>https://mlwhiz.com/blog/2020/05/24/multitextclass/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/24/multitextclass/</guid><description>&lt;p>






 
 
 
 
 

 
 
 

 
 &lt;img
 sizes="(min-width: 35em) 1200px, 100vw"
 srcset='
 
 /images/multitextclass/main_hu5347609766070859065.png 500w
 
 
 , /images/multitextclass/main_hu15737587202097698753.png 800w
 
 
 , /images/multitextclass/main_hu7039699309994025813.png 1200w
 
 
 , /images/multitextclass/main_hu5618161544359752641.png 1500w 
 '
 src="https://mlwhiz.com/images/multitextclass/main.png"

 alt="MLWhiz: Data Science, Machine Learning, Artificial Intelligence">
 

&lt;/p></description></item><item><title>Adding Interpretability to Multiclass Text Classification models</title><link>https://mlwhiz.com/blog/2019/11/08/interpret_models/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/08/interpret_models/</guid><description>&lt;p>Explain Like I am 5.&lt;/p>
&lt;p>It is the basic tenets of learning for me where I try to distill any concept in a more palatable form. As Feynman said:&lt;/p></description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>&lt;p>Chatbots are the in thing now. Every website must implement it. Every Data Scientist must know about them. Anytime we talk about AI; Chatbots must be discussed. But they look intimidating to someone very new to the field. We struggle with a lot of questions before we even begin to start working on them.
Are they hard to create? What technologies should I know before attempting to work on them? In the end, we end up discouraged reading through many posts on the internet and effectively accomplishing nothing.&lt;/p></description></item><item><title>NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid><description>&lt;p>This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the 

&lt;a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/">third post&lt;/a>
, I delved deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach to this data.&lt;/p></description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>&lt;p>This post is the third post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the different &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In the 

&lt;a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/">second post&lt;/a>
, I talked through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In this post, I delve deeper into &lt;strong>Deep learning models and the various architectures&lt;/strong> we could use to solve the text Classification problem. To make this post platform generic, I am going to code in both Keras and Pytorch. I will use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>&lt;p>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on &lt;strong>Quora Insincere questions classification&lt;/strong> in which I got a rank of &lt;strong>&lt;code>182/4037&lt;/code>&lt;/strong>. In this post, I will try to provide a summary of the things I tried. I will also try to summarize the ideas which I missed but were a part of other winning solutions.&lt;/p></description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>&lt;p>This is the second post of the NLP Text classification series. To give you a recap, recently I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. And I thought to share the knowledge via a series of blog posts on text classification. The 

&lt;a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/">first post&lt;/a>
 talked about the various &lt;strong>preprocessing techniques that work with Deep learning models&lt;/strong> and &lt;strong>increasing embeddings coverage&lt;/strong>. In this post, I will try to take you through some &lt;strong>basic conventional models&lt;/strong> like TFIDF, Count Vectorizer, Hashing etc. that have been used in text classification and try to access their performance to create a baseline. We will delve deeper into &lt;strong>Deep learning models&lt;/strong> in the third post which will focus on different architectures for solving the text classification problem. We will try to use various other models which we were not able to use in this competition like &lt;strong>ULMFit transfer learning&lt;/strong> approaches in the fourth post in the series.&lt;/p></description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>&lt;p>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge. It is an NLP Challenge on text classification and as the problem has become more clear after working through the competition as well as by going through the invaluable kernels put up by the kaggle experts, I thought of sharing the knowledge.&lt;/p></description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>&lt;div style="margin-top: 9px; margin-bottom: 10px;">
&lt;center>&lt;img src="https://mlwhiz.com/images/artificial-neural-network.png" height="350" width="700" >&lt;/center>
&lt;/div>
&lt;p>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results. Now I have always worked with Keras in the past and it has given me pretty good results, but somehow I got to know that the &lt;strong>CuDNNGRU/CuDNNLSTM layers in keras are not deterministic&lt;/strong>, even after setting the seeds. So Pytorch did come to rescue. And am I glad that I moved.&lt;/p></description></item><item><title>What Kagglers are using for Text Classification</title><link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid><description>&lt;p>With the problem of Image Classification is more or less solved by Deep learning, &lt;em>Text Classification is the next new developing theme in deep learning&lt;/em>. For those who don&amp;rsquo;t know, Text classification is a common task in natural language processing, which transforms a sequence
of text of indefinite length into a category of text. How could you use that?&lt;/p></description></item><item><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid><description>&lt;p>Recently Quora put out a 

&lt;a href="https://www.kaggle.com/c/quora-question-pairs" target="_blank" rel="nofollow noopener">Question similarity&lt;/a>
 competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn. The one thing that blew my mind away was the word2vec embeddings.&lt;/p></description></item></channel></rss>