<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on Helping You Learn Data Science!</title><link>https://mlwhiz.com/categories/natural-language-processing/</link><description>Recent content in Natural Language Processing on Helping You Learn Data Science!</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://mlwhiz.com/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Explaining BERT Simply Using Sketches</title><link>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/bert-sketches/</guid><description>In my last series of posts on Transformers, I talked about how a transformer works and how to implement one yourself for a translation task.</description></item><item><title>How Can Data Scientists Use Parallel Processing?</title><link>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/parallel-processing/</guid><description>Finally, my program is running! Should I go and get a coffee?</description></item><item><title>Understanding BERT with Huggingface</title><link>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</link><pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2021/07/24/huggingface-bert/</guid><description>In my last post on BERT , I talked in quite a detail about BERT transformers and how they work on a basic level.</description></item><item><title>Understanding Transformers, the Data Science Way</title><link>https://mlwhiz.com/blog/2020/09/20/transformers/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/20/transformers/</guid><description>Transformers have become the defacto standard for NLP tasks nowadays.</description></item><item><title>The Most Complete Guide to PyTorch for Data Scientists</title><link>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</link><pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/09/09/pytorch_guide/</guid><description>PyTorch has sort of became one of the de facto standards for creating Neural Networks now, and I love its interface.</description></item><item><title>A definitive guide for Setting up a Deep Learning Workstation with Ubuntu</title><link>https://mlwhiz.com/blog/2020/06/06/dlrig/</link><pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/06/06/dlrig/</guid><description>Creating my own workstation has been a dream for me if nothing else.</description></item><item><title>Using Deep Learning for End to End Multiclass Text Classification</title><link>https://mlwhiz.com/blog/2020/05/24/multitextclass/</link><pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2020/05/24/multitextclass/</guid><description>Have you ever thought about how toxic comments get flagged automatically on platforms like Quora or Reddit?</description></item><item><title>Adding Interpretability to Multiclass Text Classification models</title><link>https://mlwhiz.com/blog/2019/11/08/interpret_models/</link><pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/11/08/interpret_models/</guid><description>Explain Like I am 5.
It is the basic tenets of learning for me where I try to distill any concept in a more palatable form.</description></item><item><title>Chatbots aren't as difficult to make as You Think</title><link>https://mlwhiz.com/blog/2019/04/15/chatbot/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/04/15/chatbot/</guid><description>Chatbots are the in thing now. Every website must implement it.</description></item><item><title>NLP Learning Series: Part 4 - Transfer Learning Intuition for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</link><pubDate>Sat, 30 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/30/transfer_learning_text_classification/</guid><description>This post is the fourth post of the NLP Text classification series.</description></item><item><title>NLP Learning Series: Part 3 - Attention, CNN and what not for Text Classification</title><link>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</link><pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/</guid><description>This post is the third post of the NLP Text classification series.</description></item><item><title>What my first Silver Medal taught me about Text Classification and Kaggle in general?</title><link>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</link><pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/</guid><description>Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037.</description></item><item><title>NLP Learning Series: Part 2 - Conventional Methods for Text Classification</title><link>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</link><pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/</guid><description>This is the second post of the NLP Text classification series.</description></item><item><title>NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep Learning</title><link>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</link><pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/</guid><description>Recently, I started up with an NLP competition on Kaggle called Quora Question insincerity challenge.</description></item><item><title>A Layman guide to moving from Keras to Pytorch</title><link>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/</guid><description>Recently I started up with a competition on kaggle on text classification, and as a part of the competition, I had to somehow move to Pytorch to get deterministic results.</description></item><item><title>What Kagglers are using for Text Classification</title><link>https://mlwhiz.com/blog/2018/12/17/text_classification/</link><pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2018/12/17/text_classification/</guid><description>With the problem of Image Classification is more or less solved by Deep learning, Text Classification is the next new developing theme in deep learning.</description></item><item><title>Today I Learned This Part I: What are word2vec Embeddings?</title><link>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid>https://mlwhiz.com/blog/2017/04/09/word_vec_embeddings_examples_understanding/</guid><description>Recently Quora put out a Question similarity competition on Kaggle. This is the first time I was attempting an NLP problem so a lot to learn.</description></item></channel></rss>